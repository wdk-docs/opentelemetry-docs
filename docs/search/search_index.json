{"config":{"lang":["ja"],"separator":"[\\/\\s\\-\\.]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"OpenTelemetry","text":"<p>@opentelemetry</p> <p></p> <ul> <li>\u4e86\u89e3\u66f4\u591a</li> <li>\u8bd5\u8bd5\u8fd9\u4e2a\u6f14\u793a</li> </ul> <p>\u6839\u636e\u4f60\u7684\u89d2\u8272\u5f00\u59cb</p> <ul> <li>Dev</li> <li>Ops</li> </ul> <p>OpenTelemetry \u662f\u4e00\u4e2a\u5de5\u5177\u3001api \u548c sdk \u7684\u96c6\u5408\u3002\u4f7f\u7528\u5b83\u6765\u68c0\u6d4b\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b \u6570\u636e(\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a)\uff0c\u4ee5\u5e2e\u52a9\u60a8\u5206\u6790\u8f6f\u4ef6\u7684\u6027\u80fd\u548c\u884c\u4e3a\u3002</p> <p>OpenTelemetry \u901a\u5e38 \u53ef\u4ee5\u8de8\u51e0\u79cd\u8bed\u8a00\u4f7f\u7528\uff0c \u5e76\u4e14\u975e\u5e38\u9002\u5408\u4f7f\u7528\u3002</p> <p>\u4ece\u60a8\u7684\u670d\u52a1\u548c\u8f6f\u4ef6\u4e2d\u521b\u5efa\u548c\u6536\u96c6\u9065\u6d4b\u6570\u636e\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u53d1\u7ed9\u5404\u79cd\u5206\u6790\u5de5\u5177\u3002</p> <p>OpenTelemetry \u96c6\u6210\u4e86\u6d41\u884c\u7684\u5e93\u548c\u6846\u67b6\uff0c \u5982Spring\uff0cASP.NET Core\uff0c Express\uff0c Quarkus\uff0c\u4ee5\u53ca\u66f4\u591a! \u5b89\u88c5 \u548c\u96c6\u6210\u53ef\u4ee5\u50cf\u51e0\u884c\u4ee3\u7801\u4e00\u6837\u7b80\u5355\u3002</p> <p>100%\u514d\u8d39\u548c\u5f00\u6e90\uff0cOpenTelemetry \u5728\u53ef\u89c2\u5bdf\u6027\u9886\u57df \u88ab\u884c\u4e1a\u9886\u5bfc\u8005\u91c7\u7528\u548c\u652f\u6301\u3002</p> <p>OpenTelemetry \u662f\u4e00\u4e2aCNCF\u5b75\u5316\u9879\u76ee\u3002 \u7531 OpenTracing \u548c OpenCensus \u9879\u76ee\u5408\u5e76\u800c\u6210\u3002</p> <p></p>"},{"location":"status/","title":"Status","text":"<p>OpenTelemetry \u7531\u51e0\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff0c\u4e00\u4e9b\u7279\u5b9a\u4e8e\u8bed\u8a00\uff0c\u53e6\u4e00\u4e9b\u4e0e\u8bed\u8a00\u65e0\u5173\u3002\u5728 \u67e5\u627e\u72b6\u6001\u65f6\uff0c\u8bf7\u786e\u4fdd\u4ece\u6b63\u786e\u7684\u7ec4\u4ef6\u9875\u9762\u67e5\u627e\u72b6\u6001\u3002\u4f8b\u5982\uff0c\u89c4\u8303\u4e2d\u7684\u4fe1\u53f7\u72b6\u6001\u53ef\u80fd\u4e0e\u7279\u5b9a\u8bed\u8a00 SDK \u4e2d\u7684\u4fe1\u53f7\u72b6\u6001\u4e0d\u76f8\u540c\u3002</p> <p>\u5bf9\u4e8e\u8bed\u8a00 SDK\u7684\u5f00\u53d1\u72b6\u6001\u6216\u6210\u719f\u5ea6\u7ea7\u522b\uff0c\u8bf7\u53c2\u9605\u8be5 \u8bed\u8a00\u7684\u72b6\u6001\u90e8\u5206:</p> <ul> <li>C++</li> <li>.NET</li> <li>Erlang/Elixir</li> <li>Go</li> <li>Java</li> <li>JavaScript</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> <li>Rust</li> <li>Swift</li> </ul> <p>\u5173 \u4e8ecollector\u548cspecification\u7684 \u5f00\u53d1\u72b6\u6001\u6216\u6210\u719f\u5ea6\uff0c\u8bf7\u53c2\u89c1:</p> <ul> <li>Specification status</li> <li>Collector status</li> </ul>"},{"location":"tags/","title":"tags","text":"<p>{{ tag_content }}</p>"},{"location":"blog/","title":"\u535a\u5ba2","text":"<p>{{ blog_content }}</p>"},{"location":"blog/2019/opentelemetry-governance-committee-explained/","title":"OpenTelemetry Governance Committee Explained","text":"<p>This article describes the functions and responsibilities of the OpenTelemetry Governance Committee, based on the charter document found here. It is an opinion, not the formal definition. The primary role of the Governance Committee is not to centralize power, but to enable and empower the broader community by establishing processes. Let me explain.</p> <p></p> <p>The main objective of the OpenTelemetry project is to make robust, portable telemetry a built-in feature of cloud-native software. The most effective way to do it is to build a community of passionate people, from existing ecosystem with the diverse expertise and experience. This community will build a project that is attractive to users, who will use it to instrument their software, as well as telemetry vendors, who will build solutions that work with the open standards.</p> <p>In other words, success of our project depends on building community \u2014 welcoming contributions from small to large. Making sure that contributions go towards contributor\u2019s interests while keeping balance with the interests of other community members.</p> <p>As we strive to keep a lean governance, the most scalable approach to represent contributors\u2019 interests is to allow self-governance of individual special interest groups. So these groups will be self-formed and autonomous. Maintainers of special interest groups make final calls on technical questions related to the group. The Governance Committee defines a clear way to become a maintainer through continuous contributions.</p> <p>With self-governance of special interest groups (SIGs), a big part of a Governance Committee\u2019s job is to keep project spirit and maintain its direction through defining, evolving, and upholding the vision, values and scope of the project. The main instrument of a Governance Committee is advocacy and building relationships with contributors.</p> <p>That said, the Governance Committee members are not project or product managers in an industry understanding of these roles. Governance Committee members have no power over day-to-day work of the SIGs (it is typical that active SIG members are members of Governance Committee and keep making decisions in this SIG). The Governance Committee delegates responsibility for technical alignment across all special interest groups to a Technical Committee. Members of this Technical Committee make sure that SIG maintainers are aligned with the overall project goals, specifications and design principles defined by Technical Committee.</p> <p>Where Governance Committee is limited in size and elected only once a year, Technical Committee membership is more agile. It allows more diverse set of people, representing various interests, to participate in defining project goals and writing specifications.</p> <p>The Governance Committee, alongside the CNCF, also holds keys for project resources and assets like artifact repositories, build and test infrastructure, web sites and their domains, blogs, social-media accounts, etc. It is also responsible for ensuring that releases of components and artifacts are aligned with the OpenTelemetry agenda, and with the project\u2019s advocacy and marketing needs.</p> <p>The Governance Committee meets once a month in a public forum, and privately when needed. Now that you have a better idea of what the Governance Committee does, I hope you feel informed about which questions you can bring to Governance Committee attention.</p> <p>Come meet the new members of Governance Committee November 14th 10:00 PT and ask your questions!</p> <p>Thanks Sarah Novotny for review and feedback!</p> <p>A version of this article was [originally posted][] on medium.com/opentelemetry.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/auto-instrumentation-k8s/","title":"Using OpenTelemetry auto-instrumentation/agents in Kubernetes (Medium)","text":"<p>This article introduces OpenTelemetry Operator\u2019s new feature that significantly simplifies instrumenting workloads deployed on Kubernetes. Read all the details from the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/collector/","title":"OpenTelemetry Collector achieves Tracing stability milestone (Medium)","text":"<p>The OpenTelemetry Collector has made its first GA release. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/cpp/","title":"OpenTelemetry C++ v1.0 \u2014 what\u2019s there, and what next (Medium)","text":"<p>OpenTelemetry C++ released its v1.0 stable version last month, which implements the OpenTelemetry distributed tracing specification! For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/gc-election/","title":"Announcing the 2021 OpenTelemetry Governance Committee Election (Medium)","text":"<p>The OpenTelemetry project is excited to announce the 2021 OpenTelemetry Governance Committee (GC) election. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/malabi/","title":"Trace-Based Testing with OpenTelemetry: Meet Open Source Malabi (Medium)","text":"<p>This article introduces you to to Malabi, a new open source tool for trace-based testing. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/otel-gc/","title":"Welcome to the incoming 2021 OpenTelemetry Governance Committee (Medium)","text":"<p>The OpenTelemetry project just completed its 2021 election for the Governance Committee (GC). For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/python/","title":"Announcing OpenTelemetry Python 1.0 (Medium)","text":"<p>Today, OpenTelemetry Python distributed tracing API/SDK released its 1.0 version. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/swift/","title":"OpenTelemetry Swift 1.0 Beta (Medium)","text":"<p>Today, OpenTelemetry Swift distributed tracing API/SDK has released its 1.0 version. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2021/womens-day/","title":"OpenTelemetry Observes International Women's Day 2021 (Medium)","text":"<p>Happy International Women\u2019s Day! The OpenTelemetry project would like to extend our thanks to all our women contributors. For all the details, see the [original post][].</p> <p>[original post]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/announcing-community-manager/","title":"Announcing the OpenTelemetry Community Manager","text":"<p>OpenTelemetry has demonstrated massive growth since its inception in 2019. What started as a handful of OpenTracing and OpenCensus maintainers and collaborators meeting at the Google campus and over Zoom, has now grown into the second-most popular project in the CNCF behind Kubernetes itself. Over 5400 contributors and 700 companies have contributed code, issues, documentation, and invaluable feedback.</p> <p>Our community isn\u2019t just composed of these contributors, however - our end-users, partners, integrators, and a whole host of other people make up the OpenTelemetry community. In order to serve the needs of the wider end-user community, the governance committee has formed a community manager role who can focus on the needs of this large, and growing, group of humans.</p> <p>Community managers are appointed by the Governance Committee and act as public stewards of the contributor and end-user community. They\u2019re responsible for organizing events, coordinating cross-SIG and WG efforts to improve contributor experience, managing the OpenTelemetry presence on social media, and working to nurture and grow the OpenTelemetry community overall. Effectively, this is a new project maintainer whose project is the OpenTelemetry community itself.</p> <p>With that said, I\u2019m happy to inform you that I have been designated the first Community Manager for the OpenTelemetry project! As a former OpenTracing maintainer, I\u2019ve been a part of OpenTelemetry since its inception, mostly working on projects such as the website, hosting OpenTelemetry Tuesdays, and helping organize events like OpenTelemetry Community Day. With this new role, I plan to more formally establish OpenTelemetry Community Days around the world as part of KubeCon/CloudNativeCon -- and additionally, help promote new end-user and contributor events like OTel Unplugged (more on that in a week or so.)</p> <p>What\u2019s next? I hope to hear from you about what you need! I\u2019ve started a thread on the Community Discussions in GitHub, and I\u2019d love for you to post your questions, comments, and suggestions for community initiatives and programs. This isn\u2019t just limited to end-users of OpenTelemetry, either -- how can I help in improving the existing and new contributor experience? Please let me know!</p> <p>If you\u2019d like to reach out in other ways, you can find me on Twitter or on the Cloud Native Community Slack.</p>"},{"location":"blog/2022/dotnet-instrumentation-first-beta/","title":"OpenTelemetry .NET Automatic Instrumentation Releases its first Beta","text":"<p>We're excited to announce the first beta release of the OpenTelemetry .NET Automatic Instrumentation project!</p> <p>Without this project, .NET developers need to use instrumentation packages to automatically generate telemetry data. For example, to instrument inbound ASP.NET Core requests, you need to use the ASP.NET Core instrumentation package and initialize it with the OpenTelemetry SDK.</p> <p>Now, developers can use automatic instrumentation to initialize signal providers and generate telemetry data for supported instrumented libraries. This approach has several benefits:</p> <ul> <li>A technical path forward to support automatic instrumentation via   byte-code instrumentation,   which can allow for more automatic instrumentation support than relying solely   on published instrumentation libraries</li> <li>No need to install and initialize an instrumentation library</li> <li>No need to modify and rebuild an application to add automatic instrumentation</li> <li>Less code needed to get started</li> </ul> <p>This first beta release is an important milestone because it establishes the technical foundation on which a rich set of automatic instrumentation capabilities can be built on. This release includes support for:</p> <ul> <li>Gathering trace data from .NET applications without requiring code   changes1</li> <li>Gathering trace data from .NET libraries that the SDK does not   support2</li> </ul> <p>See the examples for demonstrations of different instrumentation scenarios covered by the OpenTelemetry .NET Automatic Instrumentation.</p> <p>Over the next few months we plan to:</p> <ul> <li>Support additional   instrumentation libraries</li> <li>Improve dependency management</li> <li>Enable metrics support</li> </ul> <p>Please, give us your feedback (using your preferred method):</p> <ul> <li>Submit a GitHub issue.</li> <li>Write to us on Slack.   If you are new, you can create a CNCF Slack account   here.</li> </ul> <ol> <li> <p>The supported and unsupported scenarios documentation describe the current limits.\u00a0\u21a9</p> </li> <li> <p>The instrumentation library documentation contains the list of libraries we can gather telemetry data from.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2022/dotnet-instrumentation-metrics/","title":"OpenTelemetry .NET Automatic Instrumentation metric signal support","text":"<p>We're excited to announce the 0.2.0-beta.1 release of the OpenTelemetry .NET Automatic Instrumentation which adds metric signal support!</p> <p>Now you can easily export metrics from:</p> <ul> <li>.NET Runtime,</li> <li>ASP.NET Core,</li> <li>ASP.NET Framework,</li> <li>HTTP clients (<code>System.Net.Http.HttpClient</code> and <code>System.Net.HttpWebRequest</code>),</li> <li>measurements created using   <code>System.Diagnostics.Metrics</code>.</li> </ul> <p>Over the next few months we plan to support:</p> <ul> <li>additional   instrumentation libraries,</li> <li>the log signal.</li> </ul> <p>Please, give us your feedback (using your preferred method):</p> <ul> <li>Submit a GitHub issue.</li> <li>Write to us on Slack.   If you are new, you can create a CNCF Slack account   here.</li> </ul>"},{"location":"blog/2022/gc-candidates/","title":"Final list of candidates for the 2022 OpenTelemetry Governance Committee","text":"<p>The OpenTelemetry election committee is pleased to announce the final list of candidates running for one of the four available seats. We encourage all voters to take a moment and review all candidates, picking the one that best represents your interest. You can find their pictures, profile link, and descriptions on the candidates page, but here are their names:</p> <ul> <li>Alolita Sharma</li> <li>Daniel Dyla</li> <li>Ken Finnigan</li> <li>Michael Hausenblas</li> <li>Morgan McLean</li> <li>Pavol Loffay</li> <li>Phillip Carter</li> <li>Reese Lee</li> <li>Reiley Yang</li> <li>Sean Marciniak</li> <li>Severin Neumann</li> <li>Trask Stalnaker</li> <li>Tyler Yahn</li> <li>Vijay Samuel</li> </ul> <p>You can check your eligibility by reviewing this GitHub issue. If you are not listed there but believe you have the right to vote, please fill out this registration form.</p>"},{"location":"blog/2022/gc-election-results/","title":"Announcing the 2022 Governance Committee Election Results","text":"<p>The OpenTelemetry project is proud to announce the winners of the 2022 OpenTelemetry Governance Committee (GC) election! Before we get into the results, I'd like to share some statistics about the election!</p> <p>This year, 513 project members were eligible to vote. Of those, 219 cast ballots, for a participation rate of nearly 43%! For comparison, the 2021 election saw 169 ballots cast with 386 eligible voters. Participation was roughly flat year-over-year, and we're thrilled that we've welcomed so many new project members and maintained a high participation rate.</p> <p>With that out of the way, please join me in congratulating Alolita Sharma, Daniel Dyla, and Morgan McLean on their re-election! They will be joined by long-time contributor and maintainer of OpenTelemetry Java Instrumentation Trask Stalnaker! All will serve a two-year term.</p> <p>We'd like to thank everyone who participated -- candidates, voters, and the election organizers -- for their efforts. You can find the full results of the election here.</p>"},{"location":"blog/2022/gc-elections/","title":"Announcing the 2022 OpenTelemetry Governance Committee Election","text":"<p>The OpenTelemetry project is excited to announce the 2022 OpenTelemetry Governance Committee (GC) election. Nominations are due by end-of-day on 7 October 2022, with the ratification happening by 8 October 2022 and a list of eligible candidates will be shared on 9 October 2022. Voting will take place between 18 October 2022 and 20 October 2022, and the final election results will be announced 22 October 2022.</p>"},{"location":"blog/2022/gc-elections/#vote","title":"Vote!","text":"<p>If you are a member of standing in the OpenTelemetry community, we invite you to participate in this election to ensure that the community is well-represented in the Governance Committee. In this election four people must be elected, each with two-year terms.</p> <p>If you have made contributions to our ecosystem not measured by the automatic process, you can request an exception to participate in the election before 23:59 UTC on 17 October 2022. The voter roll with all members of standing and approved exceptions will be published here by 11 October 2022 and continuously updated.</p> <p>Voting will be open between 00:00 UTC on 18 October 2022 00:00 UTC and 20 October 2022 23:59 UTC on Helios Voting; voters will need to sign in with their GitHub account.</p> <p>To learn more about the election process, read about all of the details here.</p>"},{"location":"blog/2022/gc-elections/#interested-in-joining-the-governance-committee","title":"Interested in joining the Governance Committee?","text":"<p>If you\u2019ve been working on OpenTelemetry and seeing it grow, or are an end-user that wants to help us make OpenTelemetry better, now\u2019s the time to consider running for a seat on the Governance Committee. You can read about the Governance Committee's role in this blog post or refer to the charter document. You may nominate yourself (or others!) by submitting a Pull Request against the list of candidates by 7 October 2022 23:59 UTC \u2014 more detailed requirements about the nomination and ratification process can be found here.</p> <p>We would like to thank the GC members who have helped grow OpenTelemetry, and invite them to run for re-election if they so choose: Alolita Sharma, Daniel Dyla, Liz Fong-Jones and Morgan McLean. After this election, the Governance Committee will comprise 9 members with staggered 2-year terms, all elected by the community.</p>"},{"location":"blog/2022/gc-elections/#questions","title":"Questions?","text":"<p>For any election related questions, please file an issue on the community repo here and tag @jpkrohling, @bogdandrutu, and @bhs. Or send us a message on CNCF Slack in #opentelemetry if you have an urgent access issue during voting.</p> <p>See you at the polls!</p>"},{"location":"blog/2022/kubecon-na-project-update/","title":"OpenTelemetry Project and Roadmap Update from Kubecon","text":"<p>2022 has been an incredible year for OpenTelemetry. Metrics became a first-class signal type, and are being used on production services and infrastructure alongside OpenTelemetry\u2019s existing distributed tracing support to send critical performance data to any observability backend for processing. Everywhere we look we see OpenTelemetry being tested, rolled out, or already in use in organizations everywhere, from the largest to smallest, from the most cutting edge to the historically cautious.</p> <p>Since January, we\u2019ve delivered:</p> <ul> <li>Metrics: defined in the specification, and delivered in the Java, JS, .NET,   and Python SDKs and instrumentation, with support all the way through the   Collector and protocol. More language implementations are on the way.</li> <li>More instrumentation for all languages.</li> <li>A great demo application, which   includes services written in every supported language, already instrumented   with OpenTelemetry. This is a great way to see practical examples of   OpenTelemetry that you can learn from or experiment with, and the demo can   also be used to test out different observability analytics systems.</li> <li>Tracing stability in C++, Erlang, and a number of other new languages.</li> <li>Progress on Ruby, Erlang, Swift, and .NET auto instrumentation.</li> <li>Major progress on logs, OpenTelemetry\u2019s third signal type.</li> <li>Documentation!</li> <li>Various improvements to all components.</li> </ul> <p>The community also continues to grow substantially. We now have over 800 monthly-active developers on GitHub, from 150 different organizations. More and more of these contributors are end-users - 10 out of our top 25 contributing orgs - which is a very healthy signal for the project. People and companies are getting so much usefulness out of OpenTelemetry that they\u2019re contributing back and making it even more useful for everyone.</p> <p>We\u2019re publishing this post during KubeCon, where many community members and end users will be gathered discussing OpenTelemetry and how it\u2019s being used, how it can be improved, and where we should go from here. In May of this year at KubeCon EU we started a process to create a more formal OpenTelemetry roadmap, and we\u2019ll be continuing that process in Detroit. I\u2019m writing this post in advance of the conference, so I won\u2019t be able to post the full outcome, but here are some of the items that we think are most important:</p> <ul> <li>Finishing logs: completing the full specification and then implementing this   spec across each language.</li> <li>Making OpenTelemetry easier to use, both technically (new features and   functionality like an OpenTelemetry control plane), and through documentation,   collecting user feedback, etc.</li> <li>Client instrumentation: extending OpenTelemetry to capture performance data   from web, mobile, and desktop client applications. This can be used to capture   data from true user-facing SLOs, show end-to-end latency in traces, etc.</li> <li>Profiling, which will tie service performance (captured today through metrics   and traces) to actual function performance within code.</li> <li>Improving the contributor and maintainer experience.</li> </ul> <p>Our focus for the remainder of this year and next year will be on both rounding out OpenTelemetry\u2019s existing functionality across all languages, scenarios, and integrations, and on the roadmap items mentioned above. As mentioned above, in the coming weeks we\u2019ll be publishing a more formal roadmap document that incorporates these, though it\u2019s important to note that the prioritization and progress made on each is dependent on the amount of effort and number of community members that get engaged with each.</p> <p>Many of these, like logs, client instrumentation, and profiling, are already in-flight. We\u2019re excited about these new initiatives because they not only expand the project\u2019s usefulness and bring it closer to its original vision, but they have brought in a new wave of members to the community who are already adding their knowledge, experience, and zeal to OpenTelemetry. These are exciting days for the project, and it\u2019s invigorating for everyone involved to see it grow and be adopted so rapidly.</p>"},{"location":"blog/2022/kubecon-na/","title":"Join us for OpenTelemetry Talks and Activities at Kubecon NA 2022","text":"<p>The OpenTelemetry project maintainers, and members of the governance committee and technical committee are excited to be at KubeCon NA in a few weeks! Join in to meet up in person or virtually for OpenTelemetry activities in Detroit from October 24 - 28, 2022.</p> <p>There are talks, workshops, an unconference as well as a project booth where you are welcome to stop by, say Hi! and tell us about how you are using OpenTelemetry or contributing to the project.</p> <p>The talks, maintainer sessions and other project activities are listed below.</p>"},{"location":"blog/2022/kubecon-na/#kubecon-talks-and-maintainer-sessions","title":"KubeCon Talks and Maintainer Sessions","text":"<p>Jaeger: The Future with OpenTelemetry and Metrics by Jonah Kowall, Logz.io &amp; Joe Elliott, Grafana Wednesday, October 26 \u2022 4:30pm - 5:05pm</p> <p>OpenTelemetry: Meet the Community, Build the Roadmap by Morgan McLean, Splunk; Daniel Dyla, Dynatrace; Ted Young, Lightstep; Alolita Sharma, Apple Thursday October 27, 2022 5:25pm - 6:00pm EDT</p> <p>Tips And Tricks To Successfully Migrate From Jaeger To OpenTelemetry by Vineeth Pothulapati, Timescale Inc Friday, October 28 \u2022 4:55pm - 5:30pm</p>"},{"location":"blog/2022/kubecon-na/#other-co-located-events","title":"Other Co-located Events","text":"<p>OTel Unplugged is a one-day OpenTelemetry unconference event held at Colony Club Detroit on Tuesday, Oct 25, 2022. Join in to listen, learn about the latest features and get involved in the project. Network with OpenTelemetry maintainers, governance committee and technical committee members. For details, check out the event page.</p> <p>Open Observability Day has several OpenTelemetry sessions including panel discussions, workshops, individual talks and lightning talks. This event will be on Monday Oct 24, 2022 and is co-located with KubeCon NA. For talk details, see the schedule.</p> <p>eBPF Day also has some OpenTelemetry talks focused on profiling and tracing using eBPF events. This event will be held on Monday, Oct 24 2022 and is co-located with KubeCon NA. For details, check out the schedule.</p> <p>The OpenTelemetry project meeting is to meet and network with the OpenTelemetry maintainers and core contributors. This meeting will be held 3-5 PM ET on Tuesday Oct 25, 2022 in Room 336 at the conference venue, Huntington Place.</p>"},{"location":"blog/2022/kubecon-na/#come-visit-the-opentelemetry-project-booth","title":"Come Visit the OpenTelemetry Project Booth","text":"<p>Drop by and say Hi! at the OpenTelemetry project booth in the KubeCon NA Project Pavilion. If you\u2019re lucky, you may even pick up some OpenTelemetry swag!</p> <p>The pavilion hours are:</p> <ul> <li>Wednesday, October 26, 10:30 - 20:00 ET</li> <li>Thursday, October 27, 10:30 - 17:30 ET</li> <li>Friday, October 27, 10:30 - 16:00 ET</li> </ul> <p>We\u2019re super excited to learn from all of you about your experience with OpenTelemetry, how you are using OpenTelemetry, features you may need and issues you may be running into when using OpenTelemetry.</p> <p>Come join us to listen, learn and get involved in OpenTelemetry.</p> <p>See you in Detroit!</p>"},{"location":"blog/2022/metrics-announcement/","title":"OpenTelemetry Metrics Release Candidates","text":"<p>OpenTelemetry\u2019s metrics capabilities are now available as release candidates, starting with Java, .NET, and Python! This means that the specification, APIs, SDKs, and other components that author, capture, process, and otherwise interact with metrics now have the full set of OpenTelemetry metrics functionality and are ready for use. These release candidates will be promoted to general availability throughout the next few weeks.</p> <p>The 1.0 metrics release includes the following:</p> <ul> <li>Metrics functionality included in the OpenTelemetry language-specific APIs,   which provide language-specific interfaces that can create and manipulate   metrics, and associate metadata and attributes to each. These are useful for:</li> <li>Developers of shared libraries that are distributed to end-users, so that     these end users can natively use OpenTelemetry to capture metrics from     these. For example, gRPC uses these APIs to produce latency, throughput, and     error rate metrics for each RPC method on a given service.</li> <li>Developers who create and maintain web services or client applications, so     that they can produce custom metrics or interact with existing metrics. For     example, an e-commerce company could use the API to track how many purchases     are made over time.</li> <li>Metrics functionality included in the OpenTelemetry SDKs for Java, .NET,   Python, and JS (coming next week) SDKs, which capture metrics from the APIs   and perform some amount of processing. Metrics support for other languages is   still in development. These are useful for:</li> <li>Developers of applications that are used by other organizations, like     databases, message queues, etc., who will expose metrics over OTLP (or     Prometheus) so that their own end users are able to monitor the performance     of these apps. These applications could be open or closed source.</li> <li>Developers of applications within a technology organization, who want to     capture metrics generated by OpenTelemetry APIs within their applications,     either by their own developers or from shared libraries that their     applications depend on. These metrics can be exported through OTLP or     exported through any other OpenTelemetry exporter.</li> <li>Collector support for metrics includes the Collector\u2019s capability to capture   metrics from a rich variety of data sources like host metrics or pre-packaged   applications. The Collector also provides the ability to receive metrics from   data sources using multiple data protocols such as the native OpenTelemetry   protocol (OTLP) and OpenMetrics compliant protocols such as Prometheus. Also   supported is configuration-driven metric processing and native OTLP,   Prometheus and custom exporters to send observability metrics to on-cloud and   on-premises monitoring systems of your choice. This feature-set is useful for:</li> <li>Anyone who wants to capture metrics from their hosts (Linux VMs, Windows,     VMs, Kubernetes, etc.) or pre-packaged applications (databases, message     queues, etc.).</li> <li>Anyone who wants to capture metrics from existing sources like OTLP (from     OpenTelemetry SDKs, pre-packaged applications, etc.), Prometheus, or others.</li> <li>Anyone who wants to process / modify metrics and metric metadata captured     from these sources.</li> <li>Anyone who wants to convert metrics from one format to another. For example,     the Collector can capture metrics from a mix of OTLP and Prometheus sources,     and then send all of these to a single destination using OTLP (with the     standard OpenTelemetry semantic conventions), Prometheus, or any other     exporter.</li> <li>Full OpenTelemetry Protocol (OTLP) support for efficiently serializing and   transmitting metrics between systems.</li> <li>A metrics section of the specification, which defines different types of   metrics, their shapes, how to process them, and semantic conventions. This is   primarily used by OpenTelemetry contributors but also provides guidance to   OpenTelemetry users who are authoring metrics or metadata.</li> </ul> <p>All of this functionality is additive to OpenTelemetry\u2019s existing tracing support, and both signal types share the same metadata and semantic conventions. As of this announcement, the following languages have issued metrics release candidates:</p> <ul> <li>Java</li> <li>.NET</li> <li>Python</li> </ul> <p>The RC release for JS is planned for next week, and more languages will be issuing metrics release candidates throughout the coming months. Each of these releases will be followed by general availability after we receive feedback from users.</p>"},{"location":"blog/2022/metrics-announcement/#getting-started","title":"Getting Started","text":"<p>If you\u2019re already using a mix of the OpenTelemetry APIs, SDKs, language agents, and Collector, then you can access the release candidate metrics functionality by updating your OpenTelemetry artifacts to their latest versions. We're currently updating the official OpenTelemetry documentation for each artifact's metrics capabilities. Examples and supplementary documentation are also being added to each artifact\u2019s corresponding GitHub repository.</p>"},{"location":"blog/2022/metrics-announcement/#whats-next-for-opentelemetry","title":"What\u2019s Next for OpenTelemetry","text":"<p>Distributed traces and metrics were the two halves of OpenTelemetry\u2019s core promise when we announced it at KubeCon EU in 2019. With the general availability of metrics, we have produced the capabilities that we originally set out to create, meaning that we can shift our focus to further investing into the robustness and ease of use of each component, the number of data sources that OpenTelemetry can capture telemetry from (either through the OpenTelemetry APIs, OTLP, or otherwise), and new capabilities and signal types.</p> <p>Logging is the most visible release on the horizon, and we\u2019re running full speed ahead on this effort. Expect to hear a lot more about our progress on logging throughout the year (logs already have a stable data model and OTLP support), and anyone interested in this area is welcome to join the weekly logging SIG calls. Beyond logging, major new projects include formalizing and implementing client instrumentation and investigations into eBPF. With metrics complete, we may also turn our attention to more signal types.</p>"},{"location":"blog/2022/new-end-user-resources/","title":"Introducing new resources for OpenTelemetry end users to connect and discover best practices","text":"<p>The content of this post has moved to End-user Resources, where it will be kept up-to-date as more end-user resources become available.</p>"},{"location":"blog/2022/otel-in-practice/","title":"OpenTelemetry in Practice: Kubernetes & the Collector","text":""},{"location":"blog/2022/otel-in-practice/#about-the-series","title":"About the Series","text":"<p>Welcome to the OpenTelemetry in Practice series! This is a new experiment by some OpenTelemetry contributors in the End User Working Group.</p> <p>We\u2019re aiming to:</p> <ul> <li>Address practical problems that commonly stop development teams from   succeeding with OpenTelemetry</li> <li>Build stronger connections with developers focused on specific languages</li> <li>Improve the experience of implementing OpenTelemetry in production</li> </ul> <p>Each OpenTelemetry in Practice session will include a half hour of lightning talks and a half hour of open conversation about the topic. We are looking for people to join the OpenTelemetry in Practice team and people to give talks at future events. So if you\u2019re interested in shaping these conversations, reach out in the #otel-user-research channel of the CNCF Slack.</p> <p>Our first conversation will be on August 23rd, at 10 AM PDT, on Zoom, and the topic is Kubernetes &amp; The Collector.</p>"},{"location":"blog/2022/otel-in-practice/#talks","title":"Talks","text":"<p>Our lightning talks include the following.</p>"},{"location":"blog/2022/otel-in-practice/#collector-configuration-and-troubleshooting-in-kubernetes","title":"Collector Configuration and Troubleshooting in Kubernetes","text":"<p>\u2014 by Jessica Kerr, Honeycomb</p> <p>Let\u2019s talk about getting the OpenTelemetry Collector working in Kubernetes. Jess will run the Collector using the OpenTelemetry helm chart, and then tweak the configuration until it works. She\u2019ll demonstrate two techniques for troubleshooting and warn about some pitfalls. Then, share your victories and frustrations with the Collector.</p>"},{"location":"blog/2022/otel-in-practice/#observability-pipelines-for-kubernetes","title":"Observability Pipelines for Kubernetes","text":"<p>\u2014 by Daniel Kim, New Relic</p> <p>When doing observability, we need to collect signals from many different data sources, frameworks and programming languages. We can turn heterogeneous data across so many different signals into actionable insights through a framework like OpenTelemetry. In this talk Daniel will show you how to build an observability data pipeline using the OpenTelemetry Collector and open source plugins that can perform a wide variety of data processing, such as injecting infrastructure metadata into traces and metrics, implementing tail-based sampling, and exporting data to any backend via OTLP. Finally, he will share some of the challenges with running the Collector, so that you can take them into account as you build out your own observability pipeline with OpenTelemetry.</p>"},{"location":"blog/2022/otel-tuesday-v1-sunset/","title":"OpenTelemetry Tuesdays, Signing Off!","text":"<p>When we started running OpenTelemetry Tuesday live streams back in 2019, the world was a lot different than it is today. Back then, it was very challenging for external participants and observers to understand what was going on with the overall status of the project. There weren't as many good resources for learning about OpenTelemetry, or for figuring out how to contribute.</p> <p>However, the project has matured by leaps and bounds since we started, and the associated rate of change and lack of discoverability that drove the stream as a concept has lessened. OpenTelemetry updates and news can be found in various newsletters, Twitter feeds, and a thriving community on Slack.</p> <p>With this in mind, we'll be winding down the OpenTelemetry Tuesday stream going forward. That doesn't mean it's gone forever, or that we wouldn't be willing to bring it back, but the format just really wasn't giving a lot to the community at this point. If you're a content creator that's interested in working on this in the future, please come find us in the #otel-comms channel on Slack!</p> <p>Last, but not least, I'd like to extend a huge thanks to everyone that's helped host or guest on the stream in the past. Until next time, signing off.</p>"},{"location":"blog/2022/otel-unplugged-kubecon-na/","title":"OTel Unplugged at KubeCon NA 2022!","text":"<p>Are you excited about KubeCon NA 2022 in Detroit later this month? Maybe you\u2019re attending in-person or virtually, for the first time or the fifth -- either way, the OpenTelemetry community is excited to present a hybrid event that will take place on Tuesday, October 25th -- OTel Unplugged!</p>"},{"location":"blog/2022/otel-unplugged-kubecon-na/#what-is-otel-unplugged","title":"What Is OTel Unplugged?","text":"<p>If you attended OpenTelemetry Community Day earlier this year, then you\u2019ll be familiar with the format we\u2019ll be using -- an \u201cunconference\u201d, where in lieu of a bunch of pre-planned talks, you\u2019ll get to decide the format and content of discussion groups on that day. If you\u2019ve been to a DevOpsDays open space, then you should be rather familiar with this concept. We\u2019ll be offering tailored breakout sessions for in-person as well as virtual attendees, so nobody is left out.</p> <p>These small group discussions can be used for a wide variety of topics -- everything from tips and stories about operating the OpenTelemetry Collector, the best patterns to implement tracing in your .NET applications, discussions about OpenTelemetry Metrics and how to get started with them -- it\u2019s all fair game, and there\u2019ll be plenty of maintainers and contributors on-hand to join in on the discussions!</p> <p>We\u2019ll be presenting a panel discussion with our maintainers and a group of end-users as well, which promises to be extremely informative and enlightening! It wouldn\u2019t be an OpenTelemetry event without a few surprises that we can\u2019t wait to tell you about, too. Regardless of if you\u2019ll be in Detroit this October 25th for KubeCon NA 2022, this is sure to be the can\u2019t-miss OpenTelemetry event of the fall.</p>"},{"location":"blog/2022/otel-unplugged-kubecon-na/#how-do-i-attend","title":"How Do I Attend?","text":"<p>Registration for virtual and in-person attendance is now open! We\u2019re collecting a small fee for in-person attendance, which will be donated to Equality Michigan. Virtual registration is, of course, completely free. In addition, some scholarships are available - please contact us in the #otel-unplugged channel on CNCF Slack for more details. Be sure to register today, as tickets are limited. You can find the complete schedule on our Eventbrite page -- details coming soon!</p> <p>We\u2019re looking forward to welcoming you to OTel Unplugged later this month!</p>"},{"location":"blog/2022/troubleshooting-nodejs/","title":"Checklist for TroublesShooting OpenTelemetry Node.js Tracing Issues","text":"<p>I\u2019ll try to make this one short and to the point. You are probably here because you installed OpenTelemetry in your Node.js application and did not see any traces or some expected spans were missing.</p> <p>There can be many reasons for that, but some are more common than others. In this post, I will try to enumerate the common ones, along with some diagnostic methods and tips.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#requirements","title":"Requirements","text":"<p>I assume that you already have basic knowledge of what OpenTelemetry is and how it works and that you tried to set it up in your Node.js application.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#enable-logging","title":"Enable Logging","text":"<p>OpenTelemetry JS will by default not log anything to its diagnostic logger. Most of the SDK issues below are easily detected when a logger is enabled.</p> <p>You can log everything to the console by adding the following code as early as possible in your service:</p> <pre><code>// tracing.ts or main index.ts\nimport { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\n// rest of your otel initialization code\n</code></pre> <p>This is useful for debugging. Logging everything to the console in production is not a good idea, so remember to remove or disable it when your issues are resolved.</p> <p>Pro tip: You can use the <code>OTEL_LOG_LEVEL</code> environment variable to set <code>DiagLogLevel</code> so we can easily turn it off and on.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#auto-instrumentation-libraries","title":"Auto Instrumentation Libraries","text":"<p>Many users choose to use auto Instrumentation libraries, which automatically create spans for interesting operations in popular and widely used packages (DB drivers, HTTP frameworks, cloud services SDKs, etc)</p> <p>Some initialization patterns and configuration options can cause your service to fail to create spans, to begin with.</p> <p>To rule out auto instrumentation libraries issues, try to create a manual span first. If you see manual spans but not spans from the installed auto instrumentation libraries, continue reading this section.</p> <pre><code>import { trace } from '@opentelemetry/api';\ntrace\n.getTracerProvider()\n.getTracer('debug')\n.startSpan('test manual span')\n.end();\n</code></pre>"},{"location":"blog/2022/troubleshooting-nodejs/#install-and-enable","title":"Install and Enable","text":"<p>To use an auto instrumentation library in your service, you\u2019ll need to:</p> <ol> <li>Install it: <code>npm install @opentelemetry/instrumentation-foo</code>. You can search    the OpenTelemetry Registry to find available instrumentations</li> <li>Create the instrumentation object: <code>new FooInstrumentation(config)</code></li> <li>Make sure instrumentation is enabled: call <code>registerInstrumentations(...)</code></li> <li>Verify you are using the right TracerProvider</li> </ol> <p>For most users, the following should cover it:</p> <pre><code>// First run `npm install @opentelemetry/instrumentation-foo @opentelemetry/instrumentation-bar\n// Replace foo and bar with the actual packages you need to instrument (http/mysql/redis etc)\nimport { FooInstrumentation } from '@opentelemetry/instrumentation-foo';\nimport { BarInstrumentation } from '@opentelemetry/instrumentation-bar';\nimport { registerInstrumentations } from '@opentelemetry/instrumentation';\n// create TracerProvider, SpanProcessors and SpanExporters\nregisterInstrumentations({\ninstrumentations: [new FooInstrumentation(), new BarInstrumentation()],\n});\n</code></pre> <p>For advanced users who choose to use the low-level api instead of calling <code>registerInstrumentations</code>, make sure your instrumentation is set to use the right tracer provider and that you call <code>enable()</code> if appropriate.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#enable-before-require","title":"Enable Before Require","text":"<p>All instrumentations are designed such that you first need to enable them and only then require the instrumented package. A common mistake is to require packages before enabling the instrumentation libraries for them.</p> <p>Here is a bad example:</p> <pre><code>import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nimport { registerInstrumentations } from '@opentelemetry/instrumentation';\nimport { HttpInstrumentation } from '@opentelemetry/instrumentation-http';\nimport {\nSimpleSpanProcessor,\nConsoleSpanExporter,\n} from '@opentelemetry/sdk-trace-base';\nimport http from 'http'; // \u21d0 BAD - at this point instrumentation is not registered yet\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));\nprovider.register();\nregisterInstrumentations({ instrumentations: [new HttpInstrumentation()] });\n// your application code which uses http\n</code></pre> <p>In most cases, the instrumentation code resides in a different file or package than the application code, which makes it tricky to discover. Some frameworks, such as serverless, can import packages before the instrumentation code has a chance to run. This can be easily missed.</p> <p>To diagnose this issue, enable logging and verify you are seeing your instrumentation package being loaded. For example:</p> <pre><code>@opentelemetry/instrumentation-http Applying patch for https@12.22.9\n</code></pre> <p>If missing, chances are your auto instrumentation library is not being applied.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#library-configuration","title":"Library Configuration","text":"<p>Some auto instrumentation libraries include a custom configuration that controls when instrumentation is skipped. For example, HTTP instrumentation has options such as <code>ignoreIncomingRequestHook</code> and <code>requireParentforOutgoingSpans</code></p> <p>In specific cases, some libraries are not instrumenting by default, and you have to specifically opt-in to get spans. For example, <code>ioredis</code> instrumentation should be configured with <code>requireParentSpan = true</code> to create spans for internal operation with no parent span.</p> <p>If you don\u2019t see spans for a library, maybe you need to tweak the configuration to make them appear.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#instrumented-library-version","title":"Instrumented Library Version","text":"<p>Auto instrumentation libraries usually don\u2019t support all versions of the library they instrument. If the version you are using is too old or very recent, it might not be supported and thus no spans will be created.</p> <p>Consult the documentation of the library you are using to verify if your version is compatible. This data is usually found in the README for the instrumentation, for example see the redis README.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#no-recording-and-non-sampled-spans","title":"No Recording and Non-Sampled Spans","text":"<p>Not all spans that are created in your application are exported. Spans can be marked as \u201cNot Sampled\u201d or \u201cNon-Recorded\u201d in which case you will not see them in your backend.</p> <p>To rule out these issues, you can hook in a \u201cdebug span processor\u201d which only prints the sampled decision. If \u201cspan sampled: false\u201d is printed to the console, continue reading this section.</p> <pre><code>import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nimport { ReadableSpan } from '@opentelemetry/sdk-trace-base';\nimport { trace, Span, Context, TraceFlags } from '@opentelemetry/api';\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor({\nforceFlush: async () =&gt; {},\nonStart: (_span: Span, _parentContext: Context) =&gt; {},\nonEnd: (span: ReadableSpan) =&gt; {\nconst sampled = !!(span.spanContext().traceFlags &amp; TraceFlags.SAMPLED);\nconsole.log(`span sampled: ${sampled}`);\n},\nshutdown: async () =&gt; {},\n});\nprovider.register();\n</code></pre>"},{"location":"blog/2022/troubleshooting-nodejs/#nooptracerprovider","title":"NoopTracerProvider","text":"<p>If you don\u2019t create and register a valid TracerProvider, your app will run with the default TracerProvider which starts all the spans in your app as NonRecordingSpans.</p> <p>You need to have code similar to this as early as possible in your application:</p> <pre><code>import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nimport {\nConsoleSpanExporter,\nSimpleSpanProcessor,\n} from '@opentelemetry/sdk-trace-base';\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));\nprovider.register();\n</code></pre>"},{"location":"blog/2022/troubleshooting-nodejs/#remote-sampling-decision","title":"Remote Sampling Decision","text":"<p>The default sampling behavior (and a very popular one) is that each span inherits the sampling decision from its parent. If the component that invoked your service is configured not to sample, then you will not see spans from your service as well.</p> <p>Examples include:</p> <ul> <li>An API Gateway can be configured with sampling logic or have tracing turned   off, in which case it can affect all downstream tracing (including your   innocent service, which needs to be sampled).</li> <li>External users, which are calling your service, can also be instrumented and   derive their own sampling decisions (which you have no control of). These   sampling decisions are then propagated to your service and affect it.</li> <li>Other services in your system can derive sampling decisions based on their   local needs and viewpoint. It can be easy to configure an upstream service   endpoint to not sample an uninteresting endpoint without realizing that it   calls a very interesting and important endpoint downstream (which we do want   to sample).</li> </ul>"},{"location":"blog/2022/troubleshooting-nodejs/#local-sampler","title":"Local Sampler","text":"<p>You can configure your local sampler to sample some spans or none. If the configuration was written by someone else a long time ago, or if it is complex / non-intuitive \u2014 then spans are justifiably not sampled and exported, which can be easy to miss.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#exporting-issues","title":"Exporting Issues","text":"<p>It is possible that the service is generating spans, but they are not exported correctly to your backend or are being thrown in the collector for some reason.</p> <p>To rule out exporting issues, try to add \"ConsoleExporter\". If you see spans exported to console but not in the backend you export to, continue reading this section.</p> <pre><code>import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nimport {\nConsoleSpanExporter,\nSimpleSpanProcessor,\n} from '@opentelemetry/sdk-trace-base';\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));\nprovider.register();\n</code></pre>"},{"location":"blog/2022/troubleshooting-nodejs/#configuring-an-exporter","title":"Configuring an Exporter","text":"<p>Your service should have span exporting code similar to this:</p> <pre><code>import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-proto';\n// Create TracerProvider\nconst exporter = new OTLPTraceExporter();\nprovider.addSpanProcessor(new BatchSpanProcessor(exporter));\n</code></pre> <p>In this example, I used <code>@opentelemetry/exporter-trace-otlp-proto</code>, but there are other exporters to choose from, and each one has a few configuration options. An error in one of these options will fail to export, which is silently ignored by default.</p> <p>A a few common configuration errors are covered in the following subsections.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#otlp-exporters","title":"OTLP exporters","text":"<ul> <li>Format \u2014 OTLP supports <code>http/json</code>, <code>http/proto</code>, and <code>grpc</code> formats. You   need to choose an exporter package that matches the format your OTLP collector   support.</li> <li>Path \u2014 If you set HTTP collector endpoint (via config in code or   environment variables), you must also set the path:   <code>http://my-collector-host:4318/v1/traces</code>. If you forget the path, the export   will fail. In gRPC, you must not add path: \u201cgrpc://localhost:4317\u201d. This can   be a bit confusing to get right at first.</li> <li>Secure Connection \u2014 Check if your collector expects a secure or insecure   connection. In HTTP, this is determined by the URL scheme (<code>http:</code> /   <code>https:</code>). In gRPC, the scheme has no effect and the connection security is   set exclusively by the credentials parameter: <code>grpc.credentials.createSsl()</code>,   <code>grpc.credentials.createInsecure()</code>, etc. The default security for both HTTP   and gRPC is Insecure.</li> </ul>"},{"location":"blog/2022/troubleshooting-nodejs/#jaeger-exporter","title":"Jaeger Exporter","text":"<p>Jaeger exporter can work in \u201cAgent\u201d mode (over UDP) and \u201cCollector\u201d mode (over TCP). The logic to decide which one to use is a bit confusing and lacks documentation. If you pass the <code>endpoint</code> parameter in exporter config or set <code>OTEL_EXPORTER_JAEGER_ENDPOINT</code> environment variable, then the exporter will use \u201cCollector\u201d HTTP sender. Else, it will export in \u201cAgent\u201d mode with UDP sender to the <code>host</code> configured in the <code>param</code>, or, <code>OTEL_EXPORTER_JAEGER_AGENT_HOST</code> or <code>localhost:6832</code>.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#setting-vendor-credentials","title":"Setting Vendor Credentials","text":"<p>If you are using a vendor as your tracing backend, you might need to add additional info such as authentication headers. For example, if you send traces to Aspecto, you\u2019ll need to add your Aspecto token as an Authorization header, like this:</p> <pre><code>import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-proto';\n// Create TracerProvider\nconst exporter = new OTLPTraceExporter({\nurl: 'https://otelcol.aspecto.io/v1/trace',\nheaders: {\nAuthorization: 'YOUR_API_KEY_HERE',\n},\n});\nprovider.addSpanProcessor(new BatchSpanProcessor(exporter));\n</code></pre> <p>If not applied, you will not be able to see any data in your vendor\u2019s account.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#flush-and-shutdown","title":"Flush and Shutdown","text":"<p>When your service goes down or your lambda function ends, it is possible that not all spans are successfully exported to your collector yet. You need to call the shutdown function on your tracer provider and await the returned promise to ensure all data has been sent.</p> <pre><code>import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';\nconst provider = new NodeTracerProvider();\nprovider.register();\n// when your you terminate your service, call shutdown on provider:\nprovider.shutdown();\n</code></pre>"},{"location":"blog/2022/troubleshooting-nodejs/#package-versions-compatibility","title":"Package Versions Compatibility","text":"<p>Some issues can be a result of incompatible or old versions of SDK and instrumentation packages.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#sdk-versions","title":"SDK versions","text":"<p>It is recommended to check that your SDKs and API packages are not old and are compatible with each other. Make sure you don\u2019t have any peer dependency warnings when you npm install.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#other-apm-libraries","title":"Other APM libraries","text":"<p>OpenTelemetry is not guaranteed to be compatible with other APM libraries that use monkey patching to do their magic. If you have such a package installed, try to remove or disable it and check if the problem goes away.</p>"},{"location":"blog/2022/troubleshooting-nodejs/#whats-next","title":"What\u2019s Next?","text":""},{"location":"blog/2022/troubleshooting-nodejs/#where-to-get-help","title":"Where to Get Help","text":"<p>If none of the above solved your problems, you can ask for help on the following channels:</p> <ul> <li>CNCF <code>#otel-js</code> Slack   channel</li> <li>CNCF <code>#opentelemetry-bootcamp</code>   Slack channel</li> <li>GitHub   discussions page</li> </ul>"},{"location":"blog/2022/troubleshooting-nodejs/#resources","title":"Resources","text":"<ul> <li>Opentelemetry-js GitHub repo</li> <li>The OpenTelemetry Bootcamp</li> <li>OpenTelemetry docs</li> </ul>"},{"location":"blog/2022/troubleshooting-nodejs/#should-i-use-a-vendor","title":"Should I Use a Vendor?","text":"<p>Another alternative is to use a vendor\u2019s distribution of OpenTelemetry. These distributions can save you time and effort:</p> <ul> <li>Technical support</li> <li>Preconfigured with popular features for common and advanced users</li> <li>Up to date with latest OpenTelemetry versions</li> <li>Implementing best practices and avoiding the pitfalls mentioned above</li> </ul> <p>For a list of OpenTelemetry vendors, see Vendors.</p> <p>A version of this article was [originally posted][] on the Aspecto blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/v1.0-trio/","title":"OpenTelemetry Erlang/Elixir, Javascript, and Ruby v1.0 (Medium)","text":"<p>We are kicking off the new year with a bang! In the last couple months, three new languages (Ruby, Javascript, and Erlang/Elixir) have had their first 1.0 releases, joining the existing GA releases from C++, Go, Java, .Net, Python and Swift. Read all the details from the [announcement][].</p> <p>[announcement]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/welcome/","title":"Welcome to the OpenTelemetry blog","text":"<p>Welcome to the OpenTelemetry blog! As of early 2022, we'll be publishing blog entries through the website rather than Medium. You can still find Medium posts at medium.com/opentelemetry.</p>"},{"location":"blog/2022/announcing-opentelemetry-demo-release/","title":"OpenTelemetry Demo now Generally Available!","text":"<p>Earlier this year, we announced a project to build an OpenTelemetry Demo, representing the breadth of OpenTelemetry features and languages. Today, the Demo SIG is proud to announce OpenTelemetry Demo v1.0! With this demo, you\u2019ll be able to quickly run a complete end-to-end distributed system instrumented with 100% OpenTelemetry Traces and Metrics.</p> <p></p> <p>One of our primary goals of this project has been to create a robust sample application for developers to use in learning OpenTelemetry, and we\u2019re proud to say that we\u2019ve done just that. Every OpenTelemetry language SDK except Swift is represented in this release -- yes, even PHP! We\u2019ve built complete tracing flows that demonstrate a breadth of common instrumentation tasks such as:</p> <ul> <li>Enriching spans from automatic instrumentation.</li> <li>Creating custom spans for richer, more useful traces.</li> <li>Propagating trace context automatically and manually.</li> <li>Handling observability baggage in order to pass attributes between services.</li> <li>Creating attributes, events, and other telemetry metadata.</li> </ul> <p>We\u2019ve also integrated OpenTelemetry Metrics across several services to capture runtime and business metric use cases.</p> <p>Now, it\u2019d be enough to just provide a great demonstration of OpenTelemetry, but one thing we wanted to focus on for our 1.0 release was showing not just the \u2018how\u2019, but the \u2018why\u2019, of OpenTelemetry. To that end, we\u2019ve built a framework for implementing failure scenarios gated by feature flags. In addition, we include pre-configured dashboards and walk-throughs in our docs on how to read and interpret the telemetry data each service emits to discover the underlying cause of performance regressions in the application.</p> <p>Another goal of this demo is to streamline the ability of vendors and commercial implementers of OpenTelemetry to have a standardized target for building demos around. We\u2019ve already seen quite a bit of adoption, with five companies including Datadog, Dynatrace, Honeycomb, Lightstep, and New Relic integrating the community demo application into their product demos (you can find a list here). We hope to encourage further contributions and collaboration along these lines.</p> <p>However, just because we reached 1.0, that doesn\u2019t mean we\u2019re stopping -- this demo is a living artifact, one that we intend to continue to improve. In the coming months we plan to continue to iterate and improve coverage of metrics and logs as more SDKs reach maturity.</p> <p>We also hope to add new instrumentation scenarios and patterns by extending the functionality of the application -- queues and async processing of requests, a hosted version in order to explore the demo with zero setup, adding in support for Swift, and more.</p> <p>We\u2019d love for you to take the demo for a spin and let us know what you think! Check out the docs, or run the demo using Docker or Kubernetes, and let us know your thoughts. If you\u2019d like to contribute, please file an issue on GitHub or join us on the CNCF Slack in #otel-community-demo.</p>"},{"location":"blog/2022/apisix/","title":"Apache APISIX Integrates with OpenTelemetry to Collect Tracing Data","text":"<p>This article introduces the Apache APISIX's <code>opentelemetry</code> plugin concept and how to enable and deploy the plugin.</p>"},{"location":"blog/2022/apisix/#background-information","title":"Background Information","text":"<p>OpenTelemetry is an open source telemetry data acquisition and processing system. It not only provides various SDKs for application side telemetry data collection and reporting, but also provides data collection side for data receiving, processing, and exporting. Export to any or more OpenTelemetry backends, such as Jaeger, Zipkin, and OpenCensus. You can view the list of plugins that have adapted the OpenTelemetry Collector in the registry.</p> <p></p>"},{"location":"blog/2022/apisix/#plugin-introduction","title":"Plugin Introduction","text":"<p>The <code>opentelemetry</code> plugin of Apache APISIX implements Tracing data collection and sends it to OpenTelemetry Collector through HTTP protocol. Apache APISIX starts to support this feature in v2.13.0.</p> <p>One of OpenTelemetry's special features is that the Agent/SDK of OpenTelemetry is not locked with back-end implementation, which gives users flexibilities on choosing their own back-end services. In other words, users can choose the backend services they want, such as Zipkin and Jaeger, without affecting the application side.</p> <p>The <code>opentelemetry</code> plugin is located on the Agent side. It integrates the OpenTelemetry Agent/SDK and adopts its features in Apache APISIX. It can collect traced requests, generate <code>trace</code>, and forward them to the OpenTelemetry Collector. It supports the <code>trace</code> protocol, and it will support the <code>logs</code> and <code>metrics</code> protocols of OpenTelemetry in the next version.</p>"},{"location":"blog/2022/apisix/#enable-the-plugin","title":"Enable the Plugin","text":"<p>You need to enable <code>opentelemetry</code> plugin and modify collector configuration in <code>conf/config.yaml</code> configuration file.</p> <p>We assume that you have already deployed the OpenTelemetry Collector on the same node as the APISIX and enabled the OTLP HTTP Receiver.</p> <p>Need help completing deployment of the OpenTelemetry Collector? See the scenario Example below.</p> <p>The default port of the OTLP HTTP Receiver is <code>4318</code>, and the address of the <code>collector</code> is the HTTP Receiver address of the OpenTelemetry Collector. For related fields, see the Apache APISIX documentation.</p> <p>A typical configuration might look like this:</p> <pre><code>plugins\n... # Other plugins that have been enabled\n- opentelemetry\nplugin_attr:\n...\nopentelemetry:\ntrace_id_source: x-request-id\nresource:\nservice.name: APISIX\ncollector:\naddress: 127.0.0.1:4318 # OTLP HTTP Receiver address\nrequest_timeout: 3\n</code></pre>"},{"location":"blog/2022/apisix/#method-1-enable-the-plugin-for-a-specific-route","title":"Method 1: Enable the Plugin for a Specific Route","text":"<p>In order to show the test effect more conveniently, <code>sampler</code> is temporarily set to full sampling in the example to ensure that <code>trace</code> data is generated after each request is traced, so that you can view <code>trace</code> related data on the Web UI. You can also set relevant parameters according to the actual situation.</p> <pre><code>curl http://127.0.0.1:9080/apisix/admin/routes/1 \\\n-H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' \\\n-X PUT -d '\n{\n    \"uri\": \"/get\",\n    \"plugins\": {\n        \"opentelemetry\": {\n            \"sampler\": {\n                \"name\": \"always_on\"\n            }\n        }\n    },\n    \"upstream\": {\n        \"type\": \"roundrobin\",\n        \"nodes\": {\n            \"httpbin.org:80\": 1\n        }\n    }\n}'\n</code></pre>"},{"location":"blog/2022/apisix/#method-2-enable-the-plugin-globally","title":"Method 2: Enable the Plugin Globally","text":"<p>You can also enable <code>opentelemetry</code> plugin through the Apache APISIX Plugins feature. After the global configuration is complete, you still need to create the route, otherwise it will not be possible to test.</p> <pre><code>curl 'http://127.0.0.1:9080/apisix/admin/global_rules/1' \\\n-H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' \\\n-X PUT -d '{\n    \"plugins\": {\n        \"opentelemetry\": {\n            \"sampler\": {\n                \"name\": \"always_on\"\n            }\n        }\n    }\n}'\n</code></pre>"},{"location":"blog/2022/apisix/#method-3-customize-labels-for-span-through-additional_attributes","title":"Method 3: Customize Labels for Span through additional_attributes","text":"<p>For the configuration of <code>sampler</code> and <code>additional_attributes</code>, see the Apache APISIX documentation, where <code>additional_attributes</code> is a series of <code>Key:Value</code> pairs, you can use it to customize the label for Span, and can follow Span to display on the Web UI. Add <code>route_id</code> and <code>http_x-custom-ot-key</code> to the span of a route through <code>additional_attributes</code>, see the following configuration:</p> <pre><code>curl http://127.0.0.1:9080/apisix/admin/routes/1001 \\\n-H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' \\\n-X PUT -d '\n{\n    \"uri\": \"/put\",\n    \"plugins\": {\n        \"opentelemetry\": {\n            \"sampler\": {\n                \"name\": \"always_on\"\n            },\n            \"additional_attributes\":[\n                \"route_id\",\n                \"http_x-custom-ot-key\"\n            ]\n        }\n    },\n    \"upstream\": {\n        \"type\": \"roundrobin\",\n        \"nodes\": {\n            \"httpbin.org:80\": 1\n        }\n    }\n}'\n</code></pre>"},{"location":"blog/2022/apisix/#test-and-verify-the-plugin","title":"Test and Verify the Plugin","text":"<p>You can enable <code>opentelemetry</code> plugin in any of the above three methods. The following example uses the example of method three to create a route. After the creation is successful, see the following commands to access the route:</p> <pre><code>curl -X PUT -H `x-custom-ot-key: test-ot-val` http://127.0.0.1:9080/put\n</code></pre> <p>After the access is successful, you can see the details of the span similar to <code>/put</code> in the Jaeger UI, and you can see that the custom tags in the route are displayed in the Tags list: <code>http_x-custom-ot-key</code> and <code>route_id</code>.</p> <p></p> <p>You need to note that the <code>additional_attributes</code> configuration is set to take values from Apache APISIX and NGINX variables as <code>attribute</code> values, so <code>additional_attributes</code> must be a valid Apache APISIX or NGINX variable. It also includes HTTP Header, but when fetching http*header, you need to add <code>http*</code>as the prefix of the variable name. If the variable does not exist, the<code>tag</code> will not be displayed.</p>"},{"location":"blog/2022/apisix/#example","title":"Example","text":"<p>This scenario example deploys Collector, Jaeger, and Zipkin as backend services by simply modifying the OpenTelemetry Collector example, and starts two sample applications (Client and Server), where Server provides an HTTP service, and Client will cyclically call the server provided by the server. HTTP interface, resulting in a call chain consisting of two spans.</p>"},{"location":"blog/2022/apisix/#step-1-deploy-opentelemetry","title":"Step 1: Deploy OpenTelemetry","text":"<p>The following uses <code>docker compose</code> as an example. For other deployments, see Getting Started.</p> <p>You can see the following command to deploy1:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-collector-contrib.git\ncd opentelemetry-collector-contrib/examples/demo\ndocker compose up -d\n</code></pre> <p>Visit http://127.0.0.1:16886 (Jaeger UI) or http://127.0.0.1:9411/zipkin (Zipkin UI) in your browser. If it can be accessed normally, the deployment is successful.</p> <p>The following screenshots show an example of successful access.</p> <p></p> <p></p>"},{"location":"blog/2022/apisix/#step-2-configure-the-test-environment","title":"Step 2: Configure the Test Environment","text":"<p>The Apache APISIX service is introduced, and the topology of the final application is shown in the following figure.</p> <p></p> <p>The Trace data reporting process is as follows. Among them, since Apache APISIX is deployed separately and not in the network of docker-compose, Apache APISIX accesses the OTLP HTTP Receiver of OpenTelemetry Collector through the locally mapped port (<code>127.0.0.1:4138</code>).</p> <p></p> <p>You need to make sure you have enabled the <code>opentelemetry</code> plugin and reload Apache APISIX.</p> <p>You can see the following example to create a route and enable the<code>opentelemetry</code> plugin for sampling:</p> <pre><code>curl http://127.0.0.1:9080/apisix/admin/routes/1 \\\n-H 'X-API-KEY: edd1c9f034335f136f87ad84b625c8f1' \\\n-X PUT -d '\n{\n  \"uri\": \"/hello\",\n  \"plugins\": {\n      \"opentelemetry\": {\n          \"sampler\": {\n            \"name\": \"always_on\"\n          }\n      }\n  },\n  \"upstream\": {\n      \"type\": \"roundrobin\",\n      \"nodes\": {\n          \"127.0.0.1:7080\": 1\n      }\n  }\n}'\n</code></pre> <p>Modify the <code>./examples/demo/otel-collector-config.yaml</code> file to add the OTLP HTTP Receiver.</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp: ${ip:port} # add OTLP HTTP Receiver\uff0cdefault port is 4318\n</code></pre> <p>Modify <code>docker-compose.yaml</code> file.</p> <p>You need to modify the configuration file, change the interface address of Client calling Server to the address of Apache APISIX, and map the ports of OTLP HTTP Receiver and Server services to local.</p> <p>The following example is the complete <code>docker-compose.yaml</code> after the configuration is modified:</p> <pre><code>version: '2'\nservices:\n# Jaeger\njaeger-all-in-one:\nimage: jaegertracing/all-in-one:latest\nports:\n- '16686:16686' # jaeger ui port\n- '14268'\n- '14250'\n# Zipkin\nzipkin-all-in-one:\nimage: openzipkin/zipkin:latest\nports:\n- '9411:9411'\n# Collector\notel-collector:\nimage: ${OTELCOL_IMG}\ncommand: ['--config=/etc/otel-collector-config.yaml', '${OTELCOL_ARGS}']\nvolumes:\n- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\nports:\n- '1888:1888' # pprof extension\n- '8888:8888' # Prometheus metrics exposed by the collector\n- '8889:8889' # Prometheus exporter metrics\n- '13133:13133' # health_check extension\n- '4317' # OTLP gRPC receiver\n- '4318:4318' # Add OTLP HTTP Receiver port mapping\n- '55670:55679' # zpages extension\ndepends_on:\n- jaeger-all-in-one\n- zipkin-all-in-one\ndemo-client:\nbuild:\ndockerfile: Dockerfile\ncontext: ./client\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n- DEMO_SERVER_ENDPOINT=http://172.17.0.1:9080/hello # APISIX address\ndepends_on:\n- demo-server\ndemo-server:\nbuild:\ndockerfile: Dockerfile\ncontext: ./server\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\nports:\n- '7080:7080' # Map the Server port to the host\ndepends_on:\n- otel-collector\nprometheus:\ncontainer_name: prometheus\nimage: prom/prometheus:latest\nvolumes:\n- ./prometheus.yaml:/etc/prometheus/prometheus.yml\nports:\n- '9090:9090'\n</code></pre> <p>It should be noted that <code>demo-client.environment.DEMO_SERVER_ENDPOINT</code> needs to be changed to your Apache APISIX address, and ensure that it can be accessed normally in the container.</p> <p>Of course, you can also deploy Apache APISIX through <code>docker-compose.yaml</code>. For details, see Installation via Docker.</p>"},{"location":"blog/2022/apisix/#step-3-verify-the-outputs","title":"Step 3: Verify the Outputs","text":"<p>After the redeployment is completed, you can access the Jaeger UI or Zipkin UI to see that the Span of APISIX is included in the Trace, as shown below:</p> <p></p> <p></p> <p>When demo-server is not instrumented, you can still getting visibility of the demo-server behavior by enabling this plugin. Although this is not a typical case, it is a poor-man substitute of a real instrumentation of demo-server and provides a lot of value.</p> <p></p> <p>When the request does not reach the demo-server, the output would not include the span of demo-server.</p> <p></p>"},{"location":"blog/2022/apisix/#disable-the-plugin","title":"Disable the Plugin","text":"<p>If you do not need trace collection of a route temporarily, you only need to modify the route configuration and delete the part of <code>opentelemetry</code> under <code>plugins</code> in the configuration.</p> <p>If you enabled <code>opentelemetry</code> globally by binding Global Rules, you can remove the configuration of the <code>opentelemetry</code> global plugin.</p> <p>Note that disabling the <code>opentelemetry</code> plugin only results in disconnecting the APISIX span, the client and server spans will remain connected.</p>"},{"location":"blog/2022/apisix/#summary","title":"Summary","text":"<p>After Apache APISIX integrates OpenTelemetry, it can easily connect with many Trace systems on the market. Apache APISIX is also actively cooperating with communities to create a more powerful ecosystem.</p> <p>Apache APISIX is also currently working on additional plugins to support integration with more services, if you're interested, feel free to start a discussion on GitHub, or communicate via the mailing list.</p> <p>A version of this article was [originally posted][] on the Apache APISIX blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2022/collector-builder-sample/","title":"Using the Collector Builder with Sample Configs on GCP","text":"<p>The OpenTelemetry Collector is a versatile tool for processing and exporting telemetry data. It supports ingestion from many different sources and routing to just as many observability backends thanks to its modular design. This design, based on individual receivers, processors, and exporters, enables third-party development of new backend support. It also allows users to configure telemetry pipelines that are customized to their use case.</p> <p>The collector-builder tool takes that customizability a step further, offering a way to easily compile a Collector binary built with only certain ingestion and export components. In contrast with publicly available binary releases (which bundle in a number of components by default), custom Collectors can be slimmed down to only include the components you really care about. This means the compiled Collector can be smaller, faster, and more secure than a public release.</p> <p>Google Cloud recently launched a sample GitHub repository for building and deploying your own custom OpenTelemetry Collector on GCP. This takes the guesswork out of running the collector on GCP, but leaves you free to extend it to meet your use-cases.</p> <p>This repository will help you:</p> <ul> <li> <p>Compile a custom Collector designed for GCP with the collector-builder.   Low-overhead Collector builds are possible with the upstream collector-builder   tool, so we\u2019ve provided the setup files for building a lightweight collector   with GCP services in mind. This is GCP's recommended base set of components to   use when running on GCP or working with GCP services. OpenTelemetry can be   complex and intimidating, and we are providing a curated and tested   configuration as a starting point.</p> </li> <li> <p>Adapt existing tutorials for the OpenTelemetry Collector to GCP use cases.   By distilling the wide breadth of available information on using the Collector   and abstracting setup into a few simple commands, this repository reduces the   process for setting up a custom collector to a few simple Make commands.</p> </li> <li> <p>Keep your Collector deployment up-to-date. By incorporating custom   Collectors into your CI/CD pipeline, you can automate builds to ensure your   Collector stays current with the latest features and fixes. In this repo,   we'll show one way to do that with Cloud Build.</p> </li> </ul> <p>Each of these represents part of the \u201cGetting Started\u201d process with OpenTelemetry Collector, so by identifying and consolidating these steps we hope to expedite and ease the experience down to a few clicks.</p>"},{"location":"blog/2022/collector-builder-sample/#build-an-opentelemetry-collector-that-suits-your-needs","title":"Build an OpenTelemetry Collector that suits your needs","text":"<p>While there are public Docker images for the Collector published by the OpenTelemetry community, these images can be as large as 40MB. This is due to all of the receivers, processors, and exporters that are bundled into the image by default. With all of these default components, there is also the potential for security issues to arise, part of the reason why the Collector maintainers recommend only enabling necessary components in your Collector.</p> <p>The collector-builder tool accelerates compiling stripped-down Collector images with a simple YAML config file. This file declares which components to include in the build, and collector-builder includes only those components (and nothing else). This is in contrast to a default Collector build, where many more components are included in the compiled binary (even if they are not being enabled in the Collector\u2019s runtime configuration). This difference is how you can achieve improvements over public images that include many excess components, with extra dependencies adding weight and security burden. Now, those extra components can\u2019t take up space because they aren\u2019t even available in the custom build.</p> <p>To show how this works we start with a sample config file in this repository which has been pre-filled with a few of the most relevant OpenTelemetry components for GCP. However, we encourage you to modify that sample file to fit your use case.</p> <p>For example, the following build file includes only the OTLP receiver and exporter, along with a logging exporter:</p> <pre><code>receivers:\n- import: go.opentelemetry.io/collector/receiver/otlpreceiver\ngomod: go.opentelemetry.io/collector v0.57.2\nexporters:\n- import: go.opentelemetry.io/collector/exporter/otlpexporter\ngomod: go.opentelemetry.io/collector v0.57.2\n- import: go.opentelemetry.io/collector/exporter/loggingexporter\ngomod: go.opentelemetry.io/collector v0.57.2\n</code></pre> <p>Edit this file in the repository and run <code>make build</code> to automatically generate a local binary, or <code>make docker-build</code> to compile a container image.</p>"},{"location":"blog/2022/collector-builder-sample/#get-up-and-running-quickly-on-gke","title":"Get up and running quickly on GKE","text":"<p>For convenience, this repository includes the minimum Kubernetes manifests used to deploy the Collector in a GKE cluster, with a compatible runtime configuration for the sample collector-builder components built by default. When used together, the Make commands provided to build and push an image to Artifact Registry will automatically update those manifests in the repository to use the newly-created image, offering an end-to-end build and deployment reference.</p>"},{"location":"blog/2022/collector-builder-sample/#deploying-the-collector-in-gke","title":"Deploying the Collector in GKE","text":"<p>As mentioned earlier in this post, the collector offers vendor-agnostic routing and processing for logs, metrics, and tracing telemetry data. For example, while you may already be using a collection agent on GKE (such as for metrics and logs), the collector can provide a pathway for exporting traces. Those traces can be sent to an observability backend of your choice. It then opens up flexibility to process and export your other telemetry signals to any backend.</p> <p>With the provided Kubernetes manifests, you only need one <code>kubectl</code> command to deploy the Collector:</p> <pre><code>kubectl apply -f k8s/manifest.yaml -n otel-collector\n</code></pre> <p>Using a Collector generated from the build file shown earlier in this post, a matching OpenTelemetry Collector configuration would look like the following:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nexporters:\notlp:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [otlp]\n</code></pre> <p>The Collector in GKE consumes this config file through a Kubernetes ConfigMap that is mounted by the Collector Pod. This ConfigMap is created with a basic <code>kubectl</code> command:</p> <pre><code>kubectl create configmap otel-config --from-file=./otel-config.yaml -n otel-collector\n</code></pre>"},{"location":"blog/2022/collector-builder-sample/#modifying-the-collector-config","title":"Modifying the Collector Config","text":"<p>The OpenTelemetry config file format allows hot-swapping Collector configurations to re-route telemetry data with minimal disruption. For example, it may be useful to temporarily enable the <code>logging</code> exporter, which provides debugging insights into the Collector. This is done by editing the local config file from above to add two lines:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nexporters:\notlp:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [otlp, logging]\n</code></pre> <p>The runtime config can then be re-applied with <code>kubectl apply</code>:</p> <pre><code>kubectl create configmap otel-config --from-file=./otel-config.yaml --dry-run=client -n otel-collector -o yaml | kubectl apply -f -\n</code></pre> <p>After restarting the Collector pod (such as with <code>kubectl delete</code> or by applying a new manifest, as we\u2019ll show below), the new config changes will take effect. This workflow can be used to enable or disable any OpenTelemetry exporter, with exporters available for many popular observability backends.</p>"},{"location":"blog/2022/collector-builder-sample/#adding-a-receiver-and-processor","title":"Adding a receiver and processor","text":"<p>You can add more components and follow the same process as above to build and deploy a new Collector image. For example, you can enable the Zipkin exporter (for sending traces to a Zipkin backend service) and the batch processor by editing your builder config from earlier like so:</p> <pre><code>receivers:\n- import: go.opentelemetry.io/collector/receiver/otlpreceiver\ngomod: go.opentelemetry.io/collector v0.57.2\nprocessors:\n- import: go.opentelemetry.io/collector/processor/batchprocessor\ngomod: go.opentelemetry.io/collector v0.57.2\nexporters:\n- import: go.opentelemetry.io/collector/exporter/otlpexporter\ngomod: go.opentelemetry.io/collector v0.57.2\n- import: go.opentelemetry.io/collector/exporter/loggingexporter\ngomod: go.opentelemetry.io/collector v0.57.2\n- import: github.com/open-telemetry/opentelemetry-collector-contrib/exporter/zipkinexporter\ngomod:\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/zipkinexporter\nv0.57.2\n</code></pre> <p>Running <code>make docker-build</code> then compiles a new version of your Collector image. If you have an Artifact Registry set up for hosting Collector images, you can also run <code>make docker-push</code> with environment variables set to make the image available in your GKE cluster (setup steps for this are documented in the README).</p> <p>Enabling the new receiver and processor also follows the same steps as above, starting with editing your local Collector config:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nbatch:\nsend_batch_max_size: 200\nsend_batch_size: 200\nexporters:\notlp:\nlogging:\nzipkin:\nendpoint: http://my-zipkin-service:9411/api/v2/spans\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp, logging, zipkin]\n</code></pre> <p>Update the config in your cluster by running the same <code>kubectl apply</code> command from earlier:</p> <pre><code>kubectl create configmap otel-config --from-file=./otel-config.yaml --dry-run=client -n otel-collector -o yaml | kubectl apply -f -\n</code></pre> <p>Now, once your Collector pod restarts with the new image it will start using the new exporter and processor. When you ran <code>make docker-build</code>, the command automatically updated the Kubernetes deployment manifests provided in the repository to point to your new Collector image. So updating an existing running Collector with the new image only requires running <code>kubectl apply</code> again:</p> <pre><code>kubectl apply -f manifest.yaml -n otel-collector\n</code></pre> <p>This will trigger a new rollout of your Collector deployment, picking up the config changes and new components as compiled into the updated image.</p>"},{"location":"blog/2022/collector-builder-sample/#automate-builds-for-secure-up-to-date-images","title":"Automate builds for secure, up-to-date images","text":"<p>Building your own Collector allows you to control updates and rollouts of the Collector image. In this repo, we have some samples of how you can own your Collector build on GCP.</p> <p>Using a Cloud Build configuration supports serverless, automated builds for your Collector. By doing so you can benefit from new releases, features, and bug fixes in Collector components with minimal delay. Combined with Artifact Registry, these builds can be pushed as Docker images in your GCP project. This provides portability and accessibility of your images, as well as enabling container vulnerability scanning in Artifact Registry to ensure the supply-chain safety of your Collector deployment.</p> <p>Serverless builds and vulnerability scanning are valuable aspects of a reliable CI/CD pipeline. Here, we hope to abstract away some of the complication around these processes by shipping sample configs with this repository. And while we're providing some sample steps toward setting these up with GCP, similar approaches are possible with many other vendors as well.</p>"},{"location":"blog/2022/collector-builder-sample/#wrapping-up","title":"Wrapping Up","text":"<p>The OpenTelemetry Collector makes it easy to ingest and export telemetry data. This is thanks to OpenTelemetry\u2019s vendor-agnostic data model, but also to the active community of contributors supporting custom components for different backends. These components provide the flexibility for OpenTelemetry to support a wide variety of use cases.</p> <p>We hope this sample, as well as some of the example use cases we\u2019ve discussed here, help smooth over the friction some experience when getting started with OpenTelemetry. If you would like to provide some feedback, don\u2019t hesitate to report any feature requests or issues by opening an issue on GitHub.</p>"},{"location":"blog/2022/debug-otel-with-otel/","title":"How we used OpenTelemetry to fix a bug in OpenTelemetry","text":"<p>OpenTelemetry is here to help us find the root cause of issues in our software quickly. We recently had an issue that we were able to fix by using one feature of OpenTelemetry to identify the root cause of bug in another feature.</p> <p>In this blog post, we want to share this interesting experience with you. By that, you will learn that minor differences in the language-specific implementations can have interesting implications and that you have a feature for java &amp; python, which is here to help you to debug context propagation issues.</p>"},{"location":"blog/2022/debug-otel-with-otel/#the-issue","title":"The issue","text":""},{"location":"blog/2022/debug-otel-with-otel/#describe-the-bug","title":"Describe the bug","text":"<p>For the blog post Learn how to instrument NGINX with OpenTelemetry we created a small sample app that had a frontend application in Node.js, that called an NGINX, which acted as a reverse proxy for a backend application in python.</p> <p>Our goal was to create a re-usable <code>docker-compose</code> that would not only show people how to instrument NGINX with OpenTelemetry, but also how a distributed trace crossing the web server would look like.</p> <p>While Jaeger showed us a trace flowing from the frontend application down to the NGINX, the connection between NGINX and python app was not visible: we had two disconnected traces.</p> <p>This came as a surprise, because in a prior test with a Java application as backend we were able to see traces going from NGINX to that downstream application.</p>"},{"location":"blog/2022/debug-otel-with-otel/#steps-to-reproduce","title":"Steps to reproduce","text":"<p>Follow the instructions on how you can put NGINX between two services. Replace the java-based application with a python application, e.g. put following three files into the <code>backend</code> folder instead:</p> <ul> <li><code>app.py</code>:</li> </ul> <pre><code>import time\nimport redis\nfrom flask import Flask\napp = Flask(__name__)\ncache = redis.Redis(host='redis', port=6379)\ndef get_hit_count():\nretries = 5\nwhile True:\ntry:\nreturn cache.incr('hits')\nexcept redis.exceptions.ConnectionError as exc:\nif retries == 0:\nraise exc\nretries -= 1\ntime.sleep(0.5)\n@app.route('/')\ndef hello():\ncount = get_hit_count()\nreturn 'Hello World! I have been seen {} times.\\n'.format(count)\n</code></pre> <ul> <li><code>Dockerfile</code>:</li> </ul> <pre><code>FROM python:3.10-alpine\nWORKDIR /code\nENV FLASK_APP=app.py\nENV FLASK_RUN_HOST=0.0.0.0\nRUN apk add --no-cache gcc musl-dev linux-headers\nCOPY requirements.txt requirements.txt\nRUN pip install -r requirements.txt\nRUN opentelemetry-bootstrap -a install\nEXPOSE 5000\nCOPY . .\nCMD [\"opentelemetry-instrument\", \"--traces_exporter\", \"otlp_proto_http\", \"--metrics_exporter\", \"console\", \"flask\", \"run\"]\n</code></pre> <ul> <li><code>requirements.txt</code>:</li> </ul> <pre><code>flask\nredis\nopentelemetry-distro\nopentelemetry-exporter-otlp-proto-http\n</code></pre> <p>Update the <code>docker-compose.yml</code> with the following:</p> <pre><code>version: '2'\nservices:\njaeger:\nimage: jaegertracing/all-in-one:latest\nports:\n- '16686:16686'\ncollector:\nimage: otel/opentelemetry-collector:latest\ncommand: ['--config=/etc/otel-collector-config.yaml']\nvolumes:\n- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\nnginx:\nimage: nginx-otel\nvolumes:\n- ./opentelemetry_module.conf:/etc/nginx/conf.d/opentelemetry_module.conf\n- ./default.conf:/etc/nginx/conf.d/default.conf\nbackend:\nbuild: ./backend\nimage: backend-with-otel\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/v1/traces\n- OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n- OTEL_SERVICE_NAME=python-app\nredis:\nimage: 'redis:alpine'\nfrontend:\nbuild: ./frontend\nimage: frontend-with-otel\nports:\n- '8000:8000'\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/\n- OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n- OTEL_SERVICE_NAME=frontend\n</code></pre> <p>Spin up that environment by running <code>docker compose up</code>1 and send some requests to the frontend with <code>curl localhost:8000</code></p>"},{"location":"blog/2022/debug-otel-with-otel/#what-did-you-expect-to-see","title":"What did you expect to see?","text":"<p>In the Jaeger UI at localhost:16686 you would expect to see traces going from the <code>frontend</code> through NGINX down to the <code>python-app</code>.</p>"},{"location":"blog/2022/debug-otel-with-otel/#what-did-you-see-instead","title":"What did you see instead?","text":"<p>In the Jaeger UI at localhost:16686 you will see two traces, one going from the <code>frontend</code> down to NGINX, and another one only for the <code>python-app</code>.</p>"},{"location":"blog/2022/debug-otel-with-otel/#the-solution","title":"The solution","text":""},{"location":"blog/2022/debug-otel-with-otel/#the-hints","title":"The hints","text":"<p>Since the setup worked with a java application in the backend, we knew that the problem was either caused by the python application or by the combination of the NGINX instrumentation and the python application.</p> <p>We could quickly rule out that the python application alone was the issue: trying out a simple Node.js application as backend, we got the same result: two traces, one from frontend to NGINX, another one for the Node.js application alone.</p> <p>With that, we knew that we had a propagation issue: the trace context was not transferred successfully from NGINX down to the python and Node.js application.</p>"},{"location":"blog/2022/debug-otel-with-otel/#the-analysis","title":"The analysis","text":"<p>Knowing that the issue does not occur with java and that it is likely a broken propagation, we knew what we had to do: we needed to see the trace headers.</p> <p>Gladly, the instrumentations for Java and Python have a feature that allows us to capture HTTP request &amp; response headers as span attributes easily.</p> <p>By providing a comma-separated list of HTTP header names via the environment variables <code>OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST</code> and <code>OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE</code> we can define which HTTP headers we want to capture. In our case we put all potential propagation headers:</p> <pre><code>OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST=tracestate,traceparent,baggage,X-B3-TraceId\n</code></pre> <p>In our <code>docker-compose</code>-based example we simply can add it to the definition of our backend service:</p> <pre><code>backend:\nbuild: ./backend\nimage: backend-with-otel\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/v1/traces\n- OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n- OTEL_SERVICE_NAME=python-app\n- OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST=tracestate,traceparent,baggage,X-B3-TraceId\n</code></pre> <p>Once again we ran <code>docker compose up</code>1 to bring up our sample app and we send some request with <code>curl localhost:8080</code> to the frontend application.</p> <p>In Jaeger we still see that the traces are disconnected. However, when we look into one of those traces, we can see the collected request headers from NGINX to backend:</p> <p></p> <p>There it is! The trace headers (<code>baggage</code>, <code>traceparent</code>, <code>tracestate</code>) are send as multiple header fields: the NGINX module added the value of each of those headers again and again, and since having multi value headers is covered by RFC7230, this didn't lead to an issue immediately.</p> <p>We tested the capability to correlate from NGINX to a downstream service with a Java application. And, without reading into the source code of the OTel Java SDK, it looks like that Java is flexible in taking a <code>traceparent</code> with multiple values, even though such format is invalid per the W3C Trace Context specification. So propagation from NGINX to the Java service worked, while in contrast, Python (and other languages) do not provide that flexibility and propagation from NGINX to the downstream service silently fails.</p> <p>Note, that we are not suggesting that the other languages should have the same flexibility as Java has with reading <code>traceparent</code> or vice-versa: the bug lives in the NGINX module and we need to fix that.</p>"},{"location":"blog/2022/debug-otel-with-otel/#the-fix","title":"The fix","text":"<p>To fix our problem we added some checks to the module for NGINX, that make sure that the trace headers are only set once.</p> <p>This fix is contained in the v1.0.1 release of the otel-webserver-module. This means you can update the <code>Dockerfile</code> to install the NGINX module like the following:</p> <pre><code>FROM nginx:1.18\nADD https://github.com/open-telemetry/opentelemetry-cpp-contrib/releases/download/webserver%2Fv1.0.1/opentelemetry-webserver-sdk-x64-linux.tgz /opt\nRUN cd /opt ; tar xvfz opentelemetry-webserver-sdk-x64-linux.tgz\nRUN cd /opt/opentelemetry-webserver-sdk; ./install.sh\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/opentelemetry-webserver-sdk/sdk_lib/lib\nRUN echo \"load_module /opt/opentelemetry-webserver-sdk/WebServerModule/Nginx/ngx_http_opentelemetry_module.so;\\n$(cat /etc/nginx/nginx.conf)\" &gt; /etc/nginx/nginx.conf\nCOPY default.conf /etc/nginx/conf.d\nCOPY opentelemetry_module.conf /etc/nginx/conf.d\n</code></pre> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2022/demo-announcement/","title":"Announcing a Community Demo for OpenTelemetry","text":""},{"location":"blog/2022/demo-announcement/#tldr","title":"TL;DR","text":"<p>The OpenTelemetry community has taken a good pre-existing demo (thanks, Google!) and is in the process of making it even better. Every GA SDK (besides Swift) will be represented, demo support will be extended to Metrics and Logs, and canonical scenarios will be documented for each signal, with fault injection, and more!</p> <p>If you want to skip the details then clone our repo then run <code>docker compose up</code>1 from the command line. There are a couple technology requirements so be sure to check those out too.</p> <p>The demo takes 15-20 minutes to build the first time so we encourage you to do some stretching and take a water break in the meantime.</p> <p>Your command line output should look like this:</p> <p></p> <ul> <li> <p>Once the images are built you can access the Webstore at:   http://localhost:8080</p> </li> <li> <p>And the Jaeger UI at: http://localhost:8080/jaeger/ui</p> </li> </ul> <p>Congratulations! You can now indulge in retail therapy and submit telemetry. A true victory.</p>"},{"location":"blog/2022/demo-announcement/#success-of-the-commons","title":"Success of the Commons","text":"<p>There are a couple universal problems that are the driving force behind our joint demo effort.</p> <p>As OpenTelemetry matures, users and enterprises are increasingly looking for best practice guides on how to onboard their services to the new paradigm or demo applications so that they can try out the new tools themselves. However, community working groups and vendors lack a singular sophisticated platform to demonstrate their technologies on. Greeting the world can only get us so far.</p> <p>Multiple vendors have written their own demo applications but are wholly responsible for the development and ongoing support. The existing demos are all feature incomplete in their own ways with missing languages, restrictions on backend choice, and they\u2019re overly reliant on instrumentation libraries.</p>"},{"location":"blog/2022/demo-announcement/#project-goals","title":"Project Goals","text":"<ul> <li>Provide developers with a robust sample application they can use in learning   OpenTelemetry instrumentation.</li> <li>Provide observability vendors with a single, well-supported, demo platform   that they can further customize or simply use OOB.</li> <li>Provide the OpenTelemetry community with a living artifact that demonstrates   the features and capabilities of OTel APIs, SDKs, and tools.</li> <li>Provide OpenTelemetry maintainers and working groups a platform to demonstrate   new features/concepts in real world like scenarios.</li> </ul>"},{"location":"blog/2022/demo-announcement/#current-state","title":"Current State","text":"<p>As a starting point, we have selected a fork of the popular GCP microservices demo. Our first feature additions have been to simplify local deployment by consolidating the project onto a single docker compose file, updating the documentation, and replacing a pre-existing service with a Ruby example. Otherwise the pre-existing feature set from the GCP demo remains the same:</p> <ul> <li>10 application microservice with support for 6 languages (C#, Go, Java,   Node.js, Python, and Ruby)</li> <li>Ruby support was added within the last 2 weeks of publishing date</li> <li>Designed to work on docker locally</li> <li>Uses redis cache</li> <li>Auto-instrumentation using instrumentation libraries Tracing support for the   gRPC, Redis, and HTTP libraries</li> <li>Jaeger visualizations for distributed traces, forwarded by OpenTelemetry   collector</li> <li>Always on sampling (100% of telemetry is submitted) and synthetic load   generation</li> </ul>"},{"location":"blog/2022/demo-announcement/#current-architecture","title":"Current Architecture","text":""},{"location":"blog/2022/demo-announcement/#byob-bring-your-own-backend","title":"BYOB (Bring Your Own Backend)","text":"<p>Jaeger is great (really) but what if you want to try this out with your APM vendor of choice? You can send data to your preferred backend by simply changing the Collector config file to use their Collector exporter or by using your vendor's fork of our demo.</p> <p>Lightstep has an excellent blog they just published on how to get started sending data to power their experiences from their forked version of our demo.</p>"},{"location":"blog/2022/demo-announcement/#future-state","title":"Future State","text":""},{"location":"blog/2022/demo-announcement/#upcoming-new-features","title":"Upcoming New Features","text":"<p>We have a lot of exciting improvements that are planned or in progress to turn this application into the canonical example of the full power of OpenTelemetry. Below is a semi-exhaustive list of upcoming features but we're not limiting ourselves to just the items listed here.</p> <ul> <li>Language examples for   C++,   Erlang/elixir,   PHP, and   Rust</li> <li>Extend support to   Metrics and   Logs for all   GA SDKs</li> <li>Visualization components to consume Metrics</li> <li>Implementation of multiple instrumentation techniques</li> <li>Auto-instrumentation using the agent in a sidecar</li> <li>Manual instrumentation of all signals</li> <li>Service Level Objective (SLO)   definition and tracking</li> <li>Additional instrumentation libraries introduced where needed</li> <li>Demonstratations of the ability to add   Baggage and   other custom tags</li> <li>Continue to build on other cloud-native technologies like:</li> <li>Kubernetes</li> <li>gRPC</li> <li>OpenFeature</li> <li>OpenSLO</li> <li>etc.</li> <li>An enhanced OpenTelemetry collector gateway capabilities for ingestion,   transformation, and export</li> <li>Probability based sampling</li> <li>Feature flag service to demonstrate various scenarios like fault injection and   how to emit telemetry from a feature flag reliant service</li> </ul>"},{"location":"blog/2022/demo-announcement/#future-architecture","title":"Future Architecture","text":""},{"location":"blog/2022/demo-announcement/#going-forward","title":"Going Forward","text":"<p>We\u2019re still at the beginning of our journey but there\u2019s great momentum behind this project. If you\u2019re interested in contributing we\u2019d love your support. There are links in our GitHub repo on how to get involved and you can track our overall progress from there.</p>"},{"location":"blog/2022/demo-announcement/#interesting-links","title":"Interesting Links","text":"<ul> <li>Demo Requirements</li> <li>Get Involved</li> </ul> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2022/end-user-discussion-group-apac/","title":"End User Discussion Group: New APAC Sessions Announcement!","text":"<p>Since July, end users have been getting together to discuss their OpenTelemetry adoption and implementation practices in a vendor-neutral space known as the Monthly Discussion Groups (also referenced as the End User Discussion Groups).</p> <p>Previously, they were only available in the EMEA and AMER regions; the End User Working Group is pleased to announce that APAC sessions will now be available, starting in January 2023!</p>"},{"location":"blog/2022/end-user-discussion-group-apac/#the-what","title":"The what","text":"<p>As mentioned above, these Discussion Groups are monthly meetings for end users to discuss the challenges they're facing in their OTel implementations and to learn from each other, as well as to talk about the project itself and what is working, what isn't, and what they'd like to see.</p> <p>We encourage you to attend and ask questions to learn how other users have implemented OpenTelemetry in their organizations, resolved common issues, and more. Example topics that have been discussed include tail sampling, collector scaling, and OpAMP; there really is no limit to the topics! We simply ask that you be respectful and abide by the Chatham House Rule.</p>"},{"location":"blog/2022/end-user-discussion-group-apac/#the-why","title":"The why","text":"<p>This group started as a result of direct feedback from end users in the community! Past participants have shared with us that these have been helpful and valuable for them as they migrate their observability instrumentation to OpenTelemetry.</p> <p>Additionally, the End User Working Group is also improving the process to share any collected user feedback* with the appropriate project maintainers to help improve the user experience and increase adoption.</p> <p>*Information shared in the groups can be discussed publicly, but not the identity of the person who said it or their affiliation.</p>"},{"location":"blog/2022/end-user-discussion-group-apac/#the-when","title":"The when","text":"<p>If this is your first time hearing about these meetings, you are invited to participate in any of the upcoming sessions that fit your schedule. Look for the meetings by filtering them in one of the community's publicly available calendars, and duplicate the event to your own. There will also be periodic reminders in the general CNCF Slack <code>#opentelemetry</code> channel.</p> <p>Upcoming sessions:</p> <ul> <li>EMEA: every third Tuesday of the month at 11AM CET (GMT +1), join   here</li> <li>December 20, 2022</li> <li>January 17, 2023</li> <li>APAC: every third Wednesday of the month at 11AM IST (GMT +5.5), join   here</li> <li>March 15, 2023 (register here)</li> <li>April 19, 2023</li> <li>AMER: every third Thursday of the month at 9AM PST (GMT -8), join   here</li> <li>December 15, 2022</li> <li>January 19, 2023</li> </ul> <p>See y'all soon!</p>"},{"location":"blog/2022/exponential-histograms/","title":"Exponential Histograms: Better Data, Zero Configuration","text":"<p>Histograms are a powerful tool in the observability tool belt. OpenTelemetry supports histograms because of their ability to efficiently capture and transmit distributions of measurements, enabling statistical calculations like percentiles.</p> <p>In practice, histograms come in several flavors, each with its own strategy for representing buckets and bucket counts. The first stable metric release for OpenTelemetry included explicit bucket histograms, and now OpenTelemetry is introducing a new exponential bucket histogram option. This exciting new format automatically adjusts buckets to reflect measurements and is more compressed to send over the wire. This blog post dives into the details of exponential histograms, explaining how they work, the problem they solve, and how to start using them now.</p>"},{"location":"blog/2022/exponential-histograms/#intro-to-metrics-in-opentelemetry","title":"Intro to metrics in OpenTelemetry","text":"<p>Before talking about exponential bucket histograms, let's do a quick refresher on some general OpenTelemetry metrics concepts. If you're already up to speed, skip ahead to Anatomy of a histogram.</p> <p>Metrics represent aggregations of many measurements. We use them because it's often prohibitively expensive to export and analyze measurements individually. Imagine the cost of exporting the time of each request for an HTTP server responding to one million requests per second! Metrics aggregate measurements to reduce data volume and retain a meaningful signal.</p> <p>Like tracing (and someday soon logs), OpenTelemetry metrics are broken into the API and SDK. The API is used to instrument code. Application owners can use the API to write custom instrumentation specific to their domain, but more commonly they install prebuilt instrumentation for their library or framework. The SDK is used to configure what happens with the data collected by the API. This typically includes processing it and exporting it out of process for analysis, often to an observability platform.</p> <p>The API entry point for metrics is the meter provider. It provides meters for different scopes, where a scope is just a logical unit of application code. For example, instrumentation for an HTTP client library would have a different scope and therefore a different meter than instrumentation for a database client library. You use meters to obtain instruments. You use instruments to report measurements, which consist of a value and set of attributes. This Java code snippet demonstrates the workflow:</p> <pre><code>OpenTelemetry openTelemetry = // declare OpenTelemetry instance\nMeter meter = openTelemetry.getMeter(\"my-meter-scope\");\nDoubleHistogram histogram =\nmeter\n.histogramBuilder(\"my-histogram\")\n.setDescription(\"The description\")\n.setUnit(\"ms\")\n.build();\nhistogram.record(10.2, Attributes.builder().put(\"key\", \"value\").build());\n</code></pre> <p>The SDK provides implementations of meter provider, meter, and instruments. It aggregates measurements reported by instruments and exports them as metrics according to the application configuration.</p> <p>There are currently six types of instruments in OpenTelemetry metrics: counter, up down counter, histogram, async counter, async up down counter, and async gauge. Carefully consider which instrument type to select, since each implies certain information about the nature of the measurements it records and how they are analyzed. For example, use a counter when you want to count things and when the sum of the things is more important than their individual values (such as tracking the number of bytes sent over a network). Use a histogram when the distribution of measurements is relevant for analysis. For example, a histogram is a natural choice for tracking response times for HTTP servers, because it's useful to analyze the distribution of response times to evaluate SLAs and identify trends. To learn more, see the guidelines for instrument selection.</p> <p>I mentioned earlier that the SDK aggregates measurements from instruments. Each instrument type has a default aggregation strategy (or simply aggregation) that reflects the intended use of the measurements as implied by the instrument type selection. For example, counters and up down counters aggregate to a sum of their values. Histograms aggregate to a histogram aggregation. (Note that histogram is both a type of instrument and an aggregation.)</p>"},{"location":"blog/2022/exponential-histograms/#anatomy-of-a-histogram","title":"Anatomy of a histogram","text":"<p>What is a histogram? Putting OpenTelemetry aside for a moment, we're all somewhat familiar with histograms. They consist of buckets and counts of occurrences within those buckets.</p> <p>For example, a histogram could track the number of times a particular number was rolled with the sum of two six-sided dice, with one bucket for each possible outcome, from 2-12. Over a large number of rolls, you expect the 7 bucket to have the highest count because you are more likely to roll a combined total of 7, and the 2 and 12 buckets to have the least because these are the least likely rolls, as shown in this example histogram.</p> <p></p> <p>OpenTelemetry has two types of histograms. Let's start with the relatively simpler explicit bucket histogram. It has buckets with boundaries explicitly defined during initialization. For example, if you configure it with boundaries [0,5,10], there are N+1 buckets with boundaries (-\u221e, 0],(0,5],(5,10], (10,+\u221e]. Each bucket tracks the number of occurrences of values within its boundaries. Additionally, the histogram tracks the sum of all values, the count of all values, the maximum value, and the minimum value. See the opentelemetry-proto for the complete definition.</p> <p>Before we talk about the second type of histogram, pause and think about some of the questions you can answer when data is structured like this. Assuming you're using a histogram to track the number of milliseconds it took to respond to a request, you can determine:</p> <ul> <li>The number of requests.</li> <li>The minimum, maximum, and average request latency.</li> <li>The percentage of requests that had latency less than a particular bucket   boundary. For example, if buckets boundaries are [0,5,10], you can take the   sum of the counts of buckets (-\u221e,0],(0,5],(5,10], and divide by the total   count to determine the percentage of requests that took less than 10   milliseconds. If you have an SLA that 99% of requests must be resolved in more   than 10 milliseconds, you can determine whether or not you met it.</li> <li>Patterns, by analyzing the distribution. For example, you might find that most   requests resolve quickly but a small number of requests take a long time and   bring down the average.</li> </ul> <p>The second type of OpenTelemetry histogram is the exponential bucket histogram. Exponential bucket histograms have buckets and bucket counts, but instead of explicitly defining the bucket boundaries, the boundaries are computed based on an exponential scale. More specifically, each bucket is defined by an index i and has bucket boundaries (base**i, base**(i+1)], where base**i means that base is raised to the power of i. The base is derived from a scale factor that is adjustable to reflect the range of reported measurements and is equal to 2**2**-scale. Bucket indexes must be continuous, but a non-zero positive or negative offset can be defined. For example, at scale 0, base = 2**2**-0 = 2 , and the bucket boundaries for indexes [-2,2] are defined as (.25,.5],(.5,1],(1,2],(2,4],(4,8]. By adjusting the scale, you can represent both large and small values. Like explicit bucket histograms, exponential bucket histograms also track the sum of all values, the count of all values, the maximum value, and the minimum value. See the opentelemetry-proto for the complete definition.</p>"},{"location":"blog/2022/exponential-histograms/#why-use-exponential-bucket-histograms","title":"Why use exponential bucket histograms","text":"<p>On the surface, exponential bucket histograms don't seem very different from explicit bucket histograms. In reality, their subtle differences yield dramatically different results.</p> <p>Exponential bucket histograms are a more compressed representation. Explicit bucket histograms encode data with a list of bucket counts and a list of N-1 bucket boundaries, where N is the number of buckets. Each bucket count and bucket boundary is an 8-byte value, so an N bucket explicit bucket histogram is encoded as 2N-1 8-byte values.</p> <p>In contrast, bucket boundaries for exponential bucket histograms are computed based on a scale factor and an offset defining the starting index of the buckets. Each bucket count is an 8-byte value, so an N bucket exponential bucket histogram is encoded as N+2 8-byte values (N bucket counts and 2 constants). Of course, both of these representations are commonly compressed when sent over a network, so further size reduction is likely, but exponential bucket histograms contain fundamentally less information.</p> <p>Exponential bucket histograms are basically configuration-free. Explicit bucket histograms need an explicitly defined set of bucket boundaries that need to be configured somewhere. A default set of boundaries is provided, but use cases of histograms vary wildly enough that it's likely you'll need to adjust the boundaries to better reflect your data. The view API helps, with mechanisms to select specific instruments and redefine the explicit bucket histogram aggregation bucket boundaries.</p> <p>In contrast, the only configurable parameter of exponential bucket histograms is the number of buckets, which defaults to 160 for positive values. The implementation automatically chooses the scale factor, based on the range of values recorded and the number of buckets available to maximize the bucket density around the recorded values. I can't overstate how useful this is.</p> <p>Exponential bucket histograms capture a high-density distribution of values automatically adjusted for the scale and range of measurements, with no configuration. The same histogram that captures nanosecond scale measurements is equally good at capturing second scale measurements. They retain fidelity regardless of scale.</p> <p>Consider the scenario of capturing HTTP request time milliseconds. With an explicit bucket histogram, you make guesses on bucket boundaries which you hope will accurately capture the distribution of values. But if conditions change and latency spikes, your assumptions might not hold and all values could be lumped together. Suddenly, you've lost visibility into the distribution of data. You know latency is high overall. But you can't know how many requests are high but tolerable versus terribly slow. In contrast, with an exponential bucket histogram, the scale automatically adjusts to the latency spikes to choose the optimal range of buckets. You retain insight into the distribution, even with a large range of measurement values.</p>"},{"location":"blog/2022/exponential-histograms/#example-scenario-explicit-bucket-histograms-vs-exponential-bucket-histograms","title":"Example scenario: explicit bucket histograms vs. exponential bucket histograms","text":"<p>Let's bring everything together with a proper demonstration comparing explicit bucket histograms to exponential bucket histograms. I've put together some example code that simulates tracking response time to an HTTP server in milliseconds. It records one million samples to an explicit bucket histogram with the default buckets, and to an exponential bucket histogram with a number of buckets that produces roughly the same size of OTLP -encoded, Gzip-compressed payload as the explicit bucket defaults. Through trial and error, I determined that ~40 exponential buckets produce an equivalent payload size to the default explicit bucket histogram with 11 buckets. (Your results may vary.)</p> <p>I wanted the distribution of samples to reflect what we might see in an actual HTTP server, with bands of response times corresponding to different operations. It will look something like this example:</p> <p></p> <p>To achieve this, I used a variety of different probability distributions, each corresponding to different bands in the curve, and each accounting for some percentage of the samples.</p> <p>I ran the simulation, and exported the histograms via to compare the explicit bucket histogram to the exponential bucket histogram. The next two charts show the results. The exponential bucket histogram has significantly more detail, which simply isn't available with the more limited buckets of the explicit bucket histogram.</p> <p>Note: These visualizations are from the New Relic platform, which I used because they employ me, and it's the easiest way for me to visualize histograms. Every platform will have its own mechanism for storing and retrieving histograms, which typically perform some lossy translation of buckets into a normalized storage format\u2014New Relic is no exception. Additionally, the visualization doesn't clearly delineate buckets, which causes adjacent buckets with the same count to appear as a single bucket.</p> <p>Here's the millisecond scale exponential bucket histogram:</p> <p></p> <p>Here's the millisecond scale explicit bucket histogram:</p> <p></p> <p>This demonstration is fairly generous to the explicit bucket histogram because I choose to report values in an optimum range for the default buckets (for example, 0 to 1000). The next two examples show what happens when the same values are recorded in nanosecond precision instead of milliseconds (all values are multiplied by 106). This is where the no-configuration autoscaling nature of exponential bucket histograms really shines. Left with the default explicit bucket boundaries, all the samples fall into a single bucket with the explicit bucket histogram. The exponential variety loses some definition compared to the millisecond version in the previous example, but you can still see the response time bands.</p> <p>Here's the nanosecond scale exponential bucket histogram:</p> <p></p> <p>Here's the nanosecond scale explicit bucket histogram:</p> <p></p>"},{"location":"blog/2022/exponential-histograms/#next-steps","title":"Next steps","text":"<p>Exponential bucket histograms are a powerful new tool for metrics. While implementations are still in progress at the time of publishing this post, you'll definitely want to enable them when you're using OpenTelemetry metrics.</p> <p>If you're using opentelemetry-java (and eventually other languages), the easiest way to enable exponential bucket histograms is by setting the environment variable with this command:</p> <pre><code>export OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION=exponential_bucket_histogram\n</code></pre> <p>For instructions on enabling in other languages, check the relevant documentation on instrumentation or github.com/open-telemetry.</p> <p>A version of this article was [originally posted][] on the New Relic blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/frontend-overhaul/","title":"Front-end Overhaul of the OpenTelemetry Demo (Go to Next.js)","text":"<p>One of the OpenTelemetry Project's many Special Interest Groups (SIG) is the OpenTelemetry Community demo SIG. The SIG supports a set of instrumented microservices and a front-end web app which are used to show how to instrument a distributed system with OpenTelemetry.</p> <p>The main focus of the web app is to demonstrate how to instrument an application, no matter what programming language, platform or OS it uses. The web app also shows different instrumentation techniques: automatic and manual, metrics, and baggage. All while following the standards and conventions prescribed in the official OTel documentation. More about the specific requirements can be found here.</p> <p>My company was focused on becoming part of and embracing the OpenTelemetry community. One of our goals this summer was to get more involved with a core OpenTelemetry project where we could provide a meaningful contribution. The OTel demo was the best match for our goal, as contributing would not only help the community, but also provide a great example to test and showcase our product with.</p> <p>The first thing we did was to get in contact with Carter Socha, the organizer of the OTel Demo SIG. Carter was really welcoming and helped us identify where our contributions could be the most impactful. We started looking at the issue created by Austin Parker referencing a complete front-end overhaul that would involve moving the application away from Go server-side render (SSR) to an architecture that included a browser-side client (client-side render or CSR), as well as improving the overall style, theme, and user experience.</p> <p>A fun aspect of the work was the request to move the store from a \"normal\" store to an astronomy store to match the OpenTelemetry project\u2019s overall branding.</p> <p>Once we got the green light from the rest of the OTel demo SIG, then we started working on the different changes that were part of the front-end architecture overhaul.</p>"},{"location":"blog/2022/frontend-overhaul/#opentelemetry-demo-application-description-and-tech-stack","title":"OpenTelemetry Demo Application Description and Tech Stack","text":"<p>The demo app is an astronomy store, with basic eCommerce features such as a shopping cart, currency selector, payment, and checkout. It also includes the ability to display promotions (ads) and related products depending on the user\u2019s context.</p> <p>The demo\u2019s stack includes multiple microservices in different languages, covering each of the following OTel-supported languages:</p> <ul> <li>C++</li> <li>.NET</li> <li>Erlang/Elixir</li> <li>Go</li> <li>Node.js</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> <li>Rust</li> </ul> <p>Every microservice has a specific goal and can communicate with others by using a global gRPC definition. Persistent information is saved into a PostgreSQL database and there are outbound services that connect with third-party services to trigger events (such as confirmation emails). All of the microservices, including the front-end, are connected to the same OpenTelemetry collector instance, which uses Jaeger as one of the data stores for the tracing data.</p> <p> </p> <p>Prior to re-architecting, the front-end consisted of a Golang SSR app, which sent complete HTML to the browser for display. Every request and call redirected to the server so new information was shown.</p>"},{"location":"blog/2022/frontend-overhaul/#web-app-styling-improvements-theme-updates-and-user-experience-redesign","title":"Web App Styling Improvements, Theme Updates, and User Experience Redesign","text":"<p>Before starting the development process, the front-end application wasn\u2019t matching the theme that OpenTelemetry had been using in terms of colors, products, and overall user experience. In addition, the demo lacked a real front-end (browser side) application as the current implementation was a Go application.</p> <p></p> <p>The first task at hand was to bring the demo to the modern age by updating the design, color schemes, and user experience. Olly Babiak walked into the fray to help us achieve this by creating a modernized version of the application. It included an improved way to display the products landing page, an updated product details page, a mini cart, and a fully compatible mobile version of the application.</p> <p></p> <p>Now we had an application design that would match the rest of the OpenTelemetry themes and colors and look more like the OpenTelemetry.io website.</p>"},{"location":"blog/2022/frontend-overhaul/#front-end-application-architecture-overhaul","title":"Front-end Application Architecture Overhaul","text":"<p>We worked on an initial proposal that included the following:</p> <ul> <li>Framework and tooling (Scaffolding, I/O, styling, UI library)</li> <li>Code Architecture and structure (Directories, coding patterns)</li> <li>Instrumentation</li> <li>Deployment &amp; Distribution</li> <li>Testing (E2E, unit test)</li> </ul> <p>This proposal was presented to the OpenTelemetry demo SIG during one of the weekly Monday meetings and we were given the green light to move ahead. As part of the changes, we decided to use Next.js to not only work as the primary front-end application but also to work as an aggregation layer between the front-end and the gRPC back-end services.</p> <p></p> <p>As you can see in the diagram, the application has two major connectivity points, one coming from the browser side (REST) to connect to the Next.js aggregation layer and the other from the aggregation layer to the back-end services (gRPC).</p>"},{"location":"blog/2022/frontend-overhaul/#opentelemetry-instrumentation","title":"OpenTelemetry Instrumentation","text":"<p>The next big thing we worked was a way to instrument both sides of the Next.js app. To do this we had to connect the app twice to the same collector used by all the microservices.</p> <p>A simple back-end solution was designed using the official gRPC exporter in combination with the Node.js SDK.</p> <p>You can find the full implementation here. The basic instrumentation includes auto instrumentation for most of the commonly used libraries and tools for Node.js. As part of providing a better example for users, a manual instrumentation in the form of route middleware was added. This would catch the incoming HTTP request and create a span based on it, including the context propagation. The implementation can be found here.</p> <p>The front-end was a little trickier, as the initial rendering is server-side. We had to make sure to load the tracer from the browser side when the JavaScript code is executed.</p> <p>After adding validations to check the browser side, we then loaded the custom front-end tracing module, which included creating the web tracer provider and the automatic web instrumentations.</p> <p>The automatic front-end instrumentation captures the most common user actions such as clicks, fetch requests, and page loads. In order to allow the browser side to interact with the collector, a config change is needed: enable incoming CORS requests from the web app.</p> <p>Once the setup is complete, by loading the application from Docker and interacting with the different features, we can start looking at the full traces that begin from the front-end user events all the way to the back-end gRPC services.</p> <p></p>"},{"location":"blog/2022/frontend-overhaul/#contributing-to-opentelemetry-was-rewarding","title":"Contributing to OpenTelemetry was Rewarding","text":"<p>As a team focused on building an open source tool in the Observability space, the opportunity to help the overall OpenTelemetry community was important to us. In addition, having a complex microservice-based application that uses multiple different languages and technologies is directly useful for our team. We really enjoyed the process of making a contribution to the OpenTelemetry project and are actively looking for more opportunities to contribute!</p> <p>Oscar Reyes and Olly Babiak also are working on Tracetest, an open source tool that allows you to develop and test your distributed system with OpenTelemetry. It works with any OTel compatible system and enables trace\u2013based tests to be created. Check it out at https://github.com/kubeshop/tracetest.</p>"},{"location":"blog/2022/go-web-app-instrumentation/","title":"Go Web-app Instrumentation","text":"<p>In this blog post, you will learn hands-on how to create and visualize traces with OpenTelemetry Go without prior knowledge.</p> <p>We will start with creating a simple to-do app that uses Mongo and the Gin framework. Then, we will send tracing data to Jaeger Tracing for visualization. You can find all the relevant files in this Github repository.</p> <p></p>"},{"location":"blog/2022/go-web-app-instrumentation/#hello-world-opentelemetry-go-example","title":"Hello world: OpenTelemetry Go example","text":"<p>We will start by creating our to-do service and installing two libraries (Gin and Mongo) to understand how instrumentations work.</p>"},{"location":"blog/2022/go-web-app-instrumentation/#step-1-create-maingo-file-for-our-to-do-app","title":"Step 1: Create main.go file for our to-do app","text":"<ol> <li>Install Gin and Mongo-driver</li> </ol> <pre><code>go get -u github.com/gin-gonic/gin\ngo get go.mongodb.org/mongo-driver/mongo\n</code></pre> <ol> <li> <p>Set up gin and mongo to listen on \u201c/todo\u201d</p> </li> <li> <p>Create some to-do\u2019s to seed Mongo</p> </li> </ol> <pre><code>package main\nimport (\n\"context\"\n\"net/http\"\n\"github.com/gin-gonic/gin\"\n\"go.mongodb.org/mongo-driver/bson\"\n\"go.mongodb.org/mongo-driver/mongo\"\n\"go.mongodb.org/mongo-driver/mongo/options\"\n)\nvar client * mongo.Client\nfunc main() {\nconnectMongo()\nsetupWebServer()\n}\nfunc connectMongo() {\nopts: = options.Client()\nopts.ApplyURI(\"mongodb://localhost:27017\")\nclient, _ = mongo.Connect(context.Background(), opts)\n//Seed the database with todo's\ndocs: = [] interface {} {\nbson.D {\n{\n\"id\", \"1\"\n}, {\n\"title\", \"Buy groceries\"\n}\n},\nbson.D {\n{\n\"id\", \"2\"\n}, {\n\"title\", \"install Aspecto.io\"\n}\n},\nbson.D {\n{\n\"id\", \"3\"\n}, {\n\"title\", \"Buy dogz.io domain\"\n}\n},\n}\nclient.Database(\"todo\").Collection(\"todos\").InsertMany(context.Background(), docs)\n}\nfunc setupWebServer() {\nr: = gin.Default()\nr.GET(\"/todo\", func(c * gin.Context) {\ncollection: = client.Database(\"todo\").Collection(\"todos\")\n//Important: Make sure to pass c.Request.Context() as the context and not c itself - TBD\ncur, findErr: = collection.Find(c.Request.Context(), bson.D {})\nif findErr != nil {\nc.AbortWithError(500, findErr)\nreturn\n}\nresults: = make([] interface {}, 0)\ncurErr: = cur.All(c, &amp; results)\nif curErr != nil {\nc.AbortWithError(500, curErr)\nreturn\n}\nc.JSON(http.StatusOK, results)\n})\n_ = r.Run(\":8080\")\n}\n</code></pre> <p>Now that our small todo app is ready, let\u2019s introduce OpenTelemetry.</p>"},{"location":"blog/2022/go-web-app-instrumentation/#step-2-install-opentelemetry-go","title":"Step 2: Install OpenTelemetry Go","text":"<p>We will be configuring OpenTelemetry to instrument our Go app.</p> <ol> <li>To install the OTel SDK, run:</li> </ol> <pre><code>go get go.opentelemetry.io/otel /\ngo.opentelemetry.io/otel/sdk /\n</code></pre> <ol> <li> <p>Instrument our Gin and Mongo libraries to generate traces.</p> </li> <li> <p>Gin &amp; Mongo instrumentation: Install otelgin &amp; otelmongo</p> </li> </ol> <pre><code>go get go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin /\ngo get go.opentelemetry.io/contrib/instrumentation/go.mongodb.org/mongo-driver/mongo/otelmongo\n</code></pre>"},{"location":"blog/2022/go-web-app-instrumentation/#gin-instrumentation-gincontext","title":"Gin instrumentation: gin.Context","text":"<p>We previously discussed the idea of context propagation \u2013 the way to transfer metadata between distributed services to correlate events in our system.</p> <p>The Gin framework has its own type gin.Context which gets passed as a parameter to an HTTP handler. However, the context that should be passed down to the mongo operations is the standard Go library Context object, available in gin.Context.Request.Context.</p> <pre><code>//Make sure to pass c.Request.Context() as the context and not c itself\ncur, findErr := collection.Find(c.Request.Context(), bson.D{})\n</code></pre> <p>So make sure that you pass the Context to the mongodb operation. Check out this issue for more info.</p> <p>We now have our todo app ready and instrumented. It\u2019s time to utilize OpenTelemetry to its full potential. Our ability to visualize traces is where the true troubleshooting power of this technology comes into play.</p> <p>For visualization, we\u2019ll be using the open source Jaeger Tracing.</p>"},{"location":"blog/2022/go-web-app-instrumentation/#visualization-with-jaeger","title":"Visualization with Jaeger","text":""},{"location":"blog/2022/go-web-app-instrumentation/#opentelemetry-go-and-jaeger-tracing-export-traces-to-jaeger","title":"OpenTelemetry Go and Jaeger Tracing: Export traces to Jaeger","text":"<p>Jaeger Tracing is a suite of open source projects managing the entire distributed tracing \u201cstack\u201d: client, collector, and UI. Jaeger UI is the most commonly used open source to visualize traces.</p> <p>Here\u2019s what the setup looks like:</p> <ol> <li>Install the Jaeger exporter</li> </ol> <pre><code>go get go.opentelemetry.io/otel/exporters/jaeger\n</code></pre> <ol> <li> <p>Create a tracing folder and a jaeger.go file</p> </li> <li> <p>Add the following code to the file</p> </li> </ol> <pre><code>package tracing\nimport (\n\"go.opentelemetry.io/otel/exporters/jaeger\"\n\"go.opentelemetry.io/otel/sdk/resource\"\nsdktrace \"go.opentelemetry.io/otel/sdk/trace\"\nsemconv \"go.opentelemetry.io/otel/semconv/v1.4.0\"\n)\nfunc JaegerTraceProvider()(*sdktrace.TracerProvider, error) {\nexp, err: = jaeger.New(jaeger.WithCollectorEndpoint(jaeger.WithEndpoint(\"http://localhost:14268/api/traces\")))\nif err != nil {\nreturn nil, err\n}\ntp: = sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(exp),\nsdktrace.WithResource(resource.NewWithAttributes(\nsemconv.SchemaURL,\nsemconv.ServiceNameKey.String(\"todo-service\"),\nsemconv.DeploymentEnvironmentKey.String(\"production\"),\n)),\n)\nreturn tp, nil\n}\n</code></pre> <ol> <li>Go back to the main.go file and modify our code to use the    JaegerTraceProvider function we just created</li> </ol> <pre><code>func main() {\ntp, tpErr: = tracing.JaegerTraceProvider()\nif tpErr != nil {\nlog.Fatal(tpErr)\n}\notel.SetTracerProvider(tp)\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext {}, propagation.Baggage {}))\nconnectMongo()\nsetupWebServer()\n}\n</code></pre> <p>Next, we are going to hook up the instrumentations we installed.</p> <ol> <li>Add the Mongo instrumentation. In our connectMongo function by adding this    line</li> </ol> <pre><code>opts.Monitor = otelmongo.NewMonitor()\n</code></pre> <p>The function should look like this:</p> <pre><code>func connectMongo() {\nopts: = options.Client()\n//Mongo OpenTelemetry instrumentation\nopts.Monitor = otelmongo.NewMonitor()\nopts.ApplyURI(\"mongodb://localhost:27017\")\nclient, _ = mongo.Connect(context.Background(), opts)\n//Seed the database with some todo's\ndocs: = [] interface {} {\nbson.D {\n{\n\"id\", \"1\"\n}, {\n\"title\", \"Buy groceries\"\n}\n},\nbson.D {\n{\n\"id\", \"2\"\n}, {\n\"title\", \"install Aspecto.io\"\n}\n},\nbson.D {\n{\n\"id\", \"3\"\n}, {\n\"title\", \"Buy dogz.io domain\"\n}\n},\n}\nclient.Database(\"todo\").Collection(\"todos\").InsertMany(context.Background(), docs)\n}\n</code></pre> <p>Now, add the Gin instrumentation.</p> <ol> <li>Go to the startWebServer function and add this line right after we create the    gin instance</li> </ol> <pre><code>r.Use(otelgin.Middleware(\"todo-service\"))\n</code></pre> <p>The function should look like this</p> <pre><code>func startWebServer() {\nr: = gin.Default()\n//Gin OpenTelemetry instrumentation\nr.Use(otelgin.Middleware(\"todo-service\"))\nr.GET(\"/todo\", func(c * gin.Context) {\ncollection: = client.Database(\"todo\").Collection(\"todos\")\n//make sure to pass c.Request.Context() as the context and not c itself\ncur, findErr: = collection.Find(c.Request.Context(), bson.D {})\nif findErr != nil {\nc.AbortWithError(500, findErr)\nreturn\n}\nresults: = make([] interface {}, 0)\ncurErr: = cur.All(c, &amp; results)\nif curErr != nil {\nc.AbortWithError(500, curErr)\nreturn\n}\nc.JSON(http.StatusOK, results)\n})\n_ = r.Run(\":8080\")\n}\n</code></pre> <p>For the complete <code>main.go</code> file, see below. Now we\u2019re finally ready to export    to Jaeger.</p> <pre><code>package main\nimport (\n\"context\"\n\"log\"\n\"net/http\"\n\"github.com/aspecto-io/opentelemetry-examples/tracing\"\n\"github.com/gin-gonic/gin\"\n\"go.mongodb.org/mongo-driver/bson\"\n\"go.mongodb.org/mongo-driver/mongo\"\n\"go.mongodb.org/mongo-driver/mongo/options\"\n\"go.opentelemetry.io/contrib/instrumentation/github.com/gin-gonic/gin/otelgin\"\n\"go.opentelemetry.io/contrib/instrumentation/go.mongodb.org/mongo-driver/mongo/otelmongo\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/propagation\"\n)\nvar client * mongo.Client\nfunc main() {\n//Export traces to Jaeger\ntp, tpErr: = tracing.JaegerTraceProvider()\nif tpErr != nil {\nlog.Fatal(tpErr)\n}\notel.SetTracerProvider(tp)\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext {}, propagation.Baggage {}))\nconnectMongo()\nstartWebServer()\n}\nfunc connectMongo() {\nopts: = options.Client()\n//Mongo OpenTelemetry instrumentation\nopts.Monitor = otelmongo.NewMonitor()\nopts.ApplyURI(\"mongodb://localhost:27017\")\nclient, _ = mongo.Connect(context.Background(), opts)\n//Seed the database with some todo's\ndocs: = [] interface {} {\nbson.D {\n{\n\"id\", \"1\"\n}, {\n\"title\", \"Buy groceries\"\n}\n},\nbson.D {\n{\n\"id\", \"2\"\n}, {\n\"title\", \"install Aspecto.io\"\n}\n},\nbson.D {\n{\n\"id\", \"3\"\n}, {\n\"title\", \"Buy dogz.io domain\"\n}\n},\n}\nclient.Database(\"todo\").Collection(\"todos\").InsertMany(context.Background(), docs)\n}\nfunc startWebServer() {\nr: = gin.Default()\n//gin OpenTelemetry instrumentation\nr.Use(otelgin.Middleware(\"todo-service\"))\nr.GET(\"/todo\", func(c * gin.Context) {\ncollection: = client.Database(\"todo\").Collection(\"todos\")\n//Make sure to pass c.Request.Context() as the context and not c itself\ncur, findErr: = collection.Find(c.Request.Context(), bson.D {})\nif findErr != nil {\nc.AbortWithError(500, findErr)\nreturn\n}\nresults: = make([] interface {}, 0)\ncurErr: = cur.All(c, &amp; results)\nif curErr != nil {\nc.AbortWithError(500, curErr)\nreturn\n}\nc.JSON(http.StatusOK, results)\n})\n_ = r.Run(\":8080\")\n}\n</code></pre>"},{"location":"blog/2022/go-web-app-instrumentation/#export-traces-to-jaeger","title":"Export traces to Jaeger","text":"<ol> <li>Run the todo-service with <code>go run main.go</code>.</li> <li>To generate some traces, make an HTTP GET request to    http://localhost:8080/todo.</li> <li>To view the traces, open Jaeger at http://localhost:16686/search.</li> </ol> <p>You can now see the Jaeger UI. Select todo-service and click on Find traces. You should see your trace on the right:</p> <p></p> <p>Jaeger UI displays OpenTelemetry traces in go for our todo-service By clicking the trace, you can drill down and see more details about it that allow you to further investigate on your own:</p> <p></p>"},{"location":"blog/2022/go-web-app-instrumentation/#summary","title":"Summary","text":"<p>That\u2019s all folks! We hope this guide was informative and easy to follow. You can find all files ready to use in our Github repository.</p> <p>A version of this article was [originally posted][] on the Aspecto blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/instrument-apache-httpd-server/","title":"Learn how to instrument Apache Http Server with OpenTelemetry","text":"<p>If you are using Apache HTTP Server and in dire need of some observability tool to monitor your web server, the OpenTelemetry Module for Apache HTTP Server is the right candidate for you: it enables tracing of incoming requests to the server and it will capture the response time of many modules (including <code>mod_proxy</code>) involved in such an incoming request. With that you will get hierarchical time consumption by each module. This article demonstrates the monitoring capabilities of the OpenTelemetry Module for Apache HTTP Server and quick guide to get started with the module.</p>"},{"location":"blog/2022/instrument-apache-httpd-server/#getting-started-with-opentelemetry-module","title":"Getting Started with OpenTelemetry Module","text":""},{"location":"blog/2022/instrument-apache-httpd-server/#building-the-module","title":"Building the module","text":"<p>Getting started with the OpenTelemetry module for apache httpd is pretty simple, all you need is a docker engine and git. Download the source code from github and then build the docker image on CentOS71:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-cpp-contrib\ncd  instrumentation/otel-webserver-module\ndocker compose --profile centos7 build\n</code></pre> <p>These commands download all required dependencies, builds the OpenTelemetry module for Apache HTTP Server and installs the same on the docker image.</p> <p>Note: The above commands might take around 1 hour to complete.</p> <p>When the build is finished, run the docker image, by typing the following command1:</p> <pre><code>docker compose --profile centos7 up -d\n</code></pre> <p>The above command starts up the centos7 image in a docker container named <code>webserver_centos7</code> along with the OpenTelemetry Collector and a Zipkin backend.</p> <p>OpenTelemetry Module for Apache HTTP Server will be configured and installed in the desired location and Apache HTTP Server will be started with the OpenTelemetry Module.</p>"},{"location":"blog/2022/instrument-apache-httpd-server/#viewing-spans-on-the-backend","title":"Viewing spans on the backend","text":"<p>As mentioned in docker-compose.yml, <code>webserver_centos7</code> listens on port 9004, Zipkin listens on port 9411 and the OpenTelemetry Collector listens on port 4317.</p> <p>To send a request to Apache HTTP Server you can either use curl from terminal (<code>curl localhost:9004</code>), or visit localhost:9004 in any browser. A default landing page saying \"Testing 123...\" for Apache HTTP Server on Centos will be displayed as below:</p> <p></p> <p>Now, traces and spans can be seen on the Zipkin backend. To view them, visit localhost:9411 in your browser and click on Run Query button. Following is the screenshot from Zipkin UI showing spans emitted by the Apache HTTP Server.</p> <p></p> <p>This shows a list of queries or endpoints that have been triggered to Apache HTTP Server, such as <code>/noindex/css</code>.</p> <p>To see the details click on any of the SHOW buttons. Below is the screenshot from the Zipkin UI showing the span hierarchy.</p> <p></p> <p>The above shows that as a part of this request, <code>mod_proxy</code>, <code>mod_proxy_balancer</code> and <code>mod_dav</code> got involved in the request processing and time consumed in each of the modules.</p>"},{"location":"blog/2022/instrument-apache-httpd-server/#how-can-module-level-details-be-beneficial","title":"How can module level details be beneficial?","text":"<p>To demonstrate the benefits of module level details, we'll introduce an artificial delay in a PHP script and see how the delay gets displayed in the Zipkin backend. The following steps are required to be done.</p> <ul> <li>Login to the container and install the PHP module.</li> </ul> <pre><code>docker exec -it webserver_centos7 /bin/bash\nyum install php -y\n</code></pre> <ul> <li>Add <code>AddType application/x-httpd-php .html</code> in <code>/etc/httpd/conf/httpd.conf</code> as   mentioned below:</li> </ul> <p></p> <ul> <li>Create a file named as <code>index.html</code> in the /var/www/html directory and add   the following text</li> </ul> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;PHP Test Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;?php\n      echo date('h:i:s') . \"&lt;br&gt;\"; echo \"Introduce delay of 1 seconds\" . \"&lt;br /&gt;\";\n    sleep(1); echo date('h:i:s'); ?&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <ul> <li>Restart the server:</li> </ul> <pre><code>httpd -k restart\n</code></pre> <ul> <li>Now, visit localhost:9004/index.html. You should see something like this:</li> </ul> <p></p> <ul> <li>Now, traces and spans can be seen on the Zipkin backend. To view them, type   localhost:9411 on the browser and click on the Run Query Button. To   see the details, click on the SHOW button corresponding to <code>/index.html</code>.</li> </ul> <p></p> <ul> <li>We can see that, <code>mod_php5.c_handler</code> consumes around 1 second which   contributes to the overall time-consumption of the request.</li> </ul> <p>As the HTTP request flows through individual modules, delay in execution or errors might occur at any of the modules involved in the request. To identify the root cause of any delay or errors in request processing, module wise information (such as response time of individual modules) would enhance the debuggability of the Apache HTTP Server.</p>"},{"location":"blog/2022/instrument-apache-httpd-server/#installing-opentelemetry-module-in-target-system","title":"Installing OpenTelemetry Module in Target System","text":"<p>To make use of the OpenTelemetry module for Apache HTTP Server, use the following steps to extract the package and install on the target system where Apache HTTP Server is installed.</p> <ul> <li>In order to clone the source code, execute the following</li> </ul> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-cpp-contrib\ncd  opentelemetry-cpp-contrib/instrumentation/otel-webserver-module\n</code></pre> <ul> <li>Trigger the build command to generate the package inside the docker image1</li> </ul> <pre><code>docker compose --profile centos7 build\n</code></pre> <p>The above might take around an hour to build. This would build on Centos 7 image as <code>apache_centos7</code></p> <ul> <li>Once the build is complete, it's time to extract the image. We need to startup   the container which can be done by the following command</li> </ul> <pre><code>docker run -idt --name &lt;container_name&gt; apache_centos7 /bin/bash\n</code></pre> <p>The above command would run the container and can be verified using the <code>docker ps</code> command.</p> <ul> <li>The generated package inside the container is available inside   <code>/otel-webserver-module/build</code> directory. The same can be extracted to the   host system as</li> </ul> <pre><code>docker cp &lt;container_name&gt;:/otel-webserver-module/build/opentelemetry-webserver-sdk-x64-linux.tgz &lt;target-directory&gt;\n</code></pre> <p>Note: The above package should work on any linux distribution having x86-64 instruction set and glibc version greater than 2.17. At the point of writing this blog, support for other architectures is not provided.</p> <ul> <li> <p>Transfer the above package along with opentelemetry_module.conf to the   target system.</p> </li> <li> <p>Uncompress the package <code>opentelemetry-webserver-sdk-x64-linux.tgz</code> to <code>/opt</code>   directory.</p> </li> </ul> <pre><code>tar -xvf opentelemetry-webserver-sdk-x64-linux.tgz -C /opt\n</code></pre> <ul> <li>Now, install the module by executing the following</li> </ul> <pre><code>cd /opt/opentelemetry-webserver-sdk\n./install.sh\n</code></pre> <ul> <li> <p>In the case of Centos, Apache HTTP Server configuration is generally located   in <code>/etc/httpd/conf/</code>. Hence copy the opentelemetry_module.conf to   <code>/etc/httpd/conf</code>.</p> </li> <li> <p>Edit the <code>/etc/httpd/conf/httpd.conf</code> and add   <code>Include conf/opentelemetry_module.conf</code> at the end of the file as mentioned   below:</p> </li> </ul> <p></p> <ul> <li> <p>Now let\u2019s look at opentelemetry_module.conf and its contents:</p> </li> <li> <p>The below LoadFile are the dependent libraries that come with the package.</p> <p></p> </li> <li> <p>The below configuration are for the OpenTelemetry Module</p> <p></p> <p>In the case of Apache HTTP Server 2.2, <code>libmod_apache_otel22.so</code> needs to be used instead of <code>libmod_apache_otel.so</code></p> </li> <li> <p>The following directive should be ON for the OpenTelemetry module to be     enabled, else it would be disabled.</p> <p></p> </li> <li> <p>Since the module works with the Collector and sends data in OLTP format, the     following directives are necessary.</p> <p></p> <p>ApacheModuleOtelExporterEndpoint should point to the endpoint of the collector</p> </li> <li> <p>ServiceNamespace, ServiceName and ServiceInstanceId should be provided by     the following directives.</p> <p></p> </li> <li> <p>All other directives are either optional and can be kept as it is for this     guide</p> </li> <li> <p>To verify whether the OpenTelemetry Module is properly enabled into Apache   HTTP Server, type <code>httpd -M</code> and look for <code>otel_apache_module (shared)</code></p> </li> </ul> <p></p> <ul> <li>Now, restart the Apache HTTP Server and OpenTelemetry module should be   instrumented.</li> </ul> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2022/instrument-kafka-clients/","title":"Instrumenting Apache Kafka clients with OpenTelemetry","text":"<p>Nowadays, Apache Kafka is chosen as the nervous system in a distributed environment. Different services communicate with each other by using Apache Kafka as a messaging system but even more as en event or data streaming platform.</p> <p>Taking into account the cloud-native approach for developing microservices, quite often Kubernetes is also used to run the workloads. In this scenario, you can also easily deploy and manage an Apache Kafka cluster on top of it, by using a project like Strimzi. It takes care of the overall Kafka infrastructure, while you can focus on developing applications which use it.</p> <p>Within the overall picture, because of the distributed nature, it is quite difficult to track how messages are moved around. This is where OpenTelemetry comes into the picture. It provides multiple instrumentation libraries for adding tracing to messaging based applications. Of course, there is one for Apache Kafka clients. It also defines the specification of semantic conventions for messaging systems.</p> <p>But usually, the architecture can even be more complicated: having applications not able to connect directly to the Apache Kafka cluster and talking its own custom protocol but using a different one, like for example HTTP. In this case, tracing how messages are produced and consumed via HTTP through Apache Kafka is really complex. The Strimzi project provides a bridge, with the OpenTelemetry support, for adding tracing data by using the corresponding instrumentation library.</p> <p>In this article, you will learn how it is possible to enable tracing on Apache Kafka based client applications in different ways. We will refer to the Java based instrumentation. You can also find all the examples in this repository.</p>"},{"location":"blog/2022/instrument-kafka-clients/#enable-tracing-on-the-kafka-clients","title":"Enable tracing on the Kafka clients","text":"<p>Let's assume you have an application using the Kafka clients API for producing and consuming messages. In order to simplify the scenario, let's also assume you don't want to add any additional tracing information within your business logic. You are interested to add tracing to the Kafka related parts only. You want to trace how the messages are produced and consumed via the Kafka clients.</p> <p>In order to do so, there are two different ways:</p> <ul> <li>using an external agent running alongside your application to add tracing.</li> <li>enabling the tracing directly on the Kafka clients used by your application.</li> </ul> <p>The former is actually an \"automatic\" approach which is about not touching your application at all. The agent, running alongside the application, is able to intercept messages coming in and out and adds tracing information to them.</p> <p>The latter is mostly a \"manual\" approach which is about instrumenting your application directly. It means adding some specific dependencies to your project and make code changes.</p>"},{"location":"blog/2022/instrument-kafka-clients/#instrumenting-by-using-the-agent","title":"Instrumenting by using the agent","text":"<p>The simpler and automatic approach is by adding tracing to your application with no changes or additions into your application code. You also don't need to add any dependencies to OpenTelemetry specific libraries. It is possible by using the OpenTelemetry agent you can download from here. This agent has to run alongside your application in order to inject the logic for tracing messages sent and received to/from a Kafka cluster.</p> <p>Run the producer application in the following way.</p> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar \\\n-Dotel.service.name=my-kafka-service \\\n-Dotel.traces.exporter=jaeger \\\n-Dotel.metrics.exporter=none \\\n-jar kafka-producer-agent/target/kafka-producer-agent-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre> <p>Run the consumer application similarly.</p> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar \\\n-Dotel.service.name=my-kafka-service \\\n-Dotel.traces.exporter=jaeger \\\n-Dotel.metrics.exporter=none \\\n-Dotel.instrumentation.messaging.experimental.receive-telemetry.enabled=true \\\n-jar kafka-consumer-agent/target/kafka-consumer-agent-1.0-SNAPSHOT-jar-with-dependencies.jar\n</code></pre> <p>The agent leverages the auto-configuration SDK extension, you will see in a bit, by setting the main parameters through system properties.</p>"},{"location":"blog/2022/instrument-kafka-clients/#instrumenting-the-apache-kafka-clients","title":"Instrumenting the Apache Kafka clients","text":"<p>The OpenTelemetry project provides the <code>opentelemetry-kafka-clients-2.6</code> module which provides tracing instrumentation for Kafka clients. First, you need to add the corresponding dependency to your application.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-kafka-clients-2.6&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Depending on the exporter that you want to use for exporting tracing information, you have to add the corresponding dependency as well. For example, in order to use the Jaeger exporter, the dependency is the following one.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-exporter-jaeger&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>In this way, you have the minimum set up for enabling the tracing on your Kafka based application.</p>"},{"location":"blog/2022/instrument-kafka-clients/#setting-up-the-opentelemetry-instance","title":"Setting up the OpenTelemetry instance","text":"<p>The entire tracing instrumentation is handled by an <code>OpenTelemetry</code> instance in your code. It needs to be created and registered globally in order to be made available to the Kafka clients instrumentation library.</p> <p>This can be done in two different ways:</p> <ul> <li>using an SDK extension for environment-based auto-configuration.</li> <li>using SDK builders classes for programmatic configuration.</li> </ul>"},{"location":"blog/2022/instrument-kafka-clients/#using-the-sdk-auto-configuration","title":"Using the SDK auto-configuration","text":"<p>It is possible to configure a global <code>OpenTelemetry</code> instance by using environment variables thanks to the SDK extension for auto-configuration, enabled by adding the following dependency to your application.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-sdk-extension-autoconfigure&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>When the Kafka clients instrumentation library is used, it checks if there is an <code>OpenTelemetry</code> instance already created and registered. If it is not, the library code checks if the SDK auto-configure module is in the classpath and in this case initializes it for creating the <code>OpenTelemetry</code> instance automatically. The corresponding configuration happens through environment variables (or corresponding system properties). This is really a way to simplify the initialization of tracing.</p> <p>The main environment variables to be set are the following:</p> <ul> <li><code>OTEL_SERVICE_NAME</code>: specify the logical service name. This is really useful   when using a tracing UI (i.e. Jaeger UI) for showing data and it is   recommended to set it.</li> <li><code>OTEL_TRACES_EXPORTER</code>: the list of exporters to be used for tracing. For   example, by using <code>jaeger</code> you also need to have the corresponding dependency   in the application.</li> </ul> <p>Instead of using the above environment variables, it is also possible to use corresponding system properties to be set programmatically in the code or on the command line when launching the application They are <code>otel.service.name</code> and <code>otel.traces.exporter</code>.</p>"},{"location":"blog/2022/instrument-kafka-clients/#using-the-sdk-builders","title":"Using the SDK builders","text":"<p>In order to build your own <code>OpenTelemetry</code> instance and not relying on the auto-configuration, it is possible to do so by using the SDK builders classes programmatically. The OpenTelemetry SDK dependency is needed in order to have such SDK builders available in your code.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-sdk&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The following code snippet sets the main attributes like the service name, then it configures the Jaeger exporter. Finally, it creates the <code>OpenTelemetry</code> instance and registers it globally so that it can be used by the Kafka clients instrumentation library.</p> <pre><code>Resource resource = Resource.getDefault()\n.merge(Resource.create(Attributes.of(ResourceAttributes.SERVICE_NAME, \"my-kafka-service\")));\nSdkTracerProvider sdkTracerProvider = SdkTracerProvider.builder()\n.addSpanProcessor(BatchSpanProcessor.builder(JaegerGrpcSpanExporter.builder().build()).build())\n.setSampler(Sampler.alwaysOn())\n.setResource(resource)\n.build();\nOpenTelemetry openTelemetry = OpenTelemetrySdk.builder()\n.setTracerProvider(sdkTracerProvider)\n.setPropagators(ContextPropagators.create(W3CTraceContextPropagator.getInstance()))\n.buildAndRegisterGlobal();\n</code></pre>"},{"location":"blog/2022/instrument-kafka-clients/#using-the-interceptors","title":"Using the interceptors","text":"<p>The Kafka clients API provides a way to \"intercept\" messages before they are sent to the brokers as well as messages received from the broker before being passed to the application. This approach is heavily used when you need to add some logic or content to the messages right before they are sent. At same time, it is useful to handle consumed messages right before they are passed to the upper application layer. It fits pretty well with tracing when you need to create or close spans on sending and receiving messages.</p> <p>The Kafka clients instrumentation library provides two interceptors to be configured to add tracing information automatically. The interceptor classes have to be set in the properties bag used to create the Kafka client within the application.</p> <p>Use the <code>TracingProducerInterceptor</code> for the producer in order to create a \"send\" span automatically, each time a message is sent.</p> <pre><code>props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, TracingProducerInterceptor.class.getName());\n</code></pre> <p>Use the <code>TracingConsumerInterceptor</code> for the consumer in order to create a \"receive\" span automatically, each time a message is received.</p> <pre><code>props.setProperty(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, TracingConsumerInterceptor.class.getName());\n</code></pre>"},{"location":"blog/2022/instrument-kafka-clients/#wrapping-the-clients","title":"Wrapping the clients","text":"<p>The other way is by wrapping the Kafka client with a tracing enabled one.</p> <p>On the producer side, assuming you have a <code>Producer&lt;K, V&gt;</code> instance, you can wrap it in the following way.</p> <pre><code>KafkaTelemetry telemetry = KafkaTelemetry.create(GlobalOpenTelemetry.get());\nProducer&lt;String, String&gt; tracingProducer = telemetry.wrap(producer);\n</code></pre> <p>Then use the <code>tracingProducer</code> as usual for sending messages to the Kafka cluster.</p> <p>On the consumer side, assuming you have a <code>Consumer&lt;K, V&gt;</code> instance, you can wrap it in the following way.</p> <pre><code>KafkaTelemetry telemetry = KafkaTelemetry.create(GlobalOpenTelemetry.get());\nConsumer&lt;String, String&gt; tracingConsumer = telemetry.wrap(this.consumer);\n</code></pre> <p>Then use the <code>tracingConsumer</code> as usual for receiving messages from the Kafka cluster.</p>"},{"location":"blog/2022/instrument-kafka-clients/#instrumentation-in-action","title":"Instrumentation in action","text":"<p>In order to practice with the instrumentation of Kafka clients by using the provided examples, first of all you need an Apache Kafka cluster. The easiest way is to download it from the official website and running just one ZooKeeper node together with one Kafka broker. You can follow the quick start to have such a cluster up and running in a few minutes. Analyzing tracing information is also simpler if using a Web UI, for example the one provided by Jaeger. Even in this case downloading it from the official website and running it locally is really simple.</p> <p>When the environment is ready, the first try is about running the producer and consumer applications instrumented by using interceptors or wrappers. Just sending one message and consuming it provides the following tracing.</p> <p></p> <p>As you can see the \"send\" and \"receive\" spans are both in the same trace and the \"receive\" span is defined as <code>CHILD_OF</code> the \"send\" one. You can also see that the semantic defines some specific messaging related tags on the spans with the <code>messaging.</code> prefix. This semantic is not actually right because the send operation doesn't depend on the receive (which is the meaning of <code>CHILD_OF</code> relationship). It is anyway going to change, per this GitHub discussion and the new messaging semantic conventions that is going to be stabilized via this new OTEP (OpenTelemetry Enhancement Proposal). The goal is to have the \"send\" and \"receive\" spans in two different traces but linked together with a <code>FOLLOW_FROM</code> relationship.</p> <p>This approach is more reflected when using the agent with the \"send\" span living in its own trace as following.</p> <p></p> <p>On the receiving side, there are also a \"receive\" and \"process\" spans referring to the producer one.</p> <p></p>"},{"location":"blog/2022/instrument-kafka-clients/#conclusion","title":"Conclusion","text":"<p>Apache Kafka is just one of the messaging platforms that can be used for microservices communication in a distributed system. Monitoring how the messages are exchanged and troubleshooting problems is really complex. This is where the OpenTelemetry project comes into the picture to put tracing in your hands. In this article, we saw how the Kafka clients instrumentation library makes really simple to add tracing information to your Kafka based applications. You can get more information about how producers and consumers are behaving and keeps track of each message from one end to the other. So ... what else? Give it a try!</p>"},{"location":"blog/2022/instrument-kafka-clients/#references","title":"References","text":"<ul> <li>Apache Kafka</li> <li>Strimzi</li> <li>Website</li> <li>GitHub</li> <li>Bridge</li> <li>Kubernetes</li> </ul>"},{"location":"blog/2022/instrument-nginx/","title":"Learn how to instrument NGINX with OpenTelemetry","text":"<p>Apache HTTP Server and NGINX are the most popular web servers. It's most likely that you are using one of them in your application. In a previous blog post, you learned how to use the OpenTelemetry Module for Apache HTTP Server to add observability to Apache HTTP Server. In this blog post, you will learn how you can get observability for NGINX!</p>"},{"location":"blog/2022/instrument-nginx/#install-the-module-for-nginx","title":"Install the module for NGINX","text":"<p>In the following, you are going to use docker to run a NGINX server with the <code>ngx_http_opentelemetry_module.so</code> enabled and configured. Of course, you can use the same set of commands used in the <code>Dockerfile</code> below to configure a NGINX server on a bare-metal machine.</p> <p>Start from an empty directory. Create a file called <code>Dockerfile</code> and copy the following content into it:</p> <pre><code>FROM nginx:1.18\nRUN apt-get update ; apt-get install unzip\nADD https://github.com/open-telemetry/opentelemetry-cpp-contrib/releases/download/webserver%2Fv1.0.0/opentelemetry-webserver-sdk-x64-linux.tgz.zip /opt\nRUN cd /opt ; unzip opentelemetry-webserver-sdk-x64-linux.tgz.zip; tar xvfz opentelemetry-webserver-sdk-x64-linux.tgz\nRUN cd /opt/opentelemetry-webserver-sdk; ./install.sh\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/opentelemetry-webserver-sdk/sdk_lib/lib\nRUN echo \"load_module /opt/opentelemetry-webserver-sdk/WebServerModule/Nginx/ngx_http_opentelemetry_module.so;\\n$(cat /etc/nginx/nginx.conf)\" &gt; /etc/nginx/nginx.conf\nCOPY opentelemetry_module.conf /etc/nginx/conf.d\n</code></pre> <p>What this <code>Dockerfile</code> does:</p> <ul> <li>Pull a base image with NGINX 1.18 pre-installed</li> <li>Install <code>unzip</code></li> <li>Download the opentelemetry-webserver-sdk-x64-linux package</li> <li>Unpack the package, put it into <code>/opt</code> &amp; run <code>./install.sh</code></li> <li>Add the dependencies at <code>/opt/opentelemetry-webserver-sdk/sdk_lib/lib</code> to the   library path (<code>LD_LIBRARY_PATH</code>)</li> <li>Tell NGINX to load the <code>ngx_http_opentelemetry_module.so</code></li> <li>Add the configuration of the modules to NGINX.</li> </ul> <p>Next, create another file called <code>opentelemetry_module.conf</code> and copy the following content into it:</p> <pre><code>NginxModuleEnabled ON;\nNginxModuleOtelSpanExporter otlp;\nNginxModuleOtelExporterEndpoint localhost:4317;\nNginxModuleServiceName DemoService;\nNginxModuleServiceNamespace DemoServiceNamespace;\nNginxModuleServiceInstanceId DemoInstanceId;\nNginxModuleResolveBackends ON;\nNginxModuleTraceAsError ON;\n</code></pre> <p>This will enable the OpenTelemetry and apply the following configuration:</p> <ul> <li>Send spans via OTLP to localhost:4317</li> <li>Set the attributes <code>service.name</code> to <code>DemoService</code>, <code>service.namespace</code> to   <code>DemoServiceNamespace</code> and the <code>service.instance_id</code> to <code>DemoInstanceId</code></li> <li>Report traces as errors, so you will see them in the NGINX log</li> </ul> <p>To learn all the settings available, see the full list of directives.</p> <p>With the <code>Dockerfile</code> and NGINX config in place, build your docker image and run the container:</p> <pre><code>$ docker build -t nginx-otel --platform linux/amd64 .\n$ docker run --platform linux/amd64 --rm -p 8080:80 nginx-otel\n...\n2022/08/12 09:26:42 [error] 69#69: mod_opentelemetry: ngx_http_opentelemetry_init_worker: Initializing Nginx Worker for process with PID: 69\n</code></pre> <p>With the container up and running, send requests to NGINX using, for example, <code>curl localhost:8080</code>.</p> <p>Since the configuration above has <code>NginxModuleTraceAsError</code> set to <code>ON</code> and you will see your traces dump to the error log of NGINX:</p> <pre><code>2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: startMonitoringRequest: Starting Request Monitoring for: / HTTP/1.1\nHost, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: startMonitoringRequest: WebServer Context: DemoServiceNamespaceDemoServiceDemoInstanceId, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: startMonitoringRequest: Request Monitoring begins successfully , client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_startInteraction: Starting a new module interaction for: ngx_http_realip_module, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Key : tracestate, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Value : , client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Key : baggage, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Value : , client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Key : traceparent, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_payload_decorator: Value : 00-987932d28550c0a1c0a82db380a075a8-fc0bf2248e93dc42-01, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_startInteraction: Interaction begin successful, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n2022/08/12 09:31:12 [error] 70#70: *3 mod_opentelemetry: otel_stopInteraction: Stopping the Interaction for: ngx_http_realip_module, client: 172.17.0.1, server: localhost, request: \"GET / HTTP/1.1\", host: \"localhost:8080\"\n</code></pre>"},{"location":"blog/2022/instrument-nginx/#viewing-spans-in-jaeger","title":"Viewing spans in Jaeger","text":"<p>At this point the telemetry data generated by NGINX is not send to an OpenTelemetry collector or any other observability backend. You can easily change that by creating a <code>docker-compose</code> file, that starts the NGINX server, the collector and Jaeger:</p> <p>Create a file called <code>docker-compose.yml</code> and add the following content:</p> <pre><code>version: '2'\nservices:\njaeger:\nimage: jaegertracing/all-in-one:latest\nports:\n- '16686:16686'\ncollector:\nimage: otel/opentelemetry-collector:latest\ncommand: ['--config=/etc/otel-collector-config.yaml']\nvolumes:\n- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\nnginx:\nimage: nginx-otel\nvolumes:\n- ./opentelemetry_module.conf:/etc/nginx/conf.d/opentelemetry_module.conf\nports:\n- 8080:80\n</code></pre> <p>Create a file called <code>otel-collector-config.yaml</code> containing the following:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nexporters:\njaeger:\nendpoint: jaeger:14250\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [jaeger]\n</code></pre> <p>Before spinning up the containers, update line 3 in <code>opentelemetry_module.conf</code> to have the right exporter endpoint:</p> <pre><code>NginxModuleEnabled ON; NginxModuleOtelSpanExporter otlp;\nNginxModuleOtelExporterEndpoint collector:4317;\n</code></pre> <p>You don't need to rebuild your docker image, because the <code>docker-compose.yaml</code> above loads the <code>opentelemetry_module.conf</code> as a file volume on container startup.</p> <p>Get everything up and running1:</p> <pre><code>$ docker compose up\n</code></pre> <p>In another shell, create some traffic:</p> <pre><code>$ curl localhost:8080\n</code></pre> <p>In your browser open localhost:16686 and search for traces from <code>DemoService</code> and drill into one of them.</p> <p></p> <p>You will see one span for each NGINX module being executed during the request. With that you can easily spot issues with certain modules, for example, a rewrite going mad.</p>"},{"location":"blog/2022/instrument-nginx/#put-nginx-between-two-services","title":"Put NGINX between two services","text":"<p>Of course, NGINX is rarely used as a standalone solution! Most of the time it is used as a reverse proxy or load balancer in front of another service. And, there might be a service calling NGINX to reach that down stream service.</p> <p>Add two more services to the running example:</p> <ul> <li>A Node.js service called <code>frontend</code> that sits at the front and calls the NGINX</li> <li>A java service called <code>backend</code> that sits behind the NGINX</li> </ul> <p>Update the <code>docker-compose</code> file to contain those 2 services and to overwrite the <code>default.conf</code> in NGINX:</p> <pre><code>version: '2'\nservices:\njaeger:\nimage: jaegertracing/all-in-one:latest\nports:\n- '16686:16686'\ncollector:\nimage: otel/opentelemetry-collector:latest\ncommand: ['--config=/etc/otel-collector-config.yaml']\nvolumes:\n- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\nnginx:\nimage: nginx-otel\nvolumes:\n- ./opentelemetry_module.conf:/etc/nginx/conf.d/opentelemetry_module.conf\n- ./default.conf:/etc/nginx/conf.d/default.conf\nbackend:\nbuild: ./backend\nimage: backend-with-otel\nenvironment:\n- OTEL_TRACES_EXPORTER=otlp\n- OTEL_METRICS_EXPORTER=none\n- OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/\n- OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n- OTEL_SERVICE_NAME=backend\nfrontend:\nbuild: ./frontend\nimage: frontend-with-otel\nports:\n- '8000:8000'\nenvironment:\n- OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/\n- OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\n- OTEL_SERVICE_NAME=frontend\n</code></pre> <p>Create the <code>default.conf</code> that will pass requests to NGINX down to the backend service:</p> <pre><code>server {\nlisten       80;\nlocation / {\nproxy_pass http://backend:8080;\n}\n}\n</code></pre> <p>Create two empty folders <code>backend</code> and <code>frontend</code>.</p> <p>In the frontend folder, create a simple Node.js app:</p> <pre><code>const opentelemetry = require('@opentelemetry/sdk-node');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst {\nOTLPTraceExporter,\n} = require('@opentelemetry/exporter-trace-otlp-http');\nconst sdk = new opentelemetry.NodeSDK({\ntraceExporter: new OTLPTraceExporter(),\ninstrumentations: [getNodeAutoInstrumentations()],\n});\nsdk.start().then(() =&gt; {\nconst express = require('express');\nconst http = require('http');\nconst app = express();\napp.get('/', (_, response) =&gt; {\nconst options = {\nhostname: 'nginx',\nport: 80,\npath: '/',\nmethod: 'GET',\n};\nconst req = http.request(options, (res) =&gt; {\nconsole.log(`statusCode: ${res.statusCode}`);\nres.on('data', (d) =&gt; {\nresponse.send('Hello World');\n});\n});\nreq.end();\n});\napp.listen(parseInt(8000, 10), () =&gt; {\nconsole.log('Listening for requests');\n});\n});\n</code></pre> <p>To finalize the frontend service, create an empty <code>Dockerfile</code> with the following content:</p> <pre><code>FROM node:16\nWORKDIR /app\nRUN npm install @opentelemetry/api @opentelemetry/auto-instrumentations-node @opentelemetry/exporter-trace-otlp-http @opentelemetry/sdk-node express\nCOPY app.js .\nEXPOSE 8000\nCMD [ \"node\", \"app.js\" ]\n</code></pre> <p>For the backend service, you are going to use Tomcat with the OpenTelemetry Java agent installed. For this, create a <code>Dockerfile</code> like the following in the <code>backend</code> folder</p> <pre><code>FROM tomcat\nADD https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar javaagent.jar\nENV JAVA_OPTS=\"-javaagent:javaagent.jar\"\nCMD [\"catalina.sh\", \"run\"]\n</code></pre> <p>As you can see, the <code>Dockerfile</code> downloads and adds the OpenTelemetry Java agent for you automatically.</p> <p>You should now have the following files in your top level directory:</p> <ul> <li>./default.conf</li> <li>./docker-compose.yml</li> <li>./Dockerfile</li> <li>./opentelemetry_module.conf</li> <li>./otel-collector-config.yaml</li> <li>./backend/Dockerfile</li> <li>./frontend/Dockerfile</li> <li>./frontend/app.js</li> </ul> <p>With everything in place, you can now start the demo environment1:</p> <pre><code>$ docker compose up\n</code></pre> <p>Within a few moments you should have five docker containers up and running:</p> <ul> <li>Jaeger</li> <li>OTel Collector</li> <li>Nginx</li> <li>Frontend</li> <li>Backend</li> </ul> <p>Send a few requests to the frontend with <code>curl localhost:8000</code> and then check the Jaeger UI in your browser at localhost:16686. You should see traces going from frontend to NGINX to backend.</p> <p>The frontend trace should indicate an error, since NGINX is forwarding the <code>Page Not Found</code> from Tomcat.</p> <p></p>"},{"location":"blog/2022/instrument-nginx/#whats-next","title":"What's next?","text":"<p>You should now be able to apply what you have learned from this blog post to your own installation of NGINX. We would love to hear about your experience! If you run into any problems, create an issue.</p> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"blog/2022/jaeger-native-otlp/","title":"Supporting Jaeger with native OTLP","text":"<p>Back in May of 2022, the Jaeger project announced native support for the OpenTelemetry Protocol (OTLP). This followed a generous deprecation cycle for the Jaeger client libraries across many languages. With these changes, OpenTelemetry users are now able to send traces into Jaeger with industry-standard OTLP, and the Jaeger client library repositories have been finally archived.</p> <p>We intend to deprecate Jaeger exporters from OpenTelemetry in the near future, and are looking for your feedback to determine the length of the depreation phase. The best way to provide feedback is by filling out a 4-question survey or commenting on the existing draft pull request.</p>"},{"location":"blog/2022/jaeger-native-otlp/#opentelemetry-support","title":"OpenTelemetry Support","text":"<p>This interoperability is a wonderful victory both for Jaeger users and for OpenTelemetry users. However, we're not done yet. The OpenTelemetry specification still requires support for Jaeger client exporters across languages.</p> <p>This causes challenges for both Jaeger users and OpenTelemetry maintainers:</p> <ol> <li> <p>Confusing Choices</p> <p>Currently, users are faced with a choice of exporter (Jaeger or OTLP), and this can be a source of confusion. A user might be inclined, when exporting telemetry to Jaeger, to simply choose the Jaeger exporter because the name matches (even though Jaeger now actively encourages the use of OTLP).</p> <p>If we can eliminate this potentially confusing choice, we can improve the user experience and continue standardizing on a single interoperable protocol. We love it when things \"just work\" out of the box!</p> </li> <li> <p>Maintenance and duplication</p> <p>Because the Jaeger client libraries are now archived, they will not receive updates (including security patches). To continue properly supporting Jaeger client exporters, OpenTelemetry authors would be required to re-implement some of the functionality it had previously leveraged from the Jaeger clients.</p> <p>Now that Jaeger supports OTLP, this feels like a step backwards: It results in an increased maintenance burden with very little benefit.</p> </li> </ol>"},{"location":"blog/2022/jaeger-native-otlp/#user-impact","title":"User Impact","text":"<p>The proposal is to deprecate the following exporters from OpenTelemetry in favor of using native OTLP into Jaeger:</p> <ul> <li>Jaeger Thrift over HTTP</li> <li>Jaeger protobuf via gRPC</li> <li>Jaeger Thrift over UDP</li> </ul> <p>In addition to application configuration changes, there could be other architectural considerations. HTTP and gRPC should be straightforward replacements, although it may require exposing ports 4317 and 4318 if they are not already accessible.</p> <p>Thrift over UDP implies the use of the Jaeger Agent. Users with this deployment configuration will need to make a slightly more complicated change, typically one of the following:</p> <ol> <li>Direct ingest. Applications will change from using Thrift+UDP to sending OTLP    traces directly to their <code>jaeger-collector</code> instance. This may also have    sampling implications.</li> <li>Replacing the Jaeger Agent with a sidecar    OpenTelemetry Collector    instance. This could have sampling implications and requires changes to your    infrastructure deployment code.</li> </ol>"},{"location":"blog/2022/jaeger-native-otlp/#intent-to-deprecate-wed-like-your-feedback","title":"Intent to Deprecate - We'd Like Your Feedback!","text":"<p>In order to better support users and the interop between OpenTelemetry and Jaeger, we intend to deprecate and eventually remove support for Jaeger client exporters in OpenTelemetry.</p> <p>We would like your feedback! We want to hear from users who could be impacted by this change. To better make a data-informed decision, we have put together a short 4-question survey.</p> <p>Your input will help us to choose how long to deprecate before removal.</p> <p>A draft PR has been created in the specification to support this deprecation. If would like to contribute and provide feedback, visit the link above and add some comments. We want to hear from you.</p>"},{"location":"blog/2022/k8s-metadata/","title":"Improved troubleshooting using k8s metadata","text":"<p>Attaching Kubernetes resource metadata to OpenTelemetry traces is useful to identify which resource (such as a pod) is failing or having performance problems. It is also useful for correlating across other signals, for example: you can correlated logs and spans that were generated by the same pod.</p> <p>In this article, you'll learn how to configure the OpenTelemetry collector to use the k8sattributesprocessor in different scenarios.</p> <p>Details of the OpenTelemetry collector pipeline won't be covered in this post. For those details, refer to the collector documentation.</p>"},{"location":"blog/2022/k8s-metadata/#how-k8s-attributes-are-attached","title":"How K8s attributes are attached","text":"<p>At a high level, K8s attributes are attached to traces as resources. This is for two reasons:</p> <ol> <li>K8s attributes fit the definition of what a resource is: an entity for which    telemetry is recorded</li> <li>It centralizes this metadata, which is relevant for any generated span.</li> </ol> <p>Let's dive in and see how to do it!</p>"},{"location":"blog/2022/k8s-metadata/#using-k8sattributes-processor","title":"Using k8sattributes processor","text":"<p>This is an OpenTelemetry processor that automatically discovers pod metadata and attaches it to a resource associated with the spans generated by that pod. If the pod belongs to a <code>Deployment</code> or a <code>ReplicaSet</code>, it will also discover it's attributes.</p> <p>Some attributes we can attach to the resource are:</p> <ul> <li>Node name <code>k8s.node.name</code></li> <li>Pod name <code>k8s.pod.name</code></li> <li>Pod UID <code>k8s.pod.uid</code></li> <li>Namespace <code>k8s.namespace.name</code></li> <li>Deployment name, <code>k8s.deployment.name</code> if the pod was created by a deployment</li> </ul> <p>Such attributes adhere to OpenTelemetry semantic conventions. For details, see the Kubernetes resource semantic conventions.</p> <p>The processor internally maintains a list of pods and an associated attribute, usually the IP address of the pod, and uses this attribute to know which pod generates a certain span.</p> <p></p> <p>In the figure above you can see how the data flows: The table of pods is fetched using Kubernetes API, while the pod IP is extracted from the connection context between the pod and the collector.</p> <p>The <code>k8sattributesprocessor</code> can work in different modes depending on how the collector is configured. Let's explore one common scenario, when the collector is deployed as daemonset.</p>"},{"location":"blog/2022/k8s-metadata/#daemonset-mode","title":"Daemonset mode","text":"<p>Let\u2019s take a look at how we can configure the collector in daemonset mode, also known as an agent mode in the k8sattributes documentation.</p> <p>When we deploy the collector in daemonset mode, we have one collector pod per node. We need to configure the collector service account to have permissions to fetch all pod information. In order to do that, we will create a <code>ClusterRole</code> with the necessary permissions.</p> <p>Here are the minimum permissions required to make the <code>k8sattributesprocessor</code> work:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: otel-collector\nrules:\n- apiGroups: ['']\nresources: ['pods', 'namespaces']\nverbs: ['get', 'watch', 'list']\n</code></pre> <p>Next, deploy the collector in daemonset mode. We recommend that you set a filter to only fetch the pods that belong to the node in which the collector is deployed. This is because if you have a large cluster, you don\u2019t want to maintain a huge list of pods.</p> <p>This is the manifest used in this blog to show how the processor works:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: otel-collector-daemonset\nspec:\nmode: daemonset\nimage: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0\nserviceAccount: attributes-account\nenv:\n- name: KUBE_NODE_NAME\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: spec.nodeName\nconfig: |\nreceivers:\njaeger:\nprotocols:\ngrpc:\nthrift_binary:\nthrift_compact:\nthrift_http:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nk8sattributes:\nfilter:\nnode_from_env_var: KUBE_NODE_NAME\nexporters:\njaeger:\nendpoint: jaeger-all-in-one-collector:14250\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [otlp, jaeger]\nprocessors: [k8sattributes]\nexporters: [jaeger]\n</code></pre> <p>The main parts to note are that it uses the contrib collector image. The <code>k8sattributesprocessor</code> is not part of the OpenTelemetry collector core, but the contrib distribution has it. Other things to notice are the filter mentioned above, and the use of a previously-created specific service account, which contains the permissions to fetch the pod list.</p> <p>Next, deploy the manifest and the vert.x example app to generate some traces.</p> <p></p> <p>As you can see, each span of the trace now has the corresponding pod attributes attached to it.</p> <p>You can restrict the configuration above to a certain namespace if you add the namespace on the <code>k8sattributesprocessor</code> filter like this:</p> <pre><code>processors:\nk8sattributes:\nfilter:\nnamespace: my_namespace\n</code></pre> <p>In this way, you can create a <code>Role</code> and don\u2019t need to create a <code>ClusterRole</code>, reducing the scope of the collector service account to a single namespace.</p>"},{"location":"blog/2022/k8s-metadata/#using-resource-detector-processor","title":"Using Resource detector processor","text":"<p>As of recently, the OpenTelemetry operator sets the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable on the collector container with the K8s pod attributes. This lets you to use the resource detector processor, which attaches the environment variable values to the spans. This only works when the collector is deployed in sidecar mode.</p> <p>For example, if you deploy the following manifest:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: sidecar-for-my-app\nspec:\nmode: sidecar\nimage: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0\nconfig: |\nreceivers:\njaeger:\nprotocols:\ngrpc:\nthrift_binary:\nthrift_compact:\nthrift_http:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nresourcedetection:\ndetectors: [env]\ntimeout: 2s\noverride: false\nexporters:\njaeger:\nendpoint: jaeger-all-in-one-collector:14250\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [otlp, jaeger]\nprocessors: [resourcedetection]\nexporters: [jaeger]\n</code></pre> <p>And then deploy the vert.x example app, you can see the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable gets injected with some values in the sidecar container. Some of them use the Kubernetes downward API to get the attribute values.</p> <p>Here's an example of the value of the environment variable:</p> <pre><code>- name: OTEL_RESOURCE_ATTRIBUTES\nvalue: k8s.deployment.name=dep-vert-x,k8s.deployment.uid=ef3fe26b-a690-4746-9119-d2dbd94b469f,\nk8s.namespace.name=default,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=\n(OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.pod.uid=$(OTEL_RESOURCE_ATTRIBUTES_POD_UID),k8s.replicaset\nname=dep-vert-x-59b6f76585,k8s.replicaset.uid=5127bc38-e298-40e1-95df-f4a777e3176c\n</code></pre>"},{"location":"blog/2022/k8s-metadata/#learn-more","title":"Learn more","text":"<p>This post covers how to configure the OpenTelemetry collector to attach Kubernetes resource metadata as resource attributes to OpenTelemetry traces. The scenarios covered, although basic, illustrate how to add this kind of metadata to traces so that you can incorporate the technique into other more sophisticated scenarios. If you want to learn more about different scenarios or options for configure the processors you can see the K8sattributes processor documentation where you can find more scenarios like sidecar, or when one collector as an agent report to another collector.</p>"},{"location":"blog/2022/k8s-metadata/#references","title":"References","text":"<ul> <li>K8sattributes processor documentation</li> <li>K8sattributes processor RBAC</li> <li>OpenTelemetry Kubernetes attributes</li> <li>Resource detector processor</li> </ul>"},{"location":"blog/2022/k8s-otel-expose/","title":"Exposing a Collector for cross cluster communication","text":"<p>Exposing an OpenTelemetry Collector currently requires a number of configuration steps. The goal of this blog post is to demonstrate <code>how to establish a secure communication</code> between two collectors in different Kubernetes clusters.</p> <p>Details of CRDs and dependency installations are not covered by this post.</p>"},{"location":"blog/2022/k8s-otel-expose/#overview","title":"Overview","text":"<p>When it comes to making collectors publicly accessible, the first thing that comes to mind is the secure transmission of user data via TLS. However, authentication to the server is at least as important to prevent unauthorized services from sending data.</p> <p>The OpenTelemetry Collector supports different authentication methods. The most used are probably:</p> <ol> <li>TLS Authentication</li> <li>OpenID Connect (OIDC-Authentication)</li> <li>HTTP Basic Authentication</li> </ol> <p>This article focuses on HTTP Basic Authentication for simplicity. It is intended to show how a secure setup can be operated without key management or further third party services.</p> <p>For more information about TLS configuration I would like to refer to the article How TLS provides identification, authentication, confidentiality, and integrity and the Collector TLS-Config description on Github.</p> <p>If you are interested in using an external authentication provider, I advise you to have a look at the article Securing your OpenTelemetry Collector by Juraci Paix\u00e3o Kr\u00f6hling on this topic. He explains how OpenTelemetry collectors can be secured using the OIDC-Authenticator extension, and how Keycloak can be configured as an authentication provider.</p>"},{"location":"blog/2022/k8s-otel-expose/#basic-authentication","title":"Basic Authentication","text":"<p>The HTTP Basic Authentication mechanism is quite simple. An HTTP user agent (e.g., a web browser) provides a username and password combination on every request. Transmitted credentials are included in the HTTP header by the key <code>Authorization</code> when the connection is established. As a value the authentication method <code>basic</code> is mentioned first, followed by the encoded credentials. Note that the credential form is <code>username:password</code>.</p> <p>In the following example, <code>dXNlci0xOjEyMzQK</code> is the encoding for a combination of <code>username=user-1</code> and <code>password=1234</code>. Note to encode or decode base64 values, you can use</p> <pre><code># HTTP Header key: value pair\nAuthorization: Basic &lt;credentials-base64-encoded&gt;\n# example: user: user-1 password: 1234\nAuthorization: Basic dXNlci0xOjEyMzQK\n</code></pre> <p>You can easily create your own user password combination using the base64 cli tool.</p> <pre><code># encode\n$ echo \"user-1:1234\" | base64\ndXNlci0xOjEyMzQK\n\n# decode\n$ echo \"dXNlci0xOjEyMzQK\" | base64 -d\nuser-1:1234\n</code></pre>"},{"location":"blog/2022/k8s-otel-expose/#data-flow","title":"Data flow","text":"<p>The following graph illustrates the target topology. The goal is to transfer traces generated by a test application via a dedicated collector to a publicly accessible cluster. The receiving collector uses the transmitted 'Basic' HTTP Authentication credentials to check whether the sender is authorized to store data. Finally, transmitted traces are stored in a Jaeger in-memory</p> <p></p>"},{"location":"blog/2022/k8s-otel-expose/#prerequisites","title":"Prerequisites","text":"<p>Interfaces and behavior may change in the future. Therefore, the versions used in this setup are mentioned in brackets.</p> <ul> <li>A Kubernetes[v1.23.3] cluster with a public address with   ingress-nginx-controller[v1.2.1]   installed.</li> <li>A Kubernetes[v1.23.3] edge cluster to create a test cluster. Using   Kind is recommended.</li> <li>Installed OpenTelemetry Operator[v0.58.0]   on both ends.</li> <li>Installed   Jaeger Operator[v1.37.0]   on your public cluster.</li> <li>Installed cert-manager[v1.9.1] on your public   cluster.</li> </ul>"},{"location":"blog/2022/k8s-otel-expose/#remote-cluster-configuration","title":"Remote cluster configuration","text":"<p>Since all components except the Jaeger backend depend on a following component, we begin by deploying the backend.</p> <pre><code>apiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\nname: my-in-memory\n</code></pre> <p>In the next step we create an OpenTelemetry Collector using the <code>OpenTelemetryCollector</code> CRD. The most important entries are <code>mode</code>, <code>image</code> and the configured basicauth extension. In the manifest below the mode <code>deployment</code> was chosen to guarantee that at least one collector pod is available for processing incoming information. Furthermore the default collector image was overwritten with the contrib version. This is necessary because the core version does not contain the basicauth extension. This extension was configured with the name <code>basicauth/server</code> and registered in <code>otlp/basicauth</code>. As otlp exporter endpoint the Jaeger in-memory service was configured.</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: otel-collector-app\nspec:\nmode: deployment\nimage: otel/opentelemetry-collector-contrib:0.58.0\nconfig: |\nextensions:\nbasicauth/server:\nhtpasswd:\ninline: |\n&lt;REPLACE: your backend credentials, e.g.: \"user-1:1234\"&gt;\nreceivers:\notlp/basicauth:\nprotocols:\ngrpc:\nauth:\nauthenticator: basicauth/server\nexporters:\notlp/jaeger:\nendpoint: my-in-memory-collector:4317\ntls:\ninsecure: true\ninsecure_skip_verify: true\nservice:\nextensions: [basicauth/server]\npipelines:\ntraces:\nreceivers: [otlp/basicauth]\nexporters: [otlp/jaeger]\n</code></pre> <p>After a successful installation, a pod for the Jaeger backend and the OpenTelemetry collector should be created in the selected namespace.</p> <pre><code>NAME                                            READY   STATUS    RESTARTS   AGE\nmy-in-memory-6c5f5f87c5-rnp99                   1/1     Running   0          4m\notel-collector-app-collector-55cccf4b7d-llczt   1/1     Running   0          3m\n</code></pre> <p>Also the following services should be available:</p> <pre><code>NAME                                      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                    AGE\nmy-in-memory-agent                        ClusterIP   None            &lt;none&gt;        5775/UDP,5778/TCP,6831/UDP,6832/UDP                         7m\nmy-in-memory-collector                    ClusterIP   10.245.43.185   &lt;none&gt;        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP    7m\nmy-in-memory-collector-headless           ClusterIP   None            &lt;none&gt;        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP    7m\nmy-in-memory-query                        ClusterIP   10.245.91.239   &lt;none&gt;        16686/TCP,16685/TCP                                         7m\notel-collector-app-collector              ClusterIP   10.245.5.134    &lt;none&gt;        4317/TCP                                                    5m\notel-collector-app-collector-headless     ClusterIP   None            &lt;none&gt;        4317/TCP                                                    5m\notel-collector-app-collector-monitoring   ClusterIP   10.245.116.38   &lt;none&gt;        8888/TCP                                                    5m\n</code></pre> <p>Finally, cert-manager is configured to automatically request TLS certificates from Let\u2019s Encrypt and make it available to the Ingress TLS configuration. The following <code>ClusterIssuer</code> and <code>Ingress</code> entries expose the <code>otel-collector-app-collector</code> service. Note that you'll need to replace values for the <code>email</code> and <code>host</code> fields.</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\nname: letsencrypt\nnamespace: cert-manager\nspec:\nacme:\nserver: https://acme-v02.api.letsencrypt.org/directory\nemail: your-email-address-here@example.com # REPLACE\nprivateKeySecretRef:\nname: letsencrypt\nsolvers:\n- http01:\ningress:\nclass: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: ingress-otel\nannotations:\nkubernetes.io/ingress.class: nginx\nnginx.ingress.kubernetes.io/backend-protocol: GRPC\ncert-manager.io/cluster-issuer: letsencrypt\nspec:\ntls:\n- hosts:\n- your-host # REPLACE your domain endpoint, e.g., traces@example.com\nsecretName: letsencrypt\nrules:\n- host: your-host # REPLACE your domain endpoint, e.g., traces@example.com\nhttp:\npaths:\n- pathType: Prefix\npath: '/'\nbackend:\nservice:\nname: otel-collector-app-collector\nport:\nnumber: 4317\n</code></pre>"},{"location":"blog/2022/k8s-otel-expose/#edge-cluster-configuration","title":"Edge Cluster configuration","text":"<p>In order to be able to determine the origin of the transmitted traces, the span-tags are extended by identifying metadata with the help of the k8sattributes processor. It is available in the OpenTelemetry Collector contrib version. In the next step we create a service account with the necessary permissions. If you want to learn more about the K8s metadata, you can read this post \"Improved troubleshooting using K8s metadata\".</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nname: attributes-role\nrules:\n- apiGroups:\n- ''\nresources:\n- pods\nverbs:\n- get\n- list\n- watch\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: attributes-rolebinding\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: attributes-role\nsubjects:\n- kind: ServiceAccount\nname: attributes-account\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: attributes-account\n</code></pre> <p>Let's have a quick look on the most important edge collector settings. A <code>daemonset</code> is used as deployment mode to ensure that one collector instance per node exists. The <code>basicauth</code> extension contains <code>username</code> and <code>password</code> to identify itself to the exposed remote collector. More container and node specific information are provided by the <code>k8sattributes</code> processor via the kubernetes Kubernetes downward-api. What is not covered is the cluster availability zone and the cluster name. To be able to identify the reported spans later, they are inserted manually with the help of the <code>resource</code> processor. Last, the OTLP exporter endpoint has also been given a placeholder value that must be replaced with your remote cluster domain.</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: otel-collector-app\nspec:\nmode: daemonset\nimage: otel/opentelemetry-collector-contrib:0.58.0\nserviceAccount: attributes-account\nenv:\n- name: KUBE_NODE_NAME\nvalueFrom:\nfieldRef:\napiVersion: v1\nfieldPath: spec.nodeName\nconfig: |\nextensions:\nbasicauth/client:\nclient_auth: # credentials must be consistent with those of the receiving collector.\nusername: &lt;REPLACE: your basicauth username, e.g.: \"user-1\"&gt;\npassword: &lt;REPLACE: your basicauth password, e.g.: \"1234\"&gt;\nreceivers:\notlp:\nprotocols:\ngrpc:\nprocessors:\nresource:\nattributes:\n- key: cloud.availability_zone\nvalue: &lt;REPLACE: your availability zone, e.g.: \"eu-west-1\"&gt;\naction: insert\n- key: k8s.cluster.name\nvalue: &lt;REPLACE: your cluster name, e.g.: \"edge-cluster-1\"&gt;\naction: insert\nk8sattributes:\nfilter:\nnode_from_env_var: KUBE_NODE_NAME\nexporters:\notlp:\nendpoint: \"&lt;REPLACE: your domain endpoint, e.g.: \"traces.example.com:443\"&gt;\"\nauth:\nauthenticator: basicauth/client\nlogging:\nservice:\nextensions: [basicauth/client]\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [k8sattributes]\nexporters: [otlp,logging]\n</code></pre> <p>After a successful installation, a <code>daemonset</code> with the name <code>otel-collector-app-collector</code> should have been created. This ensures that each cluster node has a local collector instance up and running.</p>"},{"location":"blog/2022/k8s-otel-expose/#deploy-trace-generator-to-generate-test-data","title":"Deploy trace generator to generate test data","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: trace-gen\nspec:\nselector:\nmatchLabels:\napp: trace-gen\ntemplate:\nmetadata:\nlabels:\napp: trace-gen\nspec:\ncontainers:\n- name: trace-gen\nimage: ghcr.io/frzifus/jaeger-otel-test:latest\nargs:\n[\n'-otel.agent.host=otel-collector-app-collector',\n'-otel.agent.port=4317',\n]\nenv:\n- name: OTEL_SERVICE_NAME\nvalue: 'local-test-service'\n</code></pre>"},{"location":"blog/2022/k8s-otel-expose/#testing","title":"Testing","text":"<p>Now spans generated in the edge cluster should be extended with origin metadata. These are then transferred to the remote cluster and stored in the Jaeger backend. Jaeger itself provides a UI for inspecting transmitted data.</p> <p>An easy way to reach the UI is by port forwarding to your local system.</p> <pre><code>$ kubectl port-forward deployments/my-in-memory 16686\nForwarding from 127.0.0.1:16686 -&gt; 16686\n</code></pre> <p></p>"},{"location":"blog/2022/k8s-otel-expose/#conclusion","title":"Conclusion","text":"<p>Configurations like <code>Ingress</code>, <code>ClusterIssuer</code> and <code>OpenTelemetryCollector</code> on client and server side have to be configured manually. Depending on installed Kubernetes components, the configurations differ a lot. Overall the configuration is very error-prone. In the future the exposing of the collector should be simplified with the help of the OpenTelemetry operator. If you are interested in the development, you can follow Github issue #902 to stay updated.</p>"},{"location":"blog/2022/k8s-otel-expose/#references","title":"References","text":"<ul> <li>Securing your OpenTelemetry Collector</li> <li>Jaeger Tracing</li> <li>OpenTelemetry-Collector</li> <li>Distributions:     contrib,     core</li> <li>Extensions:     basicauth,     oidc</li> <li>Processors:     resource,     k8sattributes</li> <li>Exporters:     otlp,     logging</li> <li>Test-Application</li> <li>Basic HTTP Authentication</li> <li>Kubernetes Downward-API</li> <li>Let\u2019s Encrypt</li> <li>Ingress NGINX gRPC example</li> <li>OpenTelemetry-Collector TLS-Config</li> <li>How TLS provides identification, authentication, confidentiality, and integrity</li> </ul>"},{"location":"blog/2022/knative/","title":"Distributed tracing in Knative","text":"<p>In this article, you will learn how distributed tracing works in Knative and we will explore how the OpenTelemetry project can make tracing support in this environment easier. We will explore Knative under the hood to understand what distributed tracing capabilities it provides out-of-the-box and which parts of the system need additional instrumentation.</p>"},{"location":"blog/2022/knative/#about-knative","title":"About Knative","text":"<p>Knative is a serverless platform built on top of Kubernetes as a set of <code>CustomResourceDefinitions</code> (CRDs). The project is split into two logical parts:</p> <ul> <li>serving - facilitates the creation, deployment and scaling of   workload/services</li> <li>eventing - facilitates event-driven communication between workloads to enable   loosely coupled architectures</li> </ul> <p>In this article we will not cover Knative fundamentals, please refer to the Knative documentation to get familiar with the project.</p>"},{"location":"blog/2022/knative/#knative-data-flow","title":"Knative data flow","text":"<p>Before we deep dive into tracing let's take a look at a data flow example. It will help us to understand Knative architecture and which parts of the system need to be instrumented in order to understand the timing characteristics of the request or transaction. On the diagram below there are two user workloads (first and second) and an incoming request marked as (1. HTTP) that goes to use workload first and then to the workload second as a cloud event message.</p> <p></p> <p>There are two important facts about this diagram:</p> <ol> <li>all the traffic goes through queue-proxy sidecar</li> <li>all traffic goes through Knative component(s). The Knative components in the    diagram are abstract. It can be a Knative activator service, Knative event    broker, dispatcher etc.</li> </ol> <p>From the telemetry perspective, the purpose of queue-proxy is similar to istio-proxy from Istio service mesh. It is a proxy that intercepts all traffic going to the workload and it emits telemetry data for any communication going to or from the workload.</p>"},{"location":"blog/2022/knative/#distributed-tracing-in-knative","title":"Distributed tracing in Knative","text":"<p>The Knative project comes with a solid distributed tracing integration. Major parts of the system are already instrumented and the system creates trace data for transactions/requests that go to user workloads.</p> <p>Internally at the moment, Knative uses OpenCensus instrumentation libraries that export data in Zipkin format. The inter-process context propagation uses Zipkin B3 and W3C Trace-Context standards. The Zipkin B3 propagation format is most likely used for legacy reasons to allow trace context propagation with older workloads instrumented with older technology. As a best practice, use the standard W3C Trace-Context which is natively used by the OpenTelemetry project.</p> <p>Now let's take a look at an example trace with two workloads (first and second). The workflow is similar to the diagram from the previous section: the first service receives an HTTP call and sends a cloud event to the second service. The full demo source code can be found in pavolloffay/knative-tracing.</p> <p></p> <p>The trace shows the following services interacting: activator, first workload, broker-ingress, imc-dispatcher, broker-filter, activator, and second workload. There are many services, right? A simple interaction of two workloads resulted in a trace that shows many Knative internal components. From the observability perspective, this is great because it can show issues in the infrastructure and additionally show cost associated with Knative request processing.</p> <p>Let's briefly example the data flow. The incoming HTTP request first goes through an activator service that is responsible for scaling up a workload, then its execution reaches the first workload. The first workload sends a cloud event which goes through the broker and dispatcher and finally reaches the second workload.</p> <p>Now let's take a closer look at the user workloads. The first service is a Golang service with a single REST API endpoint. The endpoint implementation creates a cloud event and sends it to the broker. Let's take a look at important facts from the observability perspective:</p> <ul> <li>REST API is instrumented with OpenTelemetry. This allows us to link traces   started in the Knative activator service with spans created in the workload   and further link it with outbound spans - e.g. to calls to the second service.</li> <li>The workload is using instrumented   Cloudevents client/SDK -   similarly to the previous point it allows us to continue the trace in the   outbound request (in this scenario to the second service).</li> </ul> <p>How is the trace-context (<code>traceId</code>, <code>spanId</code>, <code>sampled</code> flag) being propagated in our example applications? The trace-context is propagated in HTTP headers both for incoming HTTP requests into the first service and as well for cloud events sent to the second service. The trace-context is not attached directly to the event extensions/attributes.</p> <p>Follows log output with request headers from the first service:</p> <pre><code>2022/02/17 12:53:48 Request headers:\n2022/02/17 12:53:48     X-B3-Sampled: [1]\n2022/02/17 12:53:48     X-B3-Spanid: [af6c239eb7b39349]\n2022/02/17 12:53:48     X-B3-Traceid: [5f2c4775e0e36efc1d554a0b6c456cc1]\n2022/02/17 12:53:48     X-Forwarded-For: [10.244.0.12, 10.244.0.5]\n2022/02/17 12:53:48     Accept-Language: [en,fr;q=0.9,de;q=0.8,sk;q=0.7]\n2022/02/17 12:53:48     Cookie: [_ga=GA1.2.260863911.1644918876]\n2022/02/17 12:53:48     Accept: [text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9]\n2022/02/17 12:53:48     K-Proxy-Request: [activator]\n2022/02/17 12:53:48     Upgrade-Insecure-Requests: [1]\n2022/02/17 12:53:48     User-Agent: [Mozilla/5.0 (X11; Fedora; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36]\n2022/02/17 12:53:48     X-Request-Id: [ee2797b5-1ee9-408e-b1ff-d5e5431977e6]\n2022/02/17 12:53:48     Cache-Control: [max-age=0]\n2022/02/17 12:53:48     X-Forwarded-Proto: [http]\n2022/02/17 12:53:48     Traceparent: [00-5f2c4775e0e36efc1d554a0b6c456cc1-af6c239eb7b39349-01]\n2022/02/17 12:53:48     Accept-Encoding: [gzip, deflate]\n2022/02/17 12:53:48     Forwarded: [for=10.244.0.12;proto=http]\n2022/02/17 12:53:48 Response headers:\n2022/02/17 12:53:48     Traceparent: [00-5f2c4775e0e36efc1d554a0b6c456cc1-1cf3f827eba96bf2-01]\n2022/02/17 12:53:48\n</code></pre> <p>Now let's take a look at logging from the second service which exposes API to consume Knative events. The event API in this case is just an HTTP endpoint which is a cloud event implementation detail:</p> <pre><code>2022/02/17 13:39:36 Event received: Context Attributes,\n  specversion: 1.0\n  type: httpbody\n  source: github/com/pavolloffay\n  id: fad4139c-b3fb-48b2-b0f4-fee44addc5f1\n  time: 2022-02-17T13:39:34.426355726Z\n  datacontenttype: text/plain\nExtensions,\n  knativearrivaltime: 2022-02-17T13:39:34.491325425Z\nData,\n  hello from first, traceid=5f2c4775e0e36efc1d554a0b6c456cc1\n</code></pre> <p>We see that the trace context is not directly present in the event object. However, it is encoded in the incoming transport message - HTTP headers.</p>"},{"location":"blog/2022/knative/#future-improvements","title":"Future improvements","text":"<p>In the previous section, it was mentioned that the Knative serving and eventing components are instrumented with OpenCensus SDK. The instrumentation will change in the future to OpenTelemetry which is tracked in knative/eventing/#3126 and knative/pkg#855. The SDK change might not have an immediate impact on the user, however, it will enable users to start natively reporting data in OpenTelemetry format (OTLP).</p> <p>Another recently merged change is the addition of Cloudevents semantic attributes into the OpenTelemetry specification. The document standardizes attributes related to CloudEvents. The screenshot below is from the demo application that is still not using the standardized attribute names:</p> <p></p>"},{"location":"blog/2022/knative/#configuration","title":"Configuration","text":"<p>Tracing in Knative can be easily enabled. Please follow the official documentation for a step-by-step guide. Let's briefly describe the process here:</p> <ol> <li>Deploy a tracing system that can ingest tracing data in Zipkin format -    Zipkin, Jaeger, or OpenTelemetry collector</li> <li>Enable tracing in    Knative eventing</li> <li>Enable tracing in    Knative serving</li> </ol> <p>In the beginning, I recommended using 100% sampling rate configuration to capture trace data for all traffic in the cluster. This will help to avoid any issues with sampling, do not forget to change this configuration once moving to the production environment.</p>"},{"location":"blog/2022/knative/#conclusion","title":"Conclusion","text":"<p>We have learned what distributed tracing capabilities Knative project provides out-of-the-box and which parts need more work from the user. Generally speaking Knative emits rich tracing data, however, as always the user is responsible to instrument the workload and make sure trace-context is propagated from inbound to outbound requests or events. This is exactly the same situation as implementing distributed tracing in service meshes.</p> <p>OpenTelemetry can help to instrument the user workload and correctly propagate the trace-context. Depending on the language, the user can initialize instrumentation libraries explicitly in the code or even dynamically inject OpenTelemetry auto-instrumentation into the workload.</p>"},{"location":"blog/2022/knative/#references","title":"References","text":"<ul> <li>Knative docs</li> <li>Knative serving tracing config</li> <li>Knative eventing tracing config</li> <li>Cloud events</li> <li>Zipkin B3</li> <li>W3C Trace-Context</li> <li>OpenTelemetry instrumentation for Cloudevents Golang SDK</li> <li>Cloudevents OpenTelemetry attributes</li> <li>Knative tracing demo</li> </ul>"},{"location":"blog/2022/opamp/","title":"Using OpenTelemetry OpAMP to modify service telemetry on the go","text":"<p>How verbose should your service telemetry be? Should a service output all traces, metrics, and logs 100% of the time? How much of the service traffic should be sampled? I would like to suggest the answer of \u201cit depends\u201d. Desired telemetry data differs in a service lifecycle from development to continuous deployment. It changes when clients face an error or a service has been thrown into a major scale.</p> <p>It is possible to change a service telemetry configuration or sampling rate. Usually, it requires only a minimal code change and a deployment process. It might not seem a lot, but whenever facing a change like this across an entire system, we tend to avoid it. Instead, it is common to collect as much data as possible, which causes an issue by itself. Can we dynamically modify service telemetry without those barriers? Thanks to OpAMP protocol and the people behind it, I believe that the answer is about to change.</p> <p>OpAMP stands for Open Agent Management Protocol. It aims at managing large fleets of data collection agents, and its GoLang implementation is at the Beta stage. It allows configuration changes as well as package downloads. It defines the communication between the OpAMP server and OpAMP client but does not assume any particular client-agent relationship giving it a lot of flexibility.</p> <p>In the following example, we\u2019ll create a simple GoLang server, instrument it, and then control it with an OpAMP server and supervisor. We won\u2019t dive into OpAMP implementation itself, but rather focus on its implications using these examples.</p> <p>First, consider this basic go server:</p> <pre><code>package main\nimport (\n\"fmt\"\n\"log\"\n\"net/http\"\n)\nfunc httpHandler(w http.ResponseWriter, r *http.Request) {\nfmt.Fprintf(w, \"Hi! This action could create a trace!\")\n}\nfunc main() {\nhandler := http.HandlerFunc(httpHandler)\nhttp.Handle(\"/\", handler)\nfmt.Println(\"Starting server on port 8080\")\nlog.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n</code></pre> <p>Next, add a basic configuration file named <code>effective.yaml</code>. Place it in the same folder as our main.go file with this configuration:</p> <pre><code>instrument: false\n</code></pre> <p>Let\u2019s add a basic configuration handler to our server:</p> <pre><code>package main\nimport (\n\"fmt\"\n\"gopkg.in/yaml.v3\"\n\"io/ioutil\"\n\"log\"\n\"net/http\"\n\"path/filepath\"\n)\ntype configurations struct {\nInstrument bool\n}\nfunc httpHandler(w http.ResponseWriter, r *http.Request) {\nfmt.Fprintf(w, \"Hi! This action could create a trace!\")\n}\nfunc main() {\nfilename, _ := filepath.Abs(\"./effective.yaml\")\nyamlFile, _ := ioutil.ReadFile(filename)\nvar config configurations\nyaml.Unmarshal(yamlFile, &amp;config)\nhandler := http.HandlerFunc(httpHandler)\nhttp.Handle(\"/\", handler)\nfmt.Println(\"Starting server on port 8080\")\nlog.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n</code></pre> <p>Next, let's wrap our handler with instrumentation and condition it with our configuration file. Something like this:</p> <pre><code>package main\nimport (\n\"context\"\n\"fmt\"\n\"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/exporters/stdout/stdouttrace\"\nsdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n\"go.opentelemetry.io/otel/trace\"\n\"gopkg.in/yaml.v3\"\n\"io/ioutil\"\n\"log\"\n\"net/http\"\n\"os\"\n\"path/filepath\"\n)\ntype configurations struct {\nInstrument bool\n}\nvar tracer trace.Tracer\nfunc newConsoleExporter() (sdktrace.SpanExporter, error) {\nreturn stdouttrace.New(\nstdouttrace.WithWriter(os.Stdout),\nstdouttrace.WithPrettyPrint(),\n)\n}\nfunc httpHandler(w http.ResponseWriter, r *http.Request) {\nfmt.Fprintf(w, \"Hi! This action could create a trace!\")\n}\nfunc setHandler(handler http.Handler, config configurations) http.Handler {\nif config.Instrument {\nreturn otelhttp.NewHandler(handler, \"instrumentation activated by OpAMP\")\n}\nreturn http.HandlerFunc(httpHandler)\n}\nfunc main() {\nfilename, _ := filepath.Abs(\"./effective.yaml\")\nyamlFile, _ := ioutil.ReadFile(filename)\nvar config configurations\nyaml.Unmarshal(yamlFile, &amp;config)\nexp, _ := newConsoleExporter()\ntp := sdktrace.NewTracerProvider(sdktrace.WithBatcher(exp))\ndefer func() { _ = tp.Shutdown(context.Background()) }()\notel.SetTracerProvider(tp)\ntracer = tp.Tracer(\"ControlledOpAMPAgentDemo\")\nhandler := http.HandlerFunc(httpHandler)\nhttp.Handle(\"/\", setHandler(handler, config))\nfmt.Println(\"Starting server on port 8080\")\nlog.Fatal(http.ListenAndServe(\":8080\", nil))\n}\n</code></pre> <p>Build and run this app:</p> <pre><code>go build .\ngo run .\n</code></pre> <p>Open a browser and visit http://localhost:8080. Nothing special will be shown. It\u2019s time to add some opamp. Git clone opamp-go and run the server with:</p> <pre><code>cd internal/examples/server\ngo run .\n</code></pre> <p>Visit http://localhost:4321 to verify that the server is running. Notice that no agent is displayed:</p> <p></p> <p>Next, edit internal/examples/supervisor/bin/supervisor.yaml to point at our agent. It should look like this:</p> <pre><code>server:\nendpoint: ws://127.0.0.1:4320/v1/opamp\nagent:\nexecutable: &lt;absolute|relative path to previous build&gt;\n</code></pre> <p>Then open a new terminal and run the following command:</p> <pre><code>cd internal/examples/supervisor/bin\ngo build -o ./supervisor ../main.go\n./supervisor\n</code></pre> <p>We have now a system consisting of OpAMP server supervisor and our server</p> <p></p> <p>Via the supervisor we can now see our agent running at http://localhost:4321. Select it and pass <code>instrument: true</code> to its configurations.</p> <p></p> <p>You can see the changes over the supervisor console log:</p> <pre><code>Received remote config from server, hash=0008886301f3ccb3520216823cfa09a.\nEffective config changed.\nConfig is changed. Signal to restart the agent.\nRestarting the agent with the new config.\nStopping agent process, PID=19206\nAgent process PID=19206 successfully stopped.\nStarting agent &lt;agent path&gt;\nAgent process started, PID=19506\n</code></pre> <p>Finally, visit http://localhost:8080. Traces should now appear in <code>internal/examples/supervisor/bin/agent.log</code>.</p> <pre><code>Starting server on port 8080\n{\n   \"Name\": \"instrumentation activated by OpAMP server\",\n   \"SpanContext\": {\n      \"TraceID\": \"d2f76958023624d4c1def3f44899b6d4\",\n      \"SpanID\": \"085510f551dc31a1\",\n      \"TraceFlags\": \"01\",\n      \"TraceState\": \"\",\n      \"Remote\":false\n   ...\n</code></pre> <p>These lines are the trace itself!</p> <p>To sum up, we have here a server that controls whether our service will generate traces. Try setting it off using the <code>instrument: false</code> configuration.</p> <p>This is a very basic implementation. Wrapping a system above OpAMP could perform as an instrumentation orchestrator. The starting point is being able to externally add and match tailor-made dynamic telemetry for your system. Imagine what AI can achieve on this type of system. It could collect metrics over the entire system, automatically and dynamically adding trace/log collections onto any detected bottlenecks. Using this protocol enables many new possibilities, I believe it has the potential to change how we think about telemetry.</p>"},{"location":"blog/2022/otel-demo-app-nomad/","title":"Running the OpenTelemetry Demo App on HashiCorp Nomad","text":"<p>Y\u2019all\u2026 I\u2019m so excited, because I finally got to work on an item on my tech bucket list. Last week, I began the process of translating OpenTelemetry Demo App\u2019s Helm Charts to HashiCorp Nomad job specs. Today I\u2019ll be talking about how to run the OpenTelemetry Demo App on Nomad, using my favorite Hashi-in-a-box tool, HashiQube.</p> <p>Let\u2019s do this!</p> <p></p>"},{"location":"blog/2022/otel-demo-app-nomad/#deployment","title":"Deployment","text":""},{"location":"blog/2022/otel-demo-app-nomad/#assumptions","title":"Assumptions","text":"<p>Before we move on, I am assuming that you have a basic understanding of:</p> <ul> <li>Nomad. If not, head on over to my   Nomad intro post.   This blog post by   Daniela Baron is also great.</li> <li>Observability   (o11y) and OpenTelemetry (OTel). If not,   see Observability primer.</li> </ul>"},{"location":"blog/2022/otel-demo-app-nomad/#pre-requisites","title":"Pre-Requisites","text":"<p>In order to run the example in this tutorial, you\u2019ll need the following:</p> <ul> <li>Docker (version 20.10.21 at the time of this writing)</li> <li>Vagrant (version 2.3.1 at the time of this writing)</li> </ul>"},{"location":"blog/2022/otel-demo-app-nomad/#tutorial-repos","title":"Tutorial Repos","text":"<p>Below are the repos that we\u2019ll be using for today\u2019s tutorial:</p> <ul> <li>My modified HashiQube Repo (fork of   servian/hashiqube). If you\u2019re curious,   you can see what modifications I\u2019ve made   here.</li> <li>My Nomad Conversions repo</li> </ul>"},{"location":"blog/2022/otel-demo-app-nomad/#hashiqube-setup","title":"HashiQube Setup","text":"<p>Before you start, just a friendly reminder that HashiQube by default runs Nomad, Vault, and Consul on Docker. In addition, we\u2019ll be deploying 21 job specs to Nomad. This means that we\u2019ll need a decent amount of CPU and RAM, so please make sure that you have enough resources allocated in your Docker desktop. For reference, I\u2019m running an M1 Macbook Pro with 8 cores and 32 GB RAM. My Docker Desktop Resource settings are as follows:</p> <ul> <li>CPUs: 3</li> <li>Memory: 9.5GB</li> <li>Swap: 3GB</li> </ul> <p>Here\u2019s a screenshot of my Docker Preferences Resources settings, if you need a visual:</p> <p></p> <p>For more, check out the Docker docs on how to change your resources settings for Mac, Windows, and Linux.</p>"},{"location":"blog/2022/otel-demo-app-nomad/#1-update-etchosts","title":"1- Update /etc/hosts","text":"<p>We use the Traefik load-balancer to expose our services, which we access as subdomains of localhost. In order ensure that we can access our Traefik-exposed services (and also the Traefik dashboard itself, you\u2019ll need to add the following entries to <code>/etc/hosts</code> on your host machine:</p> <pre><code>127.0.0.1   traefik.localhost\n127.0.0.1   otel-demo.localhost\n</code></pre>"},{"location":"blog/2022/otel-demo-app-nomad/#2-provision-a-local-hashi-environment-with-hashiqube","title":"2- Provision a Local Hashi Environment with HashiQube","text":"<p>Start HashiQube by following the detailed instructions here.</p> <p>NOTE: Be sure to check out the Gotchas section, if you get stuck.</p> <p>Once everything is up and running (this will take several minutes, by the way), you\u2019ll see this in the tail-end of the startup sequence, to indicate that you are good to go:</p> <p></p> <p>You can now access the apps using the URLs below:</p> <ul> <li>Vault: http://localhost:8200</li> <li>Nomad: http://localhost:4646</li> <li>Consul: http://localhost:8500</li> <li>Traefik: http://traefik.localhost</li> </ul> <p>Don\u2019t forget to download and install the Nomad CLI and the Vault CLI.</p> <p>If you need to SSH into HashiQube, open up a new terminal window on your host machine and run the following command:</p> <pre><code>vagrant ssh\n</code></pre>"},{"location":"blog/2022/otel-demo-app-nomad/#3-deploy-the-otel-demo-app","title":"3- Deploy the OTel Demo App","text":"<p>We\u2019re finally ready to deploy the OTel Demo App!</p> <p>First, let\u2019s clone the repo, and go to our working directory:</p> <pre><code>git clone https://github.com/avillela/nomad-conversions.git\ncd nomad-conversions\n</code></pre> <p>Next, let\u2019s enable Memory Oversubscription in Nomad. This is a one-time setting.</p> <pre><code>nomad operator scheduler set-config -memory-oversubscription true\n</code></pre> <p>Memory Oversubscription allows Nomad to use more memory than is allotted to the job. For example, consider this setting in the <code>resources</code> stanza:</p> <pre><code>resources {\ncpu    = 55\nmemory = 1024\nmemory_max = 2048\n}\n</code></pre> <p>We\u2019ve allocated 55Mz of processing power to our job (<code>cpu</code> setting), along with 1024MB RAM (<code>memory</code> setting). In this case, when Memory Oversubscription is enabled, and the job requires more memory than the allotted 1024MB, Nomad will allocate as much as 2048MB RAM to the job (<code>memory_max</code> setting). Note that if Memory Oversubscription is not enabled,Nomad will ignore the <code>memory_max</code> setting.</p> <p>Next, let\u2019s deploy the services:</p> <pre><code>nomad job run -detach otel-demo-app/jobspec/traefik.nomad\nnomad job run -detach otel-demo-app/jobspec/redis.nomad\nnomad job run -detach otel-demo-app/jobspec/ffspostgres.nomad\nnomad job run -detach otel-demo-app/jobspec/otel-collector.nomad\nnomad job run -detach otel-demo-app/jobspec/adservice.nomad\nnomad job run -detach otel-demo-app/jobspec/cartservice.nomad\nnomad job run -detach otel-demo-app/jobspec/currencyservice.nomad\nnomad job run -detach otel-demo-app/jobspec/emailservice.nomad\nnomad job run -detach otel-demo-app/jobspec/featureflagservice.nomad\nnomad job run -detach otel-demo-app/jobspec/paymentservice.nomad\nnomad job run -detach otel-demo-app/jobspec/productcatalogservice.nomad\nnomad job run -detach otel-demo-app/jobspec/quoteservice.nomad\nnomad job run -detach otel-demo-app/jobspec/shippingservice.nomad\nnomad job run -detach otel-demo-app/jobspec/checkoutservice.nomad\nnomad job run -detach otel-demo-app/jobspec/recommendationservice.nomad\nnomad job run -detach otel-demo-app/jobspec/frontend.nomad\nnomad job run -detach otel-demo-app/jobspec/loadgenerator.nomad\nnomad job run -detach otel-demo-app/jobspec/frontendproxy.nomad\nnomad job run -detach otel-demo-app/jobspec/grafana.nomad\nnomad job run -detach otel-demo-app/jobspec/jaeger.nomad\nnomad job run -detach otel-demo-app/jobspec/prometheus.nomad\n</code></pre> <p>Since we\u2019re running the jobs in detached mode, Nomad won\u2019t wait to start the next job until the current one has deployed successfully. This means that your output will look something like this:</p> <pre><code>Job registration successful\nEvaluation ID: d3eaa396-954e-241f-148d-6720c35f34bf\nJob registration successful\nEvaluation ID: 6bba875d-f415-36b7-bfeb-2ca4b9982acb\nJob registration successful\nEvaluation ID: 16dc8ef8-5e26-68f4-89b6-3d96b348775b\nJob registration successful\nEvaluation ID: 34de0532-a3b5-8691-bf18-51c0cc030573\nJob registration successful\nEvaluation ID: 7310e6a2-9945-710b-1505-c01bd58ccd35\n...\n</code></pre> <p>A reminder that the <code>Evaluation ID</code> values will be different on your machine.</p>"},{"location":"blog/2022/otel-demo-app-nomad/#4-see-it-in-nomad","title":"4- See it in Nomad!","text":"<p>As things are deploying, you can mozy on over to the Nomad UI at http://localhost:4646 to see how things are coming along:</p> <p></p> <p>It will take some time for all of the services to come up (sometimes up to 10 minutes), especially since Nomad needs to download the images and initialize the services, so be patient! Since some services depend on other services in order to run, you may see services in limbo or some going up and down for a while, per the above screen capture. DON\u2019T PANIC! IT WILL ALL BE OKAY!!</p> <p>Once all of the jobs are up and running, you\u2019ll see everything look green, like this:</p> <p></p> <p>You can also head on over to Consul at http://localhost:8500 to see the health of the services:</p> <p></p> <p>By default, unhealthy services show up at the top, with a red \u201cx\u201d next to them. Since we don\u2019t see any nasty red \u201cx\u201ds in the above screen shot, we know that our services are lookin\u2019 good!</p>"},{"location":"blog/2022/otel-demo-app-nomad/#5-access-the-otel-demo-app","title":"5- Access the OTel Demo App","text":"<p>The OTel Demo App uses Envoy to expose a number of front-end services: the Webstore, Jaeger, Grafana, Load Generator, and Feature Flag. These are all managed by the frontendproxy service. Traefik makes the frontendproxy service available via the <code>otel-demo.localhost</code> address.</p> <p>This is configured via the code snippet below, in the <code>service</code> stanza of frontendproxy.nomad:</p> <pre><code>tags = [        \"traefik.http.routers.frontendproxy.rule=Host(`otel-demo.localhost`)\",\n\"traefik.http.routers.frontendproxy.entrypoints=web\",\n\"traefik.http.routers.frontendproxy.tls=false\",\n\"traefik.enable=true\",\n]\n</code></pre> <p>Note that the <code>Host</code> is set to <code>otel-demo.localhost</code>.</p> <p>The services are accessed via the URLs below.</p> <p>Webstore: http://otel-demo.localhost/</p> <p></p> <p>Go ahead and explore the amazing selection of telescopes and accessories, and buy a few. \ud83d\ude09\ud83d\udd2d</p> <p>Jaeger UI: http://otel-demo.localhost/jaeger/ui/</p> <p></p> <p>In the screen capture above, we can see a sample Trace from the checkoutservice.</p> <p>Grafana: http://otel-demo.localhost/grafana/</p> <p></p> <p></p> <p>The Demo App comes bundled with a two Grafana dashboards, which showcase Metrics emitted with OpenTelemetry.</p> <p>Feature Flags UI: http://otel-demo.localhost/feature/</p> <p></p> <p>Load Generator UI: http://otel-demo.localhost/loadgen/</p> <p></p>"},{"location":"blog/2022/otel-demo-app-nomad/#gotchas","title":"Gotchas","text":"<p>While I think I\u2019ve managed to iron out a lot of the kinks as far as running the OTel Demo App in Nomad, I have run into a few hiccups when deploying the services.</p>"},{"location":"blog/2022/otel-demo-app-nomad/#services-sometimes-cant-connect-to-the-collector","title":"Services sometimes can\u2019t connect to the Collector","text":"<p>Although all of the services appear to start up properly, in some cases, some services appear to be unable to connect to the OTel Collector. I haven\u2019t quite figured out why this is happening, so for now, I just restart otel-collector.nomad. If things are looking a little weird in the Webapp UI (like missing products or currency), I also restart frontend.nomad. Usually a good indicator that services aren\u2019t sending telemetry to the Collector is to look at the number of services showing up in Jaeger. You should see 14 services, including the <code>jaeger-query</code> service.</p> <p></p>"},{"location":"blog/2022/otel-demo-app-nomad/#low-memory-on-host-machine","title":"Low memory on host machine","text":"<p>Yup\u2026as beefy as my machine is, I do also sometimes run low on memory on my host machine. It probably doesn\u2019t help that I have a zillion tabs open in Chrome and Safari. Plus, let\u2019s face it: HashiQube + 21 jobs in Nomad can be a bit memory intensive. I\u2019ve made a few tweaks to the memory settings in HashiQube and Docker to try to minimize memory issues, but in case the Memory Monster gets you, I suggest closing browsers and other apps, and re-opening them to free up some memory. And if this does happen to you, please let me know!</p>"},{"location":"blog/2022/otel-demo-app-nomad/#a-work-in-progress","title":"A Work in Progress","text":"<p>Please bear in mind that this project is a work in progress. If you have any suggestions for improvement, or would like to collaborate further on the Nomad jobspecs, please hit me up!</p>"},{"location":"blog/2022/otel-demo-app-nomad/#final-thoughts","title":"Final Thoughts","text":"<p>Well, there you have it, folks! You now have an example of how to deploy OpenTelemetry Demo App (a multi-micro-service app running OpenTelemetry) to HashiCorp Nomad. Main highlights:</p> <ul> <li>We used HashiQube to stand up a local   HashiCorp environment in Docker via Nomad so that we could run the OTel Demo   App in Nomad using Traefik as our load balancer.</li> <li>We saw the OTel Demo App in action, by accessing the following services   exposed through the   frontendproxy:   Webstore,   Grafana,   Jaeger,   Feature Flags UI,and the   Load Generator UI.</li> </ul> <p>Before I wrap this up, I do want to give a HUGE shoutout to Luiz Aoqui of HashiCorp, who helped me tweak my Nomad jobspecs, and to Riaan Nolan, for his continued work on HashiQube. (Aside, both Luiz and Riaan were my guests on the On-Call Me Maybe Podcast!)</p> <p>I will now leave you with a picture of Phoebe the rat, peering out of a pink basket. Doesn\u2019t she look cute? \ud83e\udd70</p> <p></p> <p>Peace, love, and code. \ud83e\udd84 \ud83c\udf08 \ud83d\udcab</p> <p>Have questions about the OTel Demo App on Nomad? Feel free to connect through Mastodon or LinkedIn.</p> <p>The OpenTelemetry community is always looking for contributions! Join us! If you're on Mastodon, be sure to follow OpenTelemetry on Mastodon</p>"},{"location":"blog/2022/tail-sampling/","title":"Tail Sampling with OpenTelemetry: Why it\u2019s useful, how to do it, and what to consider","text":"<p>Tail sampling is useful for identifying issues in your distributed system while saving on observability costs. In this post, you\u2019ll learn how to implement tail sampling using the OpenTelemetry collector. I will also share some general and OpenTelemetry-specific concerns to consider as you develop your sampling strategy.</p>"},{"location":"blog/2022/tail-sampling/#what-is-sampling-and-why-should-you-do-it","title":"What is sampling, and why should you do it?","text":"<p>With distributed tracing, you observe requests as they move from one service to another in a distributed system. It\u2019s superbly practical for a number of reasons, such as understanding your service connections and diagnosing latency issues, among many other benefits. If distributed tracing is a new topic for you, make sure to read this post on distributed tracing and sampling.</p> <p>However, if the majority of all your requests are successful 200s and finish without latency or errors, do you really need all that data? Here\u2019s the thing\u2014you don\u2019t always need a ton of data to find the right insights. You just need the right sampling of data.</p> <p></p> <p>The idea behind sampling is to control the spans you send to your observability backend, resulting in lower ingest costs. Different organizations will have their own reasons for not just why they want to sample, but also what they want to sample. You might want to customize your sampling strategy to:</p> <ul> <li>Manage costs: You risk incurring heavy charges from the relevant cloud   provider or vendor if you\u2019re exporting and storing all your spans.</li> <li>Focus on interesting traces: For example, your frontend team may only want   to see traces with specific user attributes.</li> <li>Filter out noise: For example, you may want to filter out health checks.</li> </ul>"},{"location":"blog/2022/tail-sampling/#what-is-tail-based-sampling","title":"What is tail-based sampling?","text":"<p>Tail-based sampling is where the decision to sample a trace happens after all the spans in a request have been completed. This is in contrast to head-based sampling, where the decision is made at the beginning of a request when the root span begins processing. Tail-based sampling gives you the option to filter your traces based on specific criteria, which isn\u2019t an option with head-based sampling.</p> <p></p> <p>Tail sampling lets you see only the traces that are of interest to you. You also lower data ingest and storage costs because you\u2019re only exporting a predetermined subset of your traces. For instance, as an app developer, I may only be interested in traces with errors or latency for debugging.</p>"},{"location":"blog/2022/tail-sampling/#how-to-implement-tail-sampling-in-the-opentelemetry-collector","title":"How to implement tail sampling in the OpenTelemetry collector","text":"<p>To use tail sampling in OpenTelemetry, you need to implement a component called the tail sampling processor. This component samples traces based on a set of policies that you can choose from and define. First, to ensure that you\u2019re capturing all spans, use either the default sampler or the AlwaysOn sampler in your SDKs.</p> <p>Now, let\u2019s walk through a sample configuration of the tail-sampling processor, aptly placed in the <code>processors</code> section of the collector configuration file:</p> <pre><code>processors:\ntail_sampling:\ndecision_wait: 10s\nnum_traces: 100\nexpected_new_traces_per_sec: 10\npolicies:\n[\n{\n          name: errors-policy,\n          type: status_code,\n          status_code: { status_codes: [ERROR] },\n},\n{\n          name: randomized-policy,\n          type: probabilistic,\n          probabilistic: { sampling_percentage: 25 },\n},\n]\n</code></pre> <ul> <li><code>tail_sampling</code> is the name of the processor you\u2019ll use to implement tail   sampling.</li> <li>The first three lines are optional configurable settings:</li> <li><code>decision_wait</code> is the time, in seconds, after the first span of a trace is     created before the sampling decision is made. The default is 30 seconds.</li> <li><code>num_traces</code> is the number of traces to be kept in memory. The default is     50,000.</li> <li><code>expected_new_traces_per_sec</code> is the expected number of new traces, which     helps in allocating data structures. The default is 0.</li> <li><code>policies</code> is where you define your sampling policies. There is no default,   and this is the only required section of the processor configuration. In this   case, two policies are defined:</li> <li><code>status_code</code>, which is named <code>errors-policy</code>, since this example will     filter traces with the status code <code>ERROR</code>.</li> <li><code>probabilistic</code>, which is named <code>randomized-policy</code>. In addition to     filtering all traces with errors, there will also be a randomized sampling     of 25% of traces without errors.</li> </ul> <p>The next image is an example of what you might see in your backend if you implement this sample configuration.</p> <p></p> <p>The blue dots and rectangle on the right side indicate that the sampling decision occurs at the end of a trace when all the spans for a given request have been completed. The green dots represent sampled spans while the gray dots represent unsampled spans. Finally, the red dots represent spans where errors were detected. With this configuration, you\u2019ll get all traces with errors as well as a random sampling of other traces based on the rate we\u2019ve configured.</p> <p>If you want to just sample based on a specific filter such as errors, you could remove the probabilistic policy. But having a random sampling of all other traces can help surface other issues and give you a broader view of your software\u2019s performance and behavior. Here\u2019s what you'll see with only the status code policy defined.</p> <p></p> <p>You also have the flexibility to add other policies. Here are a few examples:</p> <ul> <li><code>always_sample</code>: Sample all traces.</li> <li><code>latency</code>: Sample based on the duration of the trace. For example, you could   sample all traces that take longer than 5 seconds.</li> <li><code>string_attribute</code>: Sample based on string attribute values, both exact and   regex value matches are supported. For example, you could sample based on   specific custom attribute values.</li> </ul>"},{"location":"blog/2022/tail-sampling/#potential-issues-with-tail-sampling","title":"Potential issues with tail sampling","text":"<ul> <li>Potentially unpredictable costs: While sampling generally helps manage   data ingest and storage costs, you may occasionally have spikes in activity.   For example, if you\u2019re sampling traces with latency and you experience severe   network congestion, your tracing solution will export a high number of traces   with latency issues, leading to an unexpected spike in costs during this   period. However, this is also why tail sampling exists\u2014 so we can quickly see   these kinds of problems and act on them.</li> <li>Performance: If you are storing telemetry data locally, you will need to   store spans until sampling decisions are completed. This can consume resources   from your application if it\u2019s stored locally, or additional network bandwidth   if not.</li> <li>Figuring out the right policies: Also, while you don\u2019t necessarily need a   ton of data to get the right insights, you do need the right sampling, and   figuring that out can be challenging. You\u2019ll have to ask a lot of questions,   such as: what does the baseline for a healthy request look like? How many   resources are you able to set aside for tail sampling? Otherwise, you may   inadvertently filter out requests that could otherwise reveal issues in your   system, or consume more resources than you originally expected.</li> <li>Establishing a wait period for tail sampling: Another challenge with tail   sampling is that it\u2019s hard to predict when a trace will actually be finished.   Since a trace is essentially a graph of spans where the child spans reference   their parents, a new span could be added at any given time. To resolve this,   you can establish an acceptable period of time to wait before making a   sampling decision. The assumption here is that a trace should be complete   within the configured period, but it means that you risk potentially losing   interesting spans that complete outside that time window, which can result in   fragmented traces. Fragmented traces occur when spans are missing and can   create gaps in visibility.</li> </ul>"},{"location":"blog/2022/tail-sampling/#limitations-of-opentelemetry","title":"Limitations of OpenTelemetry","text":"<p>There are also some limitations to consider that are related to OpenTelemetry. Note that some of these limitations also apply more broadly to any client-hosted tail-based sampling solution, not just OpenTelemetry.</p> <p>First, you have to stand up a collector. While the collector can ultimately be quite practical in terms of centralizing configuration and processing data, it\u2019s one more piece in your system to implement and maintain. Furthermore, for tail sampling to work, all the spans of a particular trace have to be processed in the same collector, which leads to scalability challenges.</p> <p>For a simple setup, one collector will suffice, and no load balancing is needed. However, the more requests that are being held in memory, the more memory you\u2019ll need, and you\u2019ll also need additional processing and computing power to look at each span attribute. As your system grows, you can\u2019t do all this with just a single collector, which means you have to think about your collector deployment pattern and load balancing.</p> <p>Since one collector is insufficient, you have to implement a two-layer setup in which the collector is deployed in an agent-collector configuration. You also need each collector to have a full view of the traces it receives. This means that all spans with the same trace ID need to go to the same collector instance, or you\u2019ll end up with fragmented traces. You can use a load-balancing exporter if you're running multiple instances of the agent/collector with the tail sampling processor. This exporter ensures that all spans with the same trace ID end up in the same agent/collector. But there may be additional overhead with implementing this exporter, such as configuring it.</p> <p>Another limitation to consider is that since OpenTelemetry does not propagate metadata that would let a backend re-weight counts \u2013 for example, P95, P99, and total events \u2013 you\u2019re only getting a look at the measurements of sampled data, not accurate measurements in regards to all data. Let\u2019s say you\u2019ve configured your sampler to keep 25% of all traces. If the backend doesn\u2019t know it\u2019s only operating on 25% of all data, any measurement it produces will be inaccurate. One way to get around this is to attach metadata to your spans that tells the backend what the sample rate is, that will allow the backend to then accurately measure things such as total span count for a given period of time. The Sampling SIG is currently working on this concept.</p> <p>Finally, as OpenTelemetry is still an evolving project, many components are under active development, including the tail sampling processor and the collector. For the tail sampling processor, there is currently an open issue in the collector-contrib repo to discuss the future of this processor that centers around replacing it with separate processors, so that the chain of events is well-defined and understood. One of the main issues the community is trying to figure out is whether using separate processors to do tail sampling will be more performant than just the tail sampling processor. It's important to note that nothing will be released without a backward-compatible solution. For the collector, there are limited options to monitor the collector, which is critical for troubleshooting. See here for more information.</p> <p>A version of this article was [originally posted][] on the New Relic blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/","title":"Why and How eBay Pivoted to OpenTelemetry","text":"<p>eBay makes a crucial pivot to OpenTelemetry to better align with industry standards for Observability.</p> <p></p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#introduction","title":"Introduction","text":"<p>Observability provides the eyes and ears to any organization. A major benefit to observability is in preventing the loss of revenue by efficiently surfacing ongoing issues in critical workflows that could potentially impact customer experience. The Observability landscape is an ever-changing one and recent developments in the OpenTelemetry world forced us to rethink our strategy in order to pivot to using it. eBay\u2019s observability platform Sherlock.io provides developers and Site Reliability Engineers (SREs) with a robust set of cloud-native offerings to observe the various applications that power the eBay ecosystem. Sherlock.io supports the three pillars of observability \u2014 metrics, logs and traces. The platform\u2019s metric store is a clustered and sharded implementation of the Prometheus storage engine. We use the Metricbeat agent to scrape around 1.5 million Prometheus endpoints every minute, which are ingested into the metric stores. These endpoints along with recording rules result in ingesting around 40 million samples per second. The ingested samples result in 3 billion active series being stored on Prometheus. As a result, eBay\u2019s observability platform operates at an uncommonly massive scale, which brings with it new challenges.</p> <p>As an observability platform provider, eBay was one of the first companies to use agents to scrape metric endpoints and tail log files. As we have discussed in previous blog posts, we have heavily relied on the Elastic Beats offering to accept signals into the platform. Beats is a lightweight shipper of operational data like metrics and logs. For five years, from 2016 to 2020, we ran both Filebeat and Metricbeat as DaemonSets on all our Kubernetes clusters. DaemonSets allow users to deploy a given workload on every node on a Kubernetes cluster. However, an experiment performed during an internal hack week provided some surprising conclusions and led to us reconsidering our usage of DaemonSets. In this blog post, we discuss some of the problems we ran into, especially for metrics scraping, and how we evolved our own solution. We will also discuss in detail about how we have been navigating the evolving open source landscape with regards to licensing and how we intend to align with OpenTelemetry as an initiative.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#metrics-instrumentation","title":"Metrics Instrumentation","text":"<p>Metrics instrumentation at eBay has more or less been standardized on Prometheus endpoints. Endpoints from various applications are exposed as a result of a variety of instrumentation practices such as (but not limited to):</p> <ul> <li>Official Prometheus clients (including Java, Go, Python and others)</li> <li>Micrometer</li> <li>OTel SDK with Prometheus exporter</li> <li>Custom code that emits a Prometheus endpoint when requested</li> </ul> <p>Frameworks offered by eBay\u2019s platform engineering group bake in an instrumentation client and also expose various metric endpoints that represent server-side, client-side and DB client metrics. Depending on the nature of the application, Prometheus endpoints can be exposed, which require scraping. The application owner can also expose an endpoint of their own to instrument their business KPIs.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#autodiscover","title":"Autodiscover","text":"<p>Most of the applications that power the eBay ecosystem run on Tess, eBay\u2019s internal Kubernetes provider. eBay runs hundreds of Tess-powered Kubernetes clusters, and an application can run on any number and combination of those clusters. Application owners can choose to onboard their application metrics along with the metrics that are freely available from the framework-level instrumentation. Our agents need to know exactly which endpoints are being exposed by the currently running Kubernetes Pod. In order to provide the agent this information, we helped enrich the Beats platform to perform a task called \u201chints based autodiscover.\u201d Autodiscover is a Beats construct that allows a dynamic source like the Kubernetes API server to deliver to the agent information such as:</p> <ul> <li>What is the endpoint that needs scraping?</li> <li>What kind of endpoint \u2014 Dropwizard, Prometheus, Foobar, something else \u2014 is   it?</li> <li>How often should it be scraped?</li> <li>Is there any other additional information that the agent should be aware of,   like an SSL certificate?</li> </ul> <p>With more and more complex discovery patterns required, we worked with the Beats open source community to enhance the power of autodiscover for our specific needs. Some of the features we contributed include:</p> <ul> <li>Discovering multiple sets of configurations:   Conventional annotation-based scraping is very limiting in that it only allows   the user to provide simple configurations for the scrape manager to target.   Given that each endpoint can have dynamic needs like varied processing and   scrape intervals, we enhanced autodiscover to accept more than one set of   configurations.</li> <li>Discovering targets from namespace annotations:   The prescribed method to announce scrape targets calls for adding annotations   to the Pod spec. However, adding them there would mean that a change would   incur a Pod restart. This is undesirable if the change is intended for a   metric that is being instrumented on the framework and is available on every   deployed application. To avoid restarting all Pods, we added support for   autodiscover to additionally look at namespace-level annotations.</li> </ul> <p>These features make Beats Autodiscover one of the more versatile and feature-rich discovery mechanisms for identifying targets deployed on Kubernetes clusters.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#limitations-of-metrics-scraping-via-daemonsets","title":"Limitations of Metrics Scraping via DaemonSets","text":"<p>Our first attempt to run Metricbeat at scale involved running it as DaemonSets on each and every Kubernetes cluster. Each Pod was given one CPU and 1GB of memory to handle all metrics exposed on that node. When Metricbeat starts up, it requests the API server for all the namespaces on that cluster along with Pods deployed on the node it runs on. With this information, for each Pod, it creates configurations by collating annotations on the Pod and the Pod\u2019s namespace. Some of the observations we observed as a result include:</p> <ul> <li>Resource fragmentation: Given that we run N beats per N node cluster, if a   single Beat pipeline takes up 50MB of bootstrapping cost, we essentially waste   50*N MB of resources. That adds up to 150GB on a 3000 node Kubernetes   cluster!</li> <li>OOM issues when large endpoints are polled: We have seen customers expose   endpoints as large as 150,000 entries per endpoint. Some gigantic endpoints   like \"kube-state-metrics\" reach three million entries, and generate 600MB of   data with each poll. Scraping becomes unreliable when such use cases land on a   node.</li> </ul> <p>The following diagram represents how any Beats instance, like Metricbeat, Filebeat and Auditbeat, would interface with the Sherlock.io platform when deployed as a DaemonSet:</p> <p></p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#moving-to-cluster-local-scrapes","title":"Moving To Cluster Local Scrapes","text":"<p>During work on an unrelated project, we took the shortcut of running Metricbeat as a single instance for all targets across the cluster. When we observed the total CPU plus memory usage for running Metricbeat, the numbers were simply stunning. We saw the following during the deployment:</p> <ul> <li>Number of Kubernetes nodes: 2851</li> <li>CPU usage: 29 cores</li> <li>Memory usage: 57GB</li> <li>Ingest rate: 238K samples per second</li> <li>Endpoints monitored per node: 4</li> <li>Average memory usage per node monitored: 20MB</li> <li>Average CPU usage per node monitored: 0.01 cores</li> </ul> <p>A single metricbeat instance monitoring a similar amount of endpoints on a node in DaemonSet mode was consuming about 200MB (10x) and approximately 0.6 cores (60x). Across the cluster, that would accumulate to 570GB and around 1700 CPUs. The overall savings by moving to a cluster local instance was roughly 90%.</p> <p>This forced us to rethink the approach of handling scrapes. Running a single instance for an entire cluster would mean that when the instance goes through an upgrade or failure, 100% of scrapes would be down at that point in time. In order to mitigate failures, we deployed Metricbeat as a statefulset with N replicas. The entire list of pods are sharded N-ways based on the number of Metricbeat instances, and each instance monitors its assigned shard:</p> <p><code>xxHash(pod uid) % statefulset_size == instance number</code></p> <p>Each instance makes a full scan against the API server, but ignores everything except what it alone is supposed to monitor. This model works well for Metricbeat, as it primarily scrapes Prometheus endpoints, and this activity can happen outside of the Tess node. A large 3000 node Kubernetes cluster has as many as 30 instances with higher number of CPUs and memory allowing it to scrape substantially larger endpoints than it would as a daemon on the node. If one Metricbeat instance goes through a restart, scraping blips only for endpoints that are monitored by that instance alone, the failure percentage is reduced to 1/Nth of the total number of instances.</p> <p>The new deployment pattern can be visualized as follows:</p> <p></p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#decoupling-autodiscover","title":"Decoupling Autodiscover","text":"<p>While moving to cluster local scrapes got us to scale much higher than when using DaemonSets, the model had room for improvement. A new set of problems arose, especially on larger clusters, with higher Pod density. Given that every Metricbeat instance has to scan all Pods and pick the ones that it needs to monitor, depending on how many Pods are present in a given cluster, it can take a very long time to do the initial scan. This became extremely problematic during roll-outs when a new Metricbeat Pod would take as long as 10 minutes to restart scrapes once it came back up. Depending on the number of instances, this technique also puts undue strain on the API server due to the number of WATCHes placed against it for the various resources Metricbeat requests.</p> <p>After further evaluation, we decided to move Autodiscover out of the agent and into its own control loop. The control loop would:</p> <ul> <li>Implement a similar parser to the Beats autodiscover logic;</li> <li>Discover all the agents that can do the work of scraping;</li> <li>Pick one of these agents;</li> <li>And pass a configuration to the selected agent to monitor the target.</li> </ul> <p>The control loop would make important decisions like shuffling workloads around in the event of agent crashes, agent over-allocation and other failure scenarios. Given that the logic to parse annotations has been decoupled from the agent, it is very simple to generate configurations for any agent as long as there exists a mapping between a feature that Beats exposes versus the new agent.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#advent-of-opentelemetry","title":"Advent of OpenTelemetry","text":"<p>In 2019, the Open Tracing and Open Census communities agreed to join forces and bring about OpenTelemetry. The OpenTelemetry initiative set out with a goal to provide vendor agnostic APIs, SDKs and tools for ingesting, transforming and sending data to any Observability backend. Investing in such an initiative seemed a natural fit to how we consume Open Source at eBay, given our choice of Kubernetes, which also provides vendor-agnostic APIs to manage containers on the cloud. In 2021, we started experimenting with distributed tracing to figure out how it might be useful for our developers.</p> <p>At the time, looking at the OpenTelemetry Collector codebase, we saw significant potential in some of its features, including defined types for metrics, logs and traces, and usage of the Prometheus scrape manager to collect metrics from OpenMetrics endpoints. We chose OpenTelemetry Collector, along with the OpenTelemetry SDK, for our adoption of distributed tracing. It only made sense that we should subsequently figure out how we would move metrics and logs collection into the OpenTelemetry Collector. This wouldn\u2019t be an easy effort, given that we need to plug all feature gaps, build relationships with a new open source community and swap a massive metrics collection infrastructure without downtime. In the beginning of 2022, we began the difficult undertaking of moving metrics scraping into OpenTelemetry Collector.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#the-migration","title":"The Migration","text":"<p>Given that we decoupled our discovery logic from the agent, actual migration only meant that we needed to generate configurations that the OpenTelemetry Collector could understand. These configurations had to be pushed every time a new Pod is spun up and cleaned up when that Pod dies. However, the OpenTelemetry Collector has a crucial gap: it can\u2019t dynamically reload configurations. The OpenTelemetry Collectors have the notion of \u201cpipelines\u201d that define how a metric needs to be received, processed and exported. To facilitate dynamic reloading of pipelines, we came up with a \u201cfilereloadreceiver\u201d that can look at a directory consisting of files describing \u201cpartial pipelines\u201d that plug into the overall pipeline of the collector. Each Pod that requires metrics scraping has a partial pipeline that the autodiscover controller generates and pushes into the collector. Yet another complex exercise during this process was the creation of a mapping table between every feature that we depended on in the Beats platform and the OpenTelemetry Collector. For example, fields in Beats would translate to using the attribute processor on OpenTelemetry. With such mappings and the filereloadreceiver in place, we were able to generate new configurations for the OpenTelemetry Collector as follows.</p> <p></p> <p>As shown above, we were able to keep the end user contract of Pod/Namespace annotations the same and simply swap the agent under the hood. This greatly simplified the actual task of rolling out the new agent. The final obstacle involved mismatches in semantics between Elastic Beats, OpenTelemetry and sometimes even the Prometheus scrape manager. This is where we spent months before we could finally replace all of Metricbeat in production. Some of the discrepancies we saw and helped patch up on the OpenTelemetry Collector project include:</p> <ul> <li>Align sanitizing labels and metric names that start with \u201c_\u201d with Prometheus</li> <li>Ability to disable label sanitization</li> <li>Correctly handle metric names starting with \u201c:\u201d</li> <li>Ability to extract pod labels using regular expressions</li> </ul> <p>These issues proved difficult to catch, and sometimes only surfaced when we attempted to upgrade a Kubernetes cluster to use OpenTelemetry Collector. Once we hit such an issue, rollback was the only option and we were forced to go back to the drawing board. One partial solution involved writing a comparison script that can scrape an endpoint using Metricbeat and OpenTelemetry Collector, simultaneously ingest them to the metric store and compare the metric name and labels to ensure that the scrapes are on par with each other. This greatly improved our confidence in moving forward.</p> <p>Sometimes moving forward simply means dropping support for certain features. We did just that with support for Dropwizard metrics and had users migrate away from the same. Outside of semantic differences, we are also actively working on adding features that we feel are critical for the project, like supporting Exemplars.</p> <p>After months of hard work and support from the community, we are happy to announce that we have fully decommissioned Metricbeat and replaced it with OpenTelemetry Collector. We are busy working on doing the same for Filebeat at this point and early performance benchmarks are very promising. We have so far made 10+ contributions to the project, but it is just the start to a very fruitful collaboration.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#conclusion","title":"Conclusion","text":"<p>Over the past five years, we at eBay have encountered several spikes in demand that forced us to rethink conventional wisdom. We started off with DaemonSets, and saw that it was cost-prohibitive and unreliable at scale. We moved to a cluster local model which cut the cost of agents by about 90 percent, but we had redundancies in the amount of work being done on the API server and the agents themselves. We decoupled the discovery, moved it into a control loop that performs scheduling and made the agents into stateless processes that can accept scrape targets. Given the growing maturity of OpenTelemetry we pivoted to using OpenTelemetry Collector for metrics and are actively working towards doing the same for logs. We will continue to learn from running agents at scale and keep pivoting as needed. We will continue to work with the OpenTelemetry community as it continues to pave the way for standardization within the Observability ecosystem. Now that we\u2019re using OpenTelemetry, we can provide our developers with an industry-approved open standard to send telemetry into Sherlock.io. As the community grows to offer support for newer features like profiling, we will adopt them into the platform to benefit our developer community.</p>"},{"location":"blog/2022/why-and-how-ebay-pivoted-to-opentelemetry/#credits","title":"Credits","text":"<p>A lot of these optimizations/directional changes would not be possible without the many thought leaders who have been involved in these activities:</p> <ul> <li>Premendra Singh</li> <li>Peter Deng</li> <li>Aishwarya Yandapalli</li> <li>Santanu Bhattacharya</li> <li>John Feldmeier</li> <li>Rami El-Charif</li> </ul> <p>We are extremely grateful to both the Elastic Beats community of the past and the present OpenTelemetry community for supporting and working with us as we strive to build world-class Observability offerings for our eBay developer community.</p> <p>Elastic community:</p> <ul> <li>Monica Sarbu</li> <li>Tudor Golubenco</li> <li>Nicolas Ruflin</li> <li>Steffen Siering</li> <li>Carlos P\u00e9rez-Aradros</li> <li>Andrew Kroh</li> <li>Christos Markou</li> <li>Jaime Soriano Pastor</li> </ul> <p>OpenTelemetry Collector community:</p> <ul> <li>Tigran Nigaryan</li> <li>Bogdan Drutu</li> <li>David Ashpole</li> <li>Anthony Mirabella</li> <li>Juraci Paix\u00e3o Kr\u00f6hling</li> <li>Albert Teoh</li> </ul> <p>A version of this article was [originally posted][] on the eBay Tech Blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2023/ecs-otel-semconv-convergence/","title":"Announcing the Elastic Common Schema (ECS) and OpenTelemetry Semantic Convention Convergence","text":"<p>Today, we're very excited to make a joint announcement with Elastic about the future of Elastic Common Schema (ECS) and the OpenTelemetry Semantic Conventions.</p> <p>The goal is to achieve convergence of ECS and OTel Semantic Conventions into a single open schema that is maintained by OpenTelemetry, so that OpenTelemetry Semantic Conventions truly is a successor of the Elastic Common Schema. OpenTelemetry shares the same interest of improving the convergence of observability and security in this space. We believe this schema merge brings huge value to the open source community because:</p> <ul> <li>ECS has years of proven success in the logs, metrics, traces and security   events schema, providing great coverage of the common problem domains.</li> <li>ECS provides schema for security domain fields, which is an important aspect   of telemetry.</li> <li>Converging two separate standards into one single standard will help to boost   the ecosystem (e.g. instrumentation libraries, tools and consumption   experiences), which benefits both the telemetry producers and consumers.</li> <li>This joint effort would benefit from domain experts across logging,   distributed tracing, metrics and security events. As a result, we expect to   have more consistent signals across different pillars of observability and   security events.</li> </ul> <p>Both Elastic and the OpenTelemetry community understand that converging two widely used standards into one singular common schema, and having a smooth transition is critical for users. A dedicated OpenTelemetry Semantic Convention working group will be created with domain experts from both Elastic and OpenTelemetry joining. We're also welcoming domain experts who are passionate about data schemas and semantic conventions to join us. If you're interested in contributing, join our OTel Semantic Conventions working group, and join the discussion on our Slack channel.</p>"},{"location":"blog/2023/ecs-otel-semconv-convergence/#references","title":"References","text":"<ul> <li>Announcement from Elastic</li> <li>OpenTelemetry Semantic Conventions</li> <li>OTEP 199: Merge Elastic Common Schema with OpenTelemetry Semantic   Conventions</li> <li>OTEP Issue 197: Proposal: Add support for Elastic Common Schema (ECS) in   OpenTelemetry</li> <li>OTEP Pull Request 199: Support Elastic Common Schema in OpenTelemetry</li> <li>OTEP Pull Request 222: Support Elastic Common Schema (ECS) in   OpenTelemetry</li> </ul>"},{"location":"blog/2023/end-user-discussions-01/","title":"OpenTelemetry End-User Discussions Summary for January 2023","text":"<p>With contributions from Henrik Rexed (Dynatrace), Michael Hausenblas (AWS), Pranay Prateek (Signoz), Rynn Mancuso (Honeycomb), and Reese Lee (New Relic).</p> <p>Each month, users in the OpenTelemetry (OTel) community gather to talk about how they use OpenTelemetry in real life. Sessions are held for users in the Americas (AMER), Europe Middle-East &amp; Africa (EMEA), and Asia-Pacific (APAC). The discussions take place using a Lean Coffee format, whereby folks are invited to post their topics to the Agile Coffee board like this one, and everyone in attendance votes on what they want to talk about.</p> <p>This is a great way to meet other users in the OpenTelemetry community, and to learn about and share practical experience on how OpenTelemetry is being used in the wild. Each meeting is attended by an OTel Governance Committee member and/or maintainer to help answer questions, listen to user feedback, and provide additional context and insight into the topics discussed.</p> <p>This is the first in a series of blog posts summarizing our monthly OTel End User Discussions, starting with our January 2023 sessions.</p>"},{"location":"blog/2023/end-user-discussions-01/#what-we-talked-about","title":"What we talked about","text":"<p>We saw a few common themes this month across our three sessions:</p> <ul> <li>OpenTelemetry adoption &amp; enablement</li> <li>Connectors   (Collector)</li> <li>Service Graph Processor   (Collector)</li> <li>Signal correlation (e.g., metrics/traces correlation, logs/traces correlation)</li> </ul> <p>We\u2019ll dig into these and more!</p>"},{"location":"blog/2023/end-user-discussions-01/#discussion-highlights","title":"Discussion highlights","text":"<p>Below is a summary of this month\u2019s discussions.</p>"},{"location":"blog/2023/end-user-discussions-01/#opentelemetry-collector","title":"OpenTelemetry Collector","text":""},{"location":"blog/2023/end-user-discussions-01/#1-opentelemetry-transformation-language-ottl","title":"1- OpenTelemetry Transformation Language (OTTL)","text":"<p>Q: Will exporters support OTTL (a language for transforming OpenTelemetry data)? Use case: data needs to be transformed, but don\u2019t want to do it in a processor.</p> <p>A: Due to separation of concerns, it is unlikely that OTTL will be added to exporters; however, this may be a use case for either connectors (a new Collector component that acts as an exporter/receiver pair to join pipelines together) or the routing processor. A routing processor reads data from an HTTP request or attribute, and routes it to a specified exporter.</p>"},{"location":"blog/2023/end-user-discussions-01/#2-service-graph-processor","title":"2- Service Graph Processor","text":"<p>Q: How can OpenTelemetry be used to generate a service graph, generate metrics, and send the data to a visualization tool?</p> <p>A: The Service Graph Processor generates a service graph. This processor is still in alpha, and as a result, some known issues around the Service Graph regarding the dependency mapping. One span doesn\u2019t have the entire context, and in order to get the complete picture, you will have to send the spans to a centralized service.</p>"},{"location":"blog/2023/end-user-discussions-01/#3-bifurcating-data-in-a-pipeline","title":"3- Bifurcating data in a pipeline","text":"<p>Q: If I want to use the Collector to send different sets of data to different back-ends, what\u2019s the best way to go about it?</p> <p>A: Connectors (a new Collector component that acts as an exporter/receiver pair to join pipelines together) can be used to solve this. Connectors will be launching soon. For more info, see the Connector PR here.</p> <p>Another approach would be to use a routing processor. A routing processor reads data from an HTTP request or attribute, and routes it to a specified exporter. This is done by making new network connections, which can make this approach inefficient.</p>"},{"location":"blog/2023/end-user-discussions-01/#4-managing-time-drift-in-telemetry-data","title":"4- Managing time drift in telemetry data","text":"<p>Q: When clocks on servers are not in sync, you can end up with some data points being recorded in the future. Can something be implemented on the OTel Collector to mitigate this?</p> <p>A: Clock skew is always going to happen. There is no way for clocks to be synchronized, especially in microservices architectures. The owner of the system that generates the telemetry is in a better position to understand the clock nuances. The Collector is not suited to address this.</p>"},{"location":"blog/2023/end-user-discussions-01/#5-advanced-collector-deployment-and-configuration","title":"5- Advanced Collector deployment and configuration","text":"<p>Q: When should I be horizontally scaling my pod vs modifying config of an individual collector? When do I add more collectors or change collector config?</p> <p>A: There are a few things to consider when deploying and configuring Collectors.</p> <ul> <li>If you only have stateless components in your Collector, you can scale (add   more replicas) based on metrics.</li> <li>You may want to shard your pipelines based on the type of processing that   you\u2019re doing. For example, creating one metrics pipeline, one logs pipeline,   and one traces pipeline, because the workload for each of these pipelines is   different.</li> <li>You might want to split your Collectors based on the type of data being   processed. If there\u2019s one namespace where there\u2019s more data that comes in with   personally identifiable information (PII),   you might want to have a dedicated Collector for that namespace that uses the   attributes processor.</li> </ul>"},{"location":"blog/2023/end-user-discussions-01/#opentelemetry-adoption-enablement","title":"OpenTelemetry Adoption &amp; Enablement","text":"<p>Q: So you\u2019ve decided to go with OpenTelemetry at your organization\u2026now what? What\u2019s the best way to promote OpenTelemetry adoption, and get developers excited about using OpenTelemetry, without overwhelming them?</p> <p>A: Some suggestions from the community:</p> <ul> <li>Find folks who are willing to be OpenTelemetry champions</li> <li>Pair developers new to OpenTelemetry with those who are more familiar with it</li> <li>The real value of OpenTelemetry won\u2019t be seen until you instrument a few   services, to see how things are stitched together.</li> <li>Developers must be mentally ready to start instrumenting their code. Keep in   mind that it may mean going into existing code to instrument it.</li> <li>A \u201cbig bang\u201d approach may not be the best way to adopt OpenTelemetry, as it   may be too overwhelming for an organization. Start with a component or two.</li> </ul>"},{"location":"blog/2023/end-user-discussions-01/#opentelemetry-language-api-sdks","title":"OpenTelemetry Language API &amp; SDKs","text":""},{"location":"blog/2023/end-user-discussions-01/#1-new-language-instrumentation","title":"1- New language instrumentation","text":"<p>Q: How do you find information on OTel implementations for different languages, for example, Dart and Lua?</p> <p>A: CNCF Slack is always a good place to start your search. There are language-specific channels, which follow the naming convention otel-&lt;language_name&gt;. If you don\u2019t find a channel for your language, feel free to start a discussion on the OpenTelemetry CNCF Slack channel, or on GitHub, like with this issue for OTel for Perl. Please also check out this page for more info.</p>"},{"location":"blog/2023/end-user-discussions-01/#2-python-instrumentation","title":"2- Python instrumentation","text":"<p>Q: How mature is auto-instrumentation for Python and what has been the experience of folks working with OpenTelemetry Python?</p> <p>A: Python auto-instrumentation is in beta; however, there are companies using OTel Python in production, so it likely won\u2019t cause any issues in prod. As a SIG, OTel Python tries to minimize shipping breaking changes, but as with everything, there is no guarantee that there will be no breaking changes. There is no firm timeframe on when Python instrumentation will be marked as stable.</p>"},{"location":"blog/2023/end-user-discussions-01/#misc-items","title":"Misc Items","text":""},{"location":"blog/2023/end-user-discussions-01/#1-opentelemetry-exemplars","title":"1- OpenTelemetry exemplars","text":"<p>Q: Where can users learn more about Exemplars and how they are being used in the real world?</p> <p>A: Exemplars are used to correlate OpenTelemetry metrics to traces. Exemplars are currently in the early stages of development, and more work still needs to be done. For more on the state of exemplars, check out the #otel-metrics channel on CNCF Slack. Please also check out Michael Hausenblas\u2019 recent talk on this topic.</p>"},{"location":"blog/2023/end-user-discussions-01/#2-correlation-between-traces-and-logs","title":"2- Correlation between traces and logs","text":"<p>Q: Is there a way to more easily correlate traces to logs?</p> <p>A: Implementing correlation takes time and is a work in progress. Correlation work is more mature for some languages (e.g. Java, Go) than for others. The best approach is to raise this issue in one of the language-specific repos that pertains to your situation. A possible work-around is to start traces at the log level, whereby every log will have its own associated trace.</p>"},{"location":"blog/2023/end-user-discussions-01/#3-profiling","title":"3- Profiling","text":"<p>Q: What is the status of Profiling in OpenTelemetry?</p> <p>A: There is an OTel proposal on profiling, which has been accepted and is being actively being worked on and discussed. The current focus is on finalizing the protocol, before SDK work can start. You can check out the profiling repo on GitHub, as well as the Profiling Vision pull request on GitHub.</p>"},{"location":"blog/2023/end-user-discussions-01/#4-context-propagation","title":"4- Context propagation","text":"<p>Q: Browsers cannot track context propagation automatically, and must therefore be done manually. Current workarounds have come with a lot of overhead. How can this be addressed?</p> <p>A: The way to address this is to join the JavaScript SIG and to raise the issue there. If anyone is actively working on an API to solve this internally, it would be great to contribute this back to the OTel community.</p>"},{"location":"blog/2023/end-user-discussions-01/#meeting-notes-recordings","title":"Meeting Notes &amp; Recordings","text":"<p>For a deeper dive on the above topics, check out the following:</p> <ul> <li>AMER   meeting notes +   Session Recording</li> <li>EMEA   meeting notes</li> <li>APAC   meeting notes</li> </ul> <p>Going forward, we will be recording all End-User Discussion meetings.</p>"},{"location":"blog/2023/end-user-discussions-01/#join-us","title":"Join us!","text":"<p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End-User Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/end-user-discussions-02/","title":"OpenTelemetry End-User Discussions Summary for February 2023","text":"<p>With contributions from Henrik Rexed (Dynatrace), Michael Hausenblas (AWS), Rynn Mancuso (Honeycomb), Reese Lee (New Relic), and Adriana Villela (Lightstep).</p> <p>The OpenTelemetry end-user group meet takes place every month for users in the Americas (AMER), Europe Middle-East &amp; Africa (EMEA), and Asia-Pacific (APAC).</p> <p>The discussions take place using a Lean Coffee format, whereby folks are invited to post their topics to the Agile Coffee board like this one, and everyone in attendance votes on what they want to talk about.</p>"},{"location":"blog/2023/end-user-discussions-02/#what-we-talked-about","title":"What we talked about","text":"<p>Some interesting topics that were discussed this month were:</p> <ul> <li>Sampling for traces</li> <li>Emitting business metrics</li> <li>Monitoring the health of OpenTelemetry Collector</li> <li>Backup/Buffer capabilities of OTel Collector</li> </ul>"},{"location":"blog/2023/end-user-discussions-02/#discussion-highlights","title":"Discussion Highlights","text":"<p>Below is the summary of this month's discussions.</p>"},{"location":"blog/2023/end-user-discussions-02/#opentelemetry-collector","title":"OpenTelemetry Collector","text":""},{"location":"blog/2023/end-user-discussions-02/#1-monitoring-otel-collectors-health","title":"1 - Monitoring OTel Collector's health","text":"<p>Q: Are there any suggestions for monitoring OTel Collector's health or patterns for collecting agent telemetry?</p> <p>A: Collectors can be used to collect telemetry data from other Collectors, which doesn't really need to be a disparate telemetry system. Users should also think about collecting multiple signals so that even if one signal fails, they get alerted by another. Here's an article discussing this.</p>"},{"location":"blog/2023/end-user-discussions-02/#2-timeline-for-opamp-extension","title":"2 - Timeline for OpAMP extension","text":"<p>Q: Is there any timeline for implementing the OpAMP spec for agent management?</p> <p>A: It's not a top priority as of now. It would be good to have a maintainer from the community for OpAMP. To track progress, see issue #16462.</p>"},{"location":"blog/2023/end-user-discussions-02/#3-buffer-capabilities-of-otel-collector","title":"3 - Buffer capabilities of OTel Collector","text":"<p>Q: What are some backup/retry buffer capabilities of OTel Collector when endpoints are unavailable?</p> <p>A: There is an experimental storage extension that is currently under development to support buffering and data persistence.</p>"},{"location":"blog/2023/end-user-discussions-02/#4-periodically-profiling-collectors-to-improve-performance","title":"4 - Periodically profiling Collectors to improve performance","text":"<p>Q: Is there any effort to periodically profile the Collector and improve performance on an ongoing basis?</p> <p>A: There is a GitHub action that runs load test on OpenTelemetry Collector, but no one is working to improve it.</p>"},{"location":"blog/2023/end-user-discussions-02/#opentelemetry-language-api-sdks","title":"OpenTelemetry Language API &amp; SDKs","text":""},{"location":"blog/2023/end-user-discussions-02/#1-timeline-for-go-sdk","title":"1 - Timeline for Go SDK","text":"<p>Q: What is the timeline for full specification compliance for OTel Go SDK?</p> <p>A: In the Go OTel SDK, the current progress is mostly around metrics. The logging development is frozen. Major work is being done on the metrics SDK. To track progress on Go metrics, see the Metric project tables. Once metrics are done, logs will be taken care of.</p>"},{"location":"blog/2023/end-user-discussions-02/#opentelemetry-traces","title":"OpenTelemetry Traces","text":""},{"location":"blog/2023/end-user-discussions-02/#1-sampling-for-traces","title":"1 - Sampling for traces","text":"<p>Q: Is there a way to sample traces based on the span counts? Example: Drop/truncate traces which have more than 1000 spans in a single trace.</p> <p>More context: Sometimes, due to issues in the application itself, some traces generate a lot of spans that are not needed. Is there a way in OpenTelemetry to control this? More specifically, is there a way in which we can set up a condition where if a certain trace has more than \u2018n\u2019 number of spans, we can drop or truncate the number of spans.</p> <p>A: Tail-based sampling processor provides users with a bunch of sampling policies. Span count is one such policy. You can also combine multiple policies. Here's the link to tail sampling processor. The span count policy is based on min span count. Some users might look for some kind of exclusion policy.</p>"},{"location":"blog/2023/end-user-discussions-02/#2-use-cases-of-span-links","title":"2 - Use cases of span links","text":"<p>Q: What are the use cases of span links?</p> <p>A: Span links are used for implying a causal relationship between one or more spans. It was a part of the original traces specification, and its status is now stable. It can be used to link traces that are related but runs asynchronously.</p> <p>For example, span links can be used in batched operations to link spans initiated by multiple initiating spans. Spans can have many-to-many mappings via links. Jaeger supports span links in its UI.</p>"},{"location":"blog/2023/end-user-discussions-02/#opentelemetry-metrics","title":"OpenTelemetry Metrics","text":""},{"location":"blog/2023/end-user-discussions-02/#1-supporting-other-metrics-format","title":"1 - Supporting other metrics format","text":"<p>Q: Can OTel Collector support metrics generated from other libraries like statsd library?</p> <p>A: The OpenTelemetry Collector contrib has a lot of receivers for different types of metrics that can be used. For example, if you are sending out metrics in Prometheus format, you can configure your OTel Collector to scrape Prometheus metrics. There is also a statsd receiver that is available. If you have something that is already working, then you don\u2019t need to change it. You can check the list of receivers here.</p>"},{"location":"blog/2023/end-user-discussions-02/#2-emitting-business-metrics","title":"2 - Emitting business metrics","text":"<p>Q: What signals are you using to emit business metrics? For instance, at an arbitrary point in time, emit something that resembles a counter but only emit it once.</p> <p>A: There is a current issue regarding this which you can track. An example of business metric can be users landing on a particular page which can be tracked with a counter.</p>"},{"location":"blog/2023/end-user-discussions-02/#opentelemetry-adoption-enablement","title":"OpenTelemetry Adoption &amp; Enablement","text":""},{"location":"blog/2023/end-user-discussions-02/#1-improving-contributions-from-apac-region","title":"1 - Improving contributions from APAC region","text":"<p>Q: How do we improve contributions from APAC region?</p> <p>A: Suggestions from the community:</p> <ul> <li>Reach out to current OpenTelemetry maintainers and share the challenges</li> <li>Create a list of maintainers from APAC region to whom people can reach to</li> <li>Local in-person meetups for OpenTelemetry users</li> <li>A good place to start would be <code>good first issues</code> in any of the OTel repos,   and ask for help in GitHub issues</li> <li>Join   OTel slack community   and ping in relevant channels</li> </ul>"},{"location":"blog/2023/end-user-discussions-02/#other-important-discussion-points","title":"Other Important discussion points","text":"<p>The community also discussed these important points:</p>"},{"location":"blog/2023/end-user-discussions-02/#auto-discovery-of-sources-to-collect-telemetry-data","title":"Auto-discovery of sources to collect telemetry data","text":"<p>Q: Can OTel Collector automatically discover known sources and collect telemetry from them?</p> <p>A: The idea is to let OTel Collector self-configure itself to collect telemetry from known sources. Prometheus has automatic service discovery in Kubernetes. Currently, there is nothing in the Collector which solves this.</p> <p>There is a receiver creator which can instantiate other receivers at runtime based on whether an observed endpoint matches a configured rule. To use the receiver creator, you must first configure one or more observers. Using Kubernetes observer, users should be able to detect and report Kubernetes pod, port, and node endpoints via the Kubernetes API.</p>"},{"location":"blog/2023/end-user-discussions-02/#hosting-pattern-suggestion-of-the-otel-collector-within-azure","title":"Hosting pattern suggestion of the OTel Collector within Azure","text":"<p>Q: Are there any suggestions for hosting pattern of the Collector within Azure to collect telemetry from Azure App Services and Azure functions?</p> <p>A: Usually, the community relies on folks from Microsoft to provide best practices. There is some issue with the latest version of OTel and Azure functions. You can track it here.</p>"},{"location":"blog/2023/end-user-discussions-02/#meeting-notes-recordings","title":"Meeting Notes &amp; Recordings","text":"<p>For a deeper dive into the above topics, check out the following:</p> <ul> <li>AMER   meeting notes</li> <li>EMEA   meeting notes</li> <li>APAC   meeting notes</li> </ul>"},{"location":"blog/2023/end-user-discussions-02/#join-us","title":"Join us!","text":"<p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End-User Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/end-user-discussions-03/","title":"OpenTelemetry End-User Discussions Summary for March 2023","text":"<p>With contributions from Henrik Rexed (Dynatrace), Michael Hausenblas (AWS), Rynn Mancuso (Honeycomb), Adriana Villela (Lightstep), and Pranay Prateek (SigNoz).</p> <p>The OpenTelemetry end-user group meet takes place every month for users in the Americas (AMER), Europe Middle-East &amp; Africa (EMEA), and Asia-Pacific (APAC).</p> <p>The discussions take place using a Lean Coffee format, whereby folks are invited to post their topics to the Agile Coffee board like this one, and everyone in attendance votes on what they want to talk about.</p>"},{"location":"blog/2023/end-user-discussions-03/#what-we-talked-about","title":"What we talked about","text":"<p>Sampling and collector capabilities continue to be topics of interest, along with questions about instrumentation and adoption.</p>"},{"location":"blog/2023/end-user-discussions-03/#discussion-highlights","title":"Discussion Highlights","text":"<p>Below is the summary of this month's discussions.</p>"},{"location":"blog/2023/end-user-discussions-03/#opentelemetry-collector","title":"OpenTelemetry Collector","text":""},{"location":"blog/2023/end-user-discussions-03/#1-losing-grpc-with-azure-app-services","title":"1 - Losing gRPC with Azure App Services","text":"<p>Q: When looking at the hosting models in Azure for the OTel Collector, only HTTP is supported (for running in Azure App Service). What are the risks associated with losing gRPC capability?</p> <p>A: If HTTP2 is supported in Azure, gRPC might work there, since gRPC is HTTP under the hood with extra complications built on top of HTTP2. One suggestion is to follow up with Microsoft about gRPC support, as it may have very long-running connections.</p>"},{"location":"blog/2023/end-user-discussions-03/#2-uptime-monitoringsynthetics","title":"2 - Uptime monitoring/synthetics","text":"<p>Q: Does the OTel Collector have the capability to do uptime monitoring/ synthetics? If not, are there any plans to work towards such a thing?</p> <p>A: The health check might be a helpful reference. Also check out the HTTP check receiver.</p>"},{"location":"blog/2023/end-user-discussions-03/#3-collector-distributions","title":"3 - Collector distributions","text":"<p>Q: Should I use a vendor distribution versus the community collector distribution?</p> <p>A: Each vendor distribution will come with customizations, whereas the community Collector distribution will include everything: receivers and exporters. If you need the flexibility, then you should use the OTel Collector distro.</p>"},{"location":"blog/2023/end-user-discussions-03/#4-rate-limiting-on-receivers","title":"4 - Rate limiting on receivers","text":"<p>Q: Are there any plans for enabling rate limiting and circuit breaks on receivers? Imagine having lots of clients sending telemetry to the same set of OTel collectors.</p> <p>More context: How do I rate-limit in a situation where I have collectors for traces, metrics, and logs, and I\u2019m receiving traffic from more than 100 individual apps? If I have even one customer who is generating heavy traffic, it might impact the overall health of my collectors.</p> <p>A: Use a reverse proxy. Something to note is that once the data is inside the collector, the data is already being deserialized, and you\u2019ve already started firehosing the collector, so it\u2019s a bit late to rate limit at that point. One approach might be to add additional headers when you configure your SDKs that contain the additional information, which would help with load balancing.</p>"},{"location":"blog/2023/end-user-discussions-03/#5-connectors","title":"5 - Connectors","text":"<p>Q: What is a connector?</p> <p>A: A connector is a collector component that consumes telemetry signals as an exporter in one pipeline, and emits it as a receiver in another pipeline. Read more here.</p>"},{"location":"blog/2023/end-user-discussions-03/#6-definitions-of-upstream-downstream-and-distro","title":"6 - Definitions of upstream, downstream, and distro","text":"<p>Q: What is upstream? Downstream? Distro?</p> <p>A: The terms \"upstream\" and \"downstream\" refer to how services or components in a system are connected to each other. Check out this article for more information as it applies to different situations in software.</p> <p>The term \"distro\" is short for distribution. For a list of vendors that provide distros, see Vendors.</p>"},{"location":"blog/2023/end-user-discussions-03/#sampling","title":"Sampling","text":""},{"location":"blog/2023/end-user-discussions-03/#1-tail-sampling","title":"1 - Tail sampling","text":"<p>Q: What are the perceived downsides of tail sampling, for example, on all HTTP requests that have errors or long latencies, instead of just relying on head-based sampling? Are there best practices around trace sampling? Tail sampling can get very expensive.</p> <p>A: Generally, head sampling is not recommended, as you aren\u2019t going to be able to do 100% of what you want to do with it, but it is true that tail sampling is expensive. The reason why sampling is such a complicated discussion is that there really isn\u2019t a universal answer; furthermore, it also depends on what kind of features are offered by your data analysis tool. For example, do you have a data ingest or storage cost? If you have ingest cost, you\u2019ll want to sample before the data gets ingested; if it\u2019s storage cost, you\u2019ll have to delete a lot of the data, so it depends on the tradeoffs.</p> <p>One thing to consider is that you can use tail sampling on attributes, such as if there\u2019s an error on a span, but it does require more memory. Suggested further exploration:</p> <ul> <li>Column data store for OpenTelemetry</li> <li>OpAMP</li> <li>Your backend vendor\u2019s tail-based sampling strategies</li> <li>Paper by Uber</li> <li>Tail sampling processor</li> </ul>"},{"location":"blog/2023/end-user-discussions-03/#adoption-migration-and-implementation","title":"Adoption, Migration, and Implementation","text":""},{"location":"blog/2023/end-user-discussions-03/#1-common-migration-challenges","title":"1 - Common migration challenges","text":"<p>Q: What are common challenges faced by developers when migrating to OpenTelemetry?</p> <p>More context: We have hundreds of microservices that need to be migrated, including big monolith systems with a lot of custom tracing locked into specific vendors and their libraries. Setting up agents to facilitate this migration is like having two different sets of observability systems running at the same time.</p> <p>A: One user shared their journey: They started by using a backend that supports OpenTelemetry. The two challenges they faced were: a cultural change in the engineer\u2019s mindset, and raising awareness of OpenTelemetry, which are bigger than the technical challenges. The key is to not propose one big change; the journey of moving from a vendor-based solution to OpenTelemetry should be a step-by-step process, rather than going into a full transformation.</p> <p>Additional suggestions:</p> <ul> <li>Start with dev or testing environments first to build trust in the software</li> <li>Choose a stack where OTel is more robust, such as Java and Node.js</li> <li>For countering developer resistance, using auto-instrumentation modules to</li> <li>start with is a good step</li> </ul>"},{"location":"blog/2023/end-user-discussions-03/#2-starting-and-scaling","title":"2 - Starting and scaling","text":"<p>Q: What is a good place to start from with OpenTelemetry? For example, from infra to data collection, or starting in the application? And how do you scale it up?</p> <p>More context: Our use case is end-to-end visibility; currently, we are using a vendor for monitoring logs, metrics, and traces. We are also using things like RUM (real user monitoring). Can we do the same with OpenTelemetry, and at scale?</p> <p>A: It depends on if you are starting to use OTel in a new project, or trying to re-orchestrate an existing or old project. It\u2019s best to start with a transition plan, make sure the performance impact is not bad, and scale up what you need. One suggestion is to start experimenting with Java OTel instrumentation, as the overall performance impact is negligible.</p> <p>Another suggestion is to try infrastructure monitoring with OpenTelemetry using the host metrics receiver in the Collector, as it covers a lot of metrics, and has no dependencies. One user noticed a 20% reduction in CPU usage when they moved from a vendor-specific agent to the host metrics receiver for infrastructure monitoring.</p>"},{"location":"blog/2023/end-user-discussions-03/#3-auto-instrumentation","title":"3 - Auto-instrumentation","text":"<p>Q: Is there a way to automatically create spans without code changes?</p> <p>A: It depends on the use cases:</p> <ul> <li>Auto instrumentation options are   maturing in OTel; for example, the Java JAR agent takes care of instrumenting   most libraries   that are used by applications. Auto-instrumentation is also available for   Python,   .NET, and   Node.js.</li> <li>If you\u2019re using Kubernetes, they can use the   OTel operator,   which takes care of instrumentations for applications deployed on K8s. The   OTel Operator also supports injecting and configuring auto-instrumentation   libraries where available (see point above).</li> <li>If you\u2019re using AWS lambda, you should check out the   OTel Lambda extension.</li> </ul>"},{"location":"blog/2023/end-user-discussions-03/#4-leveraging-telemetry-from-otel","title":"4 - Leveraging telemetry from OTel","text":"<p>Q: Has there been work toward telecommand standards to leverage the telemetry from OTel?</p> <p>A: Telecommand is a command sent to control a remote system or systems that are not directly connected to the place from which the telecommand is sent (per Wikipedia). Check out this paper, and OpAMP.</p>"},{"location":"blog/2023/end-user-discussions-03/#5-message-brokers","title":"5 - Message brokers","text":"<p>Q: What are some use cases for message brokers?</p> <p>A: IoT use cases (car manufacturer). There is also ongoing work for semantic conventions support for messages.</p>"},{"location":"blog/2023/end-user-discussions-03/#updates-and-communications","title":"Updates and Communications","text":""},{"location":"blog/2023/end-user-discussions-03/#1-unified-query-standard","title":"1 - Unified query standard","text":"<p>Q: Is there an update on the upcoming Unified Query Standard working group for observability data and discussion at O11y Day at KubeCon EU?</p> <p>A: The Observability TAG within CNCF is working to launch a working group that is going to analyze the various query languages that are out there and come up with use cases, such as, what are your most common alert and diagnostic types, and what are some uncommon patterns that you\u2019d like to have available? Then, we\u2019d like to see if there\u2019s any way we can come up with a recommendation for a unified standard language across vendors. Maybe SQL-ish?</p> <p>We\u2019re officially launching the working group at the end of the month; the charter is open for comments. View here. We are going to start making the conference circuit and gather feedback, the first place will be at Observability Day. Join the discussion at #telemetry-analysis in CNCF\u2019s Slack instance.</p>"},{"location":"blog/2023/end-user-discussions-03/#2-documentation-and-searches","title":"2 - Documentation and searches","text":"<p>Q: Where do you go to find documentation and answers to your questions?</p> <p>A: We have many resources, including official documentation and Github repos.</p> <p>To help us improve our resources, it would be helpful to gather feedback from you as an end user \u2013 what is your process for finding OTel information? Do you search for answers or post questions on Stack Overflow? The community is researching options that make sense so that questions can be indexed for searching. One option is Stack Overflow. Please share your answers using one of the avenues below!</p>"},{"location":"blog/2023/end-user-discussions-03/#meeting-notes-recordings","title":"Meeting Notes &amp; Recordings","text":"<p>For a deeper dive into the above topics, check out the following:</p> <ul> <li>AMER   meeting notes</li> <li>EMEA   meeting notes</li> <li>APAC   meeting notes</li> </ul>"},{"location":"blog/2023/end-user-discussions-03/#join-us","title":"Join us","text":"<p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End-User Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/end-user-discussions-04/","title":"OpenTelemetry End-User Discussions Summary for April 2023","text":"<p>For the month of April 2023, the OpenTelemetry end-user group meet took place for users in the Asia-Pacific (APAC) region. Due to KubeCon EU, the AMER and EMEA sessions did not take place; however, we will have meetings for all 3 regions again in May.</p> <p>The discussions take place using a Lean Coffee format, whereby folks are invited to post their topics to the Agile Coffee board like this one, and everyone in attendance votes on what they want to talk about.</p>"},{"location":"blog/2023/end-user-discussions-04/#what-we-talked-about","title":"What we talked about","text":"<p>We talked about evangelizing the adoption of OpenTelemetry in a big organization and also discussed how to optimize observability data at scale.</p>"},{"location":"blog/2023/end-user-discussions-04/#discussion-highlights","title":"Discussion Highlights","text":"<p>Below is the summary of this month's discussion.</p> <p>Note: The answers are provided by a mix of OTel community members and end-users to the best of their knowledge. The answers are not official recommendations by OpenTelemetry.</p>"},{"location":"blog/2023/end-user-discussions-04/#evangelizing-adoption-of-opentelemetry-in-a-big-organization","title":"Evangelizing adoption of OpenTelemetry in a big organization","text":"<p>Q: How do you evangelize the adoption of OpenTelemetry in a big organization?</p> <p>A: In a big organization, the first step would be to put out the current pain points of observability to leadership. There are benefits of having an open source standard for observability. If there is no standard in place, it gets very difficult to communicate across different teams. If you use OpenTelemetry, you do not have to depend on any vendor agents, and you have the flexibility to send data to multiple backends.</p>"},{"location":"blog/2023/end-user-discussions-04/#how-to-optimize-observability-data-at-scale","title":"How to optimize observability data at scale?","text":"<p>Q: In a big organization, observability data can be in the range of TBs per day, which comes with associated costs. But there is always a feeling that 80% of captured data is unusable. None of the vendors help you understand what data is accessed and how to bring that visibility to the engineering teams sending the data.</p> <p>A: One of the ways to optimize observability at scale is sampling. Here's an article on tail sampling with OpenTelemetry. There are a number of options for you to reduce the data volumes at the SDKs level and the collector level.</p> <p>Also, there is active work going on the OpenTelemetry Collector side to handle data at scale more efficiently. For example, there is work going around using Apache Arrow for serialization to optimize network costs.</p> <p>One of the other ways to optimize observability data at scale is to decide how much of it you want to store for future use. You should optimize data storage so that you incur less cost.</p>"},{"location":"blog/2023/end-user-discussions-04/#other-important-discussion-points","title":"Other Important discussion points","text":""},{"location":"blog/2023/end-user-discussions-04/#maturity-model-for-opentelemetry","title":"Maturity model for OpenTelemetry","text":"<p>Q: Is there some literature available around understanding steps to reach a certain maturity level in adopting OTel in your organization? For example, I should be able to go to a team and tell them to start with X, and then do Y to move ahead. In a big enterprise, you have to provide something for people to understand the maturity of OpenTelemetry.</p> <p>A: For teams adopting OpenTelemetry, a good idea is to start with minimal changes. For example, teams can start with languages that have auto-instrumentation support. Seeing value from small changes can build more confidence in the team to go deeper into OpenTelemetry adoption.</p> <p>There are also several OpenTelemetry receivers available. These receivers help to collect the telemetry end-users already have. For example, Prometheus receiver can help you receive metrics data in Prometheus format. Using these receivers, you can start sending telemetry data from different components of your application.</p>"},{"location":"blog/2023/end-user-discussions-04/#meeting-notes-recordings","title":"Meeting Notes &amp; Recordings","text":"<p>For a deeper dive into the above topics, check out the following:</p> <ul> <li>APAC   meeting notes</li> </ul>"},{"location":"blog/2023/end-user-discussions-04/#join-us","title":"Join us!","text":"<p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End-User Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/end-user-q-and-a-01/","title":"End-User Q&A Series: Using OTel with GraphQL","text":"<p>With contributions from Rynn Mancuso (Honeycomb) and Reese Lee (New Relic).</p> <p>On Thursday, January 26th, 2023, the OpenTelemetry End User Working Group hosted the first of its monthly End User Q&amp;A sessions of 2023. This series is a monthly casual discussion with a team using OpenTelemetry in production. The goal is to learn more about their environment, their successes, and the challenges that they face, and to share it with the community, so that together, we can help make OpenTelemetry awesome!</p> <p>This month, Dynatrace\u2019s Henrik Rexed spoke with J, who works at a financial services organization, about how they use OpenTelemetry with GraphQL.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#overview","title":"Overview","text":"<p>J and his team embarked on their OpenTelemetry journey for two main reasons:</p> <ul> <li>J\u2019s company uses a few different observability back-ends. His team had   switched to a vendor back-end that was different from the back-end used by   other teams that they interfaced with. OpenTelemetry allowed them to continue   to get end-to-end Traces in spite of using different vendors.</li> <li>His team was using GraphQL, and needed to be able to better understand what   was happening behind the scenes with their GraphQL calls.</li> </ul> <p>J also shared:</p> <ul> <li>His team\u2019s OpenTelemetry setup</li> <li>How he and his team have helped other teams start using OpenTelemetry</li> <li>His quest to make OpenTelemetry a standard at his organization</li> <li>Challenges that he and his team encountered in their OpenTelemetry journey,   along with a few suggestions for improvement.</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-01/#qa","title":"Q&amp;A","text":""},{"location":"blog/2023/end-user-q-and-a-01/#why-opentelemetry","title":"Why OpenTelemetry?","text":"<p>J\u2019s company has a diverse tech ecosystem, ranging from on-premise old-school mainframes, to AWS Cloud and Azure Cloud, where they run both Windows and Linux servers. They also use a number of different languages, including Node.js, .NET, Java, C, C++, and PL/I (mainframe).</p> <p>Across the organization, different teams have chosen to use different observability platforms to suit their needs, resulting in a mix of both open source and proprietary observability tools.</p> <p>J\u2019s team had recently migrated from one observability back-end to another. After this migration, they started seeing gaps in trace data, because other teams that they integrated with were still using a different observability back-end. As a result, they no longer had an end-to-end picture of their traces. The solution was to use a standard, vendor-neutral way to emit telemetry: OpenTelemetry.</p> <p>Another reason that his team took to using OpenTelemetry was GraphQL, which they had been using for four years. GraphQL is an open source language used to query and manipulate APIs. With GraphQL, everything is held in the body of data: request, response and errors, and as a result everything returns an HTTP status of 200, giving the impression that even failures are successful. This meant that J and his team had no visibility into what was going on behind the scenes.</p> <p>They pass a lot of data into a GraphQL response, because they have a main gateway that brings all of the different GraphQL endpoints into a single one, so it all looks like one massive query. OpenTelemetry exposed massive amounts of data from their GraphQL systems\u2013with traces as large as three to four thousand spans! Instrumentation has been done around Node.js GraphQL systems, and instrumentation has also started for their .NET GraphQL systems.</p> <p>Another black box that they are still facing is around AWS, and they are looking to add some distributed tracing around components like Lambdas and ECS.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#how-are-applications-deployed-into-production","title":"How are applications deployed into production?","text":"<p>The team is on GitLab, and uses GitLab pipelines for CI/CD, leveraging Ansible Tower to manage deployments. The GitLab custom pipelines deploy Kubernetes YAML files (without Helm) to an EKS cluster.</p> <p>The team is currently in the early stages of planning to use Amazon\u2019s cdk8s to deploy to Kubernetes, and Flagger to manage those deployments (including Canary deployments).</p>"},{"location":"blog/2023/end-user-q-and-a-01/#how-are-queries-built-in-graphql","title":"How are queries built in GraphQL?","text":"<p>There are two systems for building gateways in GraphQL. One is using Apollo Federation, and the other is through Schema Stitching. Schema stitching allows users to run a single query that spans across multiple GraphQL APIs. J\u2019s team chose Schema Stitching because, unlike Apollo which is getting more locked down, it is more open source, flexible, and less proprietary.</p> <p>This allows users to query or mutate as much data as they want. Uses of GraphQL include microservices development, and extracting data for analysis.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#how-do-you-generate-traces","title":"How do you generate traces?","text":"<p>To instrument their code, they configure the Node.js SDK and use a number of Node.js auto-instrumentation plug-ins. While the team is currently only using auto-instrumentation to generate traces and spans, they do occasionally add more data to a span (e.g. attributes). They do this by grabbing the context to find the span, and injecting custom attributes into that spans.</p> <p>There are currently no plans for the team to create custom spans, and in fact, J is currently discouraging teams from creating their own custom spans. Since they do a lot of asynchronous programming, it can be very difficult for developers to understand how the context is going to behave across asynchronous processes.</p> <p>Traces are sent to their observability back-end using that vendor\u2019s agent, which is installed on all of their nodes.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#besides-traces-do-you-use-other-signals","title":"Besides traces, do you use other signals?","text":"<p>The team has implemented a custom Node.js plugin for getting certain metrics data about GraphQL, such as deprecated field usage and overall query usage, which is something that they can\u2019t get from their traces. These metrics are being sent to the observability back-end through the OpenTelemetry Collector\u2019s OTLP metrics receiver.</p> <p>There is a long-term goal to have this plugin contributed back to the OpenTelemetry community. At the moment, however, the plugin is currently coupled to their own systems, and needs to be modified for more generic use cases. In addition, the plugin needs to be reviewed by the organization\u2019s open source Software group before it can be shared externally.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#do-you-do-any-logging","title":"Do you do any logging?","text":"<p>The team uses Amazon Elasticache and the ELK stack for logging. They are currently doing a proof-of-concept (POC) of migrating .NET logs to their observability back-end. The ultimate goal is to have metrics, logs, and traces under one roof.</p> <p>They have currently been able to automatically link traces to logs in ELK using Node.js Bunyan. They are hoping to leverage OpenTelemetry\u2019s Exemplars to link traces and metrics.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#how-is-the-organization-sending-telemetry-data-to-various-observability-back-ends","title":"How is the organization sending telemetry data to various observability back-ends?","text":"<p>J\u2019s team uses a combination of the proprietary back-end agent and the OpenTelemetry Collector (for metrics). They are one of the primary users of OpenTelemetry at J\u2019s company, and he hopes to help get more teams to make the switch.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#who-has-access-to-the-instrumentation-data","title":"Who has access to the instrumentation data?","text":"<p>Traces are used for diagnostic purposes. If there\u2019s an issue in production, traces help developers pinpoint where the problem might be.</p> <p>Because GraphQL mostly returns HTTP 200s, it gives the impression that it returns no errors, when in fact, there might be errors lurking behind the scenes. Having traces enables developers to see if there\u2019s actually an error in the response body. For example, when accessing a database, if there\u2019s a connection hangup, GraphQL will report HTTP 200, but the Trace will show that there\u2019s an error, and where.</p> <p>The SRE team also uses the observability data for the purposes of improving system reliability and performance.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#how-would-you-describe-the-overall-opentelemetry-adoption-experience","title":"How would you describe the overall OpenTelemetry adoption experience?","text":"<p>The team\u2019s initial adoption was super fast and easy\u201380% of their tracing needs were met right away. The next 20% required some additional proof of concept work, which was completed relatively quickly. Overall, it was a very positive experience.</p> <p>J\u2019s team has convinced a couple of other groups to use OpenTelemetry; however, they have been met with a few challenges. For example, J wants to make sure that these teams move away from proprietary software, such as Apollo Studio, since OpenTelemetry already meets these same needs.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#are-there-plans-to-use-opentelemetry-across-the-organization","title":"Are there plans to use OpenTelemetry across the organization?","text":"<p>The team has recently been talking to their internal Open Source Software (OSS) and Enterprise Architecture (EA) groups to make OpenTelemetry an enterprise standard. They are hoping to use their own success with their production-ready OpenTelemetry system to illustrate the benefits of OpenTelemetry across the organization.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#are-you-seeing-the-benefits-of-using-opentelemetry-with-graphql-in-your-production-environments","title":"Are you seeing the benefits of using OpenTelemetry with GraphQL in your production environments?","text":"<p>Using the GraphQL OpenTelemetry plugin-for Node.js made it super easy to identify an issue with a GraphQL resolver that was acting up in production.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#were-the-outputs-produced-by-the-instrumentation-libraries-that-you-used-meaningful-to-you-or-did-you-have-to-make-any-adjustments","title":"Were the outputs produced by the instrumentation libraries that you used meaningful to you, or did you have to make any adjustments?","text":"<p>On the Node.js side, the team used auto-instrumentation for HTTP, Express, GraphQL, and also the AWS SDK on some systems.</p> <p>The most useful instrumentation was GraphQL and AWS SDK. Although GraphQL auto-instrumentation has been very useful, there are still some areas for improvement, such as adding the ability to ignore certain fields. J has opened a pull request to address this.</p> <p>The team didn\u2019t see much benefit in auto-instrumentation for HTTP and Express. They found HTTP instrumentation to be a little too noisy. Express is being used very minimally, and therefore there was no real value in having that instrumentation. Also, the team plans to migrate from Express to GraphQL Yoga in the near future. They expect there to be some instrumentation gaps when they move to GraphQL Yoga, and are therefore planning on writing an OpenTelemetry plugin for it, which they intend to give back to the OpenTelemetry community.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#are-you-planning-on-instrumenting-mainframe-code","title":"Are you planning on instrumenting mainframe code?","text":"<p>The observability back-end used by J\u2019s team provided native instrumentation for the mainframe. J and his team would have loved to instrument mainframe code using OpenTelemetry. Unfortunately, there is currently no OpenTelemetry SDK for PL/I (and other mainframe languages such as FORTRAN and COBOL). The team would love to have OpenTelemetry available for the mainframe, but aren\u2019t sure if there\u2019s enough appetite out there for undertaking such an effort.</p> <p>NOTE: If anyone is interested in or ends up creating an OpenTelemetry implementation for the mainframe, please reach out to us!</p>"},{"location":"blog/2023/end-user-q-and-a-01/#challengesmoving-forward","title":"Challenges/Moving Forward","text":"<p>As part of our conversation with J, he also shared some areas and suggestions for improvement.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#javascript-maintenance","title":"JavaScript Maintenance","text":"<p>OpenTelemetry has a small number of language maintainers, and as a result, they don\u2019t necessarily have enough cycles to work on all the things. Thus, they currently focus on keeping up with spec changes to update the SDK and API. This means that they often don\u2019t have time (and sometimes not even the expertise) to manage the contrib repos (e.g. GraphQL). This is a known problem, and there is currently no solution in place. The OpenTelemetry Community welcomes any suggestions for improvement!</p> <p>There is also a huge focus on stabilizing semantic conventions, and as part of that effort, maintainers plan to go through the existing instrumentation packages and to make sure that they\u2019re all up to date with the latest conventions. While it\u2019s very well-maintained for certain languages, such as Java, that is not the case for other languages, such as Node.js.</p> <p>JavaScript environments are akin to the Wild West of Development due to:</p> <ul> <li>Multiple facets: web side vs server side</li> <li>Multiple languages: JavaScript, TypeScript, Elm</li> <li>Two similar, but different server-side runtimes: Node.js and   Deno</li> </ul> <p>One of J\u2019s suggestions is to treat OTel Javascript as a hierarchy, which starts with a Core JavaScript team that splits into two subgroups: front-end web group, and back-end group. Front-end and back-end would in turn split. For example, for the back-end, have a separate Deno and Node.js group.</p> <p>Another suggestion is to have a contrib maintainers group, separate from core SDK and API maintainers group.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#javascript-contributions","title":"JavaScript Contributions","text":"<p>Making OpenTelemetry JavaScript contributions has been slow-moving at times, specifically around plug-ins. Much of the plug-in maintenance relies on the plug-in\u2019s original owner; however, the original owner is gone in many cases, or maintainers don\u2019t check GitHub very frequently, and as a result, movement on some pull requests (PRs) is very slow. One way to mitigate this is to get contributors more involved, which could potentially help bring in more contributors.</p>"},{"location":"blog/2023/end-user-q-and-a-01/#documentation","title":"Documentation","text":"<p>J and his team have also experienced some challenges with documentation, noting that there are some gaps in the online docs:</p> <ul> <li>Under metrics for JavaScript, there is no mention of the Observable Gauge at   all. J had to go into the code to find it.</li> <li>There are some short, very high-level metric API examples. Those examples   currently don't show which libraries you need to bring in. It also doesn't   talk about how to export items.</li> <li>In .NET, it is very hard to keep a trace going in your work due to all the   async/await and it jumping between threads. .NET docs lack some detail around   context propagation in this particular scenario.</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-01/#final-thoughts","title":"Final Thoughts","text":"<p>OpenTelemetry is all about community, and we wouldn\u2019t be where we are without our contributors, maintainers, and users. Hearing stories of how OpenTelemetry is being implemented in real life is only part of the picture. We value user feedback, and encourage all of our users to share your experiences with us, so that we can continue to improve OpenTelemetry. \u2763\ufe0f</p> <p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End Users Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Sign up for one of our monthly interview/feedback sessions</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/end-user-q-and-a-02/","title":"End-User Q&A Series: Using OTel at Uplight","text":"<p>With contributions from Rynn Mancuso (Honeycomb) and Reese Lee (New Relic).</p> <p>On Thursday, March 2nd, 2023, the OpenTelemetry (OTel) End User Working Group hosted its second End User Q&amp;A session of 2023. This series is a monthly casual discussion with a team using OpenTelemetry in production. The goal is to learn more about their environment, their successes, and the challenges that they face, and to share it with the community, so that together, we can help make OpenTelemetry awesome!</p> <p>This month, I spoke with Doug Ramirez, Principal Architect at Uplight.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#overview","title":"Overview","text":"<p>Doug loves observability, and by extension, OpenTelemetry, because of the excitement that he gets from getting feedback for code that he has written.</p> <p>In this session, Doug shared:</p> <ul> <li>His organization\u2019s OpenTelemetry journey</li> <li>How he has evangelized OpenTelemetry at Uplight</li> <li>Challenges that he encountered in Uplight\u2019s OpenTelemetry journey, along with   a few suggestions for improvement.</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-02/#qa","title":"Q&amp;A","text":""},{"location":"blog/2023/end-user-q-and-a-02/#tell-us-about-your-role","title":"Tell us about your role?","text":"<p>Uplight is made up of a number of companies that were brought together as a result of mergers and acquisitions, and now all exist under the Uplight brand. Its mission is to save the planet, by helping utilities operate their grid to minimize resource consumption and offset CO2 emissions. The organization has a main data platform that centralizes large data sets for utilities. As they\u2019ve grown, the Uplight data platform has become an extremely important component that enables the apps that ultimately deliver value to its customers.</p> <p>Doug\u2019s role as Principal Architect on the platform is to help design and architect the platform in ways that satisfy business requirements, while also allowing developers to easily leverage the platform. To help achieve that, he has optimized on observability as an architecture characteristic, having spent a significant amount of time in the past year talking about and thinking about observability, and baking it into everything</p>"},{"location":"blog/2023/end-user-q-and-a-02/#what-do-you-think-that-observability-will-help-you-solve","title":"What do you think that Observability will help you solve?","text":"<p>Because Uplight is a conglomerate of companies, it means that there are different tech stacks, different design patterns, and different ways to approach the same problems.</p> <p>In spite of having all these different systems with different stacks, Doug feels that it is essential to be able observe them all running together, as a cohesive unit. He wants to create the same experience for developers across the company to observe their code, irrespective of the tech stack that they\u2019re using \u2013 i.e. a common path to observability. This is being achieved by leaning into OpenTelemetry as the standard and tool to get there.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#what-is-your-architecture-like","title":"What is your architecture like?","text":"<p>Uplight uses \u201ceverything\u201d, including a lot of Ruby, Java, Python, some .NET, and as a result, it\u2019s hard to describe the tech stack. There\u2019s a lot of legacy code. There are many monoliths. New development work is being written in Python, and they are leveraging FastAPI for micro-services work.</p> <p>With so many different languages and frameworks being used, the question was, how do you get observability and OTel injected or baked into these different platforms?</p> <p>The ultimate goal was to get folks to understand OpenTelemetry and the long-term vision around observability. Most developers are familiar and comfortable with logs \u2013 they just want to be able to write a log and see what happens. So, Doug started by getting developers to add OpenTelemetry (structured) logs to all of the services across their various platforms. In order to leverage OTel logs, developers had to add the OpenTelemetry language-specific SDKs into their code. Once they got past that initial hump and got the SDKs into their code, it then became easier for developers to add other signals (such as metrics and traces) to the code as well, since the OTel scaffolding was already in place!</p> <p>Doug and his team realized that the problem of structured logging had already been solved by OpenTelemetry. Contributors and maintainers have thought long and hard about logging and standardization on structure, and it didn\u2019t make sense to reinvent the wheel. The log spec already existed, so Uplight chose to ride the coattails of OTel, in order for developers to get to their observability path more quickly and easily. Again, in adopting OpenTelemetry logs, adopting traces and metrics became a natural next step.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#what-is-your-build-and-deployment-process-like","title":"What is your build and deployment process like?","text":"<p>Builds are done using CircleCI and Jenkins. Everything is run in containers, and they use all of the cloud providers. They are working to standardize on tooling and processing for deploying to the cloud.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#otel-logs-are-relatively-new-why-use-something-so-new","title":"OTel logs are relatively new. Why use something so new?","text":"<p>As one of the newer OpenTelemetry signals, there was a lot of concern around the maturity of logs. There were also many concerns about whether OTel itself would go away, or whether logs would be eliminated from the spec. All of that unease was put to rest once the folks at Uplight began exploring and using log correlation \u2013 i.e. linking logs to traces.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#what-were-some-of-the-challenges-on-the-road-to-otel","title":"What were some of the challenges on the road to OTel?","text":"<p>One of the biggest challenges faced internally at Uplight was defending against vendor lock-in, while still emitting meaningful telemetry data in order to achieve observability. Some folks at Uplight felt that the SDKs provided by their APM vendor did the job; however, that meant vendor lock-in.</p> <p>Providing a good developer experience was key. It was important to show developers that they could instrument their code easily, using a framework that has become the de facto standard for instrumentation, and which is also portable, so it won\u2019t keep them locked into a particular vendor.</p> <p>The hearts and minds of developers began to change after they were able to experience OpenTelemetry in action:</p> <ul> <li>Seeing structured logs, being able to correlate traces and logs, and emitting   metrics.</li> <li>Experiencing the benefits of   context propagation \u2013 i.e. spans and   traces interacting across different operations to provide an end-to-end view   of a service call.</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-02/#how-did-you-promote-opentelemetry-across-the-organization","title":"How did you promote OpenTelemetry across the organization?","text":"<p>There was a lot of internal debate on whether or not OpenTelemetry was mature enough to warrant adoption. As a result, Doug spent a lot of time educating folks on OpenTelemetry, to show that OpenTelemetry was not bleeding edge (it\u2019s been around for a while), and that it has the support of the major Observability vendors. In fact, these vendors are all talking about it on their blogs. These efforts helped get buy-in from both Uplight\u2019s leadership and engineers.</p> <p>Doug\u2019s main architecture goals at Uplight are observability, deployability, and security. Part of the observability narrative included talking about OpenTelemetry and showing folks how it all works. To do that, Doug has created a number of short internal Loom videos, inspired by Microsoft\u2019s Channel 9. The Loom videos have been a very effective means of sharing information about OpenTelemetry (both theory and code snippets) very quickly across the organization. They have been extremely well-received. Video topics have included structured logging, metrics, traces, and integrating distributed tracing with webhook platforms.</p> <p>Internal hackathons have also proven to be a very effective means of promoting OpenTelemetry, and getting folks to use it.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#how-have-developers-found-the-experience-of-integrating-the-otel-sdks-into-the-application-code","title":"How have developers found the experience of integrating the OTel SDKs into the application code?","text":"<p>One of Doug\u2019s goals with OpenTelemetry was to create a pleasant developer experience around implementing the language SDKs. There was a lot of internal debate on whether or not shared libraries would help lower the barrier to entry for implementing the OTel SDK. It was ultimately decided to allow teams to choose their own path: some teams are implementing Uplight shared libraries, others are leveraging code snippets from a reference architecture created by Doug, and others are using the SDK directly.</p> <p>Doug\u2019s main takeaway is for folks to just start using OpenTelemetry right away, get to know it, and not worry about creating shared libraries.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#manual-or-auto-instrumentation","title":"Manual or auto-instrumentation?","text":"<p>Folks at Uplight have used a combination of manual and auto-instrumentation. Doug\u2019s main advice is to do the minimum you need to get instrumentation up and running, do the minimum required to get traces and logs emitted and correlated, and then refine as needed.</p> <p>The SDKs give you everything you need. How much you decided to optimize on top of that is up to you. Doug\u2019s advice is to do the minimum you need to get started</p>"},{"location":"blog/2023/end-user-q-and-a-02/#how-do-you-deploy-your-otel-collectors","title":"How do you deploy your OTel Collectors?","text":"<p>Uplight currently has a few different Collector configurations:</p> <ul> <li>Collectors running standalone as some   sidecars</li> <li>For larger Kubernetes clusters, there\u2019s a   Collector running in each cluster</li> <li>Developers running their own Collectors   locally with Docker</li> </ul> <p>Doug\u2019s ultimate goal is for any deployment in any environment to be able to easily send telemetry to an OTel Collector gateway.</p> <p>Collectors at Uplight are typically run and maintained by the infrastructure team, unless individual teams decide to take ownership of their own Collectors. Those who do take ownership of their own Collectors have had a positive experience thus far. Uplight may revisit whether or not development teams should own their own Collectors at a later date, but for now, giving developers a quick path to standing up the Collector is more important to help further OpenTelemetry adoption.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#feedback","title":"Feedback","text":""},{"location":"blog/2023/end-user-q-and-a-02/#community-engagement","title":"Community Engagement","text":"<p>Doug has had a very positive experience with OpenTelemetry so far. He has been happy to see that the OTel community is very active on the CNCF Community Slack, and recommends for anyone new to OpenTelemetry to just join some OTel Channels (e.g. #otel-collector, #otel-logs, #otel-python) and just see what people are talking about. The conversations happening in the various channels have helped inform his decisions at Uplight.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#contribution","title":"Contribution","text":"<p>Doug has made some contributions to the Python SDK; however, it took a little bit of time to understand the logistics of contributing. He was initially unsure about how to get engaged, who to talk to in Slack, and how to nudge folks to request a review of his PRs. Anything that can be done to make it super easy and obvious for people to contribute would be super helpful.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#communication","title":"Communication","text":"<p>Doug has found it challenging to determine where to go for certain types of conversations. Is it GitHub issues, or Slack? Where do you go if you\u2019re someone new who wants to make a contribution? Where do you go if you\u2019re new to OTel and are seeing a problem? How do you ensure that the conversations are not being duplicated?</p>"},{"location":"blog/2023/end-user-q-and-a-02/#simple-reference-implementations","title":"Simple Reference Implementations","text":"<p>Doug would like to see really simple reference implementations to help folks who are starting OTel from scratch. For example, they\u2019re running a simple \u201cHello World\u201d program to send data to the Collector, and nothing is showing up, and need some guidance around this. How do we help folks who aren\u2019t super familiar with Docker and aren\u2019t super familiar with OpenTelemetry? Can we have some super simple reference implementations to hold folks\u2019 hands as they get started? For example, for a Ruby developer, clone X repo, run <code>docker compose up</code>1, and everything should be up and running. That way, they can focus on learning OpenTelemetry, rather than mess around with Docker networking and other distracting things.</p> <p>I shared with Doug that we have the OTel Demo App (and #otel-community-demo channel on Slack), which provides an OTel-example-in-a-box. I also shared the #otel-config-file Slack channel, which aims to simplify OTel bootstrapping</p> <p>Doug would like to see a more targeted, language-specific example in a box. For example, a FastAPI example with 2 Python services talking to each other, to demonstrate context propagation, going through the Collector, which sends traces to Jaeger.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#whats-next","title":"What's next?","text":"<p>If you\u2019d like to see my conversation with Doug in full, you can check out the video here.</p> <p>If anyone would like to continue the conversation with Doug, please reach out to him in the #otel-user-research Slack channel!</p> <p>Also, be sure to check out more of Doug's OTel adventures at this month's OTel in Practice series, on March 27th, 09:00 PT/11:00 ET.</p>"},{"location":"blog/2023/end-user-q-and-a-02/#final-thoughts","title":"Final Thoughts","text":"<p>OpenTelemetry is all about community, and we wouldn\u2019t be where we are without our contributors, maintainers, and users. Hearing stories of how OpenTelemetry is being implemented in real life is only part of the picture. We value user feedback, and encourage all of our users to share your experiences with us, so that we can continue to improve OpenTelemetry. \u2763\ufe0f</p> <p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End Users Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Sign up for one of our   monthly interview/feedback sessions</li> <li>Join the   OpenTelemetry group on LinkedIn</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/end-user-q-and-a-03/","title":"End-User Q&A Series: Using OTel at Farfetch","text":"<p>With contributions from Rynn Mancuso (Honeycomb) and Reese Lee (New Relic).</p> <p>On Thursday, May 25th, 2023, the OpenTelemetry (OTel) End User Working Group hosted its third End User Q&amp;A session of 2023. We had a bit of a gap due to KubeCon Europe, but now we\u2019re back! This series is a monthly casual discussion with a team using OpenTelemetry in production. The goal is to learn more about their environment, their successes, and the challenges that they face, and to share it with the community, so that together, we can help make OpenTelemetry awesome!</p> <p>This month, I spoke with Iris Dyrmishi, Platform Engineer at Farfetch.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#overview","title":"Overview","text":"<p>Iris is a huge fan of observability and OpenTelemetry, and her love of these two topics is incredibly infectious.</p> <p>In this session, Iris shared:</p> <ul> <li>Farfetch\u2019s journey to OpenTelemetry</li> <li>How metrics and traces are instrumented</li> <li>OpenTelemetry Collector deployment and configuration</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-03/#qa","title":"Q&amp;A","text":""},{"location":"blog/2023/end-user-q-and-a-03/#tell-us-about-your-role","title":"Tell us about your role?","text":"<p>Iris is part of a central team that provides tools for all the engineering teams across Farfetch to monitor their services, including traces, metrics, logs, and alerting. The team is responsible for maintaining Observability tooling, managing deployments related to Observability tooling, and educating teams on instrumenting code using OpenTelemetry.</p> <p>Iris first started her career as a software engineer, focusing on back-end development. She eventually moved to a DevOps Engineering role, and it was in this role that she was introduced to cloud monitoring through products such as Amazon CloudWatch and Azure App Insights. The more she learned about monitoring, the more it became a passion for her.</p> <p>She then moved into another role where she was introduced to OpenTelemetry, Prometheus, and Grafana, and got to dabble a little more in the world of Observability. This role became an excellent stepping stone for her current role at Farfetch, which she has been doing for a little over a year now.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-did-you-hear-about-opentelemetry","title":"How did you hear about OpenTelemetry?","text":"<p>Iris first heard about OpenTelemetry on LinkedIn. The company she was working at at the time, which was not using traces, had started exploring the possibility of using them and was looking into tracing solutions. After reading about OpenTelemetry, Iris created a small Proof-of-Concept (POC) for her manager. While nothing had moved past the POC at that role, when Iris joined Farfetch and OpenTelemetry came up again, she jumped at the chance to work with it.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#what-is-the-architecture-at-farfetch-like-how-has-opentelemetry-helped","title":"What is the architecture at Farfetch like? How has OpenTelemetry helped?","text":"<p>Farfetch currently has 2000 engineers, with a complex and varied architecture which includes cloud-native, Kubernetes, and virtual machines running on three different cloud providers. There is a lot of information coming from everywhere, with a lack of standardization on how to collect this information. For example, Prometheus is used mostly as a standard for collecting metrics; however, in some cases, engineers found that Prometheus did not suit their needs. With the introduction of OpenTelemetry, Farfetch was able to standardize the collection of both metrics and traces, and enabled them to collect telemetry signals from services where signal collection had not previously been possible.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#can-you-describe-the-build-and-deployment-process-at-farfetch","title":"Can you describe the build and deployment process at Farfetch?","text":"<p>Farfetch uses Jenkins for CI/CD, and there is a separate team that manages this.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#what-observability-tooling-do-you-use","title":"What Observability tooling do you use?","text":"<p>Iris\u2019 team uses mostly open source tooling, alongside some in-house tooling created by her team. On the open source tooling front:</p> <ul> <li>Grafana is used for dashboards</li> <li>OpenTelemetry is used for emitting traces, and   Grafana Tempo is used as a tracing back-end</li> <li>Jaeger is still used in some cases for emitting   traces and as a tracing back-end, because some teams have not yet completely   moved to OpenTelemetry for instrumenting traces   (via Jaeger\u2019s implementation of the OpenTracing API).</li> <li>Prometheus Thanos (highly-available   Prometheus) is used for metrics collection and storage</li> <li>OpenTelemetry is also being used to collect metrics</li> </ul>"},{"location":"blog/2023/end-user-q-and-a-03/#tell-us-about-farfetchs-opentelemetry-journey","title":"Tell us about Farfetch\u2019s OpenTelemetry journey","text":"<p>Farfetch is a very Observability-driven organization, so when senior leadership floated the idea of bringing OpenTelemetry into the organization, it got overwhelming support across the organization. The biggest challenge faced around OpenTelemetry was around timing for its implementation; however, once work on OpenTelemetry started, everyone embraced it.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-did-you-and-your-team-enable-observability-through-opentelemetry","title":"How did you and your team enable Observability through OpenTelemetry?","text":"<p>By the time Iris joined Farfetch, most of the big struggles and challenges around Observability had passed. When Observability was first introduced within the organization, it was very new and unknown to many engineers there, and as with all new things, there is a learning curve.</p> <p>When Iris and her team took on the task of enabling OpenTelemetry across the organization, Observability as a concept had already been embraced. Their biggest challenge in bringing OpenTelemetry to Farfetch was making sure that engineers did not experience major disruptions to their work, while still benefiting from having OpenTelemetry in place. It helped that OpenTelemetry is compatible with many of the tools in their existing Observability stack, including Jaeger and Prometheus.</p> <p>Due to the enthusiasm, drive, and push that Iris and one of her co-workers, an architect at Farfetch, made for OpenTelemetry, Iris was proud to share that they are now using OpenTelemetry in production.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-long-did-it-take-your-team-to-get-opentelemetry-in-production","title":"How long did it take your team to get OpenTelemetry in production?","text":"<p>Iris and her team planned to start using OpenTelemetry in January 2023. This included initial investigation and information-gathering. By mid-March, they had their first pieces in production.</p> <p>They are not fully there yet:</p> <ul> <li>There is still a lot of reliance on Prometheus and Jaeger for generating   metrics and traces, respectively</li> <li>Not all applications have been instrumented with OpenTelemetry</li> </ul> <p>In spite of that, Iris and her team are leveraging the power of the OpenTelemetry Collector to gather and send metrics and traces to various Observability back-ends. Since she and her team started using OpenTelemetery, they started instrumenting more traces. In fact, with their current setup, Iris has happily reported that they went from processing 1,000 spans per second, to processing 40,000 spans per second!</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-are-you-collecting-your-traces-right-now","title":"How are you collecting your traces right now?","text":"<p>Traces are being collected through a combination of manual and auto instrumentation.</p> <p>Some applications are being manually instrumented through OpenTelemetry, and others are still instrumented using the [legacy OpenTracing using shims.</p> <p>The OpenTelemetry Operator is being implemented to auto-instrument Java and .NET code. Among other things, the OTel Operator supports injecting and configuring auto-instrumentation in .NET, Java, Python, and Node.js. Iris hopes that Go auto-instrumentation will be available in the near-future. To track progress of auto-instrumentation in Go, see OpenTelemetry Go Automatic Instrumentation.</p> <p>Although this will be a lengthy and time-consuming process, the team\u2019s goal is to have all applications instrumented using OpenTelemetry.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#what-kind-of-support-does-your-team-provide-for-manual-instrumentation","title":"What kind of support does your team provide for manual instrumentation?","text":"<p>By design, Iris and her team don\u2019t instrument other teams\u2019 code. Instead, they provide documentation and guidelines on manual instrumentation, and refer teams to the OpenTelemetry docs where applicable. They also have sessions with engineers to show them best practices around instrumenting their own code. It\u2019s a team sport!</p>"},{"location":"blog/2023/end-user-q-and-a-03/#can-you-share-your-experience-around-using-the-otel-operator","title":"Can you share your experience around using the OTel Operator?","text":"<p>The OTel Operator is only partially used in production, and is currently not available for everyone. Iris and her team really love the OTel Operator; however, it did take a bit of getting used to. Iris and her team found that there is a tight coupling between cert-manager and the OTel Operator. They were not able to use our own custom certificates, and they did not support cert-manager in their clusters, so they found it hard to use the Operator in our clusters. They solved this by submitting a PR -- opentelemetry-helm-charts PR #760!</p> <p>One of the things she loves about OpenTelemetry was that, when she was trying to troubleshoot an issue whereby Prometheus was not sending metrics to the Collector, and was therefore not able to create alerts from it. Then a colleague suggested using OpenTelemetry to troubleshoot OpenTelemetry.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#have-you-or-anyone-on-your-team-or-at-farfetch-started-playing-with-otel-logging","title":"Have you or anyone on your team or at Farfetch started playing with OTel Logging?","text":"<p>Iris has played around a bit with OTel logging, mostly consuming logs from a Kafka topic. This experiment has not included log correlation, but it is something that Iris would like to explore further.</p> <p>Since logs are not yet stable, Iris doesn\u2019t expect logging to go into production at Farfetch just yet. Farfetch has a huge volume of logs (more than traces), so they don\u2019t want to start converting to OTel logging until things are more stable.</p> <p>Note: some parts of OTel logs are stable. For details, see Specification Status Summary.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-are-you-collecting-the-metrics-signal","title":"How are you collecting the metrics signal?","text":"<p>Auto-instrumentation emits some OTLP metrics; however, the majority of metrics still come from Prometheus.</p> <p>The team currently uses the Prometheus Receiver to scrape metrics from Consul. Specifically, they use Consul to get the targets and the ports where to scrape them. The Receiver\u2019s scrape configs are the same as in Prometheus, so it was relatively easy to move from Prometheus to the Prometheus Receiver (lift and shift).</p> <p>They also plan to collect OTLP metrics from Kubernetes. This is facilitated by the Prometheus Receiver\u2019s support for the OTel Operator\u2019s Target Allocator.</p> <p>Prometheus is also still currently used for metrics collection in other areas, and will probably remain this way, especially when collecting metrics from virtual machines.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-many-kubenetes-clusters-are-you-observing","title":"How many Kubenetes clusters are you observing?","text":"<p>There are 100 Kubernetes clusters being observed, and thousands of virtual machines. Iris and her team are responsible for managing the OTel Operator across all of these clusters, and are therefore also trained in Kubernetes, so that they can maintain their stack on the clusters.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#have-you-dabbled-in-any-of-the-otel-experimental-features-in-kubernetes","title":"Have you dabbled in any of the OTel experimental features in Kubernetes?","text":"<p>This question is referring to the ability for Kubernetes components to emit OTLP traces which can then be consumed by the OTel Collector. For more info, see Traces For Kubernetes System Components. This feature is currently in beta, and was first introduced in Kubernetes 1.25.</p> <p>Iris and team have not played around with this beta feature.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#how-do-you-deploy-your-otel-collectors","title":"How do you deploy your OTel Collectors?","text":"<p>Because there are so many Kubernetes clusters, having a single OTel Collector would be a bottleneck in terms of load and single point of failure. The team currently has one OpenTelemetry Collector agent per Kubernetes cluster. The end goal is to replace those agents with the OTel Operator instead, which allows you to deploy and configure the OTel Collector and inject and configure auto-instrumentation.</p> <p>Everything is then sent to a central OTel Collector (i.e. an OTel Collector gateway) per data center, where data masking (using the transform processor, or redaction processor), data sampling (e.g. tail sampling processor or probabilistic sample processor), and other things happen. It then sends traces to Grafana Tempo.</p> <p>The central OTel Collector resides on another Kubernetes cluster that belongs solely to the Farfetch Observability team, which runs the Collector and other applications that belong to the team.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#what-happens-if-the-central-collector-fails","title":"What happens if the central Collector fails?","text":"<p>The team has fallback clusters, so that if a central Collector fails, the fallback cluster will be used in its place. The satellite clusters are configured to send data to the central Collector on the fallback cluster, so if the central cluster fails, the fallback cluster can be brought up without disruption to OTel data flow.</p> <p>Having autoscaling policies in place to ensure that the Collectors have enough memory and CPU to handle data loads also helps to keep the system highly available.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#what-were-some-of-the-challenges-you-experienced-in-deploying-the-otel-collector","title":"What were some of the challenges you experienced in deploying the OTel Collector?","text":"<p>The biggest challenge was getting to know the Collector and how to use it effectively. Farfetch relies heavily on auto-scaling, so one of the first things that the team did was to enable auto-scaling for the Collectors, and tweak settings to make sure that it could handle large amounts of data.</p> <p>The team also leaned heavily on OTel Helm charts, and on the OTel Community for additional support.</p> <p>Are you currently using any processors on the OTel Collector? \\ The team is currently experimenting with processors, namely for data masking (transform processor, or redaction processor), especially as they move to using OTel Logs, which will contain sensitive data that they won\u2019t want to transmit to their Observability back-end. They currently, however, are only using the batch processor.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#are-you-aware-of-any-teams-using-span-events","title":"Are you aware of any teams using span events?","text":"<p>A span event provides additional point-in-time information in a trace. It\u2019s basically a structured log within a span.</p> <p>Not at the moment, but it is something that they would like to explore. When the Observability team first started, there was little interest in tracing. As they started implementing OpenTelemetry and tracing, they have moved to make traces first-class citizens, and now it is piquing the interest of engineers, as they begin to see the relevance of traces.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#have-you-encountered-anyone-who-was-resistant-to-opentelemetry","title":"Have you encountered anyone who was resistant to OpenTelemetry?","text":"<p>Farfetch is a very Observability-driven culture, and the Observability team hasn\u2019t really encountered anyone who is against Observability or OpenTelemetry. Some engineers might not care either way, but they are not opposed to it, either.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#have-you-or-your-team-made-any-contributions-to-opentelemetry","title":"Have you or your team made any contributions to OpenTelemetry?","text":"<p>The team, led by the architect, has made a contribution recently to the OTel Operator around certificates. The OTel Operator relied on cert-manager for certificates, rather than custom certificates. They initially put in a feature request, but then decided to develop the feature themselves, and filed a pull request.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#audience-questions","title":"Audience Questions","text":""},{"location":"blog/2023/end-user-q-and-a-03/#how-much-memory-and-cpu","title":"How much memory and CPU?","text":"<p>When their Collector was processing around 30,000 spans per second, there were 4 instances of the Collector, using around 8GB memory.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#are-you-doing-any-correlation-between-metrics-data-trace-data-and-log-data","title":"Are you doing any correlation between metrics data, trace data, and log data?","text":"<p>This is something that is currently being explored. The team is exploring traces/metrics correlation (exemplars) through OpenTelemetry; however, they found that this correlation is accomplished more easily through their tracing back-end, Tempo.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#are-you-concerned-about-the-amount-of-data-that-you-end-up-producing-transporting-and-collecting-how-do-you-ensure-data-quality","title":"Are you concerned about the amount of data that you end up producing, transporting, and collecting? How do you ensure data quality?","text":"<p>This is not a concern, since the volume of data never changed, and the team knows that they can handle these large volumes. The team is simply changing how the data is being produced, transported, and collected. Iris also recognizes that the amount of trace data is gradually increasing; however, the data increase is gradual, so that the team can prepare itself to handle larger data volumes.</p> <p>The team is working very hard to ensure that they are getting quality data. This is especially true for metrics, where they are cleaning up metrics data to make sure that they are processing meaningful data. If a team decided to drastically increase the volume of metrics it emits, the Observability team is consulted beforehand, to ensure that the increase makes sense.</p> <p>Since trace volumes were initially a low lower, they did not need to concern themselves with trace sampling. Now that trace volume is increasing, they are keeping a close eye on things.</p> <p>The team is also focusing its attention on data quality and volume of logs, which means researching log processors to see which ones suit their needs. Ultimately, they will publish a set of guidelines for development teams to follow, and evangelize practices within the company.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#feedback","title":"Feedback","text":"<p>Iris and her team have had a very positive experience with OpenTelemetry and the OpenTelemetry community.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#documentation","title":"Documentation","text":"<p>Iris shared that the docs at times are not as clear as they could be, requiring some extra digging on the part of the engineer, to understand how a certain component works or is supposed to be configured. For example, she had a hard time finding documentation on Consul SD configuration for OpenTelemetry. That being said, Iris is hoping to contribute back to docs to help improve them.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#turnaround-time-on-prs","title":"Turnaround time on PRs","text":"<p>Iris and her team were pleasantly surprised by the quick turnaround time on getting their OTel Operator PR approved and merged.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#additional-resources","title":"Additional Resources","text":"<p>My conversation with Iris, in full, is available on YouTube.</p> <p>If anyone would like to continue the conversation with Iris, reach out to her in the #otel-user-research Slack channel!</p> <p>She will also be presenting at OTel in Practice on June 8th.</p>"},{"location":"blog/2023/end-user-q-and-a-03/#final-thoughts","title":"Final Thoughts","text":"<p>OpenTelemetry is all about community, and we wouldn\u2019t be where we are without our contributors, maintainers, and users. Hearing stories of how OpenTelemetry is being implemented in real life is only part of the picture. We value user feedback, and encourage all of our users to share your experiences with us, so that we can continue to improve OpenTelemetry. \u2763\ufe0f</p> <p>If you have a story to share about how you use OpenTelemetry at your organization, we\u2019d love to hear from you! Ways to share:</p> <ul> <li>Join the #otel-endusers channel on the   CNCF Community Slack</li> <li>Join our monthly   End Users Discussion Group calls</li> <li>Join our OTel in Practice sessions</li> <li>Sign up for one of our   monthly interview/feedback sessions</li> <li>Join the   OpenTelemetry group on LinkedIn</li> <li>Share your stories on the   OpenTelemetry blog</li> </ul> <p>Be sure to follow OpenTelemetry on Mastodon and Twitter, and share your stories using the #OpenTelemetry hashtag!</p>"},{"location":"blog/2023/exponential-histograms/","title":"Exponential Histograms","text":"<p>Previously, in [Why Histograms?][] and [Histograms vs Summaries][], I went over the basics of histograms and summaries, explaining the tradeoffs, benefits, and limitations of each. Because they're easy to understand and demonstrate, those posts focused on so-called explicit bucket histograms. The exponential bucket histogram, also referred to as native histogram in Prometheus, is a low-cost, efficient alternative to explicit bucket histograms. In this post, I go through what they are, how they work, and the problems they solve that explicit bucket histograms struggle with.</p>"},{"location":"blog/2023/exponential-histograms/#types-of-histograms","title":"Types of histograms","text":"<p>For the purposes of this blog post, there are two major types of histograms: explicit bucket histograms and exponential bucket histograms. In previous posts, I've focused on what OpenTelemetry calls explicit bucket histograms and Prometheus simply refers to as histograms. As the name implies, an explicit bucket histogram has each bucket configured explicitly by either the user or some default list of buckets. Exponential histograms work by calculating bucket boundaries using an exponential growth function. This means each consecutive bucket is larger than the previous bucket and ensures a constant relative error for every bucket.</p>"},{"location":"blog/2023/exponential-histograms/#exponential-histograms","title":"Exponential histograms","text":"<p>In OpenTelemetry exponential histograms, buckets are calculated automatically from an integer scale factor, with larger scale factors offering smaller buckets and greater precision. It is important to select a scale factor that is appropriate for the distribution of values you are collecting in order to minimize error, maximize efficiency, and ensure the values being collected fit in a reasonable number of buckets. In the next few sections, I'll go over the scale and error calculations in detail.</p>"},{"location":"blog/2023/exponential-histograms/#scale-factor","title":"Scale factor","text":"<p>The most important and most fundamental part of an exponential histogram is also one of the trickiest to understand, the scale factor. From the scale factor, bucket boundaries, and by extension resolution, range, and error rates, are derived. The first step is to calculate the histogram base.</p> <p>The base is a constant derived directly from the scale using the equation <code>2 ^ (2 ^ -scale)</code>. For example, given a scale of 3, the base can be calculated as <code>2^(2^-3) ~= 1.090508</code>. Because the calculation depends on the power of the negative scale, as the scale grows, the base shrinks and vice versa. As will be shown later, this is the fundamental reason that a greater scale factor results in smaller buckets and a higher resolution histogram.</p>"},{"location":"blog/2023/exponential-histograms/#bucket-calculation","title":"Bucket calculation","text":"<p>Given a scale factor and its resulting base, we can calculate every possible bucket in the histogram. From the base, the upper bound of each bucket at index <code>i</code> is defined to be <code>base ^ (i + 1)</code>, with the first bucket lower boundary of 1. Because of this, the upper boundary of the first bucket at index 0 is also exactly the base. For now, we will only consider nonnegative indices, but negative indexed buckets are also possible and define all buckets between 0 and 1. Keeping with our example using a scale of 3 and resulting base of 1.090508, the third bucket at index 2 has an upper bound of <code>1.090508^(2+1) = 1.29684</code>. The following table shows upper bounds for the first 10 buckets of a few different scale factors:</p> index scale -1 scale 0 scale 1 scale 3 -1 1 1 1 1 0 4 2 1.4142 1.0905 1 16 4 2 1.1892 2 64 8 2.8284 1.2968 3 256 16 4 1.4142 4 1024 32 5.6569 1.5422 5 4096 64 8 1.6818 6 16384 128 11.3137 1.8340 7 65536 256 16 2 8 262144 512 22.6274 2.1810 9 1048576 1024 32 2.3784 <p>I've bolded some of the values here to show an important property of exponential histograms called perfect subsetting.</p>"},{"location":"blog/2023/exponential-histograms/#perfect-subsetting","title":"Perfect subsetting","text":"<p>In the chart above, some of the bucket boundaries are shared between histograms with differing scale factors. In fact, each time the scale factor increases by 1, exactly 1 boundary is inserted between each existing boundary. This feature is called perfect subsetting because each set of boundaries for a given scale factor is a perfect subset of the boundaries for any histogram with a greater scale factor.</p> <p>Because of this, histograms with differing scale factors can be normalized to whichever has the lesser scale factor by combining neighboring buckets. This means that histograms with different scale factors can still be combined into a single histogram with exactly the precision of the least precise histogram being combined. For example, histogram A with scale 3 and histogram B with scale 2 can be combined into a single histogram C with scale 2 by first summing each pair of neighboring buckets in A to form histogram A' with scale 2. Then, each bucket in A' is summed with the corresponding bucket of the same index in B to make C.</p>"},{"location":"blog/2023/exponential-histograms/#relative-error","title":"Relative Error","text":"<p>A histogram does not store exact values for each point, but represents each point as a bucket consisting of a range of possible points. This can be thought of as being similar to lossy compression. In the same way the it is impossible to recover an exact source image from a compressed JPEG, it is impossible to recover the exact input data set from a histogram. The difference between the input data and the estimated reconstruction of the data is the error of the histogram. It is important to understand histogram errors because it affects \u03c6-quantile estimation and may affect how you define your SLOs.</p> <p>The relative error for a histogram is defined as half the bucket width divided by the bucket midpoint. Because the relative error is the same across all buckets, we can use the first bucket with the upper bound of the base to make the math easy. An example is shown below using a scale of 3.</p> <pre><code>scale = 3\n# For base calculation, see above\nbase  = 1.090508\n\nrelative error = (bucketWidth / 2) / bucketMidpoint\n               = ((upper - lower) / 2) / ((upper + lower) / 2)\n               = ((base - 1) / 2) / ((base + 1) / 2)\n               = (base - 1) / (base + 1)\n               = (1.090508 - 1) / (1.090508 + 1)\n               = 0.04329\n               = 4.329%\n</code></pre> <p>For more information regarding histogram errors, see OTEP 149 and the specification for exponential histogram aggregations.</p>"},{"location":"blog/2023/exponential-histograms/#choosing-a-scale","title":"Choosing a scale","text":"<p>Because increasing the scale factor increases the resolution and decreases the relative error, it may be tempting to choose a large scale factor. After all, why would you want to introduce error? The answer is that there is a positive relationship between the scale factor and the number of buckets required to represent values within a specified range. For example, with 160 buckets (the OpenTelemetry default), histogram A with a scale factor of 3 can represent values between 1 and about 1 million; histogram B with a scale of 4 the same number of buckets would only be able to represent values between about 1 and about 1000, albeit at half the relative error. To represent the same range of values as A with B, twice as many buckets are required; in this case 320.</p> <p>This brings me to the first most important point of choosing a scale, data contrast. Data contrast is how you describe the difference in scale between the smallest possible value x and the largest possible value y in your dataset and is calculated as the constant multiple c such that <code>y = c * x</code>. For example, if your data is between 1 and 1000 milliseconds, your data contrast is 1000. If your data is between 1 kilobyte and 1 terabyte, your data contrast is 1,000,000,000. Data contrast, scale, and the number of buckets are all interlinked such that if you have 2, you can calculate the third.</p> <p>Fortunately, if you are using OpenTelemetry, scale choice is largely done for you. In OpenTelemetry, you configure a maximum scale (default 20) and a maximum size (default 160), or number of buckets, in the histogram. The histogram is initially assumed have the maximum scale. As additional data points are added, the histogram will rescale itself down such that the data points always fit within your maximum number of buckets. The default of 160 buckets was chosen by the OpenTelemetry authors to be able to cover typical web requests between 1ms and 10s with less than 5% relative error. If your data has less contrast, your error will be even less.</p>"},{"location":"blog/2023/exponential-histograms/#negative-or-zero-values","title":"Negative or zero values","text":"<p>For the bulk of this post we have ignored zero and negative values, but negative buckets work much the same way, growing larger as the buckets get further from zero. All of the math and explanation above applies in the same way to negative values, but they should be substituted for their absolute values, and upper bounds for buckets are lower bounds (or upper absolute value bounds). Zero values, or values with an absolute value less than a configurable threshold, go into a special zero bucket. When merging histograms with differing zero thresholds, the larger threshold is taken and any buckets with absolute value upper bounds within the zero threshold are added to the zero bucket and discarded.</p>"},{"location":"blog/2023/exponential-histograms/#opentelemetry-and-prometheus","title":"OpenTelemetry and Prometheus","text":"<p>Compatibility between OpenTelemetry and Prometheus is probably a topic large enough for its own post. For now I will just say that for all practical purposes, OpenTelemetry exponential histograms are 1:1 compatible with Prometheus native histograms. Scale calculations, bucket boundaries, error rates, zero buckets, etc are all the same. For more information, I recommend you watch this talk given by Ruslan Vovalov and Ganesh Vernekar: Using OpenTelemetry\u2019s Exponential Histograms in Prometheus</p> <p>A version of this article was [originally posted][] to the author's blog.</p> <p>[Why Histograms?]: {{% relref \"why-histograms\" %}} [Histograms vs Summaries]: {{% relref \"histograms-vs-summaries\" %}}</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2023/http-conventions-stability/","title":"Final push to HTTP semantic convention stability","text":"<p>The OpenTelemetry community is beginning the final push to declare HTTP tracing and metrics semantic conventions stable!</p> <p>Following the recently proposed Semantic Convention Process, the HTTP semantic convention stability working group will meet three times a week for the next six weeks (starting January 30) to work through the remaining list of issues. This group will submit spec PRs for any changes that need to be made prior to declaring the HTTP semantic conventions stable.</p> <p>Once this is done, the community will have four weeks to review and provide feedback on the final set of PRs submitted by the working group.</p> <p>And finally, the working group and spec approvers will have two weeks to clean up and merge the final spec PRs, and mark the HTTP semantic conventions as stable.</p> <p>If you are interested in participating in the working group, please join the meetings starting Monday, January 30 at 3pm Pacific Time, and continuing for six weeks on Mondays, Wednesdays and Fridays:</p> <ul> <li>Mon 3-3:30pm Pacific Time</li> <li>Wed 3-3:30pm Pacific Time</li> <li>Fri 9:30-10am Pacific Time</li> </ul> <p>See the OpenTelemetry calendar for Zoom link details.</p>"},{"location":"blog/2023/kubecon-eu/","title":"Join us for OpenTelemetry Talks and Activities at KubeCon EU 2023","text":"<p>The OpenTelemetry project maintainers, members of the governance committee, and technical committee are excited to be at KubeCon EU in Amsterdam from April 18 - 21, 2023!</p> <p>Read on to learn about all the things related OpenTelemetry during KubeCon.</p>"},{"location":"blog/2023/kubecon-eu/#kubecon-talks-and-maintainer-sessions","title":"KubeCon Talks and Maintainer Sessions","text":"<ul> <li>Jaeger: The Future with OpenTelemetry and Metrics   by Pavol Loffay, Red Hat &amp; Jonah Kowall, Aiven Wednesday, April 19 \u2022   11:55 - 12:30</li> <li>Using OpenTelemetry for Application Security, with a Real Life Example   by Ron Vider, Oxeye Wednesday, April 19 \u2022 11:55 - 12:30</li> <li>Understand Systems with OpenTelemetry: A Hybrid Telemetry Data Backend   by Ran Xu, Huawei &amp; Xiaochun Yang, Northeastern University Wednesday,   April 19 \u2022 14:30 - 15:05</li> <li>OTel Me About Metrics: A Metrics 101 Crash Course   by Reese Lee, New Relic Wednesday, April 19 \u2022 17:25 - 18:00</li> <li>OpenTelemetry: Using Unified Semantics to Drive Insights + Project Update   by Morgan McLean, Splunk, Alolita Sharma, Apple, Daniel Dyla, Dynatrace, &amp; Ted   Young, Lightstep Thursday, April 20 \u2022 16:30 - 17:05</li> <li>Connected Observability Pipelines in the OpenTelemetry Collector   by Daniel Jaglowski, observIQ Friday, April 21 \u2022 11:55 - 12:30</li> <li>Tutorial: Exploring the Power of OpenTelemetry on Kubernetes   by Pavol Loffay, Benedikt Bongartz &amp; Yuri Oliveira Sa, Red Hat, Severin   Neumann, Cisco &amp; Kristina Pathak, LightStep Friday, April 21 \u2022 16:00 -   17:30</li> </ul>"},{"location":"blog/2023/kubecon-eu/#co-located-events","title":"Co-located Events","text":"<p>Come network with OpenTelemetry maintainers and core contributors during the OpenTelemetry project meeting, on Tuesday April 18, 2023 from 16:00 - 17:00. You can attend with a standard in-person pass.</p> <p>Observability Day fosters collaboration, discussion, and knowledge sharing of cloud-native observability projects. This event will be held on April 18, 2023 from 9:00 - 17:00. There will be several sessions on OpenTelemetry as well.</p> <p> IMPORTANT access note: You need an in-person all-access pass for on-site access to Observability Day. For details, see KubeCon registration. If you have a virtual ticket, you will be able to follow Observability Day through a live stream.</p>"},{"location":"blog/2023/kubecon-eu/#opentelemetry-project-booth","title":"OpenTelemetry Project Booth","text":"<p>Drop by and say Hi! at the OpenTelemetry project booth in the KubeCon EU Project Pavilion. If you\u2019re lucky, you may even pick up some OpenTelemetry swag!</p> <p>You will find us in the Solutions Showcase in Hall 5, Kiosk Number 20.</p> <p>Pavilion hours are as follows:</p> <ul> <li>Wednesday, April 19: 10:30 \u2013 21:00 (includes KubeCrawl + CloudNativeFest from   18:00 \u2013 21:00)</li> <li>Thursday, April 20: 10:30 \u2013 17:30</li> <li>Friday, April 21: 10:30 \u2013 14:30</li> </ul> <p>You can help us improve the project by sharing your thoughts and feedback about your OpenTelemetry adoption, implementation, and usage! We also invite you to fill out our community survey. We will create action items from your comments as appropriate. Check #otel-user-research in CNCF's Slack instance for survey results and action item updates to come after KubeCon EU.</p> <p>Come join us to listen, learn, and get involved in OpenTelemetry.</p> <p>See you in Amsterdam!</p>"},{"location":"blog/2023/lambda-release/","title":"OpenTelemetry Updates Lambda Support","text":"<p>The Functions-as-a-Service (FaaS) SIG is incredibly excited to announce that the release of OpenTelemetry Lambda layers, and associated documentation on how to monitor Amazon Web Services (AWS) Lambdas.</p>"},{"location":"blog/2023/lambda-release/#otel-faas-repackaged","title":"OTel FaaS repackaged","text":"<p>If you've been monitoring Lambdas using OTel for a while now, you may be slightly confused by this announcement. You might think something like: OTel has had a repo for Lambda layers and they've been available on AWS for years.</p> <p>You're totally correct. Rest assured, we're not reinventing the wheel. However, there are some pre-existing problems that may impact users:</p> <ul> <li>The OTel Lambda layers were only released as part of the   AWS Distribution for OTel (ADOT), and the   community had limited control over releases which meant a delay getting new   features and fixes delivered.</li> <li>The layers available on AWS combined the Collector and auto-instrumentation   capabilities into a single package, which contributed to performance   degradations and limited user choice.</li> <li>There wasn't official OTel guidance on how to monitor Lambdas and no single   source of truth for OTel users to reference.</li> </ul> <p>The FaaS SIG has addressed the above-mentioned shortcomings:</p> <ul> <li>We have written new Github Actions to release the Lambda layers ourselves,   thus empowering the community to make its own release decisions.</li> <li>Separated the Collector and instrumentation layers to give users options when   instrumenting their Lambdas. We now offer a standalone Lambda layer for the   Collector alongside auto-instrumentation layers for JavaScript, Java, and   Python.</li> <li>Added official community Lambda documentation to the OTel website under the   new FaaS section.</li> </ul>"},{"location":"blog/2023/lambda-release/#what-next","title":"What next","text":"<p>Moving forward, the FaaS SIG plans to: enhance the documentation, add auto-instrumentation for other Cloud vendors like Azure and GCP (tentative), enhancing the existing Lambda assets, and improving OpenTelemetry performance for Function specific scenarios.</p>"},{"location":"blog/2023/lambda-release/#get-involved","title":"Get Involved","text":"<p>Interested in learning more, or if you'd like to help: join us in a SIG meeting (every Tuesday at 12 pm PST), or join us on Slack at #otel-faas.</p>"},{"location":"blog/2023/new-apac-meetings/","title":"New APAC Collector-SIG meetings","text":"<p>As the collector community grows worldwide, having the ability to meet with each other is a challenge. To help more APAC maintainers and contributors connect, we have created two new meetings times:</p> <ul> <li>1st Wednesday is EU-APAC friendly</li> <li>3rd Wednesday is NA-APAC friendly</li> </ul>"},{"location":"blog/2023/new-apac-meetings/#meeting-times","title":"Meeting times","text":"UTC\u221208:00 (PST) UTC -05:00 (EST) UTC +01:00 (CET) UTC +08:00 (China) UTC +10:00 (Sydney) EU-APAC 00:00 03:00 09:00 16:00 19:00 NA-APAC 15:00 18:00 00:00 07:00 10:00 <p>To stay up to date with these times, subscribe to the community calendar.</p> <p>We look forward to seeing more of you join the Collector community at these new meetings!</p>"},{"location":"blog/2023/otel-in-focus-01/","title":"OpenTelemetry in Focus, January 2023","text":"<p>Welcome to the first edition of OpenTelemetry In Focus! This blog is intended to be an overview of important releases, roadmap updates, and community news. This series will be focusing on our core components, such as the specification, data format, tools, and most popular API/SDKs.</p> <p>Are you a maintainer with something you\u2019d like featured here? Get in touch with me via email, or on the CNCF Slack, #otel-comms channel.</p>"},{"location":"blog/2023/otel-in-focus-01/#releases-and-updates","title":"Releases and Updates","text":"<p>New year, new code! The following repositories have released new versions this month. For more details on what\u2019s in each release, be sure to check out the full release notes.</p> <ul> <li>Specification v.1.17.0   has been released! This includes deprecation notices for the Jaeger exporter,   histogram data model stability, changes to semantic conventions, and several   other changes.</li> <li>Go v1.12.0/v0.35.0   celebrates an important release of updated semantic conventions and metric   instruments, along with a handful of bug fixes and other important changes.</li> <li>JavaScript has   released v1.4.0 of the API, v.1.9.0 of core, and v0.35.0 of experimental   packages! These releases include important bug fixes in tracing around clock   drift, as well as other deprecations and enhancements.</li> <li>Java v1.22.0   includes several fixes and enhancements for exporters, as well as other   ease-of-use and correctness issues. In addition,   Java Agent v1.22.1   has been released to align with the core API and SDK, in addition to new   instrumentations for Spring Web MVC, JMS 3.0 (Jakarta), and Spring JMS 6.0.</li> <li>Operator v0.68.0   brings with it a new OpAMP Bridge service,and a fix to allow for deployment to   OpenShift clusters, along with other bug fixes.</li> <li>Collector v1.0.0 (RC4)   and   Collector Contrib v0.70.0   have been released with a significant amount of changes, including support for   connectors in the Collector Builder.</li> <li>Demo v1.3.0   has been released with support for metric exemplars, enhanced resource   detection, and updates to OpenTelemetry API and SDKs.</li> </ul>"},{"location":"blog/2023/otel-in-focus-01/#project-updates","title":"Project Updates","text":"<p>Over the past few months, through community discussions both in-person and online, a new public roadmap has been created and published! This roadmap isn\u2019t meant to be a set-in-stone list of priorities, but more of a guide to our priorities and prioritization.</p> <p>Interested in AWS Lambda, or other Functions-as-a-Service workloads, and how to emit OpenTelemetry data from them? Our FAAS Working Group has restarted in order to work on this problem. Join a meeting, or check out their notes, for more information.</p> <p>The End User Working Group has published a summary of their discussions for the month. If you\u2019re a user of OpenTelemetry, these meetings are a great place to get connected with your peers and discuss how you\u2019re using the project.</p>"},{"location":"blog/2023/otel-in-focus-01/#news-and-upcoming-events","title":"News and Upcoming Events","text":"<p>We\u2019re proud to support Observability Day Europe as part of KubeCon EU 2023. If you\u2019re planning to be in Amsterdam for KubeCon, be sure to come a day early and meet up with OpenTelemetry contributors and maintainers!</p>"},{"location":"blog/2023/otel-in-focus-02/","title":"OpenTelemetry in Focus, February 2023","text":"<p>Welcome to this month\u2019s edition of OpenTelemetry in Focus! It might be cold and snowy in much of the Northern Hemisphere, but that hasn\u2019t frozen our progress. Read on for an overview of new releases, announcements, and other important updates.</p> <p>Are you a maintainer with something you\u2019d like featured here? Get in touch with me via email, or on the CNCF Slack #otel-comms channel.</p>"},{"location":"blog/2023/otel-in-focus-02/#releases-and-updates","title":"Releases and Updates","text":"<p>Here are the latest updates from our core repositories.</p>"},{"location":"blog/2023/otel-in-focus-02/#specification","title":"Specification","text":"<p>v1.18 has been released, with a batch of semantic convention updates and clarifications on mapping and converting between Prometheus and OpenTelemetry Metrics.</p>"},{"location":"blog/2023/otel-in-focus-02/#collector-and-contrib","title":"Collector and contrib","text":"<p>v0.72 have been released with several major changes to be aware of:</p> <ul> <li>The minimum supported Golang version is now 1.19</li> <li>The host metrics receiver has removed deprecated metrics for process memory.</li> <li>The promtail receiver has been removed from collector-contrib.</li> <li>The Jaeger exporters are now deprecated, to be removed in a future release.</li> <li>Connectors   have been added! These are components that act as exporters and receivers,   allowing you to route data through pipelines. Please see the component docs   for more information.</li> <li>Many bug fixes and enhancements.</li> </ul>"},{"location":"blog/2023/otel-in-focus-02/#go","title":"Go","text":"<p>v1.14 has been released. This is the last release to support Go 1.18; 1.19 will be required in the future. Semantic conventions have been updated, resulting in changes to constant and function names. Finally, there\u2019s a variety of bug fixes and other small changes.</p>"},{"location":"blog/2023/otel-in-focus-02/#java","title":"Java","text":"<p>v1.23 has been released, bringing with it stable base2 exponential histogram aggregations and significant metrics refactoring. Semantic convention updates, improvements to SDK shutdown, and several enhancements to the SDK extensions are also in this release. Java Instrumentation has been updated as well, most notably changing HTTP span names to reflect updated semantic conventions.</p>"},{"location":"blog/2023/otel-in-focus-02/#php","title":"PHP","text":"<p>v1 beta was announced at the end of January. The PHP SIG is looking forward to your feedback. In addition, the Communications SIG is planning a release of new documentation for PHP soon.</p>"},{"location":"blog/2023/otel-in-focus-02/#python","title":"Python","text":"<p>v1.16 has been released with deprecations to Jaeger exporters, several performance improvements and bug fixes, and changes to Prometheus export.</p>"},{"location":"blog/2023/otel-in-focus-02/#net","title":".NET","text":"<p>v1.4 removes several deprecated extension methods.</p> <p>As always, this is just a snapshot of important changes and improvements across the core projects. Make sure you thoroughly read the release notes when upgrading your OpenTelemetry dependencies.</p>"},{"location":"blog/2023/otel-in-focus-02/#project-updates","title":"Project Updates","text":"<p>The Outreachy project is looking for participants. This is an annual program that connects new open source contributors with small, self-contained projects that they can work on. There are also opportunities to volunteer to mentor these contributors. Read the blog for more information!</p> <p>The Collector SIG will be starting new APAC-friendly meetings to support contributors and maintainers worldwide.</p> <p>Our End-User Working Group has written up a Q&amp;A about using OpenTelemetry with GraphQL.</p>"},{"location":"blog/2023/otel-in-focus-02/#news-and-upcoming-events","title":"News and Upcoming Events","text":"<p>OpenTelemetry maintainers and contributors will be in attendance at Observability Day Europe on April 18th, 2023, as part of KubeCon/CloudNativeCon Europe 2023.</p>"},{"location":"blog/2023/otel-in-focus-03/","title":"OpenTelemetry in Focus, March 2023","text":"<p>Welcome to this month\u2019s edition of OpenTelemetry in Focus! It's been another busy month in the OpenTelemetry community, with some big announcements and new releases from our core repositories. I've also put together an overview of some blog, website, and project highlights - give it a look, and tell me what you think.</p> <p>Are you a maintainer with something you\u2019d like featured here? Get in touch with me via email, or on the CNCF Slack #otel-comms channel.</p>"},{"location":"blog/2023/otel-in-focus-03/#releases-and-updates","title":"Releases and Updates","text":"<p>Here are the latest updates from our core repositories.</p>"},{"location":"blog/2023/otel-in-focus-03/#specification","title":"Specification","text":"<p>Version 1.19 has been released with a number of important udates.</p> <ul> <li>OTLP/JSON has been declared stable.</li> <li>To clarify its purpose, the Logs API has been renamed to the Logs Bridge API.</li> <li>Semantic convention updates.</li> </ul>"},{"location":"blog/2023/otel-in-focus-03/#collector-and-contrib","title":"Collector and contrib","text":"<p>Version 0.74/v1.0-rc8 has been released for the collector, resulting in a new operator version as well. Highlights include:</p> <ul> <li>Connectors are enabled by default.</li> <li>The <code>spanmetricsprocessor</code> has been deprecated in favor of the   <code>spanmetricsconnector</code>. Other changes have been made to the behavior of this   component.</li> <li>A new receiver for CloudFlare logs has been added.</li> <li>Many bugfixes and enhancements.</li> </ul>"},{"location":"blog/2023/otel-in-focus-03/#go","title":"Go","text":"<p>Version v1.15.0-rc.2 has been released. Version 1.15 will ship with Metrics v1 support, and its associated stability guarantees. Other highlights of the release candidate include:</p> <ul> <li>Support for global meter providers.</li> <li>Exemplar support added for metric data.</li> <li>Several optimizations, bugfixes, and removals/deprecations.</li> </ul>"},{"location":"blog/2023/otel-in-focus-03/#java","title":"Java","text":"<p>Version 1.24.0 of the Java SDK has been released, featuring several optimizations and bugfixes to the metrics SDK.</p> <p>In addition, the Java Instrumentation package has been updated to 1.24 as well, featuring several new instrumentations and fixes:</p> <ul> <li>Apache Pulsar and Jodd-Http can now be instrumented automatically via the   agent.</li> <li>Ktor and Spring Webflux libraries can be instrumented using the library.</li> <li>Improvements to the RxJava2, Cassandra, Spring Boot, and other instrumentation   packages.</li> </ul>"},{"location":"blog/2023/otel-in-focus-03/#python","title":"Python","text":"<p>Version 1.17 has been released with a handful of fixes and improvements, most notably support for exponential histograms!</p>"},{"location":"blog/2023/otel-in-focus-03/#project-updates","title":"Project Updates","text":"<p>The proposal to merge the Elastic Common Schema (ECS) into OpenTelemetry has been passed! This is a big step towards reducing competing standards and aligning the open source observability ecosystem around a common data model.</p> <p>A proposal to donate OpenTelemetry Instrumentation for Android has been made. You can follow along with the discussion in the linked issue -- exciting to see more options for client observability in OpenTelemetry!</p> <p>We're on a mission to reduce the number of unanswered OpenTelemetry questions on Stack Overflow. Be sure to check out the Stack Overflow Watch in the Monthly Highlights to learn how you can help, and get some cool swag in the process.</p> <p>The Logs Bridge Specification is in the final stretch before merge. If you'd like to help proofread, or have any comments, now's the time to get involved!</p>"},{"location":"blog/2023/otel-in-focus-03/#news-and-upcoming-events","title":"News and Upcoming Events","text":"<p>The schedule for KubeCon EU is up, and there's a lot of OpenTelemetry to go around! We'll also be at Observability Day EU -- which will be live-streamed, including project updates and a panel discussion featuring several OpenTelemetry maintainers. Will you be there in-person? Find me (I'm the guy with the hat) and say hi -- I'd love to meet you (and I'll have some stickers to give away). We'll also have a project booth, so swing by -- and stay tuned for another blog detailing our full involvement at KubeCon EU.</p>"},{"location":"blog/2023/otel-in-focus-04/","title":"OpenTelemetry in Focus, April 2023","text":"<p>Welcome to this month\u2019s edition of OpenTelemetry in Focus! It's been another busy month in the OpenTelemetry community, with some big announcements and new releases from our core repositories. I'll also be sharing some highlights from OpenTelemetry at KubeCon EU, which was a blast. Can't wait for Chicago this fall!</p> <p>Are you a maintainer with something you\u2019d like featured here? Get in touch with me via email, or on the CNCF Slack #otel-comms channel.</p>"},{"location":"blog/2023/otel-in-focus-04/#releases-and-updates","title":"Releases and Updates","text":"<p>Here are the latest updates from some of our core repositories.</p>"},{"location":"blog/2023/otel-in-focus-04/#specification","title":"Specification","text":"<p>Version 1.20 has been released, and it's a big one!</p> <p>First, OpenTelemetry Protocol has been declared stable! Second, we've started a process to converge the Elastic Common Schema with OpenTelemetry Semantic Conventions. What does this mean? At a high level, you can expect to see that semantic conventions will split out of the specification as we proceed towards aligning our standards. Please be on the look out for more information.</p> <p>Other changes include:</p> <ul> <li>Changes to span and metric SDK details.</li> <li>Clean up the log bridge API.</li> <li>Key stability work for existing Semantic Conventions.</li> <li>Breaking change to <code>http.server.active_requests</code> metric; The   <code>http.status_code</code> attribute is no longer present.</li> </ul>"},{"location":"blog/2023/otel-in-focus-04/#collector-and-contrib","title":"Collector and contrib","text":"<p>Version 0.76.1/v1.0-rcv0011 has been released for the collector. The operator has been updated to v0.75.0, adding support for feature gates in the operator.</p> <p>This release includes several bug fixes and improvements to connectors, along with a breaking change to the <code>confmap</code> component.</p>"},{"location":"blog/2023/otel-in-focus-04/#go","title":"Go","text":"<p>Version v1.15.0 has been released! This marks the official release of OpenTelemetry Metrics v1 in Go. Please check out the full release notes, as there are several important changes and renamings, especially if you're using metrics.</p>"},{"location":"blog/2023/otel-in-focus-04/#java","title":"Java","text":"<p>Version 1.25.0 of the Java SDK has been released, with several bugfixes and improvements. Please note that this includes a change to exponential bucket histograms, please see the release notes for details if you rely on automatic configuration of histograms.</p> <p>In addition, the Java Instrumentation package has been updated to 1.25.1 as well. Highlights include:</p> <ul> <li>New instrumentation added for R2DBC, JFR streaming metrics, and ZIO 2.0</li> <li>Passwords no longer emitted from db.user when using JDBC instrumentation.</li> <li>Apache HTTP Client library now emits client metrics as well.</li> <li>Alignment with semantic conventions.</li> </ul> <p>There's much more -- be sure to check out the release notes!</p>"},{"location":"blog/2023/otel-in-focus-04/#project-updates","title":"Project Updates","text":"<p>KubeCon EU saw over ten thousand cloud-native developers gather in Amsterdam, and a lot of you stopped by the OpenTelemetry booth to say hi! Hopefully some of you got your hands on our limited-edition KubeCon stickers... if not, well, there'll be more limited edition stickers. Just not for KubeCon, because it's come and gone.</p> <p>There was a lot of great feedback that we're excited to tackle as a project over the coming months, including:</p> <ul> <li>Improving discoverability of components for the collector.</li> <li>Increasing responsiveness to PR's and issues.</li> <li>Finishing up the Logging Bridge API and getting logs to stability.</li> </ul> <p>There were also a lot of great talks from the Observability community at KubeCon, including at Observability Day Europe. Go check it out if you have some time, there's some really interesting real-world examples in there of how people are using OpenTelemetry!</p>"},{"location":"blog/2023/otel-in-focus-04/#news-and-upcoming-events","title":"News and Upcoming Events","text":"<p>OpenCensus is being sunset in July 2023. Once this has concluded, our initial goal of OpenTelemetry as a single replacement for OpenTracing and OpenCensus will have been realized!</p>"},{"location":"blog/2023/otel-in-focus-05/","title":"OpenTelemetry in Focus, May 2023","text":"<p>Welcome back to OpenTelemetry in Focus for May, 2023! The sun is shining, the sky is blue, and it's time to run down the latest updates from the OpenTelemetry project!</p> <p>Are you a maintainer with something you\u2019d like featured here? Get in touch with me via email, or on the CNCF Slack #otel-comms channel.</p>"},{"location":"blog/2023/otel-in-focus-05/#releases-and-updates","title":"Releases and Updates","text":"<p>Here are the latest updates from some of our core repositories.</p>"},{"location":"blog/2023/otel-in-focus-05/#specification","title":"Specification","text":"<p>Version 1.21 has been released with a variety of important changes, including:</p> <ul> <li>Log Bridge API and SDK have been marked stable.</li> <li>Add groundwork for file-based configuration of OpenTelemetry.</li> <li>OpenCensus compatibility specification marked stable.</li> </ul>"},{"location":"blog/2023/otel-in-focus-05/#collector","title":"Collector","text":"<p>Version 0.78.0 has been released, along with 0.77. These releases address several important core issues, including:</p> <ul> <li>Batch processor can now batch by attribute keys.</li> <li>Initial support for internal OpenTelemetry SDK usage.</li> <li>Default queue size for exporters reduced from 5000 to 1000.</li> <li>Feature gate added to disable internal metrics with high cardinality.</li> </ul> <p>In addition, collector-contrib has been updated with several changes and enhancements. The Operator now supports Golang &amp; Apache HTTP server auto-instrumentation in addition to Python, Java, Node.JS, and .NET.</p>"},{"location":"blog/2023/otel-in-focus-05/#go","title":"Go","text":"<p>Version 1.16.0/0.39.0 marks the stable release of the OpenTelemetry Metric API in Go.</p>"},{"location":"blog/2023/otel-in-focus-05/#java","title":"Java","text":"<p>Version 1.26 is the Release Candidate for the Log Bridge. This release enables log appenders to bridge logs from existing log frameworks, allowing users to configure the Log SDK and dictate how logs are processed and exported. In addition, opentelemetry-opentracing-shim is now stable, as well as other bug fixes and improvements.</p> <p>Java Instrumentation includes instrumentation support for vertx-sql-client, as well as several bug fixes.</p>"},{"location":"blog/2023/otel-in-focus-05/#javascript","title":"Javascript","text":"<p>Version 1.13 has been released, adding support for gRPC log export. In addition, a couple bugs have been fixed.</p>"},{"location":"blog/2023/otel-in-focus-05/#python","title":"Python","text":"<p>Version 1.18 adds a new feature that allows histogram aggregation to be set using an environment variable, as well as various bug fixes related to resource detection, exporting, and suppressing instrumentation.</p> <ul> <li>Add ability to select histogram aggregation with an environment variable</li> <li>Move protobuf encoding to its own package</li> <li>Add experimental feature to detect resource detectors in auto instrumentation</li> <li>Fix exporting of ExponentialBucketHistogramAggregation from   opentelemetry.sdk.metrics.view</li> <li>Fix headers types mismatch for OTLP Exporters</li> </ul>"},{"location":"blog/2023/otel-in-focus-05/#net","title":".NET","text":"<p>Version 1.5.0-rc1 includes many bug fixes across a variety of packages.</p>"},{"location":"blog/2023/otel-in-focus-05/#project-and-community-updates","title":"Project and Community Updates","text":""},{"location":"blog/2023/otel-in-focus-05/#youtube-and-meeting-recordings","title":"YouTube and Meeting Recordings","text":"<p>Recently, you may have noticed that the OpenTelemetry YouTube channel stopped publishing meeting recordings. In the future, you will be able to access recordings, transcripts, and chat history for meetings through the Zoom cloud. Please see this issue for more information.</p> <p>We'll be publishing more curated content on the OpenTelemetry channel starting in June, including interviews with end-users and more. Please keep an eye on the OpenTelemetry Blog for updates.</p>"},{"location":"blog/2023/otel-in-focus-05/#from-the-blog","title":"From the blog...","text":"<p>OpenTelemetry Lambda Layers are now available. Congratulations to the Functions-as-a-Service SIG on the release!</p> <p>A new blog series discussing Histograms vs. Summaries and Exponential Histograms has gone up on the blog, giving an overview of this important topic.</p>"},{"location":"blog/2023/otel-in-focus-05/#news-and-upcoming-events","title":"News and Upcoming Events","text":"<p>OpenTelemetry in Practice is coming up on June 8th at 10:00 PT/13:00 ET/19:00 CET featuring Iris Dyrmishi of Farfetch. Please see the #otel-comms channel on the CNCF Slack for more info.</p> <p>Observability Day is coming to KubeCon North America in Chicago! Keep an eye on the KubeCon page for more information. A call for proposals is expected to be available in early June.</p>"},{"location":"blog/2023/outreachy-may-cohort/","title":"Call for participation - Outreachy May 2023","text":"<p>It's almost time for the next Outreachy cohort and I (Juraci Paix\u00e3o Kr\u00f6hling) want to get YOU involved, even if you are not a maintainer.</p> <ol> <li>Perhaps you are NOT a maintainer, but there's a feature you dream on having    implemented? Let me know! I'm looking for project ideas for Outreachy interns    to work on. I'll try to find a mentor for your idea. Given this is all open    source, you'll have a chance to follow the development closely.</li> <li>Are you a maintainer and there's something you want to get done for your    project? Even if you prefer not to be a mentor this time, I'd appreciate to    receive project ideas from you. I'll try to find a mentor to work on it.</li> <li>I hear you want to be a mentor? That's awesome! Outreachy is a great    opportunity to improve your mentoring skills and to get someone to work on    that feature you always wanted to get done but never really got around to    actually do it. Typically, the outcome of the internship is great in many    levels, being an interesting topic for conferences such as KubeCon.</li> </ol> <p>What's a great idea, you ask? Small to medium-sized self-contained tasks, suitable for new contributors. Your intern will likely be a person who's new to our area (observability) and might not be an experienced programmer yet. That said, they will have around 3 months to complete the task, which is typically more than enough! In the past, people have been working on adding auto-instrumentation support for Python threading module, creating component generators for OpenTelemetry Collector, benchmarking the client SDKs, writing user-facing documentation, experimenting with data visualization for distributed tracing and more.</p> <p>Here's where you can get more information about Outreachy and our participation on previous cohorts:</p> <ul> <li>Outreachy</li> <li>CNCF OpenTelemetry: Past community participation in Outreachy</li> </ul> <p>The easiest way to get in touch with us if you have a project idea or want to be a mentor is via CNCF Slack, #outreachy.</p>"},{"location":"blog/2023/php-beta-release/","title":"Opentelemetry PHP Beta Release","text":"<p>The OpenTelemetry PHP SIG is very excited to announce the release of v1.0.0beta1 of OpenTelemetry PHP. This is the culmination of over 3 years of work done by the OpenTelemetry PHP team.</p> <p>We are actively soliciting feedback from the development community for this library. Try the beta release, instrument your PHP app with it, and open an issue if you\u2019d like to see a bug squashed or a new feature added.</p> <p>There are many ways you can get started with our project:</p> <ul> <li>There are examples in the repo to get   you started.</li> <li>The getting started guide can   help you to instrument a sample PHP file.</li> <li>The quote service, is a demo application built   in PHP to showcase the library.</li> </ul> <p>Questions? Feel free to reach out to us in the CNCF #otel-php Slack channel, or come to our SIG meeting, which you can find on the OTel public calendar.</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/","title":"Submitting Your First Conference Talk","text":"<p>Submitting a conference talk in the tech industry can be a thrilling and nerve-wracking experience, especially if it\u2019s your first time. The thought of presenting your ideas to a room full of experts and industry professionals can be intimidating, but also an opportunity to showcase your skills, gain recognition, and network with others in your field.</p> <p>On Wednesday, February 1st, 2023, the OpenTelemetry End User Working Group ran an Observability Abstract Workshop. The goal was to help folks who are new to speaking, and are interested in submitting an observability panel or talk. The workshop featured advice and expertise from OpenTelemetry community members Reese Lee, Daniel Kim, Rynn Mancuso, Juraci Paix\u00e3o Kr\u00f6hling, and Adriana Villela.</p> <p>What follows is a summary of the workshop's main takeaways. Whether you\u2019re a seasoned speaker or just starting out, this guide will provide you with tips to help you make the most of your conference talk submission and increase your chances of being selected to speak.</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#tip-1-just-do-it","title":"Tip #1: Just do it","text":"<p>Don\u2019t let perfection be the enemy of good. The best way to start giving talks is to just start submitting and see what happens. You don\u2019t need to craft the perfect proposal (although tailoring it for the event and target audience will certainly help!) and you don\u2019t need to be an industry-leading subject matter expert.</p> <p>Don\u2019t let any excuse stop you. So many conferences and events are desperate for anybody who simply has the willingness to speak and a little something to share. You may be surprised how happy people are to hear real world experience from their peers.</p> <p>But if you\u2019re still unsure, you may want to consider\u2026</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#tip-2-start-at-a-meetup-or-user-group","title":"Tip #2: Start at a Meetup or User Group","text":"<p>If you\u2019re not sure about your public speaking and want to start in a low-pressure environment, I would definitely recommend pitching any topics to a local meetup or user group. You can even treat your next team meeting as an opportunity to speak, paying attention how you present your thoughts.</p> <p>As a former meetup organizer, I can tell you that they are constantly looking for willing speakers.</p> <p>Meetups are also a great place to workshop a talk or topic that you are planning to present for larger audiences at a conference.</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#tip-3-you-dont-have-to-be-an-expert","title":"Tip #3: You don\u2019t have to be an expert","text":"<p>A lot of people that I talk to feel that they need to be an expert on a topic in order to give a talk about it. This couldn\u2019t be further from the truth. While conferences and audiences love to highlight deep subject matter experts, the insight of beginners can be just as valuable.</p> <p>A common tactic for conference talks is to submit a topic that you are interested in learning more about. If your talk is accepted, it gives you an excuse to learn that topic deeply \u2014 and your fresh perspective can make the topic more accessible to other beginners.</p> <p>This tactic is sometimes referred to as \u201cConference Driven Development\u201d \ud83d\ude02</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#tip-4-you-dont-have-to-be-original","title":"Tip #4: You don\u2019t have to be original","text":"<p>Choosing a topic can be difficult. There is a strong temptation to only submit original topics that the world has never seen before. Resist this urge. Your topics don\u2019t have to be unique or novel to be valuable.</p> <p>In fact, \u201cA beginners guide to X\u201d is probably going to have much broader appeal than \u201cHow we combined seven really specific and niche technologies to solve this one problem\u201d \u2014 although both of those topics could be interesting!</p> <p>If you\u2019re stumped for deeper ideas, consider:</p> <ul> <li>Sharing a learning journey (see Tip #3)</li> <li>Sharing a real world case study about how you used a specific technology to   solve a problem</li> <li>Diving deep on a specific topic or technology</li> <li>Doing a bake-off or round-up of comparable tools</li> </ul> <p>If your topic is touching some Open Source project - you can always ask for the feedback from the expert in the community. They may suggest an interesting angle for your talk. You can even try to find a co-speaker. Don't be shy asking for help and form great connections in the community.</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#where-to-submit-your-talk","title":"Where to submit your talk","text":"<p>I am thrilled to see the resurgence in conferences that is currently underway. There is no shortage of venues in need of up-and-coming speakers.</p> <p>As a starting point, I would recommend looking at smaller community-organized conferences like DevOpsDays and Code Camps. Often these conferences are hosted by a local meetup group, which makes it very easy to get connected with the organizers and ask them exactly what kind of talks they are looking for.</p> <p>Don\u2019t be afraid to submit the same talk to multiple conferences (unless the conferences disallow this, but most don\u2019t). Here are some websites I use to find conferences:</p> <ul> <li>GitHub - scraly/developers-conferences-agenda</li> <li>SeeCFP</li> <li>Sessionize</li> <li>The Linux Foundation</li> </ul>"},{"location":"blog/2023/submitting-your-first-conference-talk/#writing-your-abstract","title":"Writing your abstract","text":"<p>Some conferences just require a few sentences or bullet points about your talk, while others may require a few pages of information including a detailed abstract and bio. In either case, remember to think about the abstract from the (potential) audience\u2019s point of view.</p> <p>It\u2019s important to cover what technologies you will discuss, of course, but you also want to make sure you do a proper introduction to the use case for people who may not know the technology.</p> <p>One of the most valuable talks I ever attended was about a topic I needed (debugging strategies) in a language I had never used at that point (.NET).</p> <p>So really, what you want to tell your audience in your abstract is: what will they learn from attending your talk?</p>"},{"location":"blog/2023/submitting-your-first-conference-talk/#making-the-most-of-your-conference-experience","title":"Making the most of your conference experience","text":"<p>Making the most of a conference could be a topic for an entirely new blog post, but I want to share my top three most important tips:</p> <ol> <li>Wear comfortable shoes</li> <li>Leave room for serendipity (the hallway track)</li> <li>Collect stats, make notes, and record feedback to motivate you to do it again</li> </ol> <p>The first point should be obvious. Most of us are used to spending 10-12 hours a day on our butts. Shifting that weight to our feet for a week can be a lot.</p> <p>The second point is less obvious. At my first conference I felt that I was missing out any time I wasn\u2019t in a session. Now I know better. While the talks are amazing for revving my mental engine, the \u201cHallway Track\u201d is where the best conversations and connections happen.</p> <p>Third is important - any data that may help you remember your experience or be used to brag or convince your company to sponsor your next speaking engagement, are very valuable. You have done a lot of work, be proud of it. The only way to take advantage of this is if you leave some wiggle room in your schedule and aren\u2019t afraid to take breaks when you need them.</p>"},{"location":"blog/2023/sunsetting-opencensus/","title":"Sunsetting OpenCensus","text":"<p>In 2019, we announced that OpenTracing and OpenCensus would be merging to form the OpenTelemetry project. From the start, we considered OpenTelemetry to be the next major version of both OpenTracing and OpenCensus.</p> <p>We are excited to announce that OpenTelemetry has reached feature parity with OpenCensus in C++, .NET, Go, Java, JavaScript, PHP and Python. Stable releases of both the Tracing and Metrics SDKs are available in most of these languages with Go and PHP soon to follow. This means that OpenTelemetry can collect and export telemetry data with the same level of functionality as OpenCensus. Beyond that, OpenTelemetry offers a richer ecosystem of instrumentation libraries and exporters, and an active open source community.</p> <p>As a result, we will be archiving all OpenCensus GitHub repositories (with the exception of census-instrumentation/opencensus-python1) on July 31st, 2023. We are excited to see the long term plan for OpenTelemetry coming to fruition, and encourage all users of OpenCensus to migrate to OpenTelemetry.</p>"},{"location":"blog/2023/sunsetting-opencensus/#how-to-migrate-to-opentelemetry","title":"How to Migrate to OpenTelemetry","text":"<p>One of the key goals of the OpenTelemetry project is to provide backward compatibility with OpenCensus and a migration story for existing users.</p> <p>To help ease the migration path, we provide backward compatibility bridges for the following languages2:</p> <ul> <li>Go</li> <li>Java</li> <li>JavaScript</li> <li>Python</li> </ul> <p>Installing these bridges allows OpenCensus and OpenTelemetry instrumentation to smoothly interoperate, with all of your telemetry flowing out of OpenTelemetry exporters. This lets OpenCensus users incrementally transition all of their instrumentation from OpenCensus to OpenTelemetry, and finally remove OpenCensus libraries from their applications3.</p> <p>While OpenTelemetry was never intended to be a strict superset of OpenCensus, most of the APIs and data models are compatible. Migration should be considered a \"major version bump\" and you may notice some changes in your telemetry.</p> <p>More details on what to expect and some suggested workflows for migration are outlined in the OpenCensus Compatibility specification.</p>"},{"location":"blog/2023/sunsetting-opencensus/#what-to-expect-after-july-31-2023","title":"What to Expect After July 31, 2023","text":"<p>After July 31st, 2023, the OpenCensus project will no longer be maintained. This means that there will be no new features added to the project, and any security vulnerabilities that are found will not be patched.</p> <p>However, the OpenCensus repositories will remain archived on GitHub. This means users will still be able to download the OpenCensus code and use it in their projects. Existing releases of OpenCensus will remain available in public package repositories like NPM and PyPI. We encourage all OpenCensus users to begin planning their project's migration to OpenTelemetry now.</p> <p>One exception to this is the census-instrumentation/opencensus-python repo1.</p> <ol> <li> <p>A number of projects within the <code>opencensus-python</code> repository are still being used as recommended production solutions. These projects will continue to be maintained. For details regarding maintenance timeline, next steps for migration, and general support questions, reach out to repo maintainers.\u00a0\u21a9\u21a9</p> </li> <li> <p>Python and JavaScript shim packages will be released soon.\u00a0\u21a9</p> </li> <li> <p>These shims implement the stable OpenCensus Compatibility specification and will be supported for at least one year following OpenTelemetry's long term support guidelines.\u00a0\u21a9</p> </li> </ol>"},{"location":"blog/2023/demo-birthday/","title":"The OpenTelemetry Demo Turns 1(.4)","text":"<p>It's hard to believe as we prepare our 1.4.0 release but the OpenTelemetry demo is turning 1 year old and it's been 6 months since we declared general availability with our 1.0.0 release.</p>"},{"location":"blog/2023/demo-birthday/#project-milestones","title":"Project Milestones","text":"<p>The demo has achieved remarkable milestones in its first year, with more than 70 contributors, 20 official vendor forks, 780 Github stars, and 180K Docker pulls. The project team has been hard at work adding new capabilities and improving on existing ones with more than 460 merged PRs, 5 re-written services in new languages, and 7 brand new components / services.</p> <p>Time flies when you're stabilizing semantic conventions. But what's actually changed between our 1.0.0 and 1.4.0 releases? Quite a lot actually.</p>"},{"location":"blog/2023/demo-birthday/#the-highlights","title":"The Highlights","text":"<ul> <li>2x build time improvements despite adding additional services</li> <li>Support added for arm64 architectures (M1 and M2 Macs)</li> <li>Async support using Kafka and the new Fraud Detection (Kotlin) / Accounting   (Go) services</li> <li>Kubernetes manifest to enable Kubernetes deployment without using requiring   Helm</li> <li>More out of the box dashboards like our   Collector Data Flow Dashboard</li> <li>A myriad of frontend bug fixes</li> <li>Our first   Connector   in the Collector to demonstrate how telemetry pipelines can be linked</li> <li>New OTel SDKs like the Java logging SDK and JavaScript / Go Metric SDK</li> <li>New manual metric instruments in the Ad, Currency, Product Catalog services</li> <li>PHP no-code change auto-instrumentation</li> <li>Browser and compute resource detectors that enrich our data with   infrastructure information</li> <li>More feature flag scenarios like generating a   failure for every 10th Ad shown</li> <li>General stability improvements to fix service restarts</li> </ul> <p>For detailed changes, check out our in depth release notes or changelog.</p>"},{"location":"blog/2023/demo-birthday/#get-involved","title":"Get Involved","text":"<p>Our contributors are essential to all of this and the project team can't thank them enough. New development is constantly ongoing as we add new capabilities and the community's tools evolve. If you'd like to help, check out our contributing guidance or join our Slack channel.</p>"},{"location":"blog/2023/histograms-vs-summaries/","title":"Histograms vs Summaries","text":"<p>In many ways, histograms and summaries appear quite similar. They both roll up many data points into a data structure for efficient processing, transmission, and storage. They can also both be used to track arbitrary quantiles such as the median or p99 of your data. So how do they differ? Let's dive in.</p>"},{"location":"blog/2023/histograms-vs-summaries/#histograms","title":"Histograms","text":"<p>Since I just published a post about histograms and when they are useful, I will only provide a quick summary here. A histogram is a data structure which describes the distribution of a set of data points. For example, one may collect all response times to an HTTP endpoint and describe them as a histogram with 10 bins ranging from 0 to 1000 milliseconds. Each bin counts the number of requests that fall within its range.</p> <p></p> <p>From this we can estimate \u03c6-quantiles like the 90th percentile. We know there are 1260 requests, so the 1134th-ranked (<code>1260 * .90</code>) request represents the 90th percentile. We can then calculate that the request would fall in the 8th bucket (<code>300 &lt;= x &lt; 500</code>) by summing the bucket counts until we exceed that rank. Finally, using relative rank within the bucket of 24 (<code>1134 - 1110</code>), we can estimate the p90 value to be 360ms (<code>300 + ((24 / 80) * (500 - 300))</code>) using linear interpolation. It is important to know that this is an estimation and could be off by as much as 60ms (<code>360 - 300</code>), a relative error of 17% (<code>60 / 360</code>). This error can be mitigated by configuring more and smaller buckets around your SLO values, but never eliminated.</p> <p>One important property of histograms is that they are aggregatable, meaning that as long as the bucket boundaries line up, an arbitrary number of histograms can be combined into a single histogram with no loss of data or precision. This means that an arbitrary number of hosts can report histogram data structures to a server, which can aggregate and compute quantiles from all of them as if they were reported by a single host. By collecting histograms from 1 or more hosts over a long period of time, developers can gain a strong understanding of how their data is distributed and how that distribution changes over time.</p>"},{"location":"blog/2023/histograms-vs-summaries/#summaries","title":"Summaries","text":"<p>Summaries work in almost the opposite manner. When a summary is configured it is given a \u03c6-quantile to track, an acceptable error range, and a decay rate. For example, a summary may track p99 \u00b1 0.5% with a decay rate of 5 minutes. The math is more complex so it won't be discussed here, but one important distinction is that the value is calculated on the client before it is sent to the server. The most important consequence of this is that summaries from multiple clients cannot be aggregated. Another disadvantage is that if you cannot query arbitrary \u03c6-quantiles, only those which you have configured and collected in advance.</p> <p>Given these disadvantages, summaries do have some advantages. First, they trade off a small performance penalty on the client for a significant reduction in transmission, storage, and server processing cost. In our histogram example above, the distribution is represented as 12 separate timeseries: 1 counter for each bucket + 1 bucket for out of range values + a total sum of all values. That is for a single, relatively modest, histogram with no attributes to multiply cardinality. By comparison, the summary is only a single timeseries for the precomputed <code>p99</code> value. Second, they have very low and configurable relative error rates. In the histogram example above, we had a potential relative error of 17% where our summary is guaranteed to be within \u00b1 0.5% accuracy.</p>"},{"location":"blog/2023/histograms-vs-summaries/#so-which-should-you-choose","title":"So which should you choose?","text":"<p>The disappointing answer is \"it depends,\" and there is no one-size-fits-all solution. If you need to aggregate data from many sources, then histograms may be the right choice. If you are collecting a large number of separate metrics with very strict SLOs, or your Prometheus server is particularly resource constrained, then maybe summaries are the right choice for you. Maybe your ideal solution is a hybrid with some histograms for flexible querying and some summaries for high-accuracy, low-cost alerting. Only you can know the ins and outs of your own system and design an observability solution around it that is accurate and flexible and fits your particular needs. The key is knowing the strengths and limitations of the available tools so you can make informed decisions.</p>"},{"location":"blog/2023/histograms-vs-summaries/#bonus-round-nativeexponential-histograms","title":"Bonus round: native/exponential histograms","text":"<p>I'm planning a longer post on this so I'll keep this short, but many of the key disadvantages of histograms are mitigated by exponential histograms, called native histograms in Prometheus. They are available in Prometheus as an experimental feature since v2.40.0, and stable in the OpenTelemetry specification as of v1.17.0. Exponential histograms come with several advantages:</p> <ul> <li>Very efficient data collection and transmission</li> <li>A constant number of timeseries created (and fewer of them) per histogram</li> <li>Very low relative error rates</li> <li>Automatic bucket boundaries, making them simpler to configure and use</li> </ul> <p>These advantages are accomplished by defining bucket boundaries according to a scale factor, intelligently resizing buckets as your distribution evolves, instead of the traditional method of defining explicit buckets. If you're not happy with the state of your current histograms and summaries, I encourage you to give exponential histograms a try. As of this writing there are no official Prometheus docs on native histograms, but if you stay tuned I plan to add a thorough explanation of them in the coming days.</p> <p>Until then, here are some talks I found helpful:</p> <ul> <li>PromCon EU 2022 - Native Histograms in Prometheus - Ganesh Vernekar</li> <li>KubeCon EU 2023 - Prometheus Native Histograms in Production - Bj\u00f6rn Rabenstein, Grafana Labs</li> <li>Using OpenTelemetry\u2019s Exponential Histograms in Prometheus - Ruslan Kovalov &amp; Ganesh Vernekar</li> </ul> <p>A version of this article was [originally posted][] to the author's blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"blog/2023/jmx-metric-insight/","title":"Gaining JMX Metric Insights with the OpenTelemetry Java Agent","text":"<p>JMX (Java Management Extensions) is a technology that provides a way to manage and monitor Java-based applications. Detailed information about the performance and resource usage of the application can be derived from JMX metrics. This data can help you identify trends or potential issues with the application and take action to address them before they become serious problems. If there are any problems, we can diagnose them with the help of metrics collected, and fine-tune the system for optimal performance.</p> <p>With the addition of the JMX Metric Insight module into the OpenTelemetry Javaagent, we don't need to deploy a separate service just to collect JMX metrics for monitoring our application. The agent can now natively collect and export metrics exposed by application servers through local MBeans available within the instrumented application. The required MBeans and corresponding metrics can be described using a YAML configuration file. The individual metric configurations allow precise metric selection and identification. JMX Metric Insight comes with a number of predefined configurations containing curated sets of JMX metrics for popular application servers or frameworks, such as:</p> <ul> <li>ActiveMQ</li> <li>Hadoop</li> <li>Jetty</li> <li>Kafka Broker</li> <li>Tomcat</li> <li>Wildfly</li> </ul> <p>You can also provide your own metric definitions, through one or more YAML files. The YAML file syntax documentation is available here.</p>"},{"location":"blog/2023/jmx-metric-insight/#observe-kafka-broker-metrics","title":"Observe Kafka Broker metrics","text":"<p>Let's observe the health of our Kafka Broker by exporting the predefined set of metrics using the JMX Metric Insight module and export it to Prometheus.</p> <p>Kafka can be installed on macOS using Homebrew with the following steps:</p> <pre><code>brew install kafka\n</code></pre> <p>To start Zookeeper:</p> <pre><code>zookeeper-server-start /usr/local/etc/kafka/zookeeper.properties\n</code></pre>"},{"location":"blog/2023/jmx-metric-insight/#attach-the-opentelemetry-java-instrumentation-agent","title":"Attach the OpenTelemetry java instrumentation agent","text":"<p>Before starting the kafka broker, attach the OpenTelemetry java instrumentation agent to Kafka Broker by providing options in the KAFKA_OPTS environment variable. You can download the latest release of the agent from here.</p> <pre><code>export KAFKA_OPTS=\"-Dapplication.name=my-kafka-app\n-Dotel.metrics.exporter=prometheus\n-Dotel.exporter.prometheus.port=9464\n-Dotel.service.name=my-kafka-broker\n-Dotel.jmx.target.system=kafka-broker\n-javaagent:/path/to/opentelemetry-javaagent.jar\"\n</code></pre> <p>Now we can start the Kafka Broker:</p> <pre><code>kafka-server-start /usr/local/etc/kafka/server.properties\n</code></pre> <p>The Kafka broker should be up and running now. To test the installation, we can create a topic and use the Kafka console producer and console consumer. Create Kafka Topic:</p> <pre><code>kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic my-test-topic\n</code></pre> <p>Start Kafka console producer to produce messages to the topic we created:</p> <pre><code>$ kafka-console-producer --broker-list localhost:9092 --topic test\n&gt;First message\n&gt;Second message\n</code></pre> <p>Now we will start the Kafka console consumer which will consume messages from the topic from the beginning:</p> <pre><code>$ kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning\nFirst message\nSecond message\n</code></pre> <p>If we are able to see the two messages received by the consumer, that verifies that our Kafka installation is working as expected.</p>"},{"location":"blog/2023/jmx-metric-insight/#export-metrics-to-prometheus","title":"Export metrics to Prometheus","text":"<p>The metrics can be exported by any of the supported metric exporters, to a backend of your choice. The full list of exporters and their configuration options can be found here. For instance, you can export the metrics to an OTel collector using the OTLP exporter, perform some processing and then consume the metrics on a backend of your choice. In this example for the sake of simplicity, we are directly exporting the metrics to Prometheus.</p> <p>We will visualise the metrics on a Grafana dashboard using Prometheus as the data source. In this demo, we\u2019ll deploy Prometheus on Docker. We can create a <code>prometheus.yml</code> file containing this minimal configuration:</p> <pre><code>global:\nscrape_interval: 10s\nevaluation_interval: 10s\nscrape_configs:\n- job_name: my-kafka-broker\nscrape_interval: 5s\nstatic_configs:\n- targets: [host.docker.internal:9464]\n</code></pre> <p>Then run the command below to deploy Prometheus on Docker:</p> <pre><code>docker run -d \\\n-p 9090:9090 \\\n-v path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\\nprom/prometheus\n</code></pre> <p>The Prometheus container should be running now. You can now navigate to http://localhost:9090 and explore the Prometheus dashboard. Here we are viewing the metric <code>kafka_request_count_total</code> on Prometheus.</p> <p></p> <p>More installation options for Prometheus can be found here.</p>"},{"location":"blog/2023/jmx-metric-insight/#view-the-metrics-on-a-grafana-dashboard","title":"View the metrics on a Grafana Dashboard","text":"<p>Now, we are going to visualise the Prometheus metrics in a Grafana dashboard. To do that, first, pull the Grafana docker image using the following command:</p> <pre><code>docker run -d -p 3000:3000 grafana/grafana\n</code></pre> <p>You can now navigate to http://localhost:3000 and explore the Grafana home page. Click on Add Data Source and select Prometheus. Add the HTTP URL, default is http://localhost:9090. After that we can create new Dashboards, with multiple options of visualisations to choose from (Graph, Singlestat, Gauge, Table, Text, etc). We can then create new panels and add any metric we would like to observe. Here is an example dashboard consisting of 6 panels, we are observing a metric in each panel. We can observe the health of our Kafka Broker in real time on this dashboard.</p> <p></p>"},{"location":"blog/2023/jmx-metric-insight/#jmx-metric-insight-in-the-otel-demo-application","title":"JMX Metric Insight in the OTel demo application","text":"<p>You can also explore the official OpenTelemetry Astronomy shop demo application. The message queue service which connects the checkout service with the accounting and fraud detection services is based on Kafka and utilises the JMX Metric Insight module to export Kafka broker metrics out of the box. You can head to the documentation.</p> <p></p> <p></p>"},{"location":"blog/2023/jmx-metric-insight/#further-capabilities-of-the-module","title":"Further Capabilities of the module","text":"<p>In this example, we have only observed a few metrics from the predefined set available for Kafka Broker. Not all metrics exposed by Kafka are part of the set, so if your requirement is a metric not covered in this predefined set, fret not! The module provides you with the option to create your custom metric definition yaml files, so you can observe any metric exposed as an MBean attribute. To get a peek into the structure of the yaml file, We can take a look at a segment of the kafka-broker.yaml:</p> <pre><code>---\nrules:\n- bean: kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec\nmapping:\nCount:\nmetric: kafka.message.count\ntype: counter\ndesc: The number of messages received by the broker\nunit: '{messages}'\n</code></pre> <p>Each file can consist of multiple rules. Each rule can identify a set of one or more MBeans, by the object name. In this example <code>kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec</code> identifies a unique MBean. We are interested in the attribute <code>Count</code> of this MBean, which is specified under <code>mapping</code>. The metric reported will have the name <code>kafka.message.count</code>, instrument type will be <code>counter</code> which indicates the metric is a monotonic sum. It's unit will be <code>{messages}</code>. We have also provided a description of the metric. This yaml segment is simple, to try out more configuration options, you can head to the documentation to understand and try out all the features of the module.</p> <p>Lastly, if you feel some metrics are important to be in the predefined sets of metrics, or in general if you have any idea for enhancement of the module, feel free to contribute to the repository.</p>"},{"location":"blog/2023/k8s-runtime-observability/","title":"Creating a Kubernetes Cluster with Runtime Observability","text":"<p>With contributions from Sebastian Choren, Adnan Rahi\u0107 and Ken Hamric.</p> <p>Kubernetes is an open source system widely used in the cloud native landscape to provide ways to deploy and scale containerized applications in the cloud. Its ability to observe logs and metrics is well-known and documented, but its observability regarding application traces is new.</p> <p>Here is a brief synopsis of the recent activity in the Kubernetes ecosystem:</p> <ul> <li>The first discussions started in December 2018 with a first PR on   implementing instrumentation.</li> <li>A KEP (Kubernetes Enhancement Proposal) was created in January 2020 and later   scoped to API Server   (KEP 647 - API Server Tracing),   while a new KEP for Kubelet was proposed in July 2021   (KEP 2831 Kubelet Tracing).</li> <li>etcd (Kubernetes uses it as an internal   datastore) started to discuss tracing in November 2020   (here) and had a   first version merged in   May 2021.</li> <li>containerd and   CRI-O, two Container Runtime Interfaces for   Kubernetes, started to implement tracing in 2021   (April 2021 for CRI-O and   August 2021 for containerd).</li> <li>API Server tracing was released as   alpha in v1.22   (Aug. 2021) and   beta in v1.27   (Apr. 2023).</li> <li>Kubelet tracing was released as   alpha in v1.25   (Aug. 2022) and   beta in v1.27   (Apr. 2023).</li> </ul> <p>In investigating the current state of tracing with Kubernetes, we found very few articles documenting how to enable it, like this article on Kubernetes blog about <code>kubelet</code> observability. We decided to document our findings and provide step-by-step instructions to set Kubernetes up locally and inspect traces.</p> <p>You\u2019ll learn how to use this instrumentation with Kubernetes to start observing traces on its API (kube-apiserver), node agent (kubelet), and container runtime (containerd) by setting up a local observability environment and later doing a local install of Kubernetes with tracing enabled.</p> <p>First, install the following tools on your local machine:</p> <ul> <li>Docker: a container environment that allows us to   run containerized environments</li> <li>k3d: a wrapper to run k3s (a lightweight   Kubernetes distribution) with Docker</li> <li>kubectl: a Kubernetes CLI to   interact with clusters</li> </ul>"},{"location":"blog/2023/k8s-runtime-observability/#setting-up-an-observability-stack-to-monitor-traces","title":"Setting up an Observability Stack to Monitor Traces","text":"<p>To set up the observability stack, you\u2019ll run the OpenTelemetry (OTel) Collector, a tool that receives telemetry data from different apps and sends it to a tracing backend. As a tracing backend, you\u2019ll use Jaeger, an open source tool that collects traces and lets you query them.</p> <p>On your machine, create a directory called <code>kubetracing</code> and create a file called otel-collector.yaml, copy the contents of the following snippet, and save it in a folder of your preference.</p> <p>This file will configure the OpenTelemetry Collector to receive traces in OpenTelemetry format and export them to Jaeger.</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nprobabilistic_sampler:\nhash_seed: 22\nsampling_percentage: 100\nbatch:\ntimeout: 100ms\nexporters:\nlogging:\nlogLevel: debug\notlp/jaeger:\nendpoint: jaeger:4317\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [probabilistic_sampler, batch]\nexporters: [otlp/jaeger, logging]\n</code></pre> <p>After that, in the same folder, create a docker-compose.yaml file that will have two containers, one for Jaeger and another for the OpenTelemetry Collector.</p> <pre><code>services:\njaeger:\nhealthcheck:\ntest:\n- CMD\n- wget\n- --spider\n- localhost:16686\ntimeout: 3s\ninterval: 1s\nretries: 60\nimage: jaegertracing/all-in-one:latest\nrestart: unless-stopped\nenvironment:\n- COLLECTOR_OTLP_ENABLED=true\nports:\n- 16686:16686\notel-collector:\ncommand:\n- --config\n- /otel-local-config.yaml\ndepends_on:\njaeger:\ncondition: service_started\nimage: otel/opentelemetry-collector:0.54.0\nports:\n- 4317:4317\nvolumes:\n- ./otel-collector.yaml:/otel-local-config.yaml\n</code></pre> <p>Now, start the observability environment by running the following command in the <code>kubetracing</code> folder:</p> <pre><code>docker compose up\n</code></pre> <p>This will start both Jaeger and the OpenTelemetry Collector, enabling them to receive traces from other apps.</p>"},{"location":"blog/2023/k8s-runtime-observability/#creating-a-kubernetes-cluster-with-runtime-observability","title":"Creating a Kubernetes Cluster with Runtime Observability","text":"<p>With the observability environment set up, create the configuration files to enable OpenTelemetry tracing in <code>kube-apiserver</code>, <code>kubelet</code>, and <code>containerd</code>.</p> <p>Inside the <code>kubetracing</code> folder, create a subfolder called <code>config</code> that will have the following two files.</p> <p>First, the apiserver-tracing.yaml, which contains the tracing configuration used by <code>kube-apiserver</code> to export traces containing execution data of the Kubernetes API. In this configuration, set the API to send 100% of the traces with the <code>samplingRatePerMillion</code> config. Set the endpoint as <code>host.k3d.internal:4317</code> to allow the cluster created by <code>k3d/k3s</code> to call another API on your machine. In this case, the OpenTelemetry Collector deployed via <code>docker compose</code> on port <code>4317</code>.</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: TracingConfiguration\nendpoint: host.k3d.internal:4317\nsamplingRatePerMillion: 1000000 # 100%\n</code></pre> <p>The second file is kubelet-tracing.yaml, which provides additional configuration for <code>kubelet</code>. Here you\u2019ll enable the feature flag <code>KubeletTracing</code> (a beta feature in Kubernetes 1.27, the current version when this article was written) and set the same tracing settings that were set on <code>kube-apiserver</code>.</p> <pre><code>apiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nfeatureGates:\nKubeletTracing: true\ntracing:\nendpoint: host.k3d.internal:4317\nsamplingRatePerMillion: 1000000 # 100%\n</code></pre> <p>Returning to the <code>kubetracing</code> folder, create the last file, config.toml.tmpl, which is a template file used by <code>k3s</code> to configure <code>containerd</code>. This file is similar to the default configuration that <code>k3s</code> uses, with two more sections at the end of the file that configures <code>containerd</code> to send traces.</p> <pre><code>version = 2\n\n[plugins.\"io.containerd.internal.v1.opt\"]\n  path = \"{{ .NodeConfig.Containerd.Opt }}\"\n[plugins.\"io.containerd.grpc.v1.cri\"]\n  stream_server_address = \"127.0.0.1\"\n  stream_server_port = \"10010\"\n  enable_selinux = {{ .NodeConfig.SELinux }}\n  enable_unprivileged_ports = {{ .EnableUnprivileged }}\n  enable_unprivileged_icmp = {{ .EnableUnprivileged }}\n\n{{- if .DisableCgroup}}\n  disable_cgroup = true\n{{end}}\n{{- if .IsRunningInUserNS }}\n  disable_apparmor = true\n  restrict_oom_score_adj = true\n{{end}}\n\n{{- if .NodeConfig.AgentConfig.PauseImage }}\n  sandbox_image = \"{{ .NodeConfig.AgentConfig.PauseImage }}\"\n{{end}}\n\n{{- if .NodeConfig.AgentConfig.Snapshotter }}\n[plugins.\"io.containerd.grpc.v1.cri\".containerd]\n  snapshotter = \"{{ .NodeConfig.AgentConfig.Snapshotter }}\"\n  disable_snapshot_annotations = {{ if eq .NodeConfig.AgentConfig.Snapshotter \"stargz\" }}false{{else}}true{{end}}\n{{ if eq .NodeConfig.AgentConfig.Snapshotter \"stargz\" }}\n{{ if .NodeConfig.AgentConfig.ImageServiceSocket }}\n[plugins.\"io.containerd.snapshotter.v1.stargz\"]\ncri_keychain_image_service_path = \"{{ .NodeConfig.AgentConfig.ImageServiceSocket }}\"\n[plugins.\"io.containerd.snapshotter.v1.stargz\".cri_keychain]\nenable_keychain = true\n{{end}}\n{{ if .PrivateRegistryConfig }}\n{{ if .PrivateRegistryConfig.Mirrors }}\n[plugins.\"io.containerd.snapshotter.v1.stargz\".registry.mirrors]{{end}}\n{{range $k, $v := .PrivateRegistryConfig.Mirrors }}\n[plugins.\"io.containerd.snapshotter.v1.stargz\".registry.mirrors.\"{{$k}}\"]\n  endpoint = [{{range $i, $j := $v.Endpoints}}{{if $i}}, {{end}}{{printf \"%q\" .}}{{end}}]\n{{if $v.Rewrites}}\n  [plugins.\"io.containerd.snapshotter.v1.stargz\".registry.mirrors.\"{{$k}}\".rewrite]\n{{range $pattern, $replace := $v.Rewrites}}\n    \"{{$pattern}}\" = \"{{$replace}}\"\n{{end}}\n{{end}}\n{{end}}\n{{range $k, $v := .PrivateRegistryConfig.Configs }}\n{{ if $v.Auth }}\n[plugins.\"io.containerd.snapshotter.v1.stargz\".registry.configs.\"{{$k}}\".auth]\n  {{ if $v.Auth.Username }}username = {{ printf \"%q\" $v.Auth.Username }}{{end}}\n  {{ if $v.Auth.Password }}password = {{ printf \"%q\" $v.Auth.Password }}{{end}}\n  {{ if $v.Auth.Auth }}auth = {{ printf \"%q\" $v.Auth.Auth }}{{end}}\n  {{ if $v.Auth.IdentityToken }}identitytoken = {{ printf \"%q\" $v.Auth.IdentityToken }}{{end}}\n{{end}}\n{{ if $v.TLS }}\n[plugins.\"io.containerd.snapshotter.v1.stargz\".registry.configs.\"{{$k}}\".tls]\n  {{ if $v.TLS.CAFile }}ca_file = \"{{ $v.TLS.CAFile }}\"{{end}}\n  {{ if $v.TLS.CertFile }}cert_file = \"{{ $v.TLS.CertFile }}\"{{end}}\n  {{ if $v.TLS.KeyFile }}key_file = \"{{ $v.TLS.KeyFile }}\"{{end}}\n  {{ if $v.TLS.InsecureSkipVerify }}insecure_skip_verify = true{{end}}\n{{end}}\n{{end}}\n{{end}}\n{{end}}\n{{end}}\n\n{{- if not .NodeConfig.NoFlannel }}\n[plugins.\"io.containerd.grpc.v1.cri\".cni]\n  bin_dir = \"{{ .NodeConfig.AgentConfig.CNIBinDir }}\"\n  conf_dir = \"{{ .NodeConfig.AgentConfig.CNIConfDir }}\"\n{{end}}\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc]\n  runtime_type = \"io.containerd.runc.v2\"\n\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options]\n  SystemdCgroup = {{ .SystemdCgroup }}\n\n{{ if .PrivateRegistryConfig }}\n{{ if .PrivateRegistryConfig.Mirrors }}\n[plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors]{{end}}\n{{range $k, $v := .PrivateRegistryConfig.Mirrors }}\n[plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"{{$k}}\"]\n  endpoint = [{{range $i, $j := $v.Endpoints}}{{if $i}}, {{end}}{{printf \"%q\" .}}{{end}}]\n{{if $v.Rewrites}}\n  [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"{{$k}}\".rewrite]\n{{range $pattern, $replace := $v.Rewrites}}\n    \"{{$pattern}}\" = \"{{$replace}}\"\n{{end}}\n{{end}}\n{{end}}\n\n{{range $k, $v := .PrivateRegistryConfig.Configs }}\n{{ if $v.Auth }}\n[plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"{{$k}}\".auth]\n  {{ if $v.Auth.Username }}username = {{ printf \"%q\" $v.Auth.Username }}{{end}}\n  {{ if $v.Auth.Password }}password = {{ printf \"%q\" $v.Auth.Password }}{{end}}\n  {{ if $v.Auth.Auth }}auth = {{ printf \"%q\" $v.Auth.Auth }}{{end}}\n  {{ if $v.Auth.IdentityToken }}identitytoken = {{ printf \"%q\" $v.Auth.IdentityToken }}{{end}}\n{{end}}\n{{ if $v.TLS }}\n[plugins.\"io.containerd.grpc.v1.cri\".registry.configs.\"{{$k}}\".tls]\n  {{ if $v.TLS.CAFile }}ca_file = \"{{ $v.TLS.CAFile }}\"{{end}}\n  {{ if $v.TLS.CertFile }}cert_file = \"{{ $v.TLS.CertFile }}\"{{end}}\n  {{ if $v.TLS.KeyFile }}key_file = \"{{ $v.TLS.KeyFile }}\"{{end}}\n  {{ if $v.TLS.InsecureSkipVerify }}insecure_skip_verify = true{{end}}\n{{end}}\n{{end}}\n{{end}}\n\n{{range $k, $v := .ExtraRuntimes}}\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"{{$k}}\"]\n  runtime_type = \"{{$v.RuntimeType}}\"\n[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"{{$k}}\".options]\n  BinaryName = \"{{$v.BinaryName}}\"\n{{end}}\n\n[plugins.\"io.containerd.tracing.processor.v1.otlp\"]\n  endpoint = \"host.k3d.internal:4317\"\n  protocol = \"grpc\"\n  insecure = true\n\n[plugins.\"io.containerd.internal.v1.tracing\"]\n  sampling_ratio = 1.0\n  service_name = \"containerd\"\n</code></pre> <p>After creating these files, open a terminal inside the <code>kubetracing</code> folder and run <code>k3d</code> to create a cluster. Before running this command, replace the <code>[CURRENT_PATH]</code> placeholder for the entire path of the <code>kubetracing</code> folder. You can retrieve it by running the <code>echo $PWD</code> command in the terminal in that folder.</p> <pre><code>k3d cluster create tracingcluster \\\n--image=rancher/k3s:v1.27.1-k3s1 \\\n--volume '[CURRENT_PATH]/config.toml.tmpl:/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl@server:*' \\\n--volume '[CURRENT_PATH]/config:/etc/kube-tracing@server:*' \\\n--k3s-arg '--kube-apiserver-arg=tracing-config-file=/etc/kube-tracing/apiserver-tracing.yaml@server:*' \\\n--k3s-arg '--kube-apiserver-arg=feature-gates=APIServerTracing=true@server:*' \\\n--k3s-arg '--kubelet-arg=config=/etc/kube-tracing/kubelet-tracing.yaml@server:*'\n</code></pre> <p>This command will create a Kubernetes cluster with version <code>v1.17.1</code>, and set up in three docker containers on your machine. If you run the command <code>kubectl cluster-info</code> now, you will see this output:</p> <pre><code>Kubernetes control plane is running at https://0.0.0.0:60503\nCoreDNS is running at https://0.0.0.0:60503/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://0.0.0.0:60503/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy\n</code></pre> <p>Going back to the logs of the observability environment, you should see some spans of internal Kubernetes operations being published in OpenTelemetry Collector, like this:</p> <pre><code>Span #90\n    Trace ID       : 03a7bf9008d54f02bcd4f14aa5438202\n    Parent ID      :\n    ID             : d7a10873192f7066\n    Name           : KubernetesAPI\n    Kind           : SPAN_KIND_SERVER\n    Start time     : 2023-05-18 01:51:44.954563708 +0000 UTC\n    End time       : 2023-05-18 01:51:44.957555323 +0000 UTC\n    Status code    : STATUS_CODE_UNSET\n    Status message :\nAttributes:\n     -&gt; net.transport: STRING(ip_tcp)\n     -&gt; net.peer.ip: STRING(127.0.0.1)\n     -&gt; net.peer.port: INT(54678)\n     -&gt; net.host.ip: STRING(127.0.0.1)\n     -&gt; net.host.port: INT(6443)\n     -&gt; http.target: STRING(/api/v1/namespaces/kube-system/pods/helm-install-traefik-crd-8w4wd)\n     -&gt; http.server_name: STRING(KubernetesAPI)\n     -&gt; http.user_agent: STRING(k3s/v1.27.1+k3s1 (linux/amd64) kubernetes/bc5b42c)\n     -&gt; http.scheme: STRING(https)\n     -&gt; http.host: STRING(127.0.0.1:6443)\n     -&gt; http.flavor: STRING(2)\n     -&gt; http.method: STRING(GET)\n     -&gt; http.wrote_bytes: INT(4724)\n     -&gt; http.status_code: INT(200)\n</code></pre>"},{"location":"blog/2023/k8s-runtime-observability/#testing-the-cluster-runtime","title":"Testing the Cluster Runtime","text":"<p>With the Observability environment and the Kubernetes cluster set up, you can now trigger commands against Kubernetes and see traces of these actions in Jaeger.</p> <p>Open the browser, and navigate to the Jaeger UI located at http://localhost:16686/search. You\u2019ll see that the <code>apiserver</code>, <code>containerd</code>, and <code>kubelet</code> services are publishing traces:</p> <p></p> <p>Choose <code>apiserver</code> and click on \"Find Traces\u201d. Here you see traces from the Kubernetes control plane:</p> <p></p> <p>Let\u2019s run a sample command against Kubernetes with <code>kubectl</code>, like running an echo:</p> <pre><code>$ kubectl run -it --rm --restart=Never --image=alpine echo-command -- echo hi\n\n# Output\n# If you don't see a command prompt, try pressing enter.\n# warning: couldn't attach to pod/echo-command, falling back to streaming logs: unable to upgrade connection: container echo-command not found in pod echo-command_default\n# Hi\n# pod \"echo-command\" deleted\n</code></pre> <p>And now, open Jaeger again, choose the <code>kubelet</code> service, operation <code>syncPod</code>, and add the tag <code>k8s.pod=default/echo-command</code>, you should be able to see spans related to this pod:</p> <p></p> <p>Expanding one trace, you\u2019ll see the operations that created this pod:</p> <p></p>"},{"location":"blog/2023/k8s-runtime-observability/#conclusion","title":"Conclusion","text":"<p>Even in beta, both traces for kubelet and apiserver can help a developer understand what\u2019s happening under the hood in Kubernetes and start debugging issues.</p> <p>This will be helpful for developers that create custom tasks, like Kubernetes Operators that update internal resources to add more functionalities to Kubernetes.</p> <p>As a team focused on building an open source tool in the observability space, the opportunity to help the overall OpenTelemetry community was important to us. That\u2019s why we were researching finding new ways of collecting traces from the core Kubernetes engine. With the current level of observability being exposed by Kubernetes we wanted to publish our findings in order to help others interested in seeing the current state of distributed tracing in the Kubernetes engine. Daniel Dias and Sebastian Choren are working on Tracetest, an open-source tool that allows you to develop and test your distributed system with OpenTelemetry. It works with any OTel compatible system and enables trace\u2013based tests to be created. Check it out at https://github.com/kubeshop/tracetest.</p> <p>The example sources used in this article, and setup instructions are available from the Tracetest repo.</p>"},{"location":"blog/2023/k8s-runtime-observability/#references","title":"References","text":"<ul> <li>Traces For Kubernetes System Components</li> <li>Tracing on ContainerD</li> <li>Kubernetes: Tools for Monitoring Resources</li> <li>Getting Started with OTel Collector</li> <li>Boosting Kubernetes container runtime observability with OpenTelemetry</li> </ul>"},{"location":"blog/2023/php-auto-instrumentation/","title":"OpenTelemetry PHP Auto-Instrumentation","text":"<p>Automatic Instrumentation is a process of adding tracing capabilities into user application without modifying its source code. There are several techniques to do that, but all of them more or less work in the same way by injecting additional code into original one during compile time, link time, run-time or by extending the operating system in case of eBPF. This blog post presents method used by OpenTelemetry PHP auto-instrumentation.</p>"},{"location":"blog/2023/php-auto-instrumentation/#prerequisites","title":"Prerequisites","text":"<p>To use the PHP auto-instrumentation, you'll need three things:</p> <ul> <li>PHP 8.0 or higher. The PHP auto-instrumentation uses the Observability API   introduced in PHP 8.0.</li> <li>Composer</li> <li>A C Compiler must be available on your machine</li> </ul>"},{"location":"blog/2023/php-auto-instrumentation/#background-on-the-php-80-observability-api","title":"Background on the PHP 8.0 Observability API","text":"<p>Observability API allows you to register and execute additional code (function) before and after an original one without introducing additional performance penalties in other areas. Before PHP 8.0, the most common technique for adding tracing capabilities was altering the <code>zend_execute_ex</code> function (a monkey patching kind technique). However, this can lead to performance problems, stack overflows at runtime, and a general application overhead that may not be desirable. Another approach considered in the past was plugging into the AST and modifying it during compilation time, but there are not known production ready traces that use this technique.</p>"},{"location":"blog/2023/php-auto-instrumentation/#observability-api-from-auto-instrumentation-perspective","title":"Observability API from auto-instrumentation perspective","text":"<p>At the moment of this writing, observability API is used by c extension and exposes one function with the following interface:</p> <pre><code>function hook(\n    ?string $class,\n    string $function,\n    ?\\Closure $pre = null,\n    ?\\Closure $post = null,\n): bool {}\n</code></pre> <p>This function can be used from user application in order to add additional functionality executed before and after the observed function. The below code snippet shows how to instrument dummy <code>helloworld</code> function:</p> <pre><code>function helloWorld() {\n  echo 'helloWorld';\n}\n\\OpenTelemetry\\Instrumentation\\hook(null, 'helloWorld',\n    static function (?string $class, array $params, ?string $classname, string $functionname, ?string $filename, ?int $lineno)\n    {\n      echo 'before';\n    },\n    static function (mixed $object, array $params, mixed $return, ?Throwable $exception)\n    {\n      echo 'after';\n    }\n);\n</code></pre> <p>In the same way, we have implemented tracing support for some of the most important <code>interfaces/libraries/frameworks</code> that are parts of Contrib repo. Each <code>auto-instrumentation</code> package uses above <code>hook</code> function in order to register and provide tracing functionality. One missing thing, not mentioned yet is an <code>API</code> <code>SDK</code> used to create traces and other necessary components. This is the responsibility of the opentelemetry-php main repo which is foundation for everything.</p> <p></p>"},{"location":"blog/2023/php-auto-instrumentation/#how-to-use-it","title":"How to use it","text":"<p>All components necessary for auto-instrumentation can be installed manually, however we invested time to lower the barrier to entry and have created an installer that can do that for you. This section will show how auto-instrument a simple PHP <code>laravel</code> application created from scratch.</p> <p>The first step is to create a demo application. Here we use the popular laravel framework:</p> <pre><code>composer create-project laravel/laravel example-app\n</code></pre> <p>Next, install opentelemetry-instrumentation-installer.</p> <pre><code>cd example-app\ncomposer require open-telemetry/opentelemetry-instrumentation-installer\n</code></pre> <p>OpenTelemetry instrumentation installer works in two modes:</p> <ul> <li>basic (installs everything with most recent version)</li> <li>advanced (gives control to the user)</li> </ul> <p>After installation, run <code>install-otel-instrumentation</code> with either <code>basic</code> or <code>advanced</code> switch as below.</p> <pre><code>./vendor/bin/install-otel-instrumentation basic\n</code></pre> <p>The final step is to run your application with <code>run-with-otel-instrumentation</code>:</p> <pre><code>./vendor/bin/run-with-otel-instrumentation php -S localhost:8080 -t public public/index.php\n</code></pre> <p>The run-with-otel-instrumentation isn't magic: everything it does can be done by hand by setting environment variables and running your application normally. It is a convenience tool for rapidly testing out OpenTelemetry against an application with a working default configuration.</p> <pre><code>./vendor/bin/run-with-otel-instrumentation php -S localhost:8080 -t public public/index.php\n</code></pre> <p>Now, as a result of triggering request to http://localhost:8080 you should see following result in Jaeger</p> <p></p>"},{"location":"blog/2023/php-auto-instrumentation/#current-status-and-next-steps","title":"Current status and next steps","text":"<p>We have all necessary components in place:</p> <ul> <li>APIs and SDK as a foundation and implementation of opentelemetry   specification.</li> <li>C extension as a foundation for auto-instrumentation.</li> <li>Auto Instrumentation support (WIP) for most important and popular libraries   and frameworks.</li> <li>Development tools that can help lower barrier for users and developers   interested in instrumenting arbitrary code.</li> <li>Documentation</li> </ul> <p>One of our goals is to increase awareness of this work and involve more people that will help us improve it, extend coverage and fix bugs.</p> <p>Please try it out and give us feedback. If you encounter any problems, you can open an issue. Questions? Feel free to reach out to us in the CNCF #otel-php Slack channel, or come to our SIG meeting, which you can find on the OTel public calendar.</p>"},{"location":"blog/2023/why-histograms/","title":"Why Histograms?","text":"<p>A histogram is a multi-value counter that summarizes the distribution of data points. For example, a histogram may have 3 counters which count the occurrences of negative, positive, and zero values respectively. Given a series of numbers, <code>3</code>, <code>-9</code>, <code>7</code>, <code>6</code>, <code>0</code>, and <code>-1</code>, the histogram would count <code>2</code> negative, <code>1</code> zero, and <code>3</code> positive values. A single histogram data point is most commonly represented as a bar chart.</p> <p></p> <p>The above example has only 3 possible output values, but it is common to have many more in a single histogram. A real-world application typically exports a histogram every minute that summarizes a metric for the previous minute. By using histograms this way, you can study how the distribution of your data changes over time.</p>"},{"location":"blog/2023/why-histograms/#what-are-histograms-for","title":"What are histograms for?","text":"<p>There are many uses for histograms, but their power comes from the ability to efficiently answer queries about the distribution of your data. These queries most commonly come in some form like \"what was the median response time in the last minute?\" These are known as \u03c6-quantiles, and often are abbreviated in a shorthand like <code>p50</code> for the 50th percentile or 0.5-quantile, also known as the median. More generally, the \u03c6-quantile is the observation value that ranks at number \u03c6*N among the N observations.</p>"},{"location":"blog/2023/why-histograms/#why-are-histograms-useful","title":"Why are Histograms useful?","text":"<p>A common use-case for histograms in observability is defining service level objectives (SLOs). One example of such an SLO might be \"&gt;=99% of all queries should respond in less than 30ms,\" or \"90% of all page loads should become interactive within 100ms of first paint.\"</p> <p>In the following chart, you can see the <code>p50</code>, <code>p90</code>, and <code>p99</code> response times plotted for some requests over some time. From the data, you can see that 50% of requests are served in around 20-30ms or less, 90% of requests are served in under about 80ms, and 99% of requests are served in under around 90ms. You can very quickly see that at least 50% of your users are receiving very fast response times, but almost all of your users are experiencing response times under 90ms.</p> <p></p>"},{"location":"blog/2023/why-histograms/#other-metric-types","title":"Other metric types","text":"<p>What if you're already defining SLOs based on other metrics? You may have considered defining the SLOs to be based on gauges or counters. This approach can work, but it requires defining your SLOs before understanding your data distribution and requires non-trivial implementation at collection time. It is also inflexible; if you decide to change your SLO from 90% of requests to 99% of requests, you have to make and release code changes, then wait for the old data to age out and the new metric to collect enough data to make useful queries. Because histograms model data as a distribution from start to finish, they enable you to simply change your queries and get answers on the data you've already collected. Particularly with exponential histograms, arbitrary distribution queries can be made with very low relative error rates and minimal resource consumption on both the client and the analysis backend.</p> <p>The inflexibility of not using histograms for SLOs also impacts your ability to gauge impact when your SLO is violated. For example, imagine you are collecting a gauge that calculates the <code>p99</code> of some metric and you define an SLO based on it. When your SLO is violated and an alert is triggered, how do you know it is really only affecting 1% of queries, 10%, or 50%? A histogram allows you to answer that question by querying the percentiles you're interested in.</p> <p>Another option is to collect each quantile you're interested in as a gauge. Some systems, like Prometheus, support this natively using a metric type sometimes called a summary. Summaries can work, but they suffer the same inflexibility as gauges and counters, requiring you to decide ahead of time which quantiles to collect. They also cannot be aggregated, meaning that a <code>p90</code> cannot be accurately calculated from two separate hosts each reporting their own <code>p90</code>.</p>"},{"location":"blog/2023/why-histograms/#other-data-sources-and-metric-types","title":"Other data sources and metric types","text":"<p>You may ask, \"why would I report a separate metric rather than calculating it from my existing log and trace data?\" While it is true that for some use cases, like response times, this may be possible, it is not necessarily possible for all use cases. Even when quantiles can be calculated from existing data, you may run into other problems. You need to be sure your observability backend is able to query and analyze a large amount of existing data on-line or index and analyze it at ingestion time. If you are sampling your logs and traces or employing a data retention policy that ages data out, you need to be sure those things are not affecting derived metrics, or that they are properly re-weighted, or you risk not being able to accurately asses your SLOs. Depending on your sampling strategy, it may not even be possible. Using histograms is a way to avoid these subtle problems if they apply to you.</p> <p>A version of this article was [originally posted][] to the author's blog.</p> <p>[originally posted]: {{% param canonical_url %}}</p>"},{"location":"community/","title":"\u793e\u533a","text":"<p>{{% community-lists %}}</p>"},{"location":"community/#_1","title":"\u53c2\u4e0e\u6700\u7ec8\u7528\u6237\u7ec4","text":"<p>Interested in connecting with other end-users and providing feedback to OpenTelemetry maintainers? Check out the End User Resources to learn more.</p>"},{"location":"community/#_2","title":"\u7279\u522b\u5174\u8da3\u56e2\u4f53","text":"<p>We organize the community into Special Interest Groups (SIGs) in order to improve our workflow and more easily manage a community project. Read more from our community repo.</p>"},{"location":"community/#_3","title":"\u751f\u6001\u7cfb\u7edf","text":"<p>\u6b63\u5728\u5bfb\u627e\u7ec4\u4ef6\u3001\u793a\u4f8b\u3001\u96c6\u6210\u7b49? \u8bf7\u53c2\u9605\u751f\u6001\u7cfb\u7edf\u3002</p>"},{"location":"community/marketing-guidelines/","title":"\u8d21\u732e\u7ec4\u7ec7\u5f00\u653e\u9065\u6d4b\u8425\u9500\u6307\u5357","text":"<p>OpenTelemetry(\u53c8\u540dOTel)\u662f\u7ec8\u7aef\u7528\u6237\u3001\u76f8\u90bbOSS\u9879\u76ee\u548c\u6700\u7ec8\u9500\u552e\u57fa\u4e8eOTel\u6570\u636e\u6216\u7ec4\u4ef6\u7684\u4ea7\u54c1\u548c\u670d\u52a1\u7684\u4f9b\u5e94\u5546\u4e4b\u95f4\u7684\u534f\u4f5c\u3002 \u50cf\u8bb8\u591a\u9762\u5411\u6807\u51c6\u7684\u9879\u76ee\u4e00\u6837\uff0c\u5728OTel\u4e0a\u5408\u4f5c\u7684\u4f9b\u5e94\u5546\u4e5f\u5728\u5e02\u573a\u4e0a\u7ade\u4e89\uff0c\u56e0\u6b64\uff0c\u4e3a\u8d21\u732e\u7ec4\u7ec7\u5982\u4f55\u6c9f\u901a\u548c\u4f20\u9012\u6709\u5173OTel\u7684\u4fe1\u606f\u5efa\u7acb\u4e00\u4e9b\u57fa\u672c\u89c4\u5219\u548c\u671f\u671b\u662f\u5f88\u91cd\u8981\u7684\u3002</p> <p>\u672c\u6587\u6863\u5206\u4e3a\u4e24\u90e8\u5206:</p> <ul> <li>Goals and Guidelines: \u6211\u4eec\u60f3\u8981\u8fbe\u5230\u4ec0\u4e48\u76ee\u6807?\u6211\u4eec\u7684\u6307\u5bfc\u662f\u4ec0\u4e48?</li> <li>Concerns and consequences: \u6211\u4eec\u5982\u4f55\u786e\u5b9a\u662f\u5426\u8fdd\u53cd\u4e86\u6307\u5bfc\u65b9\u9488?\u6211\u4eec\u8be5\u600e\u4e48\u505a\u5462?</li> </ul>"},{"location":"community/marketing-guidelines/#goals-and-guidelines","title":"Goals and Guidelines","text":"<p>There are three high-level focus areas for these goals and guidelines.</p>"},{"location":"community/marketing-guidelines/#i-opentelemetry-is-a-joint-effort","title":"I: OpenTelemetry is a joint effort","text":"<ul> <li>Do\u2019s:</li> <li>Use project collateral such as logo and name in line with the Linux     Foundation\u2019s branding and     trademark usage guidelines</li> <li>Emphasize that OTel would not be possible without collaboration from many     contributors who happen to work for competing vendors and providers</li> <li>Cite names of the other contributors and vendors involved with OTel efforts</li> <li>Emphasize our common goals as a community to improve end user/developer     experiences and empower them</li> <li>Don\u2019ts:</li> <li>Imply that a single provider is responsible for OTel itself, and/or one of     its various component parts</li> <li>Diminish the contributions of another organization or of another individual</li> </ul>"},{"location":"community/marketing-guidelines/#ii-its-not-a-competition","title":"II: It\u2019s not a competition","text":"<ul> <li>Do\u2019s:</li> <li>Emphasize that all contributions are valuable, and that they come in many     shapes and sizes, including:</li> <li>Contributions to the core project code or to language- or framework-specific     SDKs</li> <li>Creating and sharing educational resources (videos, workshops, articles), or     shared resources that can be used for educational purposes (e.g. a sample     app using specific language/framework)</li> <li>Community-building activities such as organizing an event or meetup group</li> <li>Publicly recognize and thank other organizations for their contributions to     OTel</li> <li>Don\u2019ts:</li> <li>Directly compare the volume or value of different contributors to OTel     (E.g., via CNCF devstats)</li> <li>Imply that infrequent or minor contributors to OTel are necessarily     second-class citizens, and/or that their own OTel compatibility should be     questioned as a result (in fact, there\u2019s no reason that any provider needs     to contribute to OTel in order to support it)</li> </ul>"},{"location":"community/marketing-guidelines/#iii-promote-awareness-of-otel-interoperability-and-modularization","title":"III: Promote awareness of OTel interoperability and modularization","text":"<ul> <li>Do\u2019s:</li> <li>\u201cShout from the rooftops\u201d about OTel compatibility \u2013 the more that end-users     understand what they can do with OTel data, the better</li> <li>Emphasize the vendor-neutrality and portability of any OTel integration</li> <li>Don\u2019ts:</li> <li>Imply that an end-user isn\u2019t \u201cUsing OTel\u201d unless they\u2019re using some specific     set of components within OTel (OTel is a \u201cwide\u201d project with many decoupled     components)</li> <li>Publicly denigrate the OTel support of another provider, particularly     without objective evidence</li> </ul>"},{"location":"community/marketing-guidelines/#concerns-and-consequences","title":"Concerns and Consequences","text":"<p>Inevitably there will be instances where vendors (or at least their Marketing departments) run afoul of these guidelines. To date, this hasn\u2019t happened frequently, so we don\u2019t want to create an over-complicated process to handle concerns.</p> <p>Here is how we handle such circumstances:</p> <ol> <li>Whomever notices the relevant public (marketing) content should write an    email to cncf-opentelemetry-governance@lists.cncf.io and include an    explanation of why the content is problematic, ideally referencing the    relevant guidelines above.</li> <li>The OTel Governance Committee (GC) will discuss the case during its next    (weekly) meeting, or asynchronously via email if possible. The OTel GC    guarantees a response via email within two weeks of the initial report.</li> <li>If the GC agrees that there\u2019s a problem, a corrective action will be    recommended to the author of the content in question, and the GC will request    that the organization that published the content train relevant employees on    the content in this document as a further preventative measure.</li> </ol> <p>If a pattern develops with a particular vendor, the GC will meet to discuss more significant consequences \u2013 for instance, removing that vendor\u2019s name from OTel-maintained lists of compatible providers, or simply publicly documenting the pattern of poor community behavior.</p>"},{"location":"community/end-user/","title":"End-user Resources","text":"<p>The OpenTelemetry End User Working Group has heard feedback from users who desire a vendor-agnostic space to discuss adopting OpenTelemetry, so we provide multiple ways for you to connect with other users of the project and share best practices:</p> <ul> <li>A synchronous monthly discussion group</li> <li>A private slack channel</li> <li>Talks about OTel in practice</li> <li>Direct interview/feedback sessions</li> </ul> <p>These forums will bring together operations and development engineers from different organizations to discuss challenges and solutions to achieving ubiquitous observability. Share successes and failures, discover best practices, and meet others who are also on a journey to implement observability powered by OpenTelemetry.</p>"},{"location":"community/end-user/#topics","title":"Topics","text":"<p>This group is what its members make it -- whatever is of interest to the group is fair game!</p> <p>But here are some of the kinds of things we expect will be on the table:</p> <ul> <li>Refactoring with telemetry</li> <li>What is company X doing with OpenTelemetry?</li> <li>Correlating multiple observability signals</li> <li>Maintaining and scaling OpenTelemetry deployments</li> <li>Writing custom instrumentation</li> </ul>"},{"location":"community/end-user/#questions","title":"Questions","text":"<p>Is this group only for OpenTelemetry end users?</p> <p>No! Anyone is welcome to join and discuss their journey to observability. This group is hosted by the OpenTelemetry Community End-User Working Group, so we expect most participants will be from organizations that are evaluating or using OpenTelemetry.</p> <p>I have questions about this, who can I reach out to?</p> <p>You can find members of the End User Working Group in #otel-user-research.</p>"},{"location":"community/end-user/discussion-group/","title":"Monthly Discussion Group","text":"<p>Interested in learning how other end users are implementing OpenTelemetry in their organizations? Come talk shop with other OpenTelemetry end users! This is a vendor-neutral space to discuss best practices, ask questions, and meet other observability-minded folks.</p> <p>Feedback that is shared and collected in these sessions will be routed back to the relevant project maintainers to help drive prioritization of improvements and changes to the project.</p> <p>New for 2023!</p> <ul> <li>Sessions are now available for all regions!</li> <li>You can now find summaries of past discussions every month! Search the blog   for \"End-User Discussions\".</li> <li>A project maintainer and/or a Governance Committee member will be in   attendance at each session to provide additional context, insight, and plans   to user questions and feedback.</li> <li>The Chatham House Rule will no longer be applied, and sessions will be   recorded. This will help make the feedback more discoverable by the   community.</li> </ul> <p>Upcoming sessions</p> <p>Here are upcoming sessions, or you can view them on the OpenTelemetry calendar:</p> <ul> <li>EMEA (Europe, Middle East, and Africa): every third Tuesday of the month   at 11AM CET (GMT +1), join   here</li> <li>March 21</li> <li>April 18</li> <li>May 16</li> <li>APAC (Asia Pacific): every third Wednesday of the month at 11AM India ST   (GMT +5.5), register here to get the Zoom link</li> <li>March 15</li> <li>April 19</li> <li>May 17</li> <li>AMER (Americas): every third Thursday of the month at 9AM PST (GMT -8),   join   here</li> <li>March 16</li> <li>April 20</li> <li>May 18</li> </ul> <p>Past topics/questions have included:</p> <ul> <li>Best practices on monitoring collectors in production</li> <li>Using OTel in CI/CD pipelines</li> <li>What\u2019s holding you back from adopting more OpenTelemetry?</li> <li>Auto vs manual instrumentation use cases</li> <li>How to secure a publicly exposed OTel Collector?</li> </ul> <p>We use a Lean Coffee format where discussion topics are generated and democratically selected by the group at the start of the meeting. Topics are rigorously time-boxed by a facilitator.</p>"},{"location":"community/end-user/interviews-feedback/","title":"Interviews or Feedback Sessions","text":"<p>One of the core functions of the OpenTelemetry End User Working Group is to improve the project by gathering feedback from end users and sharing them with the appropriate SIGs to help drive prioritization of improvements and changes to the project.</p> <p>Direct interview or feedback sessions between an organization and the OpenTelemetry Community is one such resource. In these hour-long sessions, we learn about the organization's OTel adoption and implementation, with particular interest in the following:</p> <ul> <li>Special use cases</li> <li>Challenges with implementation -- what went well? What needs improvement?   These can be anything, from instrumentation to documentation.</li> <li>Challenges with adoption within the organization -- how can the Community help   you drive adoption with your teams?</li> </ul> <p>We will then take the feedback you shared and turn them into actionable tasks for the relevant SIGs, as well as provide context and insight for you during the session as appropriate.</p> <p>We schedule these sessions once a month, usually during one of the EUWG's meetings (alternating Thursdays at 10AM Pacific Time), but can set up a different time if that does not work for you. The sessions are typically attended by a few project maintainers and/or Governance Committee members, as well as some curious general members of the Community.</p> <p>Things to keep in mind:</p> <ul> <li>These sessions are recorded. However, if your organization has compliance   concerns, we can delete them once we've summarized the notes and feedback.</li> <li>We will write up a summary post for the OpenTelemetry blog. Our goal with   these is to drive awareness and adoption of the software, and to chronicle   interesting use cases. These can be anonymized if your org has compliance   concerns.</li> <li>These are open to the public, so other users may come and ask you questions.</li> </ul> <p>To schedule a session, reach out to us in CNCF Community Slack:</p> <ul> <li>In the   #otel-user-research   channel</li> <li>Via direct message: ping   Reese Lee,   Adriana Villela, or   Rynn Mancuso</li> </ul>"},{"location":"community/end-user/otel-in-practice/","title":"OpenTelemetry In Practice","text":"<p>We\u2019re aiming to:</p> <ul> <li>Address practical problems that commonly stop development teams from   succeeding with OpenTelemetry</li> <li>Build stronger connections with developers focused on specific languages</li> <li>Improve the experience of implementing OpenTelemetry in production</li> </ul> <p>Each OpenTelemetry in Practice session will include a half hour of lightning talks and a half hour of open conversation about the topic. We are looking for people to join the OpenTelemetry in Practice team and people to give talks at future events. So if you\u2019re interested in shaping these conversations, reach out in the #otel-user-research channel of the CNCF Slack.</p> <p>Join the OpenTelemetry in Practice Meetup Group to get invited to our next talk!</p>"},{"location":"community/end-user/slack-channel/","title":"Slack Channel","text":"<ul> <li>Confirm your agreement with channel Code of Conduct and reach out to Reese   Lee, Rynn Mancuso, or Adriana Villela on CNCF slack for an invite to   <code>#otel-endusers</code>.</li> <li>Troubleshooting or tactical SDK specific questions are still best directed to   individual SIG channels or the   #opentelemetry channel.</li> <li>Vendor specific questions are still best directed to vendor channels, or if it   doesn\u2019t exist   #otel-vendor</li> </ul>"},{"location":"docs/","title":"\u6587\u6863","text":"<p>OpenTelemetry\uff0c\u4e5f\u88ab\u7b80\u79f0\u4e3aOTel\uff0c\u662f\u4e00\u4e2a\u4f9b\u5e94\u5546\u4e2d\u7acb\u7684\u5f00\u6e90\u53ef\u89c2\u5bdf\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u3001\u751f\u6210\u3001\u6536\u96c6\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\uff0c\u5982\u8ddf\u8e2a\u3001\u5ea6\u91cf\u3001\u65e5\u5fd7\u3002 \u4f5c\u4e3a\u4e00\u4e2a\u884c\u4e1a\u6807\u51c6\uff0c\u5b83\u88ab\u8bb8\u591a\u4f9b\u5e94\u5546\u672c\u5730\u652f\u6301\u3002</p> <p></p>"},{"location":"docs/contribution-guidelines/","title":"Contribution guidelines","text":"<p>OpenTelemetry is an open source project, and we gladly accept new contributions and contributors. Please see the CONTRIBUTING.md file in each SIG repository for information on getting started.</p>"},{"location":"docs/contribution-guidelines/#contributing-to-the-opentelemetry-documentation","title":"Contributing to the OpenTelemetry Documentation","text":"<p>Individual SIGs may maintain documentation above and beyond what is offered here, but we strive for accurate general guidance on using the project from our main website.</p> <p>The per-language API, SDK, and \"Getting Started\" documentation is hosted in each language's GitHub repository. For more information, see the Mirrored Documentation section of the website repository's CONTRIBUTING.md file.</p>"},{"location":"docs/contribution-guidelines/#contributor-license-agreement-cla","title":"Contributor License Agreement (CLA)","text":"<p>Contributors to this and other OpenTelemetry projects require acceptance of our CLA, managed by EasyCLA.</p>"},{"location":"docs/contribution-guidelines/#code-reviews","title":"Code reviews","text":"<p>All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.</p>"},{"location":"docs/contribution-guidelines/#getting-started","title":"Getting started","text":"<p>See README.md for current information.</p>"},{"location":"docs/contribution-guidelines/#code-of-conduct","title":"Code of conduct","text":"<p>OpenTelemetry follows the CNCF Code of Conduct.</p>"},{"location":"docs/what-is-opentelemetry/","title":"\u4ec0\u4e48\u662f\u9065\u6d4b?","text":"<p>\u5fae\u670d\u52a1\u67b6\u6784\u4f7f\u5f00\u53d1\u4eba\u5458\u80fd\u591f\u66f4\u5feb\u3001\u66f4\u72ec\u7acb\u5730\u6784\u5efa\u548c\u53d1\u5e03\u8f6f\u4ef6\uff0c\u56e0\u4e3a\u4ed6\u4eec\u4e0d\u518d\u53d7\u5236\u4e8e\u4e0e\u5355\u7247\u67b6\u6784\u76f8\u5173\u7684\u590d\u6742\u53d1\u5e03\u8fc7\u7a0b\u3002</p> <p>\u968f\u7740\u8fd9\u4e9b\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6269\u5c55\uff0c\u5f00\u53d1\u4eba\u5458\u8d8a\u6765\u8d8a\u96be\u4ee5\u770b\u5230\u4ed6\u4eec\u81ea\u5df1\u7684\u670d\u52a1\u5982\u4f55\u4f9d\u8d56\u6216\u5f71\u54cd\u5176\u4ed6\u670d\u52a1\uff0c\u7279\u522b\u662f\u5728\u90e8\u7f72\u4e4b\u540e\u6216\u4e2d\u65ad\u671f\u95f4\uff0c\u901f\u5ea6\u548c\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002</p> <p>\u53ef\u89c2\u5bdf\u6027\u4f7f\u5f00\u53d1\u4eba\u5458\u548c\u64cd\u4f5c\u4eba\u5458\u90fd\u53ef\u4ee5\u83b7\u5f97\u5bf9\u5176\u7cfb\u7edf\u7684\u53ef\u89c1\u6027\u3002</p>"},{"location":"docs/what-is-opentelemetry/#_1","title":"\u90a3\u53c8\u600e\u6837?","text":"<p>\u4e3a\u4e86\u4f7f\u7cfb\u7edf\u53ef\u89c2\u5bdf\uff0c\u5fc5\u987b\u5bf9\u5176\u8fdb\u884c\u68c0\u6d4b\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u4ee3\u7801\u5fc5\u987b\u53d1\u51fatrace\u3001metrics\u548clogs\u3002 \u7136\u540e\u5fc5\u987b\u5c06\u68c0\u6d4b\u7684\u6570\u636e\u53d1\u9001\u5230\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\u3002 \u73b0\u5728\u6709\u5f88\u591a\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\uff0c\u4ece\u81ea\u6258\u7ba1\u7684\u5f00\u6e90\u5de5\u5177(\u4f8b\u5982Jaeger\u548cZipkin)\u5230\u5546\u4e1aSaaS\u4ea7\u54c1\u3002</p> <p>\u5728\u8fc7\u53bb\uff0c\u68c0\u6d4b\u4ee3\u7801\u7684\u65b9\u5f0f\u5404\u4e0d\u76f8\u540c\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u53ef\u89c2\u5bdf\u7684\u540e\u7aef\u90fd\u6709\u81ea\u5df1\u7684\u68c0\u6d4b\u5e93\u548c\u4ee3\u7406\uff0c\u7528\u4e8e\u5411\u5de5\u5177\u53d1\u9001\u6570\u636e\u3002</p> <p>\u8fd9\u610f\u5473\u7740\u6ca1\u6709\u6807\u51c6\u7684\u6570\u636e\u683c\u5f0f\u6765\u5c06\u6570\u636e\u53d1\u9001\u5230\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\u3002 \u6b64\u5916\uff0c\u5982\u679c\u4e00\u5bb6\u516c\u53f8\u9009\u62e9\u5207\u6362Observability\u540e\u7aef\uff0c\u8fd9\u610f\u5473\u7740\u4ed6\u4eec\u5fc5\u987b\u91cd\u65b0\u7f16\u5199\u4ee3\u7801\u5e76\u914d\u7f6e\u65b0\u7684\u4ee3\u7406\uff0c\u4ee5\u4fbf\u80fd\u591f\u5411\u6240\u9009\u62e9\u7684\u65b0\u5de5\u5177\u53d1\u9001\u9065\u6d4b\u6570\u636e\u3002</p> <p>\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u5316\uff0c\u6700\u7ec8\u7684\u7ed3\u679c\u662f\u7f3a\u4e4f\u6570\u636e\u53ef\u79fb\u690d\u6027\uff0c\u5e76\u4e14\u589e\u52a0\u4e86\u7528\u6237\u7ef4\u62a4\u4eea\u5668\u5e93\u7684\u8d1f\u62c5\u3002</p> <p>\u8ba4\u8bc6\u5230\u6807\u51c6\u5316\u7684\u9700\u8981\uff0c\u4e91\u793e\u533a\u805a\u96c6\u5728\u4e00\u8d77\uff0c\u4e24\u4e2a\u5f00\u6e90\u9879\u76ee\u8bde\u751f\u4e86:OpenTracing(\u4e00\u4e2acloud Native Computing Foundation (CNCF)\u9879\u76ee)\u548cOpenCensus(\u4e00\u4e2aGoogle opensource\u793e\u533a\u9879\u76ee)\u3002</p> <p>OpenTracing \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5382\u5546\u4e2d\u7acb\u7684API\uff0c\u7528\u4e8e\u5c06\u9065\u6d4b\u6570\u636e\u53d1\u9001\u5230\u53ef\u89c2\u5bdf\u7684\u540e\u7aef;\u7136\u800c\uff0c\u5b83\u4f9d\u8d56\u4e8e\u5f00\u53d1\u4eba\u5458\u5b9e\u73b0\u4ed6\u4eec\u81ea\u5df1\u7684\u5e93\u6765\u6ee1\u8db3\u89c4\u8303\u3002</p> <p>OpenCensus \u63d0\u4f9b\u4e86\u4e00\u7ec4\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u5e93\uff0c\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\u6765\u68c0\u6d4b\u4ed6\u4eec\u7684\u4ee3\u7801\u5e76\u5c06\u5176\u53d1\u9001\u5230\u4ed6\u4eec\u652f\u6301\u7684\u4efb\u4f55\u4e00\u4e2a\u540e\u7aef\u3002</p>"},{"location":"docs/what-is-opentelemetry/#opentelemetry","title":"\u4f60\u597d,OpenTelemetry !","text":"<p>\u4e3a\u4e86\u5b9e\u73b0\u5355\u4e00\u6807\u51c6\uff0cOpenCensus\u548cOpenTracing\u4e8e2019\u5e745\u6708\u5408\u5e76\u4e3aOpenTelemetry(\u7b80\u79f0OTel)\u3002 \u4f5c\u4e3aCNCF\u7684\u5b75\u5316\u9879\u76ee\uff0cOpenTelemetry\u517c\u6536\u5e76\u4e3e\u3002</p> <p>OTel\u7684\u76ee\u6807\u662f\u63d0\u4f9b\u4e00\u5957\u6807\u51c6\u5316\u7684\u3001\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684sdk\u3001api\u548c\u5de5\u5177(/docs/collector)\uff0c\u7528\u4e8e\u6444\u53d6\u3001\u8f6c\u6362\u548c\u53d1\u9001\u6570\u636e\u5230\u53ef\u89c2\u5bdf\u6027\u540e\u7aef(\u5373\u5f00\u6e90\u6216\u5546\u4e1a\u4f9b\u5e94\u5546)\u3002</p>"},{"location":"docs/what-is-opentelemetry/#opentelemetry_1","title":"OpenTelemetry\u80fd\u4e3a\u6211\u505a\u4ec0\u4e48?","text":"<p>OTel\u62e5\u6709\u5e7f\u6cdb\u7684\u884c\u4e1a\u652f\u6301\u548c\u4e91\u63d0\u4f9b\u5546\u3001\u5382\u5546\u548c\u7ec8\u7aef\u7528\u6237\u7684\u91c7\u7528\u3002\u5b83\u4e3a\u60a8\u63d0\u4f9b:</p> <ul> <li>\u4e00\u4e2a\u5355\u4e00\u7684\u3001\u4e0e\u5382\u5546\u65e0\u5173\u7684\u68c0\u6d4b\u5e93\u6bcf\u79cd\u8bed\u8a00\uff0c\u652f\u6301\u81ea\u52a8\u548c\u624b\u52a8\u68c0\u6d4b\u3002</li> <li>\u4e00\u4e2a\u72ec\u7acb\u4e8e\u4f9b\u5e94\u5546\u7684\u6536\u96c6\u5668\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u53ef\u4ee5\u4ee5\u591a\u79cd\u65b9\u5f0f\u90e8\u7f72\u3002</li> <li>\u751f\u6210\u3001\u53d1\u51fa\u3001\u6536\u96c6\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u7684\u7aef\u5230\u7aef\u5b9e\u73b0\u3002</li> <li>\u5b8c\u5168\u63a7\u5236\u60a8\u7684\u6570\u636e\uff0c\u5e76\u80fd\u591f\u901a\u8fc7\u914d\u7f6e\u5c06\u6570\u636e\u5e76\u884c\u53d1\u9001\u5230\u591a\u4e2a\u76ee\u7684\u5730\u3002</li> <li>\u5f00\u653e\u6807\u51c6\u8bed\u4e49\u7ea6\u5b9a\uff0c\u786e\u4fdd\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u6570\u636e\u6536\u96c6</li> <li>\u80fd\u591f\u5e76\u884c\u652f\u6301\u591a\u79cd\u4e0a\u4e0b\u6587\u4f20\u64ad\u683c\u5f0f\uff0c\u4ee5\u5e2e\u52a9\u968f\u7740\u6807\u51c6\u7684\u53d1\u5c55\u800c\u8fc1\u79fb\u3002</li> <li>\u65e0\u8bba\u4f60\u5728\u53ef\u89c2\u5bdf\u6027\u4e4b\u65c5\u7684\u54ea\u4e2a\u4f4d\u7f6e\uff0c\u90fd\u662f\u4e00\u6761\u524d\u8fdb\u7684\u9053\u8def\u3002</li> </ul> <p>\u901a\u8fc7\u5bf9\u5404\u79cd\u5f00\u6e90\u548c\u5546\u4e1a\u534f\u8bae\uff0c\u683c\u5f0f\u548c\u4e0a\u4e0b\u6587\u4f20\u64ad\u673a\u5236\u7684\u652f\u6301\uff0c\u4ee5\u53ca\u4e3aOpenTracing\u548cOpenCensus\u9879\u76ee\u63d0\u4f9bshims\uff0c\u91c7\u7528OpenTelemetry\u5f88\u5bb9\u6613\u3002</p>"},{"location":"docs/what-is-opentelemetry/#opentelemetry_2","title":"OpenTelemetry\u4e0d\u662f\u4ec0\u4e48","text":"<p>OpenTelemetry\u4e0d\u50cfJaeger\u6216Prometheus\u90a3\u6837\u662f\u4e00\u4e2a\u53ef\u89c2\u5bdf\u7684\u540e\u7aef\u3002 \u76f8\u53cd\uff0c\u5b83\u652f\u6301\u5c06\u6570\u636e\u5bfc\u51fa\u5230\u5404\u79cd\u5f00\u6e90\u548c\u5546\u4e1a\u540e\u7aef\u3002 \u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63d2\u62d4\u7684\u67b6\u6784\uff0c\u56e0\u6b64\u53ef\u4ee5\u5f88\u5bb9\u6613\u5730\u6dfb\u52a0\u5176\u4ed6\u6280\u672f\u534f\u8bae\u548c\u683c\u5f0f\u3002</p>"},{"location":"docs/what-is-opentelemetry/#_2","title":"\u4e0b\u4e00\u4e2a\u4ec0\u4e48?","text":"<ul> <li>\u5165\u95e8 \u2014\u8df3\u8fdb\u53bb\u5427!</li> <li>\u4e86\u89e3OpenTelemetry\u6982\u5ff5.</li> </ul>"},{"location":"docs/acknowledgements/","title":"Acknowledgements","text":"<p>We would like to acknowledge the following sources for some of the content on this site:</p> <ol> <li>APM is Dying and That's Okay - Lightstep</li> <li>Alexandria Pigram via    Honeycomb for tracing content in    Traces</li> <li>What is OpenTelemetry - Dynatrace</li> <li>Understanding OpenTracing, OpenCensus, and OpenMetrics - BMC</li> <li>Ask Miss O11y: Baggage in OTel - Honeycomb</li> </ol>"},{"location":"docs/collector/","title":"\u6536\u96c6\u5668","text":""},{"location":"docs/collector/#_1","title":"\u4ecb\u7ecd","text":"<p>OpenTelemetry Collector \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u5b9e\u73b0\uff0c\u7528\u4e8e\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570 \u636e\u3002\u5b83\u6d88\u9664\u4e86\u8fd0\u884c\u3001\u64cd\u4f5c\u548c\u7ef4\u62a4\u591a\u4e2a\u4ee3\u7406/\u6536\u96c6\u5668\u7684\u9700\u8981\u3002\u5b83\u5177\u6709\u6539\u8fdb\u7684\u53ef\u4f38\u7f29\u6027\uff0c\u5e76\u652f\u6301 \u5c06\u5f00\u6e90\u53ef\u89c2\u5bdf\u6570\u636e\u683c\u5f0f(\u4f8b\u5982 Jaeger\u3001Prometheus\u3001Fluent Bit \u7b49)\u53d1\u9001\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u5f00\u6e90 \u6216\u5546\u4e1a\u540e\u7aef\u3002\u672c\u5730 Collector \u4ee3\u7406\u662f\u4eea\u5668\u5e93\u5c06\u5176\u9065\u6d4b\u6570\u636e\u5bfc\u51fa\u5230\u7684\u9ed8\u8ba4\u4f4d\u7f6e\u3002</p>"},{"location":"docs/collector/#_2","title":"\u76ee\u6807","text":"<ul> <li>\u53ef\u7528\u6027: \u5408\u7406\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u652f\u6301\u6d41\u884c\u7684\u534f\u8bae\uff0c\u5f00\u7bb1\u5373\u7528\u7684\u8fd0\u884c\u548c\u6536\u96c6\u3002</li> <li>\u8868\u6f14: \u5728\u4e0d\u540c\u7684\u8d1f\u8f7d\u548c\u914d\u7f6e\u4e0b\u9ad8\u5ea6\u7a33\u5b9a\u548c\u9ad8\u6027\u80fd\u3002</li> <li>\u53ef\u89c2\u5bdf\u6027: \u53ef\u89c2\u5bdf\u670d\u52a1\u7684\u8303\u4f8b\u3002</li> <li>\u53ef\u6269\u5c55\u6027: \u65e0\u9700\u89e6\u53ca\u6838\u5fc3\u4ee3\u7801\u5373\u53ef\u81ea\u5b9a\u4e49\u3002</li> <li>\u7edf\u4e00: \u5355\u4e2a\u4ee3\u7801\u5e93\uff0c\u53ef\u4f5c\u4e3a\u652f\u6301\u8ddf\u8e2a\u3001\u5ea6\u91cf\u548c\u65e5\u5fd7(\u672a\u6765)\u7684\u4ee3\u7406\u6216\u6536\u96c6\u5668\u8fdb\u884c\u90e8\u7f72\u3002</li> </ul>"},{"location":"docs/collector/#_3","title":"\u4f55\u65f6\u4f7f\u7528\u6536\u96c6\u5668","text":"<p>\u5bf9\u4e8e\u5927\u591a\u6570\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u5de5\u5177\u5e93\uff0c\u60a8\u90fd\u6709\u9488\u5bf9\u6d41\u884c\u540e\u7aef\u548c OTLP \u7684\u5bfc\u51fa\u5668\u3002\u4f60\u53ef\u80fd\u4f1a\u60f3\uff0c</p> <p>\u5728\u4ec0\u4e48\u60c5\u51b5\u4e0b\u4f7f\u7528\u6536\u96c6\u5668\u53d1\u9001\u6570\u636e\uff0c\u800c\u4e0d\u662f\u8ba9\u6bcf\u4e2a\u670d\u52a1\u76f4\u63a5\u53d1\u9001\u5230\u540e\u7aef?</p> <p>\u5bf9\u4e8e\u5c1d\u8bd5\u548c\u5f00\u59cb\u4f7f\u7528 OpenTelemetry\uff0c\u5c06\u6570\u636e\u76f4\u63a5\u53d1\u9001\u5230\u540e\u7aef\u662f\u5feb\u901f\u83b7\u53d6\u4ef7\u503c\u7684\u597d\u65b9\u6cd5\u3002\u6b64 \u5916\uff0c\u5728\u5f00\u53d1\u6216\u5c0f\u89c4\u6a21\u73af\u5883\u4e2d\uff0c\u60a8\u53ef\u4ee5\u5728\u6ca1\u6709\u6536\u96c6\u5668\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e0d\u9519\u7684\u7ed3\u679c\u3002</p> <p>\u4f46\u662f\uff0c\u901a\u5e38\u6211\u4eec\u5efa\u8bae\u5728\u670d\u52a1\u65c1\u8fb9\u4f7f\u7528\u6536\u96c6\u5668\uff0c\u56e0\u4e3a\u5b83\u5141\u8bb8\u60a8\u7684\u670d\u52a1\u5feb\u901f\u5378\u8f7d\u6570\u636e\uff0c\u5e76\u4e14\u6536\u96c6 \u5668\u53ef\u4ee5\u5904\u7406\u989d\u5916\u7684\u5904\u7406\uff0c\u5982\u91cd\u8bd5\uff0c\u6279\u5904\u7406\uff0c\u52a0\u5bc6\uff0c\u751a\u81f3\u654f\u611f\u6570\u636e\u8fc7\u6ee4\u3002</p> <p>\u8bbe\u7f6e\u6536\u96c6\u5668\u4e5f\u6bd4\u60a8\u60f3\u8c61\u7684\u8981\u5bb9\u6613:\u6bcf\u79cd\u8bed\u8a00\u7684\u9ed8\u8ba4 OTLP \u5bfc\u51fa\u5668 \u90fd\u5047\u5b9a\u6709\u4e00\u4e2a\u672c\u5730\u6536\u96c6\u5668\u7aef\u70b9\uff0c\u56e0\u6b64\u60a8\u542f\u52a8\u6536\u96c6\u5668\u5e76\u5f00\u59cb\u8fdb\u884c\u9065\u6d4b\u3002</p>"},{"location":"docs/collector/#_4","title":"\u72b6\u6001\u548c\u53d1\u5e03","text":"<p>\u6536\u96c6\u5668 \u72b6\u6001\u4e3a:mixed\uff0c\u56e0\u4e3a\u6838\u5fc3\u6536\u96c6\u5668\u7ec4\u4ef6\u5f53\u524d\u5177\u6709\u6df7\u5408\u7684\u7a33\u5b9a\u7ea7\u522b\u3002</p> <p>\u6536\u96c6\u5668\u7ec4\u4ef6 \u6210\u719f\u5ea6\u4e0d\u540c\u3002\u6b63\u5728\u52aa\u529b\u786e\u4fdd\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u6709\u5176\u7a33\u5b9a\u6027\u6587\u6863\u3002\u8981\u8ddf\u8e2a\u6b64\u5de5\u4f5c\u7684 \u8fdb\u5c55\uff0c\u8bf7\u53c2\u9605<code>opentelemetry-collector-contrib</code> issue #10116.</p>"},{"location":"docs/collector/configuration/","title":"\u914d\u7f6e","text":"<p>\u5047\u8bbe\u719f\u6089\u4ee5\u4e0b\u9875\u9762:</p> <ul> <li>\u6570\u636e\u6536\u96c6\u6982\u5ff5\u4ee5\u4fbf\u4e86\u89e3\u9002\u7528\u4e8e OpenTelemetry   Collector \u7684\u5b58\u50a8\u5e93\u3002</li> <li>\u5b89\u5168\u6307\u5bfc</li> </ul>"},{"location":"docs/collector/configuration/#_1","title":"\u57fa\u672c","text":"<p>\u91c7\u96c6\u5668\u7531\u56db\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff0c\u7528\u4e8e\u8bbf\u95ee\u9065\u6d4b\u6570\u636e:</p> <ul> <li> Receivers</li> <li> Processors</li> <li> Exporters</li> <li> Connectors</li> </ul> <p>\u8fd9\u4e9b\u7ec4\u4ef6\u4e00\u65e6\u914d\u7f6e\u597d\uff0c\u5c31\u5fc5\u987b\u901a\u8fc7service\u90e8\u5206\u4e2d\u7684\u7ba1\u9053\u542f\u7528\u3002</p> <p>\u5176\u6b21\uff0c\u8fd8\u6709\u6269\u5c55\uff0c\u5b83\u4eec\u63d0\u4f9b\u4e86\u53ef\u4ee5\u6dfb\u52a0\u5230 Collector \u7684\u529f\u80fd\uff0c\u4f46\u4e0d\u9700\u8981\u76f4 \u63a5\u8bbf\u95ee\u9065\u6d4b\u6570\u636e\uff0c\u4e5f\u4e0d\u662f\u7ba1\u9053\u7684\u4e00\u90e8\u5206\u3002\u5b83\u4eec\u4e5f\u5728service\u90e8\u5206\u4e2d\u542f\u7528\u3002</p> <p>\u4e00\u4e2a\u793a\u4f8b\u914d\u7f6e\u5982\u4e0b:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nbatch:\nexporters:\notlp:\nendpoint: otelcol:4317\nextensions:\nhealth_check:\npprof:\nzpages:\nservice:\nextensions: [health_check, pprof, zpages]\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\nmetrics:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\nlogs:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\n</code></pre> <p>\u8bf7\u6ce8\u610f\uff0c\u63a5\u6536\u5668\u3001\u5904\u7406\u5668\u3001\u8f93\u51fa\u5668\u548c/\u6216\u7ba1\u9053\u662f\u901a\u8fc7\u7ec4\u4ef6\u6807\u8bc6\u7b26\u4ee5<code>type[/name]</code>\u683c\u5f0f\u5b9a\u4e49\u7684 (\u4f8b\u5982: <code>otlp</code> or <code>otlp/2</code>)\u3002\u53ea\u8981\u6807\u8bc6\u7b26\u662f\u552f\u4e00\u7684\uff0c\u7ed9\u5b9a\u7c7b\u578b\u7684\u7ec4\u4ef6\u5c31\u53ef\u4ee5\u5b9a\u4e49\u591a\u6b21\u3002\u4f8b \u5982:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nhttp:\notlp/2:\nprotocols:\ngrpc:\nendpoint: 0.0.0.0:55690\nprocessors:\nbatch:\nbatch/test:\nexporters:\notlp:\nendpoint: otelcol:4317\notlp/2:\nendpoint: otelcol2:4317\nextensions:\nhealth_check:\npprof:\nzpages:\nservice:\nextensions: [health_check, pprof, zpages]\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\ntraces/2:\nreceivers: [otlp/2]\nprocessors: [batch/test]\nexporters: [otlp/2]\nmetrics:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\nlogs:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\n</code></pre> <p>\u914d\u7f6e\u4e5f\u53ef\u4ee5\u5305\u62ec\u5176\u4ed6\u6587\u4ef6\uff0c\u5bfc\u81f4 Collector \u5c06\u4e24\u4e2a\u6587\u4ef6\u5408\u5e76\u5230 YAML \u914d\u7f6e\u7684\u5355\u4e2a\u5185\u5b58\u8868\u793a \u4e2d:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nexporters: ${file:exporters.yaml}\nservice:\nextensions: []\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: []\nexporters: [otlp]\n</code></pre> <p><code>exporters.yaml</code> \u6587\u4ef6\u4e3a:</p> <pre><code>otlp:\nendpoint: otelcol.observability.svc.cluster.local:443\n</code></pre> <p>\u5185\u5b58\u4e2d\u7684\u6700\u7ec8\u7ed3\u679c\u5c06\u662f:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nexporters:\notlp:\nendpoint: otelcol.observability.svc.cluster.local:443\nservice:\nextensions: []\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: []\nexporters: [otlp]\n</code></pre>"},{"location":"docs/collector/configuration/#receivers-","title":"Receivers - \u63a5\u6536\u5668","text":"<p>\u63a5\u6536\u5668\u53ef\u4ee5\u662f\u57fa\u4e8e\u63a8\u6216\u62c9\u7684\uff0c\u5b83\u662f\u6570\u636e\u8fdb\u5165\u6536\u96c6\u5668\u7684\u65b9\u5f0f\u3002\u63a5\u6536\u5668\u53ef\u4ee5\u652f\u6301\u4e00\u4e2a\u6216\u591a \u4e2a\u6570\u636e\u6e90\u3002</p> <p>The <code>receivers:</code> section is how receivers are configured. Many receivers come with default settings so simply specifying the name of the receiver is enough to configure it (for example, <code>zipkin:</code>). If configuration is required or a user wants to change the default configuration then such configuration must be defined in this section. Configuration parameters specified for which the receiver provides a default configuration are overridden.</p> <p><code>receivers:</code>\u90e8\u5206\u662f\u5982\u4f55\u914d\u7f6e\u63a5\u6536\u5668\u7684\u3002\u8bb8\u591a\u63a5\u6536\u5668\u90fd\u5e26\u6709\u9ed8\u8ba4\u8bbe\u7f6e\uff0c\u56e0\u6b64\u53ea\u9700\u6307\u5b9a\u63a5\u6536\u5668 \u7684\u540d\u79f0\u5c31\u8db3\u4ee5\u914d\u7f6e\u5b83(\u4f8b\u5982\uff0c<code>zipkin:</code>)\u3002\u5982\u679c\u9700\u8981\u914d\u7f6e\u6216\u8005\u7528\u6237\u5e0c\u671b\u66f4\u6539\u9ed8\u8ba4\u914d\u7f6e\uff0c\u5219\u5fc5 \u987b\u5728\u672c\u8282\u4e2d\u5b9a\u4e49\u8fd9\u79cd\u914d\u7f6e\u3002\u5c06\u8986\u76d6\u63a5\u6536\u5668\u4e3a\u5176\u63d0\u4f9b\u9ed8\u8ba4\u914d\u7f6e\u7684\u6307\u5b9a\u914d\u7f6e\u53c2\u6570\u3002</p> <p>\u914d\u7f6e\u63a5\u6536\u5668\u4e0d\u4f1a\u542f\u7528\u5b83\u3002\u63a5\u6536\u5668\u901a\u8fc7service\u8282\u4e2d\u7684\u7ba1\u9053\u542f\u7528\u3002</p> <p>\u5fc5\u987b\u914d\u7f6e\u4e00\u4e2a\u6216\u591a\u4e2a\u63a5\u6536\u5668\u3002\u7f3a\u7701\u60c5\u51b5\u4e0b\uff0c\u6ca1\u6709\u914d\u7f6e\u63a5\u6536\u5668\u3002\u4e0b\u9762\u63d0\u4f9b\u4e86\u4e00\u4e2a\u63a5\u6536\u5668\u7684\u57fa\u672c \u793a\u4f8b\u3002</p> <p>\u6709\u5173\u63a5\u6536\u5668\u7684\u8be6\u7ec6\u914d\u7f6e\uff0c\u8bf7\u53c2 \u9605receiver README.md.</p> <pre><code>receivers:\n# Data sources: logs\nfluentforward:\nendpoint: 0.0.0.0:8006\n# Data sources: metrics\nhostmetrics:\nscrapers:\ncpu:\ndisk:\nfilesystem:\nload:\nmemory:\nnetwork:\nprocess:\nprocesses:\npaging:\n# Data sources: traces\njaeger:\nprotocols:\ngrpc:\nthrift_binary:\nthrift_compact:\nthrift_http:\n# Data sources: traces\nkafka:\nprotocol_version: 2.0.0\n# Data sources: traces, metrics\nopencensus:\n# Data sources: traces, metrics, logs\notlp:\nprotocols:\ngrpc:\nhttp:\n# Data sources: metrics\nprometheus:\nconfig:\nscrape_configs:\n- job_name: otel-collector\nscrape_interval: 5s\nstatic_configs:\n- targets: [localhost:8888]\n# Data sources: traces\nzipkin:\n</code></pre>"},{"location":"docs/collector/configuration/#processors-","title":"Processors - \u5904\u7406\u5668","text":"<p>Processors are run on data between being received and being exported. Processors are optional though some are recommended.</p> <p>The <code>processors:</code> section is how processors are configured. Processors may come with default settings, but many require configuration. Any configuration for a processor must be done in this section. Configuration parameters specified for which the processor provides a default configuration are overridden.</p> <p>Configuring a processor does not enable it. Processors are enabled via pipelines within the service section.</p> <p>A basic example of the default processors is provided below. The full list of processors can be found by combining the list found here and here.</p> <p>For detailed processor configuration, please see the processor README.md.</p> <pre><code>processors:\n# Data sources: traces\nattributes:\nactions:\n- key: environment\nvalue: production\naction: insert\n- key: db.statement\naction: delete\n- key: email\naction: hash\n# Data sources: traces, metrics, logs\nbatch:\n# Data sources: metrics\nfilter:\nmetrics:\ninclude:\nmatch_type: regexp\nmetric_names:\n- prefix/.*\n- prefix_.*\n# Data sources: traces, metrics, logs\nmemory_limiter:\ncheck_interval: 5s\nlimit_mib: 4000\nspike_limit_mib: 500\n# Data sources: traces\nresource:\nattributes:\n- key: cloud.zone\nvalue: zone-1\naction: upsert\n- key: k8s.cluster.name\nfrom_attribute: k8s-cluster\naction: insert\n- key: redundant-attribute\naction: delete\n# Data sources: traces\nprobabilistic_sampler:\nhash_seed: 22\nsampling_percentage: 15\n# Data sources: traces\nspan:\nname:\nto_attributes:\nrules:\n- ^\\/api\\/v1\\/document\\/(?P&lt;documentId&gt;.*)\\/update$\nfrom_attributes: [db.svc, operation]\nseparator: '::'\n</code></pre>"},{"location":"docs/collector/configuration/#exporters-","title":"Exporters - \u5bfc\u51fa\u5668","text":"<p>An exporter, which can be push or pull based, is how you send data to one or more backends/destinations. Exporters may support one or more data sources.</p> <p>The <code>exporters:</code> section is how exporters are configured. Exporters may come with default settings, but many require configuration to specify at least the destination and security settings. Any configuration for an exporter must be done in this section. Configuration parameters specified for which the exporter provides a default configuration are overridden.</p> <p>Configuring an exporter does not enable it. Exporters are enabled via pipelines within the service section.</p> <p>One or more exporters must be configured. By default, no exporters are configured. A basic example of exporters is provided below. Certain exporter configurations require x.509 certificates to be created in order to be secure, as described in setting up certificates.</p> <p>For detailed exporter configuration, see the exporter README.md.</p> <pre><code>exporters:\n# Data sources: traces, metrics, logs\nfile:\npath: ./filename.json\n# Data sources: traces\njaeger:\nendpoint: jaeger-all-in-one:14250\ntls:\ncert_file: cert.pem\nkey_file: cert-key.pem\n# Data sources: traces\nkafka:\nprotocol_version: 2.0.0\n# Data sources: traces, metrics, logs\nlogging:\nloglevel: debug\n# Data sources: traces, metrics\nopencensus:\nendpoint: otelcol2:55678\n# Data sources: traces, metrics, logs\notlp:\nendpoint: otelcol2:4317\ntls:\ncert_file: cert.pem\nkey_file: cert-key.pem\n# Data sources: traces, metrics\notlphttp:\nendpoint: https://example.com:4318\n# Data sources: metrics\nprometheus:\nendpoint: prometheus:8889\nnamespace: default\n# Data sources: metrics\nprometheusremotewrite:\nendpoint: http://some.url:9411/api/prom/push\n# For official Prometheus (e.g. running via Docker)\n# endpoint: 'http://prometheus:9090/api/v1/write'\n# tls:\n#   insecure: true\n# Data sources: traces\nzipkin:\nendpoint: http://localhost:9411/api/v2/spans\n</code></pre>"},{"location":"docs/collector/configuration/#connectors-","title":"Connectors - \u8fde\u63a5\u5668","text":"<p>A connector is both an exporter and receiver. As the name suggests a Connector connects two pipelines: It consumes data as an exporter at the end of one pipeline and emits data as a receiver at the start of another pipeline. It may consume and emit data of the same data type, or of different data types. A connector may generate and emit data to summarize the consumed data, or it may simply replicate or route data.</p> <p>The <code>connectors:</code> section is how connectors are configured.</p> <p>Configuring a connectors does not enable it. Connectors are enabled via pipelines within the service section.</p> <p>One or more connectors may be configured. By default, no connectors are configured. A basic example of connectors is provided below.</p> <p>For detailed connector configuration, please see the connector README.md.</p> <pre><code>connectors:\nforward:\ncount:\nspanevents:\nmy.prod.event.count:\ndescription: The number of span events from my prod environment.\nconditions:\n- 'attributes[\"env\"] == \"prod\"'\n- 'name == \"prodevent\"'\nspanmetrics:\nhistogram:\nexplicit:\nbuckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]\ndimensions:\n- name: http.method\ndefault: GET\n- name: http.status_code\ndimensions_cache_size: 1000\naggregation_temporality: 'AGGREGATION_TEMPORALITY_CUMULATIVE'\nservicegraph:\nlatency_histogram_buckets: [1, 2, 3, 4, 5]\ndimensions:\n- dimension-1\n- dimension-2\nstore:\nttl: 1s\nmax_items: 10\n</code></pre>"},{"location":"docs/collector/configuration/#extensions-","title":"Extensions - \u6269\u5c55","text":"<p>Extensions are available primarily for tasks that do not involve processing telemetry data. Examples of extensions include health monitoring, service discovery, and data forwarding. Extensions are optional.</p> <p>The <code>extensions:</code> section is how extensions are configured. Many extensions come with default settings so simply specifying the name of the extension is enough to configure it (for example, <code>health_check:</code>). If configuration is required or a user wants to change the default configuration then such configuration must be defined in this section. Configuration parameters specified for which the extension provides a default configuration are overridden.</p> <p>Configuring an extension does not enable it. Extensions are enabled within the service section.</p> <p>By default, no extensions are configured. A basic example of extensions is provided below.</p> <p>For detailed extension configuration, please see the extension README.md.</p> <pre><code>extensions:\nhealth_check:\npprof:\nzpages:\nmemory_ballast:\nsize_mib: 512\n</code></pre>"},{"location":"docs/collector/configuration/#service-","title":"Service - \u670d\u52a1","text":"<p>The service section is used to configure what components are enabled in the Collector based on the configuration found in the receivers, processors, exporters, and extensions sections. If a component is configured, but not defined within the service section then it is not enabled. The service section consists of three sub-sections:</p> <ul> <li>extensions</li> <li>pipelines</li> <li>telemetry</li> </ul> <p>Extensions consist of a list of all extensions to enable. For example:</p> <pre><code>service:\nextensions: [health_check, pprof, zpages]\n</code></pre> <p>Pipelines can be of the following types:</p> <ul> <li>traces: collects and processes trace data.</li> <li>metrics: collects and processes metric data.</li> <li>logs: collects and processes log data.</li> </ul> <p>A pipeline consists of a set of receivers, processors and exporters. Each receiver/processor/exporter must be defined in the configuration outside of the service section to be included in a pipeline.</p> <p>Note: Each receiver/processor/exporter can be used in more than one pipeline. For processor(s) referenced in multiple pipelines, each pipeline will get a separate instance of that processor(s). This is in contrast to receiver(s)/exporter(s) referenced in multiple pipelines, where only one instance of a receiver/exporter is used for all pipelines. Also note that the order of processors dictates the order in which data is processed.</p> <p>The following is an example pipeline configuration:</p> <pre><code>service:\npipelines:\nmetrics:\nreceivers: [opencensus, prometheus]\nexporters: [opencensus, prometheus]\ntraces:\nreceivers: [opencensus, jaeger]\nprocessors: [batch]\nexporters: [opencensus, zipkin]\n</code></pre> <p>Telemetry is where the telemetry for the collector itself can be configured. It has two subsections: <code>logs</code> and <code>metrics</code>.</p> <p>The <code>logs</code> subsection allows configuration of the logs generated by the collector. By default the collector will write its logs to stderr with a log level of <code>INFO</code>. You can also add static key-value pairs to all logs using the <code>initial_fields</code> section. View the full list of <code>logs</code> options here.</p> <p>The <code>metrics</code> subsection allows configuration of the metrics generated by the collector. By default the collector will generate basic metrics about itself and expose them for scraping at <code>localhost:8888/metrics</code> View the full list of <code>metrics</code> options here.</p> <p>The following is an example telemetry configuration:</p> <pre><code>service:\ntelemetry:\nlogs:\nlevel: debug\ninitial_fields:\nservice: my-instance\nmetrics:\nlevel: detailed\naddress: 0.0.0.0:8888\n</code></pre>"},{"location":"docs/collector/configuration/#_2","title":"\u5176\u4ed6\u4fe1\u606f","text":""},{"location":"docs/collector/configuration/#_3","title":"\u914d\u7f6e\u73af\u5883\u53d8\u91cf","text":"<p>\u5728 Collector \u914d\u7f6e\u4e2d\u652f\u6301\u73af\u5883\u53d8\u91cf\u7684\u4f7f\u7528\u548c\u6269\u5c55\u3002\u4f8b\u5982\uff0c\u8981\u4f7f\u7528\u5b58\u50a8 \u5728<code>DB_KEY</code>\u548c<code>OPERATION</code>\u73af\u5883\u53d8\u91cf\u4e0a\u7684\u503c\uff0c\u4f60\u53ef\u4ee5\u8fd9\u6837\u5199:</p> <pre><code>processors:\nattributes/example:\nactions:\n- key: ${env:DB_KEY}\naction: ${env:OPERATION}\n</code></pre> <p>\u4f7f\u7528<code>$$</code>\u8868\u793a\u6587\u5b57<code>$</code>\u3002\u4f8b\u5982\uff0c\u8868\u793a<code>$DataVisualization</code>\u770b\u8d77\u6765\u50cf\u8fd9\u6837:</p> <pre><code>exporters:\nprometheus:\nendpoint: prometheus:8889\nnamespace: $$DataVisualization\n</code></pre>"},{"location":"docs/collector/configuration/#_4","title":"\u4ee3\u7406\u652f\u6301","text":"<p>\u5229\u7528<code>net/http</code>\u5305\u7684\u51fa\u53e3\u5546(\u4eca\u5929\u90fd\u662f\u8fd9\u6837)\u5c0a\u91cd\u4ee5\u4e0b\u4ee3\u7406\u73af\u5883\u53d8\u91cf:</p> <ul> <li>HTTP_PROXY</li> <li>HTTPS_PROXY</li> <li>NO_PROXY</li> </ul> <p>\u5982\u679c\u5728 Collector \u542f\u52a8\u65f6\u95f4\u8bbe\u7f6e\uff0c\u90a3\u4e48\u65e0\u8bba\u534f\u8bae\u5982\u4f55\uff0c\u5bfc\u51fa\u7a0b\u5e8f\u5c06\u4f1a\u6216\u4e0d\u4f1a\u6309\u7167\u8fd9\u4e9b\u73af\u5883 \u53d8\u91cf\u5b9a\u4e49\u4ee3\u7406\u6d41\u91cf\u3002</p>"},{"location":"docs/collector/configuration/#_5","title":"\u8eab\u4efd\u9a8c\u8bc1","text":"<p>\u5927\u591a\u6570\u66b4\u9732 HTTP \u6216 gRPC \u7aef\u53e3\u7684\u63a5\u6536\u5668\u90fd\u80fd\u591f\u4f7f\u7528\u6536\u96c6\u5668\u7684\u8eab\u4efd\u9a8c\u8bc1\u673a\u5236\u6765\u4fdd\u62a4\uff0c\u5e76\u4e14\u5927 \u591a\u6570\u4f7f\u7528 HTTP \u6216 gRPC \u5ba2\u6237\u7aef\u7684\u5bfc\u51fa\u5668\u90fd\u80fd\u591f\u5411\u4f20\u51fa\u8bf7\u6c42\u6dfb\u52a0\u8eab\u4efd\u9a8c\u8bc1\u6570\u636e\u3002</p> <p>\u6536\u96c6\u5668\u4e2d\u7684\u8eab\u4efd\u9a8c\u8bc1\u673a\u5236\u4f7f\u7528\u6269\u5c55\u673a\u5236\uff0c\u5141\u8bb8\u5c06\u81ea\u5b9a\u4e49\u8eab\u4efd\u9a8c\u8bc1\u5668\u63d2\u5165\u6536\u96c6\u5668\u53d1\u884c\u7248\u4e2d\u3002\u5982 \u679c\u60a8\u5bf9\u5f00\u53d1\u81ea\u5b9a\u4e49\u8eab\u4efd\u9a8c\u8bc1\u5668\u611f\u5174\u8da3\uff0c\u8bf7\u67e5 \u770b\u6784\u5efa\u81ea\u5b9a\u4e49\u8eab\u4efd\u9a8c\u8bc1\u5668\u6587\u6863\u3002</p> <p>\u6bcf\u4e2a\u8eab\u4efd\u9a8c\u8bc1\u6269\u5c55\u90fd\u6709\u4e24\u79cd\u53ef\u80fd\u7684\u7528\u6cd5:\u4f5c\u4e3a\u5bfc\u51fa\u8005\u7684\u5ba2\u6237\u7aef\u8eab\u4efd\u9a8c\u8bc1\u5668\uff0c\u5411\u4f20\u51fa\u8bf7\u6c42\u6dfb\u52a0 \u8eab\u4efd\u9a8c\u8bc1\u6570\u636e;\u4f5c\u4e3a\u63a5\u6536\u65b9\u7684\u670d\u52a1\u5668\u8eab\u4efd\u9a8c\u8bc1\u5668\uff0c\u5bf9\u4f20\u5165\u8fde\u63a5\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\u3002\u8bf7\u53c2\u8003\u8eab\u4efd\u9a8c \u8bc1\u6269\u5c55\u4ee5\u4e86\u89e3\u5176\u529f\u80fd\u5217\u8868\uff0c\u4f46\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u8eab\u4efd\u9a8c\u8bc1\u6269\u5c55\u53ea\u80fd\u5b9e\u73b0\u5176\u4e2d\u4e00\u4e2a\u7279\u5f81\u3002\u6709\u5173\u5df2\u77e5 \u8eab\u4efd\u9a8c\u8bc1\u5668\u7684\u5217\u8868\uff0c\u8bf7\u4f7f\u7528\u672c\u7f51\u7ad9\u63d0\u4f9b \u7684\u6ce8\u518c\u8868\u3002</p> <p>\u82e5\u8981\u5c06\u670d\u52a1\u5668\u8eab\u4efd\u9a8c\u8bc1\u5668\u6dfb\u52a0\u5230\u6536\u96c6\u5668\u4e2d\u7684\u63a5\u6536\u5668\uff0c\u8bf7\u786e\u4fdd:</p> <ol> <li>\u5728<code>.extensions</code>\u4e0b\u6dfb\u52a0\u9a8c\u8bc1\u5668\u6269\u5c55\u53ca\u5176\u914d\u7f6e</li> <li>\u5411<code>.services.extensions</code>\u4e2d\u6dfb\u52a0\u5bf9\u9a8c\u8bc1\u5668\u7684\u5f15\u7528\uff0c\u4ee5\u4fbf\u6536\u96c6\u5668\u52a0\u8f7d\u5b83</li> <li>\u5728<code>.receivers.&lt;your-receiver&gt;.&lt;http-or-grpc-config&gt;.auth</code>\u4e0b\u6dfb\u52a0\u5bf9\u9a8c\u8bc1\u5668\u7684\u5f15\u7528</li> </ol> <p>\u4e0b\u9762\u662f\u4e00\u4e2a\u5728\u63a5\u6536\u7aef\u4f7f\u7528 OIDC \u9a8c\u8bc1\u5668\u7684\u793a\u4f8b\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u4ece\u5145\u5f53\u4ee3\u7406\u7684 OpenTelemetry collector \u63a5\u6536\u6570\u636e\u7684\u8fdc\u7a0b\u6536\u96c6\u5668:</p> <pre><code>extensions:\noidc:\nissuer_url: http://localhost:8080/auth/realms/opentelemetry\naudience: collector\nreceivers:\notlp/auth:\nprotocols:\ngrpc:\nauth:\nauthenticator: oidc\nprocessors:\nexporters:\nlogging:\nservice:\nextensions:\n- oidc\npipelines:\ntraces:\nreceivers:\n- otlp/auth\nprocessors: []\nexporters:\n- logging\n</code></pre> <p>\u5728\u4ee3\u7406\u7aef\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f7f OTLP \u5bfc\u51fa\u5668\u83b7\u5f97 OIDC \u4ee4\u724c\u7684\u793a\u4f8b\uff0c\u5e76\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230\u8fdc\u7a0b\u6536\u96c6\u5668\u7684 \u6bcf\u4e2a RPC \u4e2d:</p> <pre><code>extensions:\noauth2client:\nclient_id: agent\nclient_secret: some-secret\ntoken_url: http://localhost:8080/auth/realms/opentelemetry/protocol/openid-connect/token\nreceivers:\notlp:\nprotocols:\ngrpc:\nendpoint: localhost:4317\nprocessors:\nexporters:\notlp/auth:\nendpoint: remote-collector:4317\nauth:\nauthenticator: oauth2client\nservice:\nextensions:\n- oauth2client\npipelines:\ntraces:\nreceivers:\n- otlp\nprocessors: []\nexporters:\n- otlp/auth\n</code></pre>"},{"location":"docs/collector/configuration/#_6","title":"\u8bbe\u7f6e\u8bc1\u4e66","text":"<p>\u5bf9\u4e8e\u751f\u4ea7\u8bbe\u7f6e\uff0c\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 TLS \u8bc1\u4e66\uff0c\u7528\u4e8e\u5b89\u5168\u901a\u4fe1\u6216 mTLS \u7528\u4e8e\u76f8\u4e92\u8eab\u4efd\u9a8c\u8bc1\u3002 \u8bf7\u53c2\u89c1\u4ee5\u4e0b\u6b65\u9aa4\u751f\u6210\u672c\u793a\u4f8b\u4e2d\u4f7f\u7528\u7684\u81ea\u7b7e\u540d\u8bc1\u4e66\u3002\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528\u5f53\u524d\u7684\u8bc1\u4e66\u4f9b\u5e94\u8fc7\u7a0b\u6765\u83b7 \u53d6\u7528\u4e8e\u751f\u4ea7\u7684\u8bc1\u4e66\u3002</p> <p>\u5b89\u88c5cfssl\uff0c\u5e76\u521b\u5efa\u5982\u4e0b <code>csr.json</code> \u6587\u4ef6:</p> <pre><code>{\n\"hosts\": [\"localhost\", \"127.0.0.1\"],\n\"key\": {\n\"algo\": \"rsa\",\n\"size\": 2048\n},\n\"names\": [\n{\n\"O\": \"OpenTelemetry Example\"\n}\n]\n}\n</code></pre> <p>\u73b0\u5728\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>cfssl genkey -initca csr.json | cfssljson -bare ca\ncfssl gencert -ca ca.pem -ca-key ca-key.pem csr.json | cfssljson -bare cert\n</code></pre> <p>\u8fd9\u5c06\u521b\u5efa\u4e24\u4e2a\u8bc1\u4e66;\u9996\u5148\uff0c<code>ca.pem</code>\u4e2d\u7684\"OpenTelemetry \u793a\u4f8b\"\u8bc1\u4e66\u9881\u53d1\u673a\u6784(CA) \u548c<code>ca-key.pem</code>\u4e2d\u7684\u5173\u8054\u5bc6\u94a5\u5ba2\u6237\u7aef\u8bc1\u4e66<code>cert.pem</code>(\u7531 OpenTelemetry \u793a\u4f8b CA \u7b7e\u540d)\u548c \u5173\u8054\u5bc6\u94a5<code>cert-key.pem</code>\u3002</p>"},{"location":"docs/collector/custom-auth/","title":"\u6784\u5efa\u81ea\u5b9a\u4e49\u8eab\u4efd\u9a8c\u8bc1\u5668","text":"<p>The OpenTelemetry Collector allows receivers and exporters to be connected to authenticators, providing a way to both authenticate incoming connections at the receiver's side, as well as adding authentication data to outgoing requests at the exporter's side.</p> <p>This mechanism is implemented on top of the <code>extensions</code> framework and this document will guide you on implementing your own authenticators. If you are looking for documentation on how to use an existing authenticator, refer to the Getting Started page and to your authenticator's documentation. You can find a list of existing authenticators in this website's registry.</p> <p>Use this guide for general directions on how to build a custom authenticator and refer to the up-to-date API Reference Guide for the actual semantics of each type and function.</p> <p>If at anytime you need assistance, join the #opentelemetry-collector room at the CNCF Slack workspace.</p>"},{"location":"docs/collector/custom-auth/#architecture","title":"Architecture","text":"<p>Authenticators are regular extensions that also satisfy one or more interfaces related to the authentication mechanism:</p> <ul> <li>go.opentelemetry.io/collector/config/configauth/ServerAuthenticator</li> <li>go.opentelemetry.io/collector/config/configauth/GRPCClientAuthenticator</li> <li>go.opentelemetry.io/collector/config/configauth/HTTPClientAuthenticator</li> </ul> <p>Server authenticators are used with receivers, and are able to intercept HTTP and gRPC requests, while client authenticators are used with exporters, able to add authentication data to HTTP and gRPC requests. It is possible for authenticators to implement both interfaces at the same time, allowing a single instance of the extension to be used both for the incoming and outgoing requests. Note that users might still want to have different authenticators for the incoming and outgoing requests, so, don't make your authenticator required to be used at both ends.</p> <p>Once an authenticator extension is available in the collector distribution, it can be referenced in the configuration file as a regular extension:</p> <pre><code>extensions:\noidc:\nreceivers:\nprocessors:\nexporters:\nservice:\nextensions:\n- oidc\npipelines:\ntraces:\nreceivers: []\nprocessors: []\nexporters: []\n</code></pre> <p>However, an authenticator will need to be referenced by a consuming component to be effective. The following example shows the same extension as above, now being used by a receiver named <code>otlp/auth</code>:</p> <pre><code>extensions:\noidc:\nreceivers:\notlp/auth:\nprotocols:\ngrpc:\nauth:\nauthenticator: oidc\nprocessors:\nexporters:\nservice:\nextensions:\n- oidc\npipelines:\ntraces:\nreceivers:\n- otlp/auth\nprocessors: []\nexporters: []\n</code></pre> <p>When multiple instances of a given authenticator are needed, they can have different names:</p> <pre><code>extensions:\noidc/some-provider:\noidc/another-provider:\nreceivers:\notlp/auth:\nprotocols:\ngrpc:\nauth:\nauthenticator: oidc/some-provider\nprocessors:\nexporters:\nservice:\nextensions:\n- oidc/some-provider\n- oidc/another-provider\npipelines:\ntraces:\nreceivers:\n- otlp/auth\nprocessors: []\nexporters: []\n</code></pre>"},{"location":"docs/collector/custom-auth/#server-authenticators","title":"Server authenticators","text":"<p>A server authenticator is essentially an extension with an <code>Authenticate</code> function, receiving the payload headers as parameter. If the authenticator is able to authenticate the incoming connection, it should return a <code>nil</code> error, or the concrete error if it can't. As an extension, the authenticator should make sure to initialize all the resources it needs during the <code>Start</code> phase, and is expected to clean them up upon <code>Shutdown</code>.</p> <p>The <code>Authenticate</code> call is part of the hot path for incoming requests and will block the pipeline, so make sure to properly handle any blocking operations you need to make. Concretely, respect the deadline set by the context, in case one is provided. Also make sure to add enough observability to your extension, especially in the form of metrics and traces, so that users can get setup a notification system in case error rates go up beyond a certain level and can debug specific failures.</p>"},{"location":"docs/collector/custom-auth/#client-authenticators","title":"Client authenticators","text":"<p>A client authenticator is one that implements one or more of the following interfaces:</p> <ul> <li>go.opentelemetry.io/collector/config/configauth/GRPCClientAuthenticator</li> <li>go.opentelemetry.io/collector/config/configauth/HTTPClientAuthenticator</li> </ul> <p>Similar to server authenticators, they are essentially extensions with extra functions, each receiving an object that gives the authenticator an opportunity to inject the authentication data into. For instance, the HTTP client authenticator provides an <code>http.RoundTripper</code>, while the gRPC client authenticator can produce a <code>credentials.PerRPCCredentials</code>.</p>"},{"location":"docs/collector/custom-auth/#adding-your-custom-authenticator-to-a-distribution","title":"Adding your custom authenticator to a distribution","text":"<p>Custom authenticators have to be part of the same binary as the main collector. When building your own authenticator, you'll likely have to build a custom distribution as well, or provide means for your users to consume your extension as part of their own distributions. Fortunately, building a custom distribution can be done using the OpenTelemetry Collector Builder utility.</p>"},{"location":"docs/collector/custom-collector/","title":"\u6784\u5efa\u81ea\u5b9a\u4e49\u6536\u96c6\u5668","text":"<p>If you are planning to build and debug custom collector receivers, processors, extensions, or exporters, you are going to need your own Collector instance. That will allow you to launch and debug your OpenTelemetry Collector components directly within your favorite Golang IDE.</p> <p>The other interesting aspect of approaching the component development this way is that you can use all the debugging features from your IDE (stack traces are great teachers!) to understand how the Collector itself interacts with your component code.</p> <p>The OpenTelemetry Community developed a tool called OpenTelemetry Collector builder (or <code>ocb</code> for short) to assist people in assembling their own distribution, making it easy to build a distribution that includes their custom components along with components that are publicly available.</p> <p>As part of the process the <code>builder</code> will generate the Collector's source code, which you can use to help build and debug your own custom components, so let's get started.</p>"},{"location":"docs/collector/custom-collector/#step-1-install-the-builder","title":"Step 1 - Install the builder","text":"<p>The <code>ocb</code> binary is available as a downloadable asset from OpenTelemetry Collector releases. You will find the list of assets at the bottom of the page. Assets are named based on OS and chipset, so download the one that fits your configuration.</p> <p>The binary has a pretty long name, so you can simply rename it to <code>ocb</code>; and if you are running Linux or macOS, you will also need to provide execution permissions for the binary.</p> <p>Open your terminal and type the following commands to accomplish both operations:</p> <pre><code>mv ocb_{{% param collectorVersion %}}_darwin_amd64 ocb\nchmod 777 ocb\n</code></pre> <p>To make sure the <code>ocb</code> is ready to be used, go to your terminal and type <code>./ocb help</code>, and once you hit enter you should have the output of the <code>help</code> command showing up in your console.</p>"},{"location":"docs/collector/custom-collector/#step-2-create-a-builder-manifest-file","title":"Step 2 - Create a builder manifest file","text":"<p>The builder's <code>manifest</code> file is a <code>yaml</code> where you pass information about the code generation and compile process combined with the components that you would like to add to your Collector's distribution.</p> <p>The <code>manifest</code> starts with a map named <code>dist</code> which contains tags to help you configure the code generation and compile process. In fact, all the tags for <code>dist</code> are the equivalent of the <code>ocb</code> command line <code>flags</code>.</p> <p>Here are the tags for the <code>dist</code> map:</p> Tag Description Optional Default Value module: The module name for the new distribution, following Go mod conventions. Optional, but recommended. Yes <code>go.opentelemetry.io/collector/cmd/builder</code> name: The binary name for your distribution Yes <code>otelcol-custom</code> description: A long name for the application. Yes <code>Custom OpenTelemetry Collector distribution</code> otelcol_version: The OpenTelemetry Collector version to use as base for the distribution. Yes <code>{{% param collectorVersion %}}</code> output_path: The path to write the output (sources and binary). Yes <code>/var/folders/86/s7l1czb16g124tng0d7wyrtw0000gn/T/otelcol-distribution3618633831</code> version: The version for your custom OpenTelemetry Collector. Yes <code>1.0.0</code> go: Which Go binary to use to compile the generated sources. Yes go from the PATH <p>As you can see on the table above, all the <code>dist</code> tags are optional, so you will be adding custom values for them depending if your intentions to make your custom Collector distribution available for consumption by other users or if you are simply leveraging the <code>ocb</code> to bootstrap your component development and testing environment.</p> <p>For this tutorial, you will be creating a Collector's distribution to support the development and testing of components.</p> <p>Go ahead and create a manifest file named <code>builder-config.yaml</code> with the following content:</p> <p>builder-config.yaml</p> <pre><code>dist:\nname: otelcol-dev\ndescription: Basic OTel Collector distribution for Developers\noutput_path: ./otelcol-dev\n</code></pre> <p>Now you need to add the modules representing the components you want to be incorporated in this custom Collector distribution. Take a look at the ocb configuration documentation to understand the different modules and how to add the components.</p> <p>We will be adding the following components to our development and testing collector distribution:</p> <ul> <li>Exporters: Jaeger and Logging</li> <li>Receivers: OTLP</li> <li>Processors: Batch</li> </ul> <p>Here is what my <code>builder-config.yaml</code> manifest file looks after adding the modules for the components above:</p> <pre><code>dist:\nname: otelcol-dev\ndescription: Basic OTel Collector distribution for Developers\noutput_path: ./otelcol-dev\notelcol_version: {{% param collectorVersion %}}\nexporters:\n- gomod:\ngo.opentelemetry.io/collector/exporter/loggingexporter v{{% param collectorVersion %}}\n- gomod:\ngithub.com/open-telemetry/opentelemetry-collector-contrib/exporter/jaegerexporter\nv{{% param collectorVersion %}}\nprocessors:\n- gomod:\ngo.opentelemetry.io/collector/processor/batchprocessor v{{% param collectorVersion %}}\nreceivers:\n- gomod:\ngo.opentelemetry.io/collector/receiver/otlpreceiver v{{% param collectorVersion %}}\n</code></pre>"},{"location":"docs/collector/custom-collector/#step-3-generating-the-code-and-building-your-collectors-distribution","title":"Step 3 - Generating the Code and Building your Collector's distribution.","text":"<p>All you need now is to let the <code>ocb</code> do it's job, so go to your terminal and type the following command:</p> <pre><code>./ocb --config builder-config.yaml\n</code></pre> <p>If everything went well, here is what the output of the command should look like:</p> <pre><code>2022-06-13T14:25:03.037-0500    INFO    internal/command.go:85  OpenTelemetry Collector distribution builder    {\"version\": \"{{% param collectorVersion %}}\", \"date\": \"2023-01-03T15:05:37Z\"}\n2022-06-13T14:25:03.039-0500    INFO    internal/command.go:108 Using config file   {\"path\": \"builder-config.yaml\"}\n2022-06-13T14:25:03.040-0500    INFO    builder/config.go:99    Using go    {\"go-executable\": \"/usr/local/go/bin/go\"}\n2022-06-13T14:25:03.041-0500    INFO    builder/main.go:76  Sources created {\"path\": \"./otelcol-dev\"}\n2022-06-13T14:25:03.445-0500    INFO    builder/main.go:108 Getting go modules\n2022-06-13T14:25:04.675-0500    INFO    builder/main.go:87  Compiling\n2022-06-13T14:25:17.259-0500    INFO    builder/main.go:94  Compiled    {\"binary\": \"./otelcol-dev/otelcol-dev\"}\n</code></pre> <p>As defined in the <code>dist</code> section of your config file, you now have a folder named <code>otelcol-dev</code> containing all the source code and the binary for your Collector's distribution.</p> <p>You can now use the generated code to bootstrap your component development projects and easily build and distribute your own collector distribution with your components.</p>"},{"location":"docs/collector/distributions/","title":"\u5206\u5e03","text":"<p>The OpenTelemetry project currently offers pre-built distributions of the collector. The components included in the distributions can be found by in the <code>manifest.yaml</code> of each distribution.</p> <p>{{% latest_release \"collector-releases\" /%}}</p>"},{"location":"docs/collector/distributions/#custom-distributions","title":"Custom Distributions","text":"<p>For various reasons the existing distributions provided by the OpenTelemetry project may not meet your needs. Whether you want a smaller version, or have the need to implement custom functionality like custom authenticators, receivers, processors, or exporters. The tool used to build distributions ocb (OpenTelemetry Collector Builder) is available to build your own distributions.</p>"},{"location":"docs/collector/getting-started/","title":"\u5165\u95e8","text":"<p>\u5982\u679c\u60a8\u4e0d\u719f\u6089\u9002\u7528\u4e8e OpenTelemetry Collector \u7684\u90e8\u7f72\u6a21\u578b\u3001\u7ec4\u4ef6\u548c\u5b58\u50a8\u5e93\uff0c\u8bf7\u9996\u5148\u67e5 \u770bData Collection\u548cdeployment Methods\u9875\u9762\u3002</p>"},{"location":"docs/collector/getting-started/#_1","title":"\u6f14\u793a","text":"<p>\u90e8\u7f72\u8d1f\u8f7d\u751f\u6210\u5668\u3001\u4ee3\u7406\u548c\u7f51\u5173\u4ee5\u53ca Jaeger\u3001Zipkin \u548c Prometheus \u540e\u7aef\u3002\u66f4\u591a\u4fe1\u606f\u53ef\u4ee5\u5728 \u6f14\u793aREADME.md\u4e2d\u627e\u5230\u3002</p> <pre><code>git clone git@github.com:open-telemetry/opentelemetry-collector-contrib.git --depth 1; \\\ncd opentelemetry-collector-contrib/examples/demo; \\\ndocker compose up -d\n</code></pre>"},{"location":"docs/collector/getting-started/#docker","title":"Docker","text":"<p>\u63d0\u53d6 docker \u6620\u50cf\u5e76\u5728\u5bb9\u5668\u4e2d\u8fd0\u884c\u6536\u96c6\u5668\u3002\u5c06<code>{{% param collectorVersion %}}</code>\u66ff\u6362\u4e3a\u8981 \u8fd0\u884c\u7684\u6536\u96c6\u5668\u7684\u7248\u672c\u3002</p> DockerHubghcr.io <pre><code>docker pull otel/opentelemetry-collector-contrib:{{% param collectorVersion %}}\ndocker run otel/opentelemetry-collector-contrib:{{% param collectorVersion %}}\n</code></pre> <pre><code>docker pull ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:{{% param collectorVersion %}}\ndocker run ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:{{% param collectorVersion %}}\n</code></pre> <p>\u8981\u4ece\u5f53\u524d\u5de5\u4f5c\u76ee\u5f55\u52a0\u8f7d\u81ea\u5b9a\u4e49\u914d\u7f6e <code>config.yaml</code> \uff0c\u5c06\u8be5\u6587\u4ef6\u6302\u8f7d\u4e3a\u5377:</p> DockerHubghcr.io <pre><code>docker run -v $(pwd)/config.yaml:/etc/otelcol-contrib/config.yaml otel/opentelemetry-collector-contrib:{{% param collectorVersion %}}\n</code></pre> <pre><code>docker run -v $(pwd)/config.yaml:/etc/otelcol-contrib/config.yaml ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:{{% param collectorVersion %}}\n</code></pre>"},{"location":"docs/collector/getting-started/#docker-compose","title":"Docker Compose","text":"<p>\u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u6dfb\u52a0 OpenTelemetry \u6536\u96c6\u5668\u5230\u4f60\u73b0\u6709\u7684<code>docker-compose.yaml</code>\u4e2d:</p> <pre><code>otel-collector:\nimage: otel/opentelemetry-collector-contrib\ncommand: [--config=/etc/otel-collector-config.yaml]\nvolumes:\n- ./otel-collector-config.yaml:/etc/otel-collector-config.yaml\nports:\n- 1888:1888 # pprof extension\n- 8888:8888 # Prometheus metrics exposed by the collector\n- 8889:8889 # Prometheus exporter metrics\n- 13133:13133 # health_check extension\n- 4317:4317 # OTLP gRPC receiver\n- 4318:4318 # OTLP http receiver\n- 55679:55679 # zpages extension\n</code></pre>"},{"location":"docs/collector/getting-started/#kubernetes","title":"Kubernetes","text":"<p>\u5c06\u4ee3\u7406\u90e8\u7f72\u4e3a\u5b88\u62a4\u8fdb\u7a0b\u548c\u5355\u4e2a\u7f51\u5173\u5b9e\u4f8b\u3002</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/open-telemetry/opentelemetry-collector/main/examples/k8s/otel-config.yaml\n</code></pre> <p>\u4e0a\u9762\u7684\u793a\u4f8b\u65e8\u5728\u4f5c\u4e3a\u4e00\u4e2a\u8d77\u70b9\uff0c\u5728\u5b9e\u9645\u751f\u4ea7\u4f7f\u7528\u4e4b\u524d\u8fdb\u884c\u6269\u5c55\u548c\u5b9a\u5236\u3002\u5bf9\u4e8e\u751f\u4ea7\u5c31\u7eea\u7684\u5b9a\u5236 \u548c\u5b89\u88c5\uff0c\u8bf7\u53c2\u89c1OpenTelemetry Helm Charts\u3002</p> <p>OpenTelemetry Operator\u4e5f\u53ef\u7528\u4e8e\u63d0\u4f9b\u548c\u7ef4\u62a4 OpenTelemetry Collector \u5b9e\u4f8b\uff0c\u5177\u6709 \u81ea\u52a8\u5347\u7ea7\u5904\u7406\u3001\u57fa\u4e8e OpenTelemetry \u914d\u7f6e\u7684\u201c\u670d\u52a1\u201d\u914d\u7f6e\u3001\u81ea\u52a8\u4fa7\u8f66\u6ce8\u5165\u90e8\u7f72\u7b49\u529f\u80fd\u3002</p>"},{"location":"docs/collector/getting-started/#nomad","title":"Nomad","text":"<p>\u5c06 Collector \u90e8\u7f72\u4e3a\u4ee3\u7406\u3001\u7f51\u5173\u548c\u5b8c\u6574\u6f14\u793a\u7684\u53c2\u8003\u4f5c\u4e1a\u6587\u4ef6\u53ef\u4ee5\u5728Getting Started with OpenTelemetry on HashiCorp Nomad\u4e2d\u627e\u5230\u3002</p>"},{"location":"docs/collector/getting-started/#linux","title":"Linux \u5305","text":"<p>\u6bcf\u4e2a Collector \u7248\u672c\u90fd\u5305\u542b Linux amd64/arm64/i386 \u7cfb\u7edf\u7684 APK\u3001DEB \u548c RPM \u5305\u3002\u5305\u4e2d \u5305\u542b\u4e00\u4e2a\u9ed8\u8ba4\u914d\u7f6e\uff0c\u53ef\u4ee5\u5728\u201c/etc/otelcol/config\u201d\u4e2d\u627e\u5230\u3002yaml \u7684\u5b89\u88c5\u540e\u3002</p> <p>Note: \u81ea\u52a8\u670d\u52a1\u914d\u7f6e\u9700\u8981 <code>systemd</code>\u3002</p>"},{"location":"docs/collector/getting-started/#apk","title":"APK \u5b89\u88c5","text":"<p>\u8981\u5728 alpine \u7cfb\u7edf\u4e0a\u5f00\u59cb\u8fd0\u884c\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 <code>v{{% param collectorVersion %}}</code> \u66ff\u6362\u4e3a\u60a8\u5e0c\u671b\u8fd0\u884c\u7684 Collector \u7248\u672c\u3002</p> AMD64ARM64i386 <pre><code>apk update\napk add wget shadow\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_amd64.apk\napk add --allow-untrusted otelcol_{{% param collectorVersion %}}_linux_amd64.apk\n</code></pre> <pre><code>apk update\napk add wget shadow\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_arm64.apk\napk add --allow-untrusted otelcol_{{% param collectorVersion %}}_linux_arm64.apk\n</code></pre> <pre><code>apk update\napk add wget shadow\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_386.apk\napk add --allow-untrusted otelcol_{{% param collectorVersion %}}_linux_386.apk\n</code></pre>"},{"location":"docs/collector/getting-started/#deb","title":"DEB \u5b89\u88c5","text":"<p>To get started on Debian systems run the following replacing <code>v{{% param collectorVersion %}}</code> with the version of the Collector you wish to run and <code>amd64</code> with the appropriate architecture.</p> AMD64ARM64i386 <pre><code>sudo apt-get update\nsudo apt-get -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_amd64.deb\nsudo dpkg -i otelcol_{{% param collectorVersion %}}_linux_amd64.deb\n</code></pre> <pre><code>sudo apt-get update\nsudo apt-get -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_arm64.deb\nsudo dpkg -i otelcol_{{% param collectorVersion %}}_linux_arm64.deb\n</code></pre> <pre><code>sudo apt-get update\nsudo apt-get -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_386.deb\nsudo dpkg -i otelcol_{{% param collectorVersion %}}_linux_386.deb\n</code></pre>"},{"location":"docs/collector/getting-started/#rpm","title":"RPM \u5b89\u88c5","text":"<p>To get started on Red Hat systems run the following replacing <code>v{{% param collectorVersion %}}</code> with the version of the Collector you wish to run and <code>x86_64</code> with the appropriate architecture.</p> AMD64ARM64i386 <pre><code>sudo yum update\nsudo yum -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_amd64.rpm\nsudo rpm -ivh otelcol_{{% param collectorVersion %}}_linux_amd64.rpm\n</code></pre> <pre><code>sudo yum update\nsudo yum -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_arm64.rpm\nsudo rpm -ivh otelcol_{{% param collectorVersion %}}_linux_arm64.rpm\n</code></pre> <pre><code>sudo yum update\nsudo yum -y install wget systemctl\nwget https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_386.rpm\nsudo rpm -ivh otelcol_{{% param collectorVersion %}}_linux_386.rpm\n</code></pre>"},{"location":"docs/collector/getting-started/#_2","title":"\u624b\u52a8\u5b89\u88c5","text":"<p>Linux releases are available for various architectures. It's possible to download the archive containing the binary and install it on your machine manually:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}} {{&lt; tab AMD64 &gt;}}</p> AMD64ARM64i386ppc64le <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_amd64.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_linux_amd64.tar.gz\n</code></pre> <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_arm64.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_linux_arm64.tar.gz\n</code></pre> <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_386.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_linux_386.tar.gz\n</code></pre> <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_linux_ppc64le.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_linux_ppc64le.tar.gz\n</code></pre>"},{"location":"docs/collector/getting-started/#_3","title":"\u4e1a\u52a1\u81ea\u52a8\u914d\u7f6e","text":"<p>By default, the <code>otelcol</code> systemd service will be started with the <code>--config=/etc/otelcol/config.yaml</code> option after installation. To customize these options, modify the <code>OTELCOL_OPTIONS</code> variable in the <code>/etc/otelcol/otelcol.conf</code> systemd environment file with the appropriate command-line options (run <code>/usr/bin/otelcol --help</code> to see all available options). Additional environment variables can also be passed to the <code>otelcol</code> service by adding them to this file.</p> <p>If either the Collector configuration file or <code>/etc/otelcol/otelcol.conf</code> are modified, restart the <code>otelcol</code> service to apply the changes by running:</p> <pre><code>sudo systemctl restart otelcol\n</code></pre> <p>To check the output from the <code>otelcol</code> service, run:</p> <pre><code>sudo journalctl -u otelcol\n</code></pre>"},{"location":"docs/collector/getting-started/#macos","title":"MacOS \u5305","text":"<p>MacOS releases are available for Intel- &amp; ARM-based systems. They are packaged as gzipped tarballs (<code>.tar.gz</code>) and will need to be unpacked with a tool that supports this compression format:</p> IntelARM <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_darwin_amd64.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_darwin_amd64.tar.gz\n</code></pre> <pre><code>curl --proto '=https' --tlsv1.2 -fOL https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v{{% param collectorVersion %}}/otelcol_{{% param collectorVersion %}}_darwin_arm64.tar.gz\ntar -xvf otelcol_{{% param collectorVersion %}}_darwin_arm64.tar.gz\n</code></pre> <p>Every Collector release includes an <code>otelcol</code> executable that you can run after unpacking.</p>"},{"location":"docs/collector/getting-started/#windows","title":"Windows \u5305","text":"<p>Windows releases are packaged as gzipped tarballs (<code>.tar.gz</code>) and will need to be unpacked with a tool that supports this compression format.</p> <p>Every Collector release includes an <code>otelcol.exe</code> executable that you can run after unpacking.</p>"},{"location":"docs/collector/getting-started/#_4","title":"\u672c\u5730","text":"<p>Builds the latest version of the collector based on the local operating system, runs the binary with all receivers enabled and exports all the data it receives locally to a file. Data is sent to the container and the container scrapes its own Prometheus metrics. The following example uses two terminal windows to better illustrate the collector. In the first terminal window run the following:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-collector.git\ncd opentelemetry-collector\nmake install-tools\nmake otelcorecol\n./bin/otelcorecol_* --config ./examples/local/otel-config.yaml\n</code></pre> <p>In a second terminal window, you can test the newly built collector by doing the following:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-collector-contrib.git\ncd opentelemetry-collector-contrib/examples/demo/server\ngo build -o main main.go; ./main &amp; pid1=\"$!\"\ncd ../client\ngo build -o main main.go; ./main\n</code></pre> <p>To stop the client, use type Ctrl-C. To stop the server, use the <code>kill $pid1</code> command. To stop the collector, type Ctrl-C in its terminal window as well.</p> <p>Note</p> <p>\u4e0a\u9762\u663e\u793a\u7684\u547d\u4ee4\u5728bash shell\u4e2d\u6f14\u793a\u4e86\u8fd9\u4e2a\u8fc7\u7a0b\u3002 \u5bf9\u4e8e\u5176\u4ed6shell\uff0c\u8fd9\u4e9b\u547d\u4ee4\u53ef\u80fd\u7565\u6709\u4e0d\u540c\u3002</p>"},{"location":"docs/collector/management/","title":"\u7ba1\u7406","text":"<p>This document describes how you can manage your OpenTelemetry collector deployment at scale.</p> <p>To get the most out of this page you should know how to install and configure the collector. These topics are covered elsewhere:</p> <ul> <li>Getting Started to understand how to install   the OpenTelemetry collector.</li> <li>Configuration for how to configure the   OpenTelemetry collector, setting up telemetry pipelines.</li> </ul>"},{"location":"docs/collector/management/#basics","title":"Basics","text":"<p>Telemetry collection at scale requires a structured approach to manage agents. Typical agent management tasks include:</p> <ol> <li>Querying the agent information and configuration. The agent information can    include its version, operating system related information, or capabilities.    The configuration of the agent refers to its telemetry collection setup, for    example, the OpenTelemetry collector    configuration.</li> <li>Upgrading/downgrading agents and management of agent-specific packages,    including the base agent functionality and plugins.</li> <li>Applying new configurations to agents. This might be required because of    changes in the environment or due to policy changes.</li> <li>Health and performance monitoring of the agents, typically CPU and memory    usage and also agent-specific metrics, for example, the rate of processing or    backpressure-related information.</li> <li>Connection management between a control plane and the agent such as handling    of TLS certificates (revocation and rotation).</li> </ol> <p>Not every use case requires support for all of the above agent management tasks. In the context of OpenTelemetry task 4. Health and performance monitoring is ideally done using OpenTelemetry.</p>"},{"location":"docs/collector/management/#opamp","title":"OpAMP","text":"<p>Observability vendors and cloud providers offer proprietary solutions for agent management. In the open source observability space, there is an emerging standard that you can use for agent management: Open Agent Management Protocol (OpAMP).</p> <p>The OpAMP specification defines how to manage a fleet of telemetry data agents. These agents can be OpenTelemetry collectors, Fluent Bit or other agents in any arbitrary combination.</p> <p>Note The term \"agent\" is used here as a catch-all term for OpenTelemetry components that respond to OpAMP, this could be the collector but also SDK components.</p> <p>OpAMP is a client/server protocol that supports communication over HTTP and over WebSockets:</p> <ul> <li>The OpAMP server is part of the control plane and acts as the   orchestrator, managing a fleet of telemetry agents.</li> <li>The OpAMP client is part of the data plane. The client side of OpAMP can   be implemented in-process, for example, as the case in OpAMP support in the   OpenTelemetry collector. The client side of OpAMP   could alternatively be implemented out-of-process. For this latter option, you   can use a supervisor that takes care of the OpAMP specific communication with   the OpAMP server and at the same time controls the telemetry agent, for   example to apply a configuration or to upgrade it. Note that the   supervisor/telemetry communication is not part of OpAMP.</li> </ul> <p>Let's have a look at a concrete setup:</p> <p></p> <ol> <li>The OpenTelemetry collector, configured with pipeline(s) to:</li> <li>(A) receive signals from downstream sources</li> <li>(B) export signals to upstream destinations, potentially including      telemetry about the collector itself (represented by the OpAMP <code>own_xxx</code>      connection settings).</li> <li>The bi-directional OpAMP control flow between the control plane implementing    the server-side OpAMP part and the collector (or a supervisor controlling the    collector) implementing OpAMP client-side.</li> </ol> <p>You can try out a simple OpAMP setup yourself by using the OpAMP protocol implementation in Go. For the following walkthrough you will need to have Go in version 1.19 or above available.</p> <p>We will set up a simple OpAMP control plane consisting of an example OpAMP server and let an OpenTelemetry collector connect to it via an example OpAMP supervisor.</p> <p>First, clone the <code>open-telemetry/opamp-go</code> repo:</p> <pre><code>git clone https://github.com/open-telemetry/opamp-go.git\n</code></pre> <p>Next, we need an OpenTelemetry collector binary that the OpAMP supervisor can manage. For that, install the OpenTelemetry Collector Contrib distro. The path to the collector binary (where you installed it into) is referred to as <code>$OTEL_COLLECTOR_BINARY</code> in the following.</p> <p>In the <code>./opamp-go/internal/examples/server</code> directory, launch the OpAMP server:</p> <pre><code>$ go run .\n2023/02/08 13:31:32.004501 [MAIN] OpAMP Server starting...\n2023/02/08 13:31:32.004815 [MAIN] OpAMP Server running...\n</code></pre> <p>In the <code>./opamp-go/internal/examples/supervisor</code> directory create a file named <code>supervisor.yaml</code> with the following content (telling the supervisor where to find the server and what OpenTelemetry collector binary to manage):</p> <pre><code>server:\nendpoint: ws://127.0.0.1:4320/v1/opamp\nagent:\nexecutable: $OTEL_COLLECTOR_BINARY\n</code></pre> <p>Note Make sure to replace <code>$OTEL_COLLECTOR_BINARY</code> with the actual file path. For example, in Linux or macOS, if you installed the collector in <code>/usr/local/bin/</code> then you would replace <code>$OTEL_COLLECTOR_BINARY</code> with <code>/usr/local/bin/otelcol</code>.</p> <p>Next, create a collector configuration as follows (save it in a file called <code>effective.yaml</code> in the <code>./opamp-go/internal/examples/supervisor</code> directory):</p> <pre><code>receivers:\nprometheus/own_metrics:\nconfig:\nscrape_configs:\n- job_name: otel-collector\nscrape_interval: 10s\nstatic_configs:\n- targets: [0.0.0.0:8888]\nhostmetrics:\ncollection_interval: 10s\nscrapers:\nload:\nfilesystem:\nmemory:\nnetwork:\nexporters:\nlogging:\nverbosity: detailed\nservice:\npipelines:\nmetrics:\nreceivers: [hostmetrics, prometheus/own_metrics]\nexporters: [logging]\n</code></pre> <p>Now it's time to launch the supervisor (which in turn will launch your OpenTelemetry collector):</p> <pre><code>$ go run .\n2023/02/08 13:32:54 Supervisor starting, id=01GRRKNBJE06AFVGQT5ZYC0GEK, type=io.opentelemetry.collector, version=1.0.0.\n2023/02/08 13:32:54 Starting OpAMP client...\n2023/02/08 13:32:54 OpAMP Client started.\n2023/02/08 13:32:54 Starting agent /usr/local/bin/otelcol\n2023/02/08 13:32:54 Connected to the server.\n2023/02/08 13:32:54 Received remote config from server, hash=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.\n2023/02/08 13:32:54 Agent process started, PID=13553\n2023/02/08 13:32:54 Effective config changed.\n2023/02/08 13:32:54 Enabling own metrics pipeline in the config&lt;F11&gt;\n2023/02/08 13:32:54 Effective config changed.\n2023/02/08 13:32:54 Config is changed. Signal to restart the agent.\n2023/02/08 13:32:54 Agent is not healthy: Get \"http://localhost:13133\": dial tcp [::1]:13133: connect: connection refused\n2023/02/08 13:32:54 Stopping the agent to apply new config.\n2023/02/08 13:32:54 Stopping agent process, PID=13553\n2023/02/08 13:32:54 Agent process PID=13553 successfully stopped.\n2023/02/08 13:32:54 Starting agent /usr/local/bin/otelcol\n2023/02/08 13:32:54 Agent process started, PID=13554\n2023/02/08 13:32:54 Agent is not healthy: Get \"http://localhost:13133\": dial tcp [::1]:13133: connect: connection refused\n2023/02/08 13:32:55 Agent is not healthy: health check on http://localhost:13133 returned 503\n2023/02/08 13:32:55 Agent is not healthy: health check on http://localhost:13133 returned 503\n2023/02/08 13:32:56 Agent is not healthy: health check on http://localhost:13133 returned 503\n2023/02/08 13:32:57 Agent is healthy.\n</code></pre> <p>If everything worked out you should now be able to go to http://localhost:4321/ and access the OpAMP server UI where you should see your collector listed, managed by the supervisor:</p> <p></p> <p>You can also query the collector for the metrics exported (note the label values):</p> <pre><code>$ curl localhost:8888/metrics\n...\n# HELP otelcol_receiver_accepted_metric_points Number of metric points successfully pushed into the pipeline.\n# TYPE otelcol_receiver_accepted_metric_points counter\notelcol_receiver_accepted_metric_points{receiver=\"prometheus/own_metrics\",service_instance_id=\"01GRRKNBJE06AFVGQT5ZYC0GEK\",service_name=\"io.opentelemetry.collector\",service_version=\"1.0.0\",transport=\"http\"} 322\n# HELP otelcol_receiver_refused_metric_points Number of metric points that could not be pushed into the pipeline.\n# TYPE otelcol_receiver_refused_metric_points counter\notelcol_receiver_refused_metric_points{receiver=\"prometheus/own_metrics\",service_instance_id=\"01GRRKNBJE06AFVGQT5ZYC0GEK\",service_name=\"io.opentelemetry.collector\",service_version=\"1.0.0\",transport=\"http\"} 0\n</code></pre>"},{"location":"docs/collector/management/#other-information","title":"Other information","text":"<ul> <li>Blog post Using OpenTelemetry OpAMP to modify service telemetry on the   go</li> <li>YouTube videos:</li> <li>Lightning Talk: Managing OpenTelemetry Through the OpAMP     Protocol</li> <li>What is OpAMP &amp; What is BindPlane</li> </ul>"},{"location":"docs/collector/scaling/","title":"\u6269\u5bb9\u91c7\u96c6\u5668","text":"<p>When planning your observability pipeline with the OpenTelemetry Collector, you should consider ways to scale the pipeline as your telemetry collection increases.</p> <p>The following sections will guide you through the planning phase discussing which components to scale, how to determine when it\u2019s time to scale up, and how to execute the plan.</p>"},{"location":"docs/collector/scaling/#what-to-scale","title":"What to Scale","text":"<p>While the OpenTelemetry Collector handles all telemetry signal types in a single binary, the reality is that each type may have different scaling needs and might require different scaling strategies. Start by looking at your workload to determine which signal type is expected to have the biggest share of the load and which formats are expected to be received by the Collector. For instance, scaling a scraping cluster differs significantly from scaling log receivers. Think also about how elastic the workload is: do you have peaks at specific times of the day, or is the load similar across all 24 hours? Once you gather that information, you will understand what needs to be scaled.</p> <p>For example, suppose you have hundreds of Prometheus endpoints to be scraped, a terabyte of logs coming from fluentd instances every minute, and some application metrics and traces arriving in OTLP format from your newest microservices. In that scenario, you\u2019ll want an architecture that can scale each signal individually: scaling the Prometheus receivers requires coordination among the scrapers to decide which scraper goes to which endpoint. In contrast, we can horizontally scale the stateless log receivers on demand. Having the OTLP receiver for metrics and traces in a third cluster of Collectors would allow us to isolate failures and iterate faster without fear of restarting a busy pipeline. Given that the OTLP receiver enables the ingestion of all telemetry types, we can keep the application metrics and traces on the same instance, scaling them horizontally when needed.</p>"},{"location":"docs/collector/scaling/#when-to-scale","title":"When to Scale","text":"<p>Once again, we should understand our workload to decide when it\u2019s time to scale up or down, but a few metrics emitted by the Collector can give you good hints on when to take action.</p> <p>One helpful hint the Collector can give you when the memory_limiter processor is part of the pipeline is the metric <code>otelcol_processor_refused_spans</code> . This processor allows you to restrict the amount of memory the Collector can use. While the Collector may consume a bit more than the maximum amount configured in this processor, new data will eventually be blocked from passing through the pipeline by the memory_limiter, which will record the fact in this metric. The same metric exists for all other telemetry data types. If data is being refused from entering the pipeline too often, you\u2019ll probably want to scale up your Collector cluster. You can scale down once the memory consumption across the nodes is significantly lower than the limit set in this processor.</p> <p>Another set of metrics to keep in sight are the ones related to the queue sizes for exporters: <code>otelcol_exporter_queue_capacity</code> and <code>otelcol_exporter_queue_size</code>. The Collector will queue data in memory while waiting for a worker to become available to send the data. If there aren\u2019t enough workers or the backend is too slow, data starts piling up in the queue. Once the queue has hit its capacity (<code>otelcol_exporter_queue_size</code> &gt; <code>otelcol_exporter_queue_capacity</code>) it rejects data (<code>otelcol_exporter_enqueue_failed_spans</code>). Adding more workers will often make the Collector export more data, which might not necessarily be what you want (see When NOT to scale).</p> <p>It\u2019s also worth getting familiar with the components that you intend to use, as different components might produce other metrics. For instance, the load-balancing exporter will record timing information about the export operations, exposing this as part of the histogram <code>otelcol_loadbalancer_backend_latency</code>. You can extract this information to determine whether all backends are taking a similar amount of time to process requests: single backends being slow might indicate problems external to the Collector.</p> <p>For receivers doing scraping, such as the Prometheus receiver, the scraping should be scaled, or sharded, once the time it takes to finish scraping all targets often becomes critically close to the scrape interval. When that happens, it\u2019s time to add more scrapers, usually new instances of the Collector.</p>"},{"location":"docs/collector/scaling/#when-not-to-scale","title":"When NOT to scale","text":"<p>Perhaps as important as knowing when to scale is to understand which signs indicate that a scaling operation won\u2019t bring any benefits. One example is when a telemetry database can\u2019t keep up with the load: adding Collectors to the cluster won\u2019t help without scaling up the database. Similarly, when the network connection between the Collector and the backend is saturated, adding more Collectors might cause a harmful side effect.</p> <p>Again, one way to catch this situation is by looking at the metrics <code>otelcol_exporter_queue_size</code> and <code>otelcol_exporter_queue_capacity</code>. If you keep having the queue size close to the queue capacity, it\u2019s a sign that exporting data is slower than receiving data. You can try to increase the queue size, which will cause the Collector to consume more memory, but it will also give some room for the backend to breathe without permanently dropping telemetry data. But if you keep increasing the queue capacity and the queue size keeps rising at the same proportion, it\u2019s indicative that you might want to look outside of the Collector. It\u2019s also important to note that adding more workers here would not be helpful: you\u2019ll only be putting more pressure on a system already suffering from a high load.</p> <p>Another sign that the backend might be having problems is an increase in the <code>otelcol_exporter_send_failed_spans</code> metric: this indicates that sending data to the backend failed permanently. Scaling up the Collector will likely only worsen the situation when this is consistently happening.</p>"},{"location":"docs/collector/scaling/#how-to-scale","title":"How to Scale","text":"<p>At this point, we know which parts of our pipeline needs scaling. Regarding scaling, we have three types of components: stateless, scrapers, and stateful.</p> <p>Most Collector components are stateless. Even if they hold some state in memory, it isn\u2019t relevant for scaling purposes.</p> <p>Scrapers, like the Prometheus receiver, are configured to obtain telemetry data from external locations. The receiver will then scrape target by target, putting data into the pipeline.</p> <p>Components like the tail sampling processor cannot be easily scaled, as they keep some relevant state in memory for their business. Those components require some careful consideration before being scaled up.</p>"},{"location":"docs/collector/scaling/#scaling-stateless-collectors","title":"Scaling Stateless Collectors","text":"<p>The good news is that most of the time, scaling the Collector is easy, as it\u2019s just a matter of adding new replicas and using an off-the-shelf load balancer. When gRPC is used to receive the data, we recommend using a load-balancer that understands gRPC. Otherwise, clients will always hit the same backing Collector.</p> <p>You should still consider splitting your collection pipeline with reliability in mind. For instance, when your workloads run on Kubernetes, you might want to use DaemonSets to have a Collector on the same physical node as your workloads and a remote central Collector responsible for pre-processing the data before sending the data to the storage. When the number of nodes is low and the number of pods is high, Sidecars might make more sense, as you\u2019ll get a better load balancing for the gRPC connections among Collector layers without needing a gRPC-specific load balancer. Using a Sidecar also makes sense to avoid bringing down a crucial component for all pods in a node when one DaemonSet pod fails.</p> <p>The sidecar pattern consists in adding a container into the workload pod. The OpenTelemetry Operator can automatically add that for you. To accomplish that, you\u2019ll need an OpenTelemetry Collector CR and you\u2019ll need to annotate your PodSpec or Pod telling the operator to inject a sidecar:</p> <pre><code>---\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: sidecar-for-my-workload\nspec:\nmode: sidecar\nconfig: |\nreceivers:\notlp:\nprotocols:\ngrpc:\nprocessors:\nexporters:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: []\nexporters: [logging]\n---\napiVersion: v1\nkind: Pod\nmetadata:\nname: my-microservice\nannotations:\nsidecar.opentelemetry.io/inject: 'true'\nspec:\ncontainers:\n- name: my-microservice\nimage: my-org/my-microservice:v0.0.0\nports:\n- containerPort: 8080\nprotocol: TCP\n</code></pre> <p>In case you prefer to bypass the operator and add a sidecar manually, here\u2019s an example:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: my-microservice\nspec:\ncontainers:\n- name: my-microservice\nimage: my-org/my-microservice:v0.0.0\nports:\n- containerPort: 8080\nprotocol: TCP\n- name: sidecar\nimage: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector:0.69.0\nports:\n- containerPort: 8888\nname: metrics\nprotocol: TCP\n- containerPort: 4317\nname: otlp-grpc\nprotocol: TCP\nargs:\n- --config=/conf/collector.yaml\nvolumeMounts:\n- mountPath: /conf\nname: sidecar-conf\nvolumes:\n- name: sidecar-conf\nconfigMap:\nname: sidecar-for-my-workload\nitems:\n- key: collector.yaml\npath: collector.yaml\n</code></pre>"},{"location":"docs/collector/scaling/#scaling-the-scrapers","title":"Scaling the Scrapers","text":"<p>Some receivers are actively obtaining telemetry data to place in the pipeline, like the hostmetrics and prometheus receivers. While getting host metrics isn\u2019t something we\u2019d typically scale up, we might need to split the job of scraping thousands of endpoints for the Prometheus receiver. And we can\u2019t simply add more instances with the same configuration, as each Collector would try to scrape the same endpoints as every other Collector in the cluster, causing even more problems, like out-of-order samples.</p> <p>The solution is to shard the endpoints by Collector instances so that if we add another replica of the Collector, each one will act on a different set of endpoints.</p> <p>One way of doing that is by having one configuration file for each Collector so that each Collector would discover only the relevant endpoints for that Collector. For instance, each Collector could be responsible for one Kubernetes namespace or specific labels on the workloads.</p> <p>Another way of scaling the Prometheus receiver is to use the Target Allocator: it\u2019s an extra binary that can be deployed as part of the OpenTelemetry Operator and will split the share of Prometheus jobs for a given configuration across the cluster of Collectors using a consistent hashing algorithm. You can use a Custom Resource (CR) like the following to make use of the Target Allocator:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: collector-with-ta\nspec:\nmode: statefulset\ntargetAllocator:\nenabled: true\nconfig: |\nreceivers:\nprometheus:\nconfig:\nscrape_configs:\n- job_name: 'otel-collector'\nscrape_interval: 10s\nstatic_configs:\n- targets: [ '0.0.0.0:8888' ]\nexporters:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre> <p>After the reconciliation, the OpenTelemetry Operator will convert the Collector\u2019s configuration into the following:</p> <pre><code>   exporters:\nlogging: null\nreceivers:\nprometheus:\nconfig:\nglobal:\nscrape_interval: 1m\nscrape_timeout: 10s\nevaluation_interval: 1m\nscrape_configs:\n- job_name: otel-collector\nhonor_timestamps: true\nscrape_interval: 10s\nscrape_timeout: 10s\nmetrics_path: /metrics\nscheme: http\nfollow_redirects: true\nhttp_sd_configs:\n- follow_redirects: false\nurl: http://collector-with-ta-targetallocator:80/jobs/otel-collector/targets?collector_id=$POD_NAME\nservice:\npipelines:\ntraces:\nexporters:\n- logging\nprocessors: []\nreceivers:\n- prometheus\n</code></pre> <p>Note how the Operator added a <code>global</code> section and a <code>new http_sd_configs</code> to the <code>otel-collector</code> scrape config, pointing to a Target Allocator instance it provisioned. Now, to scale the collectors, change the \u201creplicas\u201d attribute of the CR and the Target Allocator will distribute the load accordingly by providing a custom <code>http_sd_config</code> per collector instance (pod).</p>"},{"location":"docs/collector/scaling/#scaling-stateful-collectors","title":"Scaling Stateful Collectors","text":"<p>Certain components might hold data in memory, yielding different results when scaled up. It is the case for the tail-sampling processor, which holds spans in memory for a given period, evaluating the sampling decision only when the trace is considered complete. Scaling a Collector cluster by adding more replicas means that different collectors will receive spans for a given trace, causing each collector to evaluate whether that trace should be sampled, potentially coming to different answers. This behavior results in traces missing spans, misrepresenting what happened in that transaction.</p> <p>A similar situation happens when using the span-to-metrics processor to generate service metrics. When different collectors receive data related to the same service, aggregations based on the service name will be inaccurate.</p> <p>To overcome this, you can deploy a layer of Collectors containing the load-balancing exporter in front of your Collectors doing the tail-sampling or the span-to-metrics processing. The load-balancing exporter will hash the trace ID or the service name consistently and determine which collector backend should receive spans for that trace. You can configure the load-balancing exporter to use the list of hosts behind a given DNS A entry, such as a Kubernetes headless service. When the deployment backing that service is scaled up or down, the load-balancing exporter will eventually see the updated list of hosts. Alternatively, you can specify a list of static hosts to be used by the load-balancing exporter. You can scale up the layer of Collectors configured with the load-balancing exporter by increasing the number of replicas. Note that each Collector will potentially run the DNS query at different times, causing a difference in the cluster view for a few moments. We recommend lowering the interval value so that the cluster view is different only for a short period in highly-elastic environments.</p> <p>Here\u2019s an example configuration using a DNS A record (Kubernetes service otelcol on the observability namespace) as the input for the backend information:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nprocessors:\nexporters:\nloadbalancing:\nprotocol:\notlp:\nresolver:\ndns:\nhostname: otelcol.observability.svc.cluster.local\nservice:\npipelines:\ntraces:\nreceivers:\n- otlp\nprocessors: []\nexporters:\n- loadbalancing\n</code></pre>"},{"location":"docs/collector/trace-receiver/","title":"\u5efa\u7acb\u8ddf\u8e2a\u63a5\u6536\u5668","text":"<p>If you are reading this tutorial, you probably already have an idea of the OpenTelemetry concepts behind distributed tracing, but if you don't you can quickly read through it here.</p> <p>Here is the definition of those concepts according to OpenTelemetry:</p> <p>Traces track the progression of a single request, called a trace, as it is handled by services that make up an application. The request may be initiated by a user or an application. Distributed tracing is a form of tracing that traverses process, network and security boundaries.</p> <p>Although the definition seems very application centric, you can leverage the OpenTelemetry trace model as a way to represent a request and quickly understand its duration and the details about every step involved in completing it.</p> <p>Assuming you already have a system generating some kind of tracing telemetry, the OpenTelemetry Collector is the doorway to help you make it available into the OTel world.</p> <p>Within the Collector, a trace receiver has the role to receive and convert your request telemetry from its original format into the OTel trace model, so the information can be properly processed through the Collector's pipelines.</p> <p>In order to implement a traces receiver you will need the following:</p> <ul> <li> <p>A <code>Config</code> implementation to enable the trace receiver to gather and validate   its configurations within the Collector's config.yaml.</p> </li> <li> <p>A <code>receiver.Factory</code> implementation so the Collector can properly instantiate   the trace receiver component.</p> </li> <li> <p>A <code>TracesReceiver</code> implementation that is responsible to collect the   telemetry, convert it to the internal trace representation, and hand the   information to the next consumer in the pipeline.</p> </li> </ul> <p>In this tutorial we will create a sample trace receiver called <code>tailtracer</code> that simulates a pull operation and generates traces as an outcome of that operation. The next sections will guide you through the process of implementing the steps above in order to create the receiver, so let's get started.</p>"},{"location":"docs/collector/trace-receiver/#setting-up-your-receiver-development-and-testing-environment","title":"Setting up your receiver development and testing environment","text":"<p>First use the Building a Custom Collector tutorial to create a Collector instance named <code>otelcol-dev</code>; all you need is to copy the <code>builder-config.yaml</code> described on Step 2 and make the following changes:</p> <pre><code>dist:\nname: otelcol-dev # the binary name. Optional.\noutput_path: ./otelcol-dev # the path to write the output (sources and binary). Optional.\n</code></pre> <p>As an outcome you should now have a <code>otelcol-dev</code> folder with your Collector's development instance ready to go.</p> <p>In order to properly test your trace receiver, you will need a distributed tracing backend so the Collector can send the telemetry to it. We will be using Jaeger, if you don't have a <code>Jaeger</code> instance running, you can easily start one using Docker with the following command:</p> <pre><code>docker run -d --name jaeger \\\n-p 16686:16686 \\\n-p 14268:14268 \\\n-p 14250:14250 \\\njaegertracing/all-in-one:1.29\n</code></pre> <p>Now, create a <code>config.yaml</code> file so you can set up your Collector's components.</p> <pre><code>cd otelcol-dev\ntouch config.yaml\n</code></pre> <p>For now, you just need a basic traces pipeline with the <code>otlp</code> receiver, the <code>jaeger</code> and <code>logging</code> exporters, here is what your <code>config.yaml</code> file should look like:</p> <p>config.yaml</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nprocessors:\nexporters:\nlogging:\nverbosity: detailed\njaeger:\nendpoint: localhost:14250\ntls:\ninsecure: true\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: []\nexporters: [jaeger, logging]\ntelemetry:\nlogs:\nlevel: debug\n</code></pre> <p>Notice that I am only using the <code>insecure</code> flag in my <code>jaeger</code> receiver config to make my local development setup easier; you should not use this flag when running your collector in production.</p> <p>In order to verify that your initial pipeline is properly set up, you should have the following output after running your <code>otelcol-dev</code> command:</p> <pre><code>$ ./otelcol-dev --config config.yaml\n2022-06-21T13:02:09.253-0500    info    builder/exporters_builder.go:255        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-06-21T13:02:09.254-0500    info    builder/exporters_builder.go:255        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-06-21T13:02:09.254-0500    info    builder/pipelines_builder.go:224        Pipeline was built.     {\"kind\": \"pipeline\", \"name\": \"traces\"}\n2022-06-21T13:02:09.254-0500    info    builder/receivers_builder.go:225        Receiver was built.     {\"kind\": \"receiver\", \"name\": \"otlp\", \"datatype\": \"traces\"}\n2022-06-21T13:02:09.254-0500    info    service/telemetry.go:102        Setting up own telemetry...\n2022-06-21T13:02:09.255-0500    info    service/telemetry.go:141        Serving Prometheus metrics      {\"address\": \":8888\", \"level\": \"basic\"}\n2022-06-21T13:02:09.255-0500    info    service/service.go:93   Starting extensions...\n2022-06-21T13:02:09.255-0500    info    service/service.go:98   Starting exporters...\n2022-06-21T13:02:09.255-0500    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-06-21T13:02:09.258-0500    info    builder/exporters_builder.go:48 Exporter started.       {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-06-21T13:02:09.258-0500    info    jaegerexporter@v0.53.0/exporter.go:186  State of the connection with the Jaeger Collector backend       {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"IDLE\"}\n2022-06-21T13:02:09.258-0500    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-06-21T13:02:09.258-0500    info    builder/exporters_builder.go:48 Exporter started.       {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-06-21T13:02:09.258-0500    info    service/service.go:103  Starting processors...\n2022-06-21T13:02:09.258-0500    info    builder/pipelines_builder.go:54 Pipeline is starting... {\"kind\": \"pipeline\", \"name\": \"traces\"}\n2022-06-21T13:02:09.258-0500    info    builder/pipelines_builder.go:65 Pipeline is started.    {\"kind\": \"pipeline\", \"name\": \"traces\"}\n2022-06-21T13:02:09.258-0500    info    service/service.go:108  Starting receivers...\n2022-06-21T13:02:09.258-0500    info    builder/receivers_builder.go:67 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-06-21T13:02:09.258-0500    info    otlpreceiver/otlp.go:70 Starting GRPC server on endpoint localhost:55690        {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-06-21T13:02:09.261-0500    info    builder/receivers_builder.go:72 Receiver started.       {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-06-21T13:02:09.262-0500    info    service/collector.go:226        Starting otelcol-dev... {\"Version\": \"1.0.0\", \"NumCPU\": 12}\n2022-06-21T13:02:09.262-0500    info    service/collector.go:134        Everything is ready. Begin running and processing data.\n2022-06-21T13:02:10.258-0500    info    jaegerexporter@v0.53.0/exporter.go:186  State of the connection with the Jaeger Collector backend       {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"READY\"}\n</code></pre> <p>Make sure you see the last line, that will confirm that the Jaeger exporter has successfully established a connection to your local Jaeger instance. Now that we have our environment ready, let's start writing your receiver's code.</p> <p>Now, create another folder called <code>tailtracer</code> so we can have a place to host all of our receiver code.</p> <pre><code>mkdir tailtracer\n</code></pre> <p>Every Collector's component should be created as a Go module, so you will need to properly initialize the <code>tailtracer</code> module. In my case here is what the command looked like:</p> <pre><code>cd tailtracer\ngo mod init github.com/rquedas/otel4devs/collector/receiver/trace-receiver/tailtracer\n</code></pre>"},{"location":"docs/collector/trace-receiver/#reading-and-validating-your-receiver-settings","title":"Reading and Validating your Receiver Settings","text":"<p>In order to be instantiated and participate in pipelines the Collector needs to identify your receiver and properly load its settings from within its configuration file.</p> <p>The <code>tailtracer</code> receiver will have the following settings:</p> <ul> <li><code>interval</code>: a string representing the time interval (in minutes) between   telemetry pull operations</li> <li><code>number_of_traces</code>: the number of mock traces generated for each interval</li> </ul> <p>Here is what the <code>tailtracer</code> receiver settings will look like:</p> <pre><code>receivers:\ntailtracer: # this line represents the ID of your receiver\ninterval: 1m\nnumber_of_traces: 1\n</code></pre> <p>Under the <code>tailtracer</code> folder, create a file named <code>config.go</code> where you will write all the code to support your receiver settings.</p> <pre><code>cd tailtracer\ntouch config.go\n</code></pre> <p>To implement the configuration aspects of a receiver you need create a <code>Config</code> struct. Add the following code to your <code>config.go</code> file:</p> <pre><code>package tailtracer\ntype Config struct{\n}\n</code></pre> <p>In order to be able to give your receiver access to its settings the <code>Config</code> struct must have a field for each of the receiver's settings.</p> <p>Here is what your <code>config.go</code> file should look like after you implemented the requirements above.</p> <p>config.go</p> <pre><code>package tailtracer\n// Config represents the receiver config settings within the collector's config.yaml\ntype Config struct {\nInterval    string `mapstructure:\"interval\"`\nNumberOfTraces int `mapstructure:\"number_of_traces\"`\n}\n</code></pre> <p>Now that you have access to the settings, you can provide any kind of validation needed for those values by implementing the <code>Validate</code> method according to the optional ConfigValidator interface.</p> <p>In this case, the <code>interval</code> value will be optional (we will look at generating default values later) but when defined should be at least 1 minute (1m) and the <code>number_of_traces</code> will be a required value. Here is what the config.go looks like after implementing the <code>Validate</code> method.</p> <p>config.go</p> <pre><code>package tailtracer\nimport (\n\"fmt\"\n\"time\"\n)\n// Config represents the receiver config settings within the collector's config.yaml\ntype Config struct {\nInterval       string `mapstructure:\"interval\"`\nNumberOfTraces int    `mapstructure:\"number_of_traces\"`\n}\n// Validate checks if the receiver configuration is valid\nfunc (cfg *Config) Validate() error {\ninterval, _ := time.ParseDuration(cfg.Interval)\nif interval.Minutes() &lt; 1 {\nreturn fmt.Errorf(\"when defined, the interval has to be set to at least 1 minute (1m)\")\n}\nif cfg.NumberOfTraces &lt; 1 {\nreturn fmt.Errorf(\"number_of_traces must be greater or equal to 1\")\n}\nreturn nil\n}\n</code></pre> <p>If you want to take a closer look at the structs and interfaces involved in the configuration aspects of a component, take a look at the component/config.go file inside the Collector's GitHub project.</p>"},{"location":"docs/collector/trace-receiver/#check-your-work","title":"Check your work","text":"<ul> <li>I added the <code>Interval</code> and the <code>NumberOfTraces</code> fields so I can properly   have access to their values from the config.yaml.</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_1","title":"Check your work","text":"<ul> <li>I imported the <code>fmt</code> package, so I can properly format print my error   messages.</li> <li>I added the <code>Validate</code> method to my Config struct where I am checking if the   <code>interval</code> setting value is at least 1 minute (1m) and if the   <code>number_of_traces</code> setting value is greater or equal to 1. If that is not   true the Collector will generate an error during its startup process and   display the message accordingly.</li> </ul>"},{"location":"docs/collector/trace-receiver/#enabling-the-collector-to-instantiate-your-receiver","title":"Enabling the Collector to instantiate your receiver","text":"<p>At the beginning of this tutorial, you created your <code>otelcol-dev</code> instance, which is bootstrapped with the following components:</p> <ul> <li>Receivers: OTLP Receiver</li> <li>Processors: Batch Processor</li> <li>Exporters: Logging and Jaeger Exporters</li> </ul> <p>Go ahead and open the <code>components.go</code> file under the <code>otelcol-dev</code> folder, and let's take a look at the <code>components()</code> function.</p> <pre><code>func components() (otelcol.Factories, error) {\nvar err error\nfactories := otelcol.Factories{}\nfactories.Extensions, err = extension.MakeFactoryMap(\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Receivers, err = receiver.MakeFactoryMap(\notlpreceiver.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Exporters, err = exporter.MakeFactoryMap(\nloggingexporter.NewFactory(),\njaegerexporter.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Processors, err = processor.MakeFactoryMap(\nbatchprocessor.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nreturn factories, nil\n}\n</code></pre> <p>As you can see, the <code>components()</code> function is responsible to provide the Collector the factories for all its components which is represented by a variable called <code>factories</code> of type <code>otelcol.Factories</code> (here is the declaration of the otelcol.Factories struct), which will then be used to instantiate the components that are configured and consumed by the Collector's pipelines.</p> <p>Notice that <code>factories.Receivers</code> is the field holding a map to all the receiver factories (instances of <code>receiver.Factory</code>), and it currently has the <code>otlpreceiver</code> factory only which is instantiated through the <code>otlpreceiver.NewFactory()</code> function call.</p> <p>The <code>tailtracer</code> receiver has to provide a <code>receiver.Factory</code> implementation, and although you will find a <code>receiver.Factory</code> interface (you can find its definition in the receiver/receiver.go file within the Collector's project ), the right way to provide the implementation is by using the functions available within the <code>go.opentelemetry.io/collector/receiver</code> package.</p>"},{"location":"docs/collector/trace-receiver/#implementing-your-receiverfactory","title":"Implementing your receiver.Factory","text":"<p>Start by creating a file named factory.go within the <code>tailtracer</code> folder.</p> <pre><code>cd tailtracer\ntouch factory.go\n</code></pre> <p>Now let's follow the convention and add a function named <code>NewFactory()</code> that will be responsible to instantiate the <code>tailtracer</code> factory. Go ahead the add the following code to your <code>factory.go</code> file:</p> <pre><code>package tailtracer\nimport (\n\"go.opentelemetry.io/collector/receiver\"\n)\n// NewFactory creates a factory for tailtracer receiver.\nfunc NewFactory() receiver.Factory {\n}\n</code></pre> <p>In order to instantiate your <code>tailtracer</code> receiver factory, you will use the following function from the <code>receiver</code> package:</p> <pre><code>func NewFactory(cfgType component.Type, createDefaultConfig component.CreateDefaultConfigFunc, options ...FactoryOption) Factory\n</code></pre> <p>The <code>receiver.NewFactory()</code> instantiates and returns a <code>receiver.Factory</code> and it requires the following parameters:</p> <ul> <li> <p><code>component.Type</code>: a unique string identifier for your receiver across all   Collector's components.</p> </li> <li> <p><code>component.CreateDefaultConfigFunc</code>: a reference to a function that returns   the component.Config instance for your receiver.</p> </li> <li> <p><code>...FactoryOption</code>: the slice of <code>receiver.FactoryOption</code>s that will determine   what type of signal your receiver is capable of processing.</p> </li> </ul> <p>Let's now implement the code to support all the parameters required by <code>receiver.NewFactory()</code>.</p>"},{"location":"docs/collector/trace-receiver/#identifying-and-providing-default-settings-for-the-receiver","title":"Identifying and Providing default settings for the receiver","text":"<p>Previously, we said that the <code>interval</code> setting for our <code>tailtracer</code> receiver would be optional, in that case you will need to provide a default value for it so it can be used as part of the default settings.</p> <p>Go ahead and add the following code to your <code>factory.go</code> file:</p> <pre><code>const (\ntypeStr = \"tailtracer\"\ndefaultInterval = 1 * time.Minute\n)\n</code></pre> <p>As for default settings, you just need to add a function that returns a <code>component.Config</code> holding the default configurations for the <code>tailtracer</code> receiver.</p> <p>To accomplish that, go ahead and add the following code to your <code>factory.go</code> file:</p> <pre><code>func createDefaultConfig() component.Config {\nreturn &amp;Config{\nInterval: string(defaultInterval),\n}\n}\n</code></pre> <p>After these two changes you will notice a few imports are missing, so here is what your <code>factory.go</code> file should look like with the proper imports:</p> <p>factory.go</p> <pre><code>package tailtracer\nimport (\n\"time\"\n\"go.opentelemetry.io/collector/component\"\n\"go.opentelemetry.io/collector/receiver\"\n)\nconst (\ntypeStr = \"tailtracer\"\ndefaultInterval = 1 * time.Minute\n)\nfunc createDefaultConfig() component.Config {\nreturn &amp;Config{\nInterval: string(defaultInterval),\n}\n}\n// NewFactory creates a factory for tailtracer receiver.\nfunc NewFactory() receiver.Factory {\nreturn nil\n}\n</code></pre>"},{"location":"docs/collector/trace-receiver/#check-your-work_2","title":"Check your work","text":"<ul> <li>Importing the <code>time</code> package in order to support the time.Duration type for   the defaultInterval</li> <li>Importing the <code>go.opentelemetry.io/collector/component</code> package, which is   where <code>component.Config</code> is declared.</li> <li>Importing the <code>go.opentelemetry.io/collector/receiver</code> package, which is   where <code>receiver.Factory</code> is declared.</li> <li>Added a <code>time.Duration</code> constant called <code>defaultInterval</code> to represent the   default value for our receiver's <code>Interval</code> setting. We will be setting the   default value for 1 minute hence the assignment of <code>1 * time.Minute</code> as its   value.</li> <li>Added a function called <code>createDefaultConfig</code> which is responsible to return   a component.Config implementation, which in this case is going to be an   instance of our <code>tailtracer.Config</code> struct.</li> <li>The <code>tailtracer.Config.Interval</code> field was initialized with the   <code>defaultInterval</code> constant.</li> </ul>"},{"location":"docs/collector/trace-receiver/#enabling-the-factory-to-describe-the-receiver-as-capable-of-processing-traces","title":"Enabling the factory to describe the receiver as capable of processing traces","text":"<p>The same receiver component can process traces, metrics, and logs. The receiver's factory is responsible for describing those capabilities.</p> <p>Given that traces are the subject of the tutorial, that's the only signal we will enable the <code>tailtracer</code> receiver to work with. The <code>receiver</code> package provides the following function and type to help the factory describe the trace processing capabilities:</p> <pre><code>func WithTraces(createTracesReceiver CreateTracesFunc, sl component.StabilityLevel) FactoryOption\n</code></pre> <p>The <code>receiver.WithTraces()</code> instantiates and returns a <code>receiver.FactoryOption</code> and it requires the following parameters:</p> <ul> <li><code>createTracesReceiver</code>: A reference to a function that matches the   <code>receiver.CreateTracesFunc</code> type</li> </ul> <p>The <code>receiver.CreateTracesFunc</code> type is a pointer to a function that is responsible to instantiate and return a <code>receiver.Traces</code> instance and it requires the following parameters:</p> <ul> <li><code>context.Context</code>: the reference to the Collector's <code>context.Context</code> so your   trace receiver can properly manage its execution context.</li> <li><code>receiver.CreateSettings</code>: the reference to some of the Collector's settings   under which your receiver is created.</li> <li><code>component.Config</code>: the reference for the receiver config settings passed by   the Collector to the factory so it can properly read its settings from the   Collector config.</li> <li><code>consumer.Traces</code>: the reference to the next <code>consumer.Traces</code> in the   pipeline, which is where received traces will go. This is either a processor   or an exporter.</li> </ul> <p>Start by adding the bootstrap code to properly implement the <code>receiver.CreateTracesFunc</code> function pointer. Go ahead and add the following code to your <code>factory.go</code> file:</p> <pre><code>func createTracesReceiver(_ context.Context, params receiver.CreateSettings, baseCfg component.Config, consumer consumer.Traces) (receiver.Traces, error) {\nreturn nil, nil\n}\n</code></pre> <p>You now have all the necessary components to successfully instantiate your receiver factory using the <code>receiver.NewFactory</code> function. Go ahead and and update your <code>NewFactory()</code> function in your <code>factory.go</code> file as follow:</p> <pre><code>// NewFactory creates a factory for tailtracer receiver.\nfunc NewFactory() receiver.Factory {\nreturn receiver.NewFactory(\ntypeStr,\ncreateDefaultConfig,\nreceiver.WithTraces(createTracesReceiver, component.StabilityLevelAlpha))\n}\n</code></pre> <p>After these two changes you will notice a few imports are missing, so here is what your <code>factory.go</code> file should look like with the proper imports:</p> <p>factory.go</p> <pre><code>package tailtracer\nimport (\n\"context\"\n\"time\"\n\"go.opentelemetry.io/collector/component\"\n\"go.opentelemetry.io/collector/consumer\"\n\"go.opentelemetry.io/collector/receiver\"\n)\nconst (\ntypeStr = \"tailtracer\"\ndefaultInterval = 1 * time.Minute\n)\nfunc createDefaultConfig() component.Config {\nreturn &amp;Config{\nInterval: string(defaultInterval),\n}\n}\nfunc createTracesReceiver(_ context.Context, params receiver.CreateSettings, baseCfg component.Config, consumer consumer.Traces) (receiver.Traces, error) {\nreturn nil, nil\n}\n// NewFactory creates a factory for tailtracer receiver.\nfunc NewFactory() receiver.Factory {\nreturn receiver.NewFactory(\ntypeStr,\ncreateDefaultConfig,\nreceiver.WithTraces(createTracesReceiver, component.StabilityLevelAlpha))\n}\n</code></pre> <p>At this point, you have the <code>tailtracer</code> factory and config code needed for the Collector to validate the <code>tailtracer</code> receiver settings if they are defined within the <code>config.yaml</code>. You just need to add it to the Collector's initialization process.</p>"},{"location":"docs/collector/trace-receiver/#check-your-work_3","title":"Check your work","text":"<ul> <li>Importing the <code>context</code> package in order to support the <code>context.Context</code>   type referenced in the <code>createTracesReceiver</code> function</li> <li>Importing the <code>go.opentelemetry.io/collector/consumer</code> package in order to   support the <code>consumer.Traces</code> type referenced in the <code>createTracesReceiver</code>   function</li> <li>Updated the <code>NewFactory()</code> function so it returns the <code>receiver.Factory</code>   generated by the <code>receiver.NewFactory()</code> call with the required parameters.   The generated receiver factory will be capable of processing traces through   the call to   <code>receiver.WithTraces(createTracesReceiver, component.StabilityLevelAlpha)</code></li> </ul>"},{"location":"docs/collector/trace-receiver/#adding-the-receiver-factory-to-the-collectors-initialization","title":"Adding the receiver factory to the Collector's initialization","text":"<p>As explained before, all the Collector components are instantiated by the <code>components()</code> function within the <code>components.go</code> file.</p> <p>The <code>tailtracer</code> receiver factory instance has to be added to the <code>factories</code> map so the Collector can load it properly as part of its initialization process.</p> <p>Here is what the <code>components.go</code> file looks like after making the changes to support that:</p> <p>components.go</p> <pre><code>// Code generated by \"go.opentelemetry.io/collector/cmd/builder\". DO NOT EDIT.\npackage main\nimport (\n\"go.opentelemetry.io/collector/exporter\"\n\"go.opentelemetry.io/collector/extension\"\n\"go.opentelemetry.io/collector/otelcol\"\n\"go.opentelemetry.io/collector/processor\"\n\"go.opentelemetry.io/collector/receiver\"\nloggingexporter \"go.opentelemetry.io/collector/exporter/loggingexporter\"\njaegerexporter \"github.com/open-telemetry/opentelemetry-collector-contrib/exporter/jaegerexporter\"\nbatchprocessor \"go.opentelemetry.io/collector/processor/batchprocessor\"\notlpreceiver \"go.opentelemetry.io/collector/receiver/otlpreceiver\"\ntailtracer \"github.com/rquedas/otel4devs/collector/receiver/trace-receiver/tailtracer\"\n)\nfunc components() (otelcol.Factories, error) {\nvar err error\nfactories := otelcol.Factories{}\nfactories.Extensions, err = extension.MakeFactoryMap(\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Receivers, err = receiver.MakeFactoryMap(\notlpreceiver.NewFactory(),\ntailtracer.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Exporters, err = exporter.MakeFactoryMap(\nloggingexporter.NewFactory(),\njaegerexporter.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nfactories.Processors, err = processor.MakeFactoryMap(\nbatchprocessor.NewFactory(),\n)\nif err != nil {\nreturn otelcol.Factories{}, err\n}\nreturn factories, nil\n}\n</code></pre> <p>We added the <code>tailtracer</code> receiver settings to the <code>config.yaml</code> previously, so here is what the beginning of the output for running your Collector with <code>otelcol-dev</code> command should look like after building it with the current codebase:</p> <pre><code>$ ./otelcol-dev --config config.yaml\n2022-02-24T12:17:41.454-0600    info    service/collector.go:190        Applying configuration...\n2022-02-24T12:17:41.454-0600    info    builder/exporters_builder.go:254        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-02-24T12:17:41.454-0600    info    builder/exporters_builder.go:254        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-02-24T12:17:41.454-0600    info    builder/pipelines_builder.go:222        Pipeline was built.     {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-02-24T12:17:41.454-0600    info    builder/receivers_builder.go:111        Ignoring receiver as it is not used by any pipeline      {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-02-24T12:17:41.454-0600    info    builder/receivers_builder.go:224        Receiver was built.     {\"kind\": \"receiver\", \"name\": \"otlp\", \"datatype\": \"traces\"}\n2022-02-24T12:17:41.454-0600    info    service/service.go:86   Starting extensions...\n2022-02-24T12:17:41.454-0600    info    service/service.go:91   Starting exporters...\n</code></pre> <p>Look for the log line for \"builder/receivers_builder.go:111\" (it's the 4th line from the bottom at the snippet showed here), you can see that the Collector found the settings for the <code>tailtracer</code> receiver, validated them (the current settings are all correct), but ignores the receiver given that it's not used in any pipeline.</p> <p>Let's check if the <code>tailtracer</code> factory is validating the receiver settings correctly, the <code>interval</code> setting isn't required, so if you remove it from the <code>config.yaml</code> and run the command again you should get the same output.</p> <p>Now, let's test one of the <code>tailtracer</code> settings validation rules. Remove the <code>number_of_traces</code> setting from the <code>config.yaml</code>, and here is what the output for running the Collector will look like:</p> <pre><code>$ ./otelcol-dev --config config.yaml\nError: invalid configuration: receiver \"tailtracer\" has invalid configuration: number_of_traces must be at least 1\n2022/02/24 13:00:20 collector server run finished with error: invalid configuration: receiver \"tailtracer\" has invalid configuration: number_of_traces must be at least 1\n</code></pre> <p>The <code>tailtracer</code> receiver factory and config requirements are done and the Collector is properly loading your component. You can now move to the core of your receiver, the implementation of the component itself.</p>"},{"location":"docs/collector/trace-receiver/#check-your-work_4","title":"Check your work","text":"<ul> <li>Importing the   <code>github.com/rquedas/otel4devs/collector/receiver/trace-receiver/tailtracer</code>   module which is where the receiver types and function are.</li> <li>Added a call to <code>tailtracer.NewFactory()</code> as a parameter of the   <code>receiver.MakeFactoryMap()</code> call so your <code>tailtracer</code> receiver factory is   properly added to the <code>factories</code> map.</li> </ul>"},{"location":"docs/collector/trace-receiver/#implementing-the-trace-receiver-component","title":"Implementing the trace receiver component","text":"<p>In the previous section, I mentioned the fact that a receiver can process any of the OpenTelemetry signals, and the Collector's API is designed to help you accomplish that.</p> <p>All the receiver APIs responsible to enable the signals are currently declared in the receiver/receiver.go file within the OTel Collector's project in GitHub, open the file and take a minute to browse through all the interfaces declared in it.</p> <p>Notice that <code>receiver.Traces</code> (and its siblings <code>receiver.Metrics</code> and <code>receiver.Logs</code>) at this point in time, doesn't describe any specific methods other than the ones it \"inherits\" from <code>component.Component</code>.</p> <p>It might feel weird, but remember, the Collector's API was meant to be extensible, and the components and their signals might evolve in different ways, so the role of those interfaces exist to help support that.</p> <p>So, to create a <code>receiver.Traces</code>, you just need to implement the following methods described by <code>component.Component</code> interface:</p> <pre><code>Start(ctx context.Context, host Host) error\nShutdown(ctx context.Context) error\n</code></pre> <p>Both methods actually act as event handlers used by the Collector to communicate with its components as part of their lifecycle.</p> <p>The <code>Start()</code> represents a signal of the Collector telling the component to start its processing. As part of the event, the Collector will pass the following information:</p> <ul> <li><code>context.Context</code>: Most of the time, a receiver will be processing a   long-running operation, so the recommendation is to ignore this context and   actually create a new one from context.Background().</li> <li><code>Host</code>: The host is meant to enable the receiver to communicate with the   Collector's host once it's up and running.</li> </ul> <p>The <code>Shutdown()</code> represents a signal of the Collector telling the component that the service is getting shutdown and as such the component should stop its processing and make all the necessary cleanup work required:</p> <ul> <li><code>context.Context</code>: the context passed by the Collector as part of the shutdown   operation.</li> </ul> <p>You will start the implementation by creating a new file called <code>trace-receiver.go</code> within your project's <code>tailtracer</code> folder and add the declaration to a type type called <code>tailtracerReceiver</code> as follow:</p> <pre><code>type tailtracerReceiver struct{\n}\n</code></pre> <p>Now that you have the <code>tailtracerReceiver</code> type you can implement the Start() and Shutdown() methods so the receiver type can be compliant with the <code>receiver.Traces</code> interface.</p> <p>Here is what the <code>tailtracer/trace-receiver.go</code> file should look like with the methods implementation:</p> <p>trace-receiver.go</p> <pre><code>package tailtracer\nimport (\n\"context\"\n\"go.opentelemetry.io/collector/component\"\n)\ntype tailtracerReceiver struct {\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Start(ctx context.Context, host component.Host) error {\nreturn nil\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Shutdown(ctx context.Context) error {\nreturn nil\n}\n</code></pre> <p>The <code>Start()</code> method is passing 2 references (<code>context.Context</code> and <code>component.Host</code>) that your receiver might need to keep so they can be used as part of its processing operations.</p> <p>The <code>context.Context</code> reference should be used for creating a new context to support you receiver processing operations, and in that case you will need to decide the best way to handle context cancellation so you can finalize it properly as part of the component's shutdown within the <code>Shutdown()</code> method.</p> <p>The <code>component.Host</code> can be useful during the whole lifecycle of the receiver so you should keep that reference within your <code>tailtracerReceiver</code> type.</p> <p>Here is what the <code>tailtracerReceiver</code> type declaration will look like after you include the fields for keeping the references suggested above:</p> <pre><code>type tailtracerReceiver struct {\nhost component.Host\ncancel context.CancelFunc\n}\n</code></pre> <p>Now you need to update the <code>Start()</code> methods so the receiver can properly initialize its own processing context and have the cancellation function kept in the <code>cancel</code> field and also initialize its <code>host</code> field value. You will also update the <code>Stop()</code> method in order to finalize the context by calling the <code>cancel</code> function.</p> <p>Here is what the <code>trace-receiver.go</code> file look like after making the changes above:</p> <p>trace-receiver.go</p> <pre><code>package tailtracer\nimport (\n\"context\"\n\"go.opentelemetry.io/collector/component\"\n)\ntype tailtracerReceiver struct {\nhost component.Host\ncancel context.CancelFunc\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Start(ctx context.Context, host component.Host) error {\ntailtracerRcvr.host = host\nctx = context.Background()\nctx, tailtracerRcvr.cancel = context.WithCancel(ctx)\nreturn nil\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Shutdown(ctx context.Context) error {\ntailtracerRcvr.cancel()\nreturn nil\n}\n</code></pre>"},{"location":"docs/collector/trace-receiver/#check-your-work_5","title":"Check your work","text":"<ul> <li>Importing the <code>context</code> package which is where the <code>Context</code> type and   functions are declared</li> <li>Importing the <code>go.opentelemetry.io/collector/component</code> package which is   where the <code>Host</code> type is declared</li> <li>Added a bootstrap implementation of the   <code>Start(ctx context.Context, host component.Host)</code> method to comply with the   <code>receiver.Traces</code> interface.</li> <li>Added a bootstrap implementation of the <code>Shutdown(ctx context.Context)</code>   method to comply with the <code>receiver.Traces</code> interface.</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_6","title":"Check your work","text":"<ul> <li>Updated the <code>Start()</code> method by adding the initialization to the <code>host</code>   field with the <code>component.Host</code> reference passed by the Collector and the   <code>cancel</code> function field with the cancellation based on a new context created   with <code>context.Background()</code> (according the Collector's API documentation   suggestions).</li> <li>Updated the <code>Stop()</code> method by adding a call to the <code>cancel()</code> context   cancellation function.</li> </ul>"},{"location":"docs/collector/trace-receiver/#keeping-information-passed-by-the-receivers-factory","title":"Keeping information passed by the receiver's factory","text":"<p>Now that you have implemented the <code>receiver.Traces</code> interface methods, your <code>tailtracer</code> receiver component is ready to be instantiated and returned by its factory.</p> <p>Open the <code>tailtracer/factory.go</code> file and navigate to the <code>createTracesReceiver()</code> function. Notice that the factory will pass references as part of the <code>createTracesReceiver()</code> function parameters that your receiver actually requires to work properly like its configuration settings (<code>component.Config</code>), the next <code>Consumer</code> in the pipeline that will consume the generated traces (<code>consumer.Traces</code>) and the Collector's logger so the <code>tailtracer</code> receiver can add meaningful events to it (<code>receiver.CreateSettings</code>).</p> <p>Given that all this information will be only be made available to the receiver at the moment its instantiated by the factory, The <code>tailtracerReceiver</code> type will need fields to keep that information and use it within other stages of its lifecycle.</p> <p>Here is what the <code>trace-receiver.go</code> file looks like with the updated <code>tailtracerReceiver</code> type declaration:</p> <p>trace-receiver.go</p> <pre><code>package tailtracer\nimport (\n\"context\"\n\"time\"\n\"go.opentelemetry.io/collector/component\"\n\"go.opentelemetry.io/collector/consumer\"\n\"go.uber.org/zap\"\n)\ntype tailtracerReceiver struct {\nhost         component.Host\ncancel       context.CancelFunc\nlogger       *zap.Logger\nnextConsumer consumer.Traces\nconfig       *Config\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Start(ctx context.Context, host component.Host) error {\ntailtracerRcvr.host = host\nctx = context.Background()\nctx, tailtracerRcvr.cancel = context.WithCancel(ctx)\ninterval, _ := time.ParseDuration(tailtracerRcvr.config.Interval)\ngo func() {\nticker := time.NewTicker(interval)\ndefer ticker.Stop()\nfor {\nselect {\ncase &lt;-ticker.C:\ntailtracerRcvr.logger.Info(\"I should start processing traces now!\")\ncase &lt;-ctx.Done():\nreturn\n}\n}\n}()\nreturn nil\n}\nfunc (tailtracerRcvr *tailtracerReceiver) Shutdown(ctx context.Context) error {\ntailtracerRcvr.cancel()\nreturn nil\n}\n</code></pre> <p>The <code>tailtracerReceiver</code> type is now ready to be instantiated and keep all meaningful information passed by its factory.</p> <p>Open the <code>tailtracer/factory.go</code> file and navigate to the <code>createTracesReceiver()</code> function.</p> <p>The receiver is only instantiated if it's declared as a component within a pipeline and the factory is responsible to make sure the next consumer (either a processor or exporter) in the pipeline is valid otherwise it should generate an error.</p> <p>The Collector's API provides some standard error types to help the factory handle pipeline configurations. Your receiver factory should throw a <code>component.ErrNilNextConsumer</code> in case the next consumer has an issue and is passed as nil.</p> <p>The <code>createTracesReceiver()</code> function will need a guard clause to make that validation.</p> <p>You will also need variables to properly initialize the <code>config</code> and the <code>logger</code> fields of the <code>tailtracerReceiver</code> instance.</p> <p>Here is what the <code>factory.go</code> file looks like with the updated <code>createTracesReceiver()</code> function:</p> <p>factory.go</p> <pre><code>package tailtracer\nimport (\n\"context\"\n\"time\"\n\"go.opentelemetry.io/collector/component\"\n\"go.opentelemetry.io/collector/consumer\"\n\"go.opentelemetry.io/collector/receiver\"\n)\nconst (\ntypeStr = \"tailtracer\"\ndefaultInterval = 1 * time.Minute\n)\nfunc createDefaultConfig() component.Config {\nreturn &amp;Config{\nInterval: string(defaultInterval),\n}\n}\nfunc createTracesReceiver(_ context.Context, params receiver.CreateSettings, baseCfg component.Config, consumer consumer.Traces) (receiver.Traces, error) {\nif consumer == nil {\nreturn nil, component.ErrNilNextConsumer\n}\nlogger := params.Logger\ntailtracerCfg := baseCfg.(*Config)\ntraceRcvr := &amp;tailtracerReceiver{\nlogger:       logger,\nnextConsumer: consumer,\nconfig:       tailtracerCfg,\n}\nreturn traceRcvr, nil\n}\n// NewFactory creates a factory for tailtracer receiver.\nfunc NewFactory() receiver.Factory {\nreturn receiver.NewFactory(\ntypeStr,\ncreateDefaultConfig,\nreceiver.WithTraces(createTracesReceiver, component.StabilityLevelAlpha))\n}\n</code></pre> <p>With the factory fully implemented and instantiating the trace receiver component you are ready to test the receiver as part of a pipeline. Go ahead and add the <code>tailtracer</code> receiver to your <code>traces</code> pipeline in the <code>config.yaml</code> as follow:</p> <pre><code>service:\npipelines:\ntraces:\nreceivers: [otlp, tailtracer]\nprocessors: []\nexporters: [jaeger, logging]\n</code></pre> <p>Here is what the output for running your Collector with <code>otelcol-dev</code> command should look like after you updated the <code>traces</code> pipeline:</p> <pre><code>$ ./otelcol-dev --config config.yaml\n2022-03-03T11:19:50.779-0600    info    service/collector.go:190        Applying configuration...\n2022-03-03T11:19:50.780-0600    info    builder/exporters_builder.go:254        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-03T11:19:50.780-0600    info    builder/exporters_builder.go:254        Exporter was built.     {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-03T11:19:50.780-0600    info    builder/pipelines_builder.go:222        Pipeline was built.     {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-03T11:19:50.780-0600    info    builder/receivers_builder.go:224        Receiver was built.     {\"kind\": \"receiver\", \"name\": \"otlp\", \"datatype\": \"traces\"}\n2022-03-03T11:19:50.780-0600    info    builder/receivers_builder.go:224        Receiver was built.     {\"kind\": \"receiver\", \"name\": \"tailtracer\", \"datatype\": \"traces\"}\n2022-03-03T11:19:50.780-0600    info    service/service.go:86   Starting extensions...\n2022-03-03T11:19:50.780-0600    info    service/service.go:91   Starting exporters...\n2022-03-03T11:19:50.780-0600    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-03T11:19:50.781-0600    info    builder/exporters_builder.go:48 Exporter started.       {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-03T11:19:50.781-0600    info    jaegerexporter@v0.41.0/exporter.go:186  State of the connection with the Jaeger Collector backend       {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"IDLE\"}\n2022-03-03T11:19:50.781-0600    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-03T11:19:50.781-0600    info    builder/exporters_builder.go:48 Exporter started.       {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-03T11:19:50.781-0600    info    service/service.go:96   Starting processors...\n2022-03-03T11:19:50.781-0600    info    builder/pipelines_builder.go:54 Pipeline is starting... {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-03T11:19:50.781-0600    info    builder/pipelines_builder.go:65 Pipeline is started.    {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-03T11:19:50.781-0600    info    service/service.go:101  Starting receivers...\n2022-03-03T11:19:50.781-0600    info    builder/receivers_builder.go:68 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-03T11:19:50.781-0600    info    otlpreceiver/otlp.go:69 Starting GRPC server on endpoint localhost:55680        {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-03T11:19:50.783-0600    info    builder/receivers_builder.go:73 Receiver started.       {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-03T11:19:50.783-0600    info    builder/receivers_builder.go:68 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-03T11:19:50.783-0600    info    builder/receivers_builder.go:73 Receiver started.       {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-03T11:19:50.783-0600    info    service/telemetry.go:92 Setting up own telemetry...\n2022-03-03T11:19:50.788-0600    info    service/telemetry.go:116        Serving Prometheus metrics      {\"address\": \":8888\", \"level\": \"basic\", \"service.instance.id\": \"0ca4907c-6fda-4fe1-b0e9-b73d789354a4\", \"service.version\": \"latest\"}\n2022-03-03T11:19:50.788-0600    info    service/collector.go:239        Starting otelcol-dev... {\"Version\": \"1.0.0\", \"NumCPU\": 12}\n2022-03-03T11:19:50.788-0600    info    service/collector.go:135        Everything is ready. Begin running and processing data.\n2022-03-21T15:19:51.717-0500    info    jaegerexporter@v0.46.0/exporter.go:186  State of the connection with the Jaeger Collector backend   {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"READY\"}\n2022-03-03T11:20:51.783-0600    info    tailtracer/trace-receiver.go:23  I should start processing traces now!   {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n</code></pre> <p>Look for the log line for \"builder/receivers_builder.go:68 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\", you can see that the Collector found the settings for the <code>tailtracer</code> receiver within the <code>traces</code> pipeline and is now instantiating it and starting it given that 1 minute after the Collector has started, you can see the info line we added to the <code>ticker</code> function within the <code>Start()</code> method.</p> <p>Now, go ahead and press Ctrl + C in your Collector's terminal so you want watch the shutdown process happening. Here is what the output should look like:</p> <pre><code>^C2022-03-03T11:20:14.652-0600  info    service/collector.go:166        Received signal from OS {\"signal\": \"interrupt\"}\n2022-03-03T11:20:14.652-0600    info    service/collector.go:255        Starting shutdown...\n2022-03-03T11:20:14.652-0600    info    service/service.go:121  Stopping receivers...\n2022-03-03T11:20:14.653-0600    info    tailtracer/trace-receiver.go:29  I am done and ready to shutdown!        {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-03T11:20:14.653-0600    info    service/service.go:126  Stopping processors...\n2022-03-03T11:20:14.653-0600    info    builder/pipelines_builder.go:73 Pipeline is shutting down...    {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-03T11:20:14.653-0600    info    builder/pipelines_builder.go:77 Pipeline is shutdown.   {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-03T11:20:14.653-0600    info    service/service.go:131  Stopping exporters...\n2022-03-03T11:20:14.653-0600    info    service/service.go:136  Stopping extensions...\n2022-03-03T11:20:14.653-0600    info    service/collector.go:273        Shutdown complete.\n</code></pre> <p>As you can see there is an info log line for the <code>tailtracer</code> receiver which means the component is responding correctly to the <code>Shutdown()</code> event. In the next section you will learn more about the OpenTelemetry Trace data model so the <code>tailtracer</code> receiver can finally generate traces!</p>"},{"location":"docs/collector/trace-receiver/#check-your-work_7","title":"Check your work","text":"<ul> <li>Importing the <code>go.opentelemetry.io/collector/consumer</code> which is where the   pipeline's consumer types and interfaces are declared.</li> <li>Importing the <code>go.uber.org/zap</code> package, which is what the Collector uses   for its logging capabilities.</li> <li>Added a <code>zap.Logger</code> field named <code>logger</code> so we can have access to the   Collector's logger reference from within the receiver.</li> <li>Added a <code>consumer.Traces</code> field named <code>nextConsumer</code> so we can push the   traces generated by the <code>tailtracer</code> receiver to the next consumer declared   in the Collector's pipeline.</li> <li>Added a <code>Config</code> field named <code>config</code> so we can have access to receiver's   configuration settings defined within the Collector's config.</li> <li>Added a variable named <code>interval</code> that will be initialized as a   <code>time.Duration</code> based on the value of the <code>interval</code> settings of the   <code>tailtracer</code> receiver defined within the Collector's config.</li> <li>Added a <code>go func()</code> to implement the <code>ticker</code> mechanism so our receiver can   generate traces every time the <code>ticker</code> reaches the amount of time specified   by the <code>interval</code> variable and used the <code>tailtracerRcvr.logger</code> field to   generate a info message every time the receiver supposed to be generating   traces.</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_8","title":"Check your work","text":"<ul> <li>Added a guard clause that verifies if the consumer is properly instantiated   and if not returns the <code>component.ErrNilNextConsumer</code>error.</li> <li>Added a variable called <code>logger</code> and initialized it with the Collector's   logger that is available as a field named <code>Logger</code> within the   <code>receiver.CreateSettings</code> reference.</li> <li>Added a variable called <code>tailtracerCfg</code> and initialized it by casting the   <code>component.Config</code> reference to the <code>tailtracer</code> receiver <code>Config</code>.</li> <li>Added a variable called <code>traceRcvr</code> and initialized it with the   <code>tailtracerReceiver</code> instance using the factory information stored within   the variables.</li> <li>Updated the return statement to now include the <code>traceRcvr</code> instance.</li> </ul>"},{"location":"docs/collector/trace-receiver/#the-collectors-trace-data-model","title":"The Collector's Trace Data Model","text":"<p>You might be familiar with OpenTelemetry traces by using the SDKs and instrumenting an application so you can see and evaluate your traces within a distributed tracing backend like Jaeger.</p> <p>Here is what a trace looks like in Jaeger:</p> <p></p> <p>Granted, this is a Jaeger trace, but it was generated by a trace pipeline within the Collector, therefore you can use it to learn a few things about the OTel trace data model :</p> <ul> <li>A trace is made of one or multiple spans structured within a hierarchy to   represent dependencies.</li> <li>The spans can represent operations within a service and/or across services.</li> </ul> <p>Creating a trace within the trace receiver will be slightly different than the way you would do it with the SDKs, so let's start reviewing the high level concepts.</p>"},{"location":"docs/collector/trace-receiver/#working-with-resources","title":"Working with Resources","text":"<p>In the OTel world, all telemetry is generated by a <code>Resource</code>, here is the definition according to the OTel spec:</p> <p>A <code>Resource</code> is an immutable representation of the entity producing telemetry as Attributes. For example, a process producing telemetry that is running in a container on Kubernetes has a Pod name, it is in a namespace and possibly is part of a Deployment which also has a name. All three of these attributes can be included in the <code>Resource</code>.</p> <p>Traces are most commonly used to represent a service request (the Services entity described by Jaeger's model), which are normally implemented as processes running in a compute unit, but OTel's API approach to describe a <code>Resource</code> through attributes is flexible enough to represent any entity that you may require like ATMs, IoT sensors, the sky is the limit.</p> <p>So it's safe to say that for a trace to exist, a <code>Resource</code> will have to start it.</p> <p>In this tutorial we will simulate a system that has telemetry that demonstrate ATMs located in 2 different states (eg: Illinois and California) accessing the Account's backend system to execute balance, deposit and withdraw operations, therefore we will have to implement code to create the <code>Resource</code> types representing the ATM and the backend system.</p> <p>Go ahead and create a file named <code>model.go</code> inside the <code>tailtracer</code> folder</p> <pre><code>cd tailtracer\ntouch model.go\n</code></pre> <p>Now, within the <code>model.go</code> file, add the definition for the <code>Atm</code> and the <code>BackendSystem</code> types as follow:</p> <p>model.go</p> <pre><code>package tailtracer\ntype Atm struct{\nID           int64\nVersion      string\nName         string\nStateID      string\nSerialNumber string\nISPNetwork   string\n}\ntype BackendSystem struct{\nVersion       string\nProcessName   string\nOSType        string\nOSVersion     string\nCloudProvider string\nCloudRegion   string\nServiceName   string\nEndpoint      string\n}\n</code></pre> <p>These types are meant to represent the entities as they are within the system being observed and they contain information that would be quite meaningful to be added to the traces as part of the <code>Resource</code> definition. You will add some helper functions to generate the instances of those types.</p> <p>Here is what the <code>model.go</code> file will look with the helper functions:</p> <p>model.go</p> <pre><code>package tailtracer\nimport (\n\"math/rand\"\n\"time\"\n)\ntype Atm struct{\nID           int64\nVersion      string\nName         string\nStateID      string\nSerialNumber string\nISPNetwork   string\n}\ntype BackendSystem struct{\nVersion       string\nProcessName   string\nOSType        string\nOSVersion     string\nCloudProvider string\nCloudRegion   string\nEndpoint      string\n}\nfunc generateAtm() Atm{\ni := getRandomNumber(1, 2)\nvar newAtm Atm\nswitch i {\ncase 1:\nnewAtm = Atm{\nID: 111,\nName: \"ATM-111-IL\",\nSerialNumber: \"atmxph-2022-111\",\nVersion: \"v1.0\",\nISPNetwork: \"comcast-chicago\",\nStateID: \"IL\",\n}\ncase 2:\nnewAtm = Atm{\nID: 222,\nName: \"ATM-222-CA\",\nSerialNumber: \"atmxph-2022-222\",\nVersion: \"v1.0\",\nISPNetwork: \"comcast-sanfrancisco\",\nStateID: \"CA\",\n}\n}\nreturn newAtm\n}\nfunc generateBackendSystem() BackendSystem{\ni := getRandomNumber(1, 3)\nnewBackend := BackendSystem{\nProcessName: \"accounts\",\nVersion: \"v2.5\",\nOSType: \"lnx\",\nOSVersion: \"4.16.10-300.fc28.x86_64\",\nCloudProvider: \"amzn\",\nCloudRegion: \"us-east-2\",\n}\nswitch i {\ncase 1:\nnewBackend.Endpoint = \"api/v2.5/balance\"\ncase 2:\nnewBackend.Endpoint = \"api/v2.5/deposit\"\ncase 3:\nnewBackend.Endpoint = \"api/v2.5/withdrawn\"\n}\nreturn newBackend\n}\nfunc getRandomNumber(min int, max int) int {\nrand.Seed(time.Now().UnixNano())\ni := (rand.Intn(max - min + 1) + min)\nreturn i\n}\n</code></pre> <p>Now that you have the functions to generate object instances representing the entities generating telemetry, you are ready to represent those entities in the OTel Collector world.</p> <p>The Collector's API provides a package named <code>ptrace</code> (nested under the <code>pdata</code> package) with all the types, interfaces and helper functions required to work with traces within the Collector's pipeline components.</p> <p>Open the <code>tailtracer/model.go</code> file and add <code>go.opentelemetry.io/collector/pdata/ptrace</code> to the <code>import</code> clause so you can have access to the <code>ptrace</code> package capabilities.</p> <p>Before you can define a <code>Resource</code>, you need to create a <code>ptrace.Traces</code> that will be responsible to propagate the traces through the Collector's pipeline and you can use the helper function <code>ptrace.NewTraces()</code> to instantiate it. You will also need to create instances of the <code>Atm</code> and <code>BackendSystem</code> types so you can have data to represent the telemetry sources involved in your trace.</p> <p>Open the <code>tailtracer/model.go</code> file and add the following function to it:</p> <pre><code>func generateTraces(numberOfTraces int) ptrace.Traces{\ntraces := ptrace.NewTraces()\nfor i := 0; i &lt;= numberOfTraces; i++{\nnewAtm := generateAtm()\nnewBackendSystem := generateBackendSystem()\n}\nreturn traces\n}\n</code></pre> <p>By now you have heard and read enough about how traces are made up of Spans. You have probably also written some instrumentation code using the SDK's functions and types available to create them, but what you probably didn't know, is that within the Collector's API, that there are other types of \"spans\" involved in creating a trace.</p> <p>You will start with a type called <code>ptrace.ResourceSpans</code> which represents the resource and all the operations that it either originated or received while participating in a trace. You can find its definition within the /pdata/internal/data/protogen/trace/v1/trace.pb.go.</p> <p><code>ptrace.Traces</code> has a method named <code>ResourceSpans()</code> which returns an instance of a helper type called <code>ptrace.ResourceSpansSlice</code>. The <code>ptrace.ResourceSpansSlice</code> type has methods to help you handle the array of <code>ptrace.ResourceSpans</code> that will contain as many items as the number of <code>Resource</code> entities participating in the request represented by the trace.</p> <p><code>ptrace.ResourceSpansSlice</code> has a method named <code>AppendEmpty()</code> that adds a new <code>ptrace.ResourceSpan</code> to the array and return its reference.</p> <p>Once you have an instance of a <code>ptrace.ResourceSpan</code> you will use a method named <code>Resource()</code> which will return the instance of the <code>pcommon.Resource</code> associated with the <code>ResourceSpan</code>.</p> <p>Update the <code>generateTrace()</code> function with the following changes:</p> <ul> <li>add a variable named <code>resourceSpan</code> to represent the <code>ResourceSpan</code></li> <li>add a variable named <code>atmResource</code> to represent the <code>pcommon.Resource</code>   associated with the <code>ResourceSpan</code>.</li> <li>Use the methods mentioned above to initialize both variables respectively.</li> </ul> <p>Here is what the function should look like after you implemented these changes:</p> <pre><code>func generateTraces(numberOfTraces int) ptrace.Traces{\ntraces := ptrace.NewTraces()\nfor i := 0; i &lt;= numberOfTraces; i++{\nnewAtm := generateAtm()\nnewBackendSystem := generateBackendSystem()\nresourceSpan := traces.ResourceSpans().AppendEmpty()\natmResource := resourceSpan.Resource()\n}\nreturn traces\n}\n</code></pre>"},{"location":"docs/collector/trace-receiver/#check-your-work_9","title":"Check your work","text":"<ul> <li>Imported the <code>math/rand</code> and <code>time</code> packages to support the implementation   of the <code>generateRandomNumber</code> function</li> <li>Added the <code>generateAtm</code> function that instantiates an <code>Atm</code> type and   randomly assign either Illinois or California as values for <code>StateID</code> and   the equivalent value for <code>ISPNetwork</code></li> <li>Added the <code>generateBackendSystem</code> function that instantiates a   <code>BackendSystem</code>type and randomly assign service endpoint values for the   <code>Endpoint</code> field</li> <li>Added the <code>generateRandomNumber</code> function to help generating random numbers   between a desired range.</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_10","title":"Check your work","text":"<ul> <li>Added the <code>resourceSpan</code> variable and initialized it with the <code>ResourceSpan</code>   reference returned by the <code>traces.ResourceSpans().AppendEmpty()</code> call</li> <li>Added the <code>atmResource</code> variable and initialized it with the   <code>pcommon.Resource</code> reference returned by the <code>resourceSpan.Resource()</code> call</li> </ul>"},{"location":"docs/collector/trace-receiver/#describing-resources-through-attributes","title":"Describing Resources through attributes","text":"<p>The Collector's API provides a package named <code>pcommon</code> (nested under the <code>pdata</code> package) with all the types and helper functions required to describe a <code>Resource</code>.</p> <p>In the Collector's world, a <code>Resource</code> is described by attributes in a key/value pair format represented by the <code>pcommon.Map</code> type.</p> <p>You can check the definition of the <code>pcommon.Map</code> type and the related helper functions to create attribute values using the supported formats in the /pdata/pcommon/common.go file within the Otel Collector's GitHub project.</p> <p>Key/value pairs provide a lot of flexibility to help model your <code>Resource</code> data, so the OTel specification has some guidelines in place to help organize and minimize the conflicts across all the different types of telemetry generation entities that it may need to represent.</p> <p>These guidelines are known as Resource Semantic Conventions and are documented in the OTel specification.</p> <p>When creating your own attributes to represent your own telemetry generation entities, you should follow the guideline provided by the specification:</p> <p>Attributes are grouped logically by the type of the concept that they described. Attributes in the same group have a common prefix that ends with a dot. For example all attributes that describe Kubernetes properties start with <code>k8s.</code></p> <p>Let's start by opening the <code>tailtracer/model.go</code> and adding <code>go.opentelemetry.io/collector/pdata/pcommon</code> to the <code>import</code> clause so you can have access to the <code>pcommon</code> package capabilities.</p> <p>Now go ahead and add a function to read the field values from an <code>Atm</code> instance and write them as attributes (grouped by the prefix \"atm.\") into a <code>pcommon.Resource</code> instance. Here is what the function looks like:</p> <pre><code>func fillResourceWithAtm(resource *pcommon.Resource, atm Atm){\natmAttrs := resource.Attributes()\natmAttrs.PutInt(\"atm.id\", atm.ID)\natmAttrs.PutStr(\"atm.stateid\", atm.StateID)\natmAttrs.PutStr(\"atm.ispnetwork\", atm.ISPNetwork)\natmAttrs.PutStr(\"atm.serialnumber\", atm.SerialNumber)\n}\n</code></pre> <p>The resource semantic conventions also have prescriptive attribute names and well-known values to represent telemetry generation entities that are common and applicable across different domains like compute unit, environment and others.</p> <p>So, when you look at the <code>BackendSystem</code> entity, it has fields representing OS related information and Cloud related information, and we will use the attribute names and values prescribed by the resource semantic convention to represent that information on its <code>Resource</code>.</p> <p>All the resource semantic convention attribute names and well known-values are kept within the /semconv/v1.9.0/generated_resource.go file within the Collector's GitHub project.</p> <p>Let's create a function to read the field values from an <code>BackendSystem</code> instance and write them as attributes into a <code>pcommon.Resource</code> instance. Open the <code>tailtracer/model.go</code> file and add the following function:</p> <pre><code>func fillResourceWithBackendSystem(resource *pcommon.Resource, backend BackendSystem){\nbackendAttrs := resource.Attributes()\nvar osType, cloudProvider string\nswitch {\ncase backend.CloudProvider == \"amzn\":\ncloudProvider = conventions.AttributeCloudProviderAWS\ncase backend.OSType == \"mcrsft\":\ncloudProvider = conventions.AttributeCloudProviderAzure\ncase backend.OSType == \"gogl\":\ncloudProvider = conventions.AttributeCloudProviderGCP\n}\nbackendAttrs.PutStr(conventions.AttributeCloudProvider, cloudProvider)\nbackendAttrs.PutStr(conventions.AttributeCloudRegion, backend.CloudRegion)\nswitch {\ncase backend.OSType == \"lnx\":\nosType = conventions.AttributeOSTypeLinux\ncase backend.OSType == \"wndws\":\nosType = conventions.AttributeOSTypeWindows\ncase backend.OSType == \"slrs\":\nosType = conventions.AttributeOSTypeSolaris\n}\nbackendAttrs.PutStr(conventions.AttributeOSType, osType)\nbackendAttrs.PutStr(conventions.AttributeOSVersion, backend.OSVersion)\n}\n</code></pre> <p>Notice that I didn't add an attribute named \"atm.name\" or \"backendsystem.name\" to the <code>pcommon.Resource</code> representing the <code>Atm</code> and <code>BackendSystem</code> entity names, that's because most (not to say all) distributed tracing backend systems that are compatible with the OTel trace specification, interpret the <code>pcommon.Resource</code> described in a trace as a <code>Service</code>, therefore they expect the <code>pcommon.Resource</code> to carry a required attribute named <code>service.name</code> as prescribed by the resource semantic convention.</p> <p>We will also use non-required attribute named <code>service.version</code> to represent the version information for both <code>Atm</code> and <code>BackendSystem</code> entities.</p> <p>Here is what the <code>tailtracer/model.go</code> file looks like after adding the code for properly assign the \"service.\" group attributes:</p> <p>model.go</p> <pre><code>package tailtracer\nimport (\n\"math/rand\"\n\"time\"\n\"go.opentelemetry.io/collector/pdata/pcommon\"\n\"go.opentelemetry.io/collector/pdata/ptrace\"\nconventions \"go.opentelemetry.io/collector/semconv/v1.9.0\"\"\n)\ntype Atm struct {\n    ID           int64\n    Version      string\n    Name         string\n    StateID      string\n    SerialNumber string\n    ISPNetwork   string\n}\ntype BackendSystem struct {\n    Version       string\n    ProcessName   string\n    OSType        string\n    OSVersion     string\n    CloudProvider string\n    CloudRegion   string\n    Endpoint      string\n}\nfunc generateAtm() Atm {\n    i := getRandomNumber(1, 2)\n    var newAtm Atm\n    switch i {\n        case 1:\n            newAtm = Atm{\n                ID:           111,\n                Name:         \"ATM-111-IL\",\n                SerialNumber: \"atmxph-2022-111\",\n                Version:      \"v1.0\",\n                ISPNetwork:   \"comcast-chicago\",\n                StateID:      \"IL\",\n            }\n        case 2:\n            newAtm = Atm{\n                ID:           222,\n                Name:         \"ATM-222-CA\",\n                SerialNumber: \"atmxph-2022-222\",\n                Version:      \"v1.0\",\n                ISPNetwork:   \"comcast-sanfrancisco\",\n                StateID:      \"CA\",\n            }\n    }\n    return newAtm\n}\nfunc generateBackendSystem() BackendSystem {\n    i := getRandomNumber(1, 3)\n    newBackend := BackendSystem{\n        ProcessName:   \"accounts\",\n        Version:       \"v2.5\",\n        OSType:        \"lnx\",\n        OSVersion:     \"4.16.10-300.fc28.x86_64\",\n        CloudProvider: \"amzn\",\n        CloudRegion:   \"us-east-2\",\n    }\n    switch i {\n        case 1:\n            newBackend.Endpoint = \"api/v2.5/balance\"\n        case 2:\n            newBackend.Endpoint = \"api/v2.5/deposit\"\n        case 3:\n            newBackend.Endpoint = \"api/v2.5/withdrawn\"\n    }\n    return newBackend\n}\nfunc getRandomNumber(min int, max int) int {\n    rand.Seed(time.Now().UnixNano())\n    i := (rand.Intn(max-min+1) + min)\n    return i\n}\nfunc generateTraces(numberOfTraces int) ptrace.Traces {\n    traces := ptrace.NewTraces()\n    for i := 0; i &lt;= numberOfTraces; i++ {\n        newAtm := generateAtm()\n        newBackendSystem := generateBackendSystem()\n        resourceSpan := traces.ResourceSpans().AppendEmpty()\n        atmResource := resourceSpan.Resource()\n        fillResourceWithAtm(&amp;atmResource, newAtm)\n        resourceSpan = traces.ResourceSpans().AppendEmpty()\n        backendResource := resourceSpan.Resource()\n        fillResourceWithBackendSystem(&amp;backendResource, newBackendSystem)\n    }\n    return traces\n}\nfunc fillResourceWithAtm(resource *pcommon.Resource, atm Atm) {\n    atmAttrs := resource.Attributes()\n    atmAttrs.PutInt(\"atm.id\", atm.ID)\n    atmAttrs.PutStr(\"atm.stateid\", atm.StateID)\n    atmAttrs.PutStr(\"atm.ispnetwork\", atm.ISPNetwork)\n    atmAttrs.PutStr(\"atm.serialnumber\", atm.SerialNumber)\n    atmAttrs.PutStr(conventions.AttributeServiceName, atm.Name)\n    atmAttrs.PutStr(conventions.AttributeServiceVersion, atm.Version)\n}\nfunc fillResourceWithBackendSystem(resource *pcommon.Resource, backend BackendSystem) {\n    backendAttrs := resource.Attributes()\n    var osType, cloudProvider string\n    switch {\n        case backend.CloudProvider == \"amzn\":\n            cloudProvider = conventions.AttributeCloudProviderAWS\n        case backend.OSType == \"mcrsft\":\n            cloudProvider = conventions.AttributeCloudProviderAzure\n        case backend.OSType == \"gogl\":\n            cloudProvider = conventions.AttributeCloudProviderGCP\n    }\n    backendAttrs.PutStr(conventions.AttributeCloudProvider, cloudProvider)\n    backendAttrs.PutStr(conventions.AttributeCloudRegion, backend.CloudRegion)\n    switch {\n        case backend.OSType == \"lnx\":\n            osType = conventions.AttributeOSTypeLinux\n        case backend.OSType == \"wndws\":\n            osType = conventions.AttributeOSTypeWindows\n        case backend.OSType == \"slrs\":\nosType = conventions.AttributeOSTypeSolaris\n}\nbackendAttrs.PutStr(conventions.AttributeOSType, osType)\nbackendAttrs.PutStr(conventions.AttributeOSVersion, backend.OSVersion)\nbackendAttrs.PutStr(conventions.AttributeServiceName, backend.ProcessName)\nbackendAttrs.PutStr(conventions.AttributeServiceVersion, backend.Version)\n}\n</code></pre>"},{"location":"docs/collector/trace-receiver/#check-your-work_11","title":"Check your work","text":"<ul> <li>Declared a variable called <code>atmAttrs</code> and initialized it with the   <code>pcommon.Map</code> reference returned by the <code>resource.Attributes()</code> call</li> <li>Used the <code>PutInt()</code> and <code>PutStr()</code> methods from <code>pcommon.Map</code> to add int and   string attributes based on the equivalent <code>Atm</code> field types. Notice that   because those attributes are very specific and only represent the <code>Atm</code>   entity, they are all grouped within the \"atm.\" prefix.</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_12","title":"Check your work","text":"<ul> <li>Imported the <code>go.opentelemetry.io/collector/semconv/v1.9.0</code> package as   <code>conventions</code>, in order to have access to all resource semantic conventions   attribute names and values.</li> <li>Updated the <code>fillResourceWithAtm()</code> function by adding lines to properly   assign the \"service.name\" and \"service.version\" attributes to the   <code>pcommon.Resource</code> representing the <code>Atm</code> entity</li> <li>Updated the <code>fillResourceWithBackendSystem()</code> function by adding lines to   properly assign the \"service.name\" and \"service.version\" attributes to the   <code>pcommon.Resource</code> representing the <code>BackendSystem</code> entity</li> <li>Updated the <code>generateTraces</code> function by adding lines to properly   instantiate a <code>pcommon.Resource</code> and fill in the attribute information for   both <code>Atm</code> and <code>BackendSystem</code> entities using the <code>fillResourceWithAtm()</code>   and <code>fillResourceWithBackendSystem()</code> functions</li> </ul>"},{"location":"docs/collector/trace-receiver/#representing-operations-with-spans","title":"Representing operations with spans","text":"<p>You now have a <code>ResourceSpan</code> instance with their respective <code>Resource</code> properly filled with attributes to represent the <code>Atm</code> and <code>BackendSystem</code> entities, you are ready to represent the operations that each <code>Resource</code> execute as part of a trace within the <code>ResourceSpan</code>.</p> <p>In the OTel world, in order for a system to generate telemetry, it needs to be instrumented either manually or automatically through an instrumentation library.</p> <p>The instrumentation libraries are responsible to set the scope (also known as the instrumentation scope) in which the operations participating on a trace happened and then describe those operations as spans within the context of the trace.</p> <p><code>pdata.ResourceSpans</code> has a method named <code>ScopeSpans()</code> which returns an instance of a helper type called <code>ptrace.ScopeSpansSlice</code>. The <code>ptrace.ScopeSpansSlice</code> type has methods to help you handle the array of <code>ptrace.ScopeSpans</code> that will contain as many items as the number of <code>ptrace.ScopeSpan</code> representing the different instrumentation scopes and the spans it generated within the context of a trace.</p> <p><code>ptrace.ScopeSpansSlice</code> has a method named <code>AppendEmpty()</code> that adds a new <code>ptrace.ScopeSpans</code> to the array and return its reference.</p> <p>Let's create a function to instantiate a <code>ptrace.ScopeSpans</code> representing for the ATM system's instrumentation scope and its spans. Open the <code>tailtracer/model.go</code> file and add the following function:</p> <pre><code> func appendAtmSystemInstrScopeSpans(resourceSpans *ptrace.ResourceSpans) (ptrace.ScopeSpans){\nscopeSpans := resourceSpans.ScopeSpans().AppendEmpty()\nreturn scopeSpans\n}\n</code></pre> <p>The <code>ptrace.ScopeSpans</code> has a method named <code>Scope()</code> that returns a reference for the <code>pcommon.InstrumentationScope</code> instance representing the instrumentation scope that generated the spans.</p> <p><code>pcommon.InstrumentationScope</code> has the following methods to describe an instrumentation scope:</p> <ul> <li> <p><code>SetName(v string)</code> sets the name for the instrumentation library</p> </li> <li> <p><code>SetVersion(v string)</code> sets the version for the instrumentation library</p> </li> <li> <p><code>Name() string</code> returns the name associated with the instrumentation library</p> </li> <li> <p><code>Version() string</code> returns the version associated with the instrumentation   library</p> </li> </ul> <p>Let's update the <code>appendAtmSystemInstrScopeSpans</code> function so we can set the name and version of the instrumentation scope for the new <code>ptrace.ScopeSpans</code>. Here is what <code>appendAtmSystemInstrScopeSpans</code> looks like after the update:</p> <pre><code> func appendAtmSystemInstrScopeSpans(resourceSpans *ptrace.ResourceSpans) (ptrace.ScopeSpans){\nscopeSpans := resourceSpans.ScopeSpans().AppendEmpty()\nscopeSpans.Scope().SetName(\"atm-system\")\nscopeSpans.Scope().SetVersion(\"v1.0\")\nreturn scopeSpans\n}\n</code></pre> <p>You can now update the <code>generateTraces</code> function and add variables to represent the instrumentation scope used by both <code>Atm</code> and <code>BackendSystem</code> entities by initializing them with the <code>appendAtmSystemInstrScopeSpans()</code>. Here is what <code>generateTraces()</code> looks like after the update:</p> <pre><code>func generateTraces(numberOfTraces int) ptrace.Traces{\ntraces := ptraces.NewTraces()\nfor i := 0; i &lt;= numberOfTraces; i++{\nnewAtm := generateAtm()\nnewBackendSystem := generateBackendSystem()\nresourceSpan := traces.ResourceSpans().AppendEmpty()\natmResource := resourceSpan.Resource()\nfillResourceWithAtm(&amp;atmResource, newAtm)\natmInstScope := appendAtmSystemInstrScopeSpans(&amp;resourceSpan)\nresourceSpan = traces.ResourceSpans().AppendEmpty()\nbackendResource := resourceSpan.Resource()\nfillResourceWithBackendSystem(&amp;backendResource, newBackendSystem)\nbackendInstScope := appendAtmSystemInstrScopeSpans(&amp;resourceSpan)\n}\nreturn traces\n}\n</code></pre> <p>At this point, you have everything needed to represent the telemetry generation entities in your system and the instrumentation scope that is responsible to identify operations and generate the traces for the system. The next step is to finally create the spans representing the operations that the given instrumentation scope generated as part of a trace.</p> <p><code>ptrace.ScopeSpans</code> has a method named <code>Spans()</code> which returns an instance of a helper type called <code>ptrace.SpanSlice</code>. The <code>ptrace.SpanSlice</code> type has methods to help you handle the array of <code>ptrace.Span</code> that will contain as many items as the number of operations the instrumentation scope was able to identify and describe as part of the trace.</p> <p><code>ptrace.SpanSlice</code> has a method named <code>AppendEmpty()</code> that adds a new <code>ptrace.Span</code> to the array and return its reference.</p> <p><code>ptrace.Span</code> has the following methods to describe an operation:</p> <ul> <li> <p><code>SetTraceID(v pcommon.TraceID)</code> sets the <code>pcommon.TraceID</code> uniquely   identifying the trace which this span is associated with</p> </li> <li> <p><code>SetSpanID(v pcommon.SpanID)</code> sets the <code>pcommon.SpanID</code> uniquely identifying   this span within the context of the trace it is associated with</p> </li> <li> <p><code>SetParentSpanID(v pcommon.SpanID)</code> sets <code>pcommon.SpanID</code> for the parent   span/operation in case the operation represented by this span is executed as   part of the parent (nested)</p> </li> <li> <p><code>SetName(v string)</code> sets the name of the operation for the span</p> </li> <li> <p><code>SetKind(v ptrace.SpanKind)</code> sets <code>ptrace.SpanKind</code> defining what kind of   operation the span represents.</p> </li> <li> <p><code>SetStartTimestamp(v pcommon.Timestamp)</code> sets the <code>pcommon.Timestamp</code>   representing the date and time when the operation represented by the span has   started</p> </li> <li> <p><code>SetEndTimestamp(v pcommon.Timestamp)</code> sets the <code>pcommon.Timestamp</code>   representing the date and time when the operation represented by the span has   ended</p> </li> </ul> <p>As you can see per the methods above, a <code>ptrace.Span</code> is uniquely identified by 2 required IDs; their own unique ID represented by the <code>pcommon.SpanID</code> type and the ID of the trace they are associated with represented by a <code>pcommon.TraceID</code> type.</p> <p>The <code>pcommon.TraceID</code> has to carry a globally unique ID represented through a 16 byte array and should follow the W3C Trace Context specification while the <code>pcommon.SpanID</code> is a unique ID within the context of the trace they are associated with and it's represented through a 8 byte array.</p> <p>The <code>pcommon</code> package provides the following types to generate the span's IDs:</p> <ul> <li> <p><code>type TraceID [16]byte</code></p> </li> <li> <p><code>type SpanID [8]byte</code></p> </li> </ul> <p>For this tutorial, you will be creating the IDs using functions from <code>github.com/google/uuid</code> package for the <code>pcommon.TraceID</code> and functions from the <code>crypto/rand</code> package to randomly generate the <code>pcommon.SpanID</code>. Open the <code>tailtracer/model.go</code> file and add both packages to the <code>import</code> statement; after that, add the following functions to help generate both IDs:</p> <pre><code>func NewTraceID() pcommon.TraceID {\nreturn pcommon.TraceID(uuid.New())\n}\nfunc NewSpanID() pcommon.SpanID {\nvar rngSeed int64\n_ = binary.Read(crand.Reader, binary.LittleEndian, &amp;rngSeed)\nrandSource := rand.New(rand.NewSource(rngSeed))\nvar sid [8]byte\nrandSource.Read(sid[:])\nspanID := pcommon.SpanID(sid)\nreturn spanID\n}\n</code></pre> <p>Now that you have a way to properly identify the spans, you can start creating them to represent the operations within and across the entities in your system.</p> <p>As part of the <code>generateBackendSystem()</code> function, we have randomly assigned the operations that the <code>BackEndSystem</code> entity can provide as services to the system. We will now open the <code>tailtracer/model.go</code> file and a function called <code>appendTraceSpans()</code> that will be responsible to create a trace and append spans representing the <code>BackendSystem</code> operations. Here is what the initial implementation for the <code>appendTraceSpans()</code> function looks like:</p> <pre><code>func appendTraceSpans(backend *BackendSystem, backendScopeSpans *ptrace.ScopeSpans, atmScopeSpans *ptrace.ScopeSpans) {\ntraceId := NewTraceID()\nbackendSpanId := NewSpanID()\nbackendDuration, _ := time.ParseDuration(\"1s\")\nbackendSpanStartTime := time.Now()\nbackendSpanFinishTime := backendSpanStartTime.Add(backendDuration)\nbackendSpan := backendScopeSpans.Spans().AppendEmpty()\nbackendSpan.SetTraceID(traceId)\nbackendSpan.SetSpanID(backendSpanId)\nbackendSpan.SetName(backend.Endpoint)\nbackendSpan.SetKind(ptrace.SpanKindServer)\nbackendSpan.SetStartTimestamp(pcommon.NewTimestampFromTime(backendSpanStartTime))\nbackendSpan.SetEndTimestamp(pcommon.NewTimestampFromTime(backendSpanFinishTime))\n}\n</code></pre> <p>You probably noticed that there are 2 references to <code>ptrace.ScopeSpans</code> as parameters in the <code>appendTraceSpans()</code> function, but we only used one of them. Don't worry about it for now, we will get back to it later.</p> <p>You will now update the <code>generateTraces()</code> function so it can actually generate the trace by calling the <code>appendTraceSpans()</code> function. Here is what the updated <code>generateTraces()</code> function looks like:</p> <pre><code>func generateTraces(numberOfTraces int) ptrace.Traces {\ntraces := ptrace.NewTraces()\nfor i := 0; i &lt;= numberOfTraces; i++ {\nnewAtm := generateAtm()\nnewBackendSystem := generateBackendSystem()\nresourceSpan := traces.ResourceSpans().AppendEmpty()\natmResource := resourceSpan.Resource()\nfillResourceWithAtm(&amp;atmResource, newAtm)\natmInstScope := appendAtmSystemInstrScopeSpans(&amp;resourceSpan)\nresourceSpan = traces.ResourceSpans().AppendEmpty()\nbackendResource := resourceSpan.Resource()\nfillResourceWithBackendSystem(&amp;backendResource, newBackendSystem)\nbackendInstScope := appendAtmSystemInstrScopeSpans(&amp;resourceSpan)\nappendTraceSpans(&amp;newBackendSystem, &amp;backendInstScope, &amp;atmInstScope)\n}\nreturn traces\n}\n</code></pre> <p>You now have the <code>BackendSystem</code> entity and its operations represented in spans within a proper trace context! All you need to do is to push the generated trace through the pipeline so the next consumer (either a processor or an exporter) can receive and process it.</p> <p><code>consumer.Traces</code> has a method called <code>ConsumeTraces()</code> which is responsible to push the generated traces to the next consumer in the pipeline. All you need to do now is to update the <code>Start()</code> method within the <code>tailtracerReceiver</code> type and add the code to use it.</p> <p>Open the <code>tailtracer/trace-receiver.go</code> file and update the <code>Start()</code> method as follow:</p> <pre><code>func (tailtracerRcvr *tailtracerReceiver) Start(ctx context.Context, host component.Host) error {\ntailtracerRcvr.host = host\nctx = context.Background()\nctx, tailtracerRcvr.cancel = context.WithCancel(ctx)\ninterval, _ := time.ParseDuration(tailtracerRcvr.config.Interval)\ngo func() {\nticker := time.NewTicker(interval)\ndefer ticker.Stop()\nfor {\nselect {\ncase &lt;-ticker.C:\ntailtracerRcvr.logger.Info(\"I should start processing traces now!\")\ntailtracerRcvr.nextConsumer.ConsumeTraces(ctx, generateTraces(tailtracerRcvr.config.NumberOfTraces))\ncase &lt;-ctx.Done():\nreturn\n}\n}\n}()\nreturn nil\n}\n</code></pre> <p>If you run your <code>otelcol-dev</code>, here is what the output should look like after 2 minutes running:</p> <pre><code>Starting: /Users/rquedas/go/bin/dlv dap --check-go-version=false --listen=127.0.0.1:54625 --log-dest=3 from /Users/rquedas/Documents/vscode-workspace/otel4devs/collector/receiver/trace-receiver/otelcol-dev\nDAP server listening at: 127.0.0.1:54625\n2022-03-21T15:44:22.737-0500    info    builder/exporters_builder.go:255    Exporter was built. {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-21T15:44:22.737-0500    info    builder/exporters_builder.go:255    Exporter was built. {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-21T15:44:22.737-0500    info    builder/pipelines_builder.go:223    Pipeline was built. {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-21T15:44:22.738-0500    info    builder/receivers_builder.go:226    Receiver was built. {\"kind\": \"receiver\", \"name\": \"otlp\", \"datatype\": \"traces\"}\n2022-03-21T15:44:22.738-0500    info    builder/receivers_builder.go:226    Receiver was built. {\"kind\": \"receiver\", \"name\": \"tailtracer\", \"datatype\": \"traces\"}\n2022-03-21T15:44:22.738-0500    info    service/service.go:82   Starting extensions...\n2022-03-21T15:44:22.738-0500    info    service/service.go:87   Starting exporters...\n2022-03-21T15:44:22.738-0500    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-21T15:44:22.738-0500    info    builder/exporters_builder.go:48 Exporter started.   {\"kind\": \"exporter\", \"name\": \"logging\"}\n2022-03-21T15:44:22.738-0500    info    builder/exporters_builder.go:40 Exporter is starting... {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-21T15:44:22.738-0500    info    builder/exporters_builder.go:48 Exporter started.   {\"kind\": \"exporter\", \"name\": \"jaeger\"}\n2022-03-21T15:44:22.738-0500    info    service/service.go:92   Starting processors...\n2022-03-21T15:44:22.738-0500    info    jaegerexporter@v0.46.0/exporter.go:186  State of the connection with the Jaeger Collector backend   {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"IDLE\"}\n2022-03-21T15:44:22.738-0500    info    builder/pipelines_builder.go:54 Pipeline is starting... {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-21T15:44:22.738-0500    info    builder/pipelines_builder.go:65 Pipeline is started.    {\"name\": \"pipeline\", \"name\": \"traces\"}\n2022-03-21T15:44:22.738-0500    info    service/service.go:97   Starting receivers...\n2022-03-21T15:44:22.738-0500    info    builder/receivers_builder.go:68 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-21T15:44:22.738-0500    info    otlpreceiver/otlp.go:69 Starting GRPC server on endpoint localhost:55680    {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-21T15:44:22.741-0500    info    builder/receivers_builder.go:73 Receiver started.   {\"kind\": \"receiver\", \"name\": \"otlp\"}\n2022-03-21T15:44:22.741-0500    info    builder/receivers_builder.go:68 Receiver is starting... {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-21T15:44:22.741-0500    info    builder/receivers_builder.go:73 Receiver started.   {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-21T15:44:22.741-0500    info    service/telemetry.go:109    Setting up own telemetry...\n2022-03-21T15:44:22.741-0500    info    service/telemetry.go:129    Serving Prometheus metrics  {\"address\": \":8888\", \"level\": \"basic\", \"service.instance.id\": \"4b134d3e-2822-4360-b2c6-7030bea0beec\", \"service.version\": \"latest\"}\n2022-03-21T15:44:22.742-0500    info    service/collector.go:248    Starting otelcol-dev... {\"Version\": \"1.0.0\", \"NumCPU\": 12}\n2022-03-21T15:44:22.742-0500    info    service/collector.go:144    Everything is ready. Begin running and processing data.\n2022-03-21T15:44:23.739-0500    info    jaegerexporter@v0.46.0/exporter.go:186  State of the connection with the Jaeger Collector backend   {\"kind\": \"exporter\", \"name\": \"jaeger\", \"state\": \"READY\"}\n2022-03-21T15:45:22.743-0500    info    tailtracer/trace-receiver.go:33 I should start processing traces now!   {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-21T15:45:22.743-0500    INFO    loggingexporter/logging_exporter.go:40  TracesExporter  {\"#spans\": 1}\n2022-03-21T15:45:22.743-0500    DEBUG   loggingexporter/logging_exporter.go:49  ResourceSpans #0\nResource SchemaURL:\nResource labels:\n     -&gt; atm.id: INT(222)\n     -&gt; atm.stateid: STRING(CA)\n     -&gt; atm.ispnetwork: STRING(comcast-sanfrancisco)\n     -&gt; atm.serialnumber: STRING(atmxph-2022-222)\n     -&gt; service.name: STRING(ATM-222-CA)\n     -&gt; service.version: STRING(v1.0)\nScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope atm-system v1.0\nResourceSpans #1\nResource SchemaURL:\nResource labels:\n     -&gt; cloud.provider: STRING(aws)\n     -&gt; cloud.region: STRING(us-east-2)\n     -&gt; os.type: STRING(linux)\n     -&gt; os.version: STRING(4.16.10-300.fc28.x86_64)\n     -&gt; service.name: STRING(accounts)\n     -&gt; service.version: STRING(v2.5)\nScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope atm-system v1.0\nSpan #0\n    Trace ID       : 5cce8a774d4546c2a5cbdeb607ec74c9\n    Parent ID      :\n    ID             : bb25c05c7fb13084\n    Name           : api/v2.5/balance\n    Kind           : SPAN_KIND_SERVER\n    Start time     : 2022-03-21 20:45:22.743385 +0000 UTC\n    End time       : 2022-03-21 20:45:23.743385 +0000 UTC\n    Status code    : STATUS_CODE_OK\n    Status message :\n2022-03-21T15:46:22.743-0500    info    tailtracer/trace-receiver.go:33 I should start processing traces now!   {\"kind\": \"receiver\", \"name\": \"tailtracer\"}\n2022-03-21T15:46:22.744-0500    INFO    loggingexporter/logging_exporter.go:40  TracesExporter  {\"#spans\": 1}\n2022-03-21T15:46:22.744-0500    DEBUG   loggingexporter/logging_exporter.go:49  ResourceSpans #0\nResource SchemaURL:\nResource labels:\n     -&gt; atm.id: INT(111)\n     -&gt; atm.stateid: STRING(IL)\n     -&gt; atm.ispnetwork: STRING(comcast-chicago)\n     -&gt; atm.serialnumber: STRING(atmxph-2022-111)\n     -&gt; service.name: STRING(ATM-111-IL)\n     -&gt; service.version: STRING(v1.0)\nScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope atm-system v1.0\nResourceSpans #1\nResource SchemaURL:\nResource labels:\n     -&gt; cloud.provider: STRING(aws)\n     -&gt; cloud.region: STRING(us-east-2)\n     -&gt; os.type: STRING(linux)\n     -&gt; os.version: STRING(4.16.10-300.fc28.x86_64)\n     -&gt; service.name: STRING(accounts)\n     -&gt; service.version: STRING(v2.5)\nScopeSpans #0\nScopeSpans SchemaURL:\nInstrumentationScope atm-system v1.0\nSpan #0\n    Trace ID       : 8a6ca822db0847f48facfebbb08bbb9e\n    Parent ID      :\n    ID             : 7cf668c1273ecee5\n    Name           : api/v2.5/withdrawn\n    Kind           : SPAN_KIND_SERVER\n    Start time     : 2022-03-21 20:46:22.74404 +0000 UTC\n    End time       : 2022-03-21 20:46:23.74404 +0000 UTC\n    Status code    : STATUS_CODE_OK\n    Status message :\n</code></pre> <p>Here is what the generated trace looks like in Jaeger: </p> <p>What you currently see in Jaeger is the representation of a service that is receiving a request from an external entity that isn't instrumented by an OTel SDK, therefore it can't be identified as the origin/start of the trace. In order for a <code>ptrace.Span</code> to understand it is representing an operation that was execute as a result of another operation originated either within or outside (nested/child) of the <code>Resource</code> within the same trace context you will need to:</p> <ul> <li>Set the same trace context as the caller operation by calling the   <code>SetTraceID()</code> method and passing the <code>pcommon.TraceID</code> of the parent/caller   <code>ptrace.Span</code> as a parameter.</li> <li>Define who is the caller operation within the context of the trace by calling   <code>SetParentId()</code> method and passing the <code>pcommon.SpanID</code> of the parent/caller   <code>ptrace.Span</code> as a parameter.</li> </ul> <p>You will now create a <code>ptrace.Span</code> representing the <code>Atm</code> entity operations and set it as the parent for <code>BackendSystem</code> span. Open the <code>tailtracer/model.go</code> file and update the <code>appendTraceSpans()</code> function as follow:</p> <pre><code>func appendTraceSpans(backend *BackendSystem, backendScopeSpans *ptrace.ScopeSpans, atmScopeSpans *ptrace.ScopeSpans) {\ntraceId := NewTraceID()\nvar atmOperationName string\nswitch {\ncase strings.Contains(backend.Endpoint, \"balance\"):\natmOperationName = \"Check Balance\"\ncase strings.Contains(backend.Endpoint, \"deposit\"):\natmOperationName = \"Make Deposit\"\ncase strings.Contains(backend.Endpoint, \"withdraw\"):\natmOperationName = \"Fast Cash\"\n}\natmSpanId := NewSpanID()\natmSpanStartTime := time.Now()\natmDuration, _ := time.ParseDuration(\"4s\")\natmSpanFinishTime := atmSpanStartTime.Add(atmDuration)\natmSpan := atmScopeSpans.Spans().AppendEmpty()\natmSpan.SetTraceID(traceId)\natmSpan.SetSpanID(atmSpanId)\natmSpan.SetName(atmOperationName)\natmSpan.SetKind(ptrace.SpanKindClient)\natmSpan.Status().SetCode(ptrace.StatusCodeOk)\natmSpan.SetStartTimestamp(pcommon.NewTimestampFromTime(atmSpanStartTime))\natmSpan.SetEndTimestamp(pcommon.NewTimestampFromTime(atmSpanFinishTime))\nbackendSpanId := NewSpanID()\nbackendDuration, _ := time.ParseDuration(\"2s\")\nbackendSpanStartTime := atmSpanStartTime.Add(backendDuration)\nbackendSpan := backendScopeSpans.Spans().AppendEmpty()\nbackendSpan.SetTraceID(atmSpan.TraceID())\nbackendSpan.SetSpanID(backendSpanId)\nbackendSpan.SetParentSpanID(atmSpan.SpanID())\nbackendSpan.SetName(backend.Endpoint)\nbackendSpan.SetKind(ptrace.SpanKindServer)\nbackendSpan.Status().SetCode(ptrace.StatusCodeOk)\nbackendSpan.SetStartTimestamp(pcommon.NewTimestampFromTime(backendSpanStartTime))\nbackendSpan.SetEndTimestamp(atmSpan.EndTimestamp())\n}\n</code></pre> <p>Go ahead and run your <code>otelcol-dev</code> again and after 2 minutes running, you should start seeing traces in Jaeger like the following: </p> <p>We now have services representing both the <code>Atm</code> and the <code>BackendSystem</code> telemetry generation entities in our system and can fully understand how both entities are being used and contributing to the performance of an operation executed by an user.</p> <p>Here is the detailed view of one of those traces in Jaeger: </p> <p>That's it! You have now reached the end of this tutorial and successfully implemented a trace receiver, congratulations!</p>"},{"location":"docs/collector/trace-receiver/#check-your-work_13","title":"Check your work","text":"<ul> <li>Added <code>traceId</code> and <code>backendSpanId</code> variables to respectively represent the   trace and the span id and initialized them with the helper functions created   previously</li> <li>Added <code>backendSpanStartTime</code> and <code>backendSpanFinishTime</code> to represent the   start and the end time of the operation. For the tutorial, any   <code>BackendSystem</code> operation will take 1 second.</li> <li>Added a variable called <code>backendSpan</code> which will hold the instance of the   <code>ptrace.Span</code> representing this operation.</li> <li>Setting the <code>Name</code> of the span with the <code>Endpoint</code> field value from the   <code>BackendSystem</code> instance</li> <li>Setting the <code>Kind</code> of the span as <code>ptrace.SpanKindServer</code>. Take a look at   SpanKind section within the trace   specification to understand how to properly define SpanKind.</li> <li>Used all the methods mentioned before to fill the <code>ptrace.Span</code> with the   proper values to represent the <code>BackendSystem</code> operation</li> </ul>"},{"location":"docs/collector/trace-receiver/#check-your-work_14","title":"Check your work","text":"<ul> <li>Added a line under the <code>case &lt;=ticker.C</code> condition calling the   <code>tailtracerRcvr.nextConsumer.ConsumeTraces()</code> method passing the new context   created within the <code>Start()</code> method (<code>ctx</code>) and a call to the   <code>generateTraces</code> function so the generated traces can be pushed to the next   consumer in the pipeline</li> </ul>"},{"location":"docs/collector/transforming-telemetry/","title":"\u6539\u53d8\u9065\u6d4b","text":"<p>The OpenTelemetry Collector is a convenient place to transform data before sending it to a vendor or other systems. This is frequently done for data quality, governance, cost, and security reasons.</p> <p>Processors available from the the Collector Contrib repository support dozens of different transformations on metric, span and log data. The following sections provide some basic examples on getting started with a few frequently-used processors.</p> <p>The configuration of processors, particularly advanced transformations, may have a significant impact on collector performance.</p>"},{"location":"docs/collector/transforming-telemetry/#basic-filtering","title":"Basic filtering","text":"<p>Processor: filter processor</p> <p>The filter processor allows users to filter telemetry based on <code>include</code> or <code>exclude</code> rules. Include rules are used for defining \"allow lists\" where anything that does not match include rules is dropped from the collector. Exclude rules are used for defining \"deny lists\" where telemetry that matches rules is dropped from the collector.</p> <p>For example, to only allow span data from services app1, app2, and app3 and drop data from all other services:</p> <pre><code>processors:\nfilter/allowlist:\nspans:\ninclude:\nmatch_type: strict\nservices:\n- app1\n- app2\n- app3\n</code></pre> <p>To only block spans from a service called development while allowing all other spans, an exclude rule is used:</p> <pre><code>processors:\nfilter/denylist:\nspans:\nexclude:\nmatch_type: strict\nservices:\n- development\n</code></pre> <p>The filter processor docs have more examples, including filtering on logs and metrics.</p>"},{"location":"docs/collector/transforming-telemetry/#adding-or-deleting-attributes","title":"Adding or Deleting Attributes","text":"<p>Processor: attributes processor or resource processor</p> <p>The attributes processor can be used to update, insert, delete, or replace existing attributes on metrics or traces. For example, here\u2019s a configuration that adds an attribute called account_id to all spans:</p> <pre><code>processors:\nattributes/accountid:\nactions:\n- key: account_id\nvalue: 2245\naction: insert\n</code></pre> <p>The resource processor has an identical configuration, but applies only to resource attributes. Use the resource processor to modify infrastructure metadata related to telemetry. For example, this inserts the Kubernetes cluster name:</p> <pre><code>processors:\nresource/k8s:\nattributes:\n- key: k8s.cluster.name\nfrom_attribute: k8s-cluster\naction: insert\n</code></pre>"},{"location":"docs/collector/transforming-telemetry/#renaming-metrics-or-metric-labels","title":"Renaming Metrics or Metric Labels","text":"<p>Processor: metrics transform processor</p> <p>The metrics transform processor shares some functionality with the attributes processor, but also supports renaming and other metric-specific functionality.</p> <pre><code>processors:\nmetricstransform/rename:\ntransforms:\ninclude: system.cpu.usage\naction: update\nnew_name: system.cpu.usage_time\n</code></pre> <p>The metrics transform processor also supports regular expressions to apply transform rules to multiple metric names or metric labels at the same time. This example renames cluster_name to cluster-name for all metrics:</p> <pre><code>processors:\nmetricstransform/clustername:\ntransforms:\n- include: ^.*$\nmatch_type: regexp\naction: update\noperations:\n- action: update_label\nlabel: cluster_name\nnew_label: cluster-name\n</code></pre>"},{"location":"docs/collector/transforming-telemetry/#enriching-telemetry-with-resource-attributes","title":"Enriching Telemetry with Resource Attributes","text":"<p>Processor: resource detection processor and k8sattributes processor</p> <p>These processors can be used for enriching telemetry with relevant infrastructure metadata to help teams quickly identify when underlying infrastructure is impacting service health or performance.</p> <p>The resource detection processor adds relevant cloud or host-level information to telemetry:</p> <pre><code>processors:\nresourcedetection/system:\n# Modify the list of detectors to match the cloud environment\ndetectors: [env, system, gcp, ec2, azure]\ntimeout: 2s\noverride: false\n</code></pre> <p>Similarly, the K8s processor enriches telemetry with relevant Kubernetes metadata like pod name, node name, or workload name. The collector pod must be configured to have read access to certain Kubernetes RBAC APIs, which is documented here. To use the default options, it can be configured with an empty block:</p> <pre><code>processors:\nk8sattributes/default:\n</code></pre>"},{"location":"docs/collector/transforming-telemetry/#advanced-transformations","title":"Advanced Transformations","text":"<p>More advanced attribute transformations are also available in the transform processor. The transform processor allows end-users to specify transformations on metrics, logs, and traces using the OpenTelemetry Transformation Language.</p>"},{"location":"docs/collector/troubleshooting/","title":"\u6545\u969c\u6392\u9664","text":"<p>This page describes some options when troubleshooting the health or performance of the OpenTelemetry Collector. The Collector provides a variety of metrics, logs, and extensions for debugging issues.</p>"},{"location":"docs/collector/troubleshooting/#sending-test-data","title":"Sending test data","text":"<p>For certain types of issues, particularly verifying configuration and debugging network issues, it can be helpful to send a small amount of data to a collector configured to output to local logs. For details, see Local exporters.</p>"},{"location":"docs/collector/troubleshooting/#checklist-for-debugging-complex-pipelines","title":"Checklist for debugging complex pipelines","text":"<p>It can be difficult to isolate problems when telemetry flows through multiple collectors and networks. For each \"hop\" of telemetry data through a collector or other component in your telemetry pipeline, it\u2019s important to verify the following:</p> <ul> <li>Are there error messages in the logs of the collector?</li> <li>How is the telemetry being ingested into this component?</li> <li>How is the telemetry being modified (i.e. sampling, redacting) by this   component?</li> <li>How is the telemetry being exported from this component?</li> <li>What format is the telemetry in?</li> <li>How is the next hop configured?</li> <li>Are there any network policies that prevent data from getting in or out?</li> </ul>"},{"location":"docs/collector/troubleshooting/#more","title":"More","text":"<p>For detailed recommendations, including common problems, see Troubleshooting from the Collector repo.</p>"},{"location":"docs/collector/deployment/","title":"\u90e8\u7f72","text":"<p>OpenTelemetry \u6536\u96c6\u5668\u7531\u5355\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u7ec4\u6210\uff0c\u60a8\u53ef\u4ee5\u9488\u5bf9\u4e0d\u540c\u7684\u7528\u4f8b\u4ee5\u4e0d\u540c\u7684\u65b9\u5f0f\u4f7f\u7528\u5b83 \u3002\u672c\u8282\u63cf\u8ff0\u90e8\u7f72\u6a21\u5f0f\u3001\u5b83\u4eec\u7684\u7528\u4f8b\u3001\u4f18\u7f3a\u70b9\u4ee5\u53ca\u7528\u4e8e\u8de8\u73af\u5883\u548c\u591a\u540e\u7aef\u90e8\u7f72\u7684\u6536\u96c6\u5668\u914d\u7f6e\u7684\u6700 \u4f73\u5b9e\u8df5\u3002</p>"},{"location":"docs/collector/deployment/#_1","title":"\u8d44\u6e90","text":"<ul> <li>KubeCon NA 2021 \u8bb2\u5ea7OpenTelemetry \u6536\u96c6\u5668\u90e8\u7f72\u6a21\u5f0f</li> <li>\u4f34\u968f\u7740\u6f14\u8bb2\u7684\u90e8\u7f72\u6a21\u5f0f</li> </ul>"},{"location":"docs/collector/deployment/agent/","title":"\u4ee3\u7406","text":"<p>\u4ee3\u7406\u6536\u96c6\u5668\u90e8\u7f72\u6a21\u5f0f\u7531\u5e94\u7528\u7a0b\u5e8f(\u4f7f\u7528 OpenTelemetry \u534f\u8bae(OTLP)\u4f7f\u7528 OpenTelemetry SDK \u8fdb\u884c\u68c0\u6d4b)\u6216\u5176\u4ed6\u6536\u96c6\u5668(\u4f7f\u7528 OTLP \u5bfc\u51fa\u5668)\u7ec4 \u6210\uff0c\u8fd9\u4e9b\u6536\u96c6\u5668\u5c06\u9065\u6d4b\u4fe1\u53f7\u53d1\u9001\u5230\u4e0e\u5e94\u7528\u7a0b\u5e8f\u4e00\u8d77\u8fd0\u884c\u7684\u6536\u96c6 \u5668\u5b9e\u4f8b\u6216\u4e0e\u5e94\u7528\u7a0b\u5e8f\u5728\u540c\u4e00\u53f0\u4e3b\u673a\u4e0a\u8fd0\u884c\u7684\u6536\u96c6\u5668\u5b9e\u4f8b(\u4f8b\u5982 sidecar \u6216\u5b88\u62a4 \u8fdb\u7a0b)\u3002</p> <p>\u6bcf\u4e2a\u5ba2\u6237\u7aef SDK \u6216\u4e0b\u6e38\u6536\u96c6\u5668\u90fd\u914d\u7f6e\u4e86\u4e00\u4e2a\u6536\u96c6\u5668\u4f4d\u7f6e:</p> <p></p> <ol> <li>\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0cSDK \u88ab\u914d\u7f6e\u4e3a\u5c06 OTLP \u6570\u636e\u53d1\u9001\u5230\u6536\u96c6\u5668\u3002</li> <li>\u91c7\u96c6\u5668\u914d\u7f6e\u4e3a\u5c06\u9065\u6d4b\u6570\u636e\u53d1\u9001\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u540e\u7aef\u3002</li> </ol>"},{"location":"docs/collector/deployment/agent/#_1","title":"\u793a\u4f8b","text":"<p>\u4ee3\u7406\u6536\u96c6\u5668\u90e8\u7f72\u6a21\u5f0f\u7684\u5177\u4f53\u793a\u4f8b\u5982\u4e0b:\u60a8\u53ef\u4ee5\u4f7f\u7528 OpenTelemetry Java SDK \u624b\u52a8\u8bbe\u7f6e\u4e00 \u4e2a\u7528\u4e8e\u5bfc\u51fa\u6307\u6807\u7684 Java \u5e94\u7528\u7a0b\u5e8f\u3002\u5728\u5e94\u7528\u7a0b\u5e8f\u7684\u4e0a\u4e0b\u6587\u4e2d \uff0c\u4f60\u53ef\u4ee5\u5c06<code>OTEL_METRICS_EXPORTER</code>\u8bbe\u7f6e\u4e3a<code>otlp</code>(\u8fd9\u662f\u9ed8\u8ba4\u503c)\uff0c\u5e76\u4f7f\u7528\u6536\u96c6\u5668\u7684\u5730\u5740\u914d \u7f6eotlp \u5bfc\u51fa\u5668\uff0c\u4f8b\u5982(\u5728 Bash \u6216 <code>zsh</code> shell \u4e2d):</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://collector.example.com:4318\n</code></pre> <p>\u670d\u52a1\u4e8e<code>collector.example.com:4318</code>\u7684\u6536\u96c6\u5668\u5c06\u88ab\u914d\u7f6e\u5982\u4e0b:</p> TracesMetricsLogs <pre><code>receivers:\n  otlp: # the OTLP receiver the app is sending traces to\n    protocols:\n      grpc:\n\nprocessors:\n  batch:\n\nexporters:\n  jaeger: # the Jaeger exporter, to ingest traces to backend\n    endpoint: 'https://jaeger.example.com:14250'\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    traces/dev:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [jaeger]\n</code></pre> <pre><code>receivers:\n  otlp: # the OTLP receiver the app is sending metrics to\n    protocols:\n      grpc:\n\nprocessors:\n  batch:\n\nexporters:\n  prometheusremotewrite: # the PRW exporter, to ingest metrics to backend\n    endpoint: 'https://prw.example.com/v1/api/remote_write'\n\nservice:\n  pipelines:\n    metrics/prod:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [prometheusremotewrite]\n</code></pre> <pre><code>receivers:\n  otlp: # the OTLP receiver the app is sending logs to\n    protocols:\n      grpc:\n\nprocessors:\n  batch:\n\nexporters:\n  file: # the File Exporter, to ingest logs to local file\n    path: \"./app42_example.log\"\n    rotation:\n\nservice:\n  pipelines:\n    logs/dev:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [file]\n</code></pre> <p>\u5982\u679c\u60a8\u60f3\u4eb2\u81ea\u5c1d\u8bd5\u4e00\u4e0b\uff0c\u53ef\u4ee5\u770b\u770b\u7aef\u5230\u7aef \u7684Java\u6216Python\u793a\u4f8b\u3002</p>"},{"location":"docs/collector/deployment/agent/#_2","title":"\u6743\u8861","text":"<p>\u6709\u70b9:</p> <ul> <li>\u5165\u95e8\u5f88\u7b80\u5355</li> <li>\u6e05\u9664\u5e94\u7528\u7a0b\u5e8f\u4e0e\u91c7\u96c6\u5668 1:1 \u7684\u6620\u5c04\u5173\u7cfb</li> </ul> <p>\u7f3a\u70b9:</p> <ul> <li>\u53ef\u4f38\u7f29\u6027(\u4eba\u529b\u548c\u8d1f\u8f7d\u65b9\u9762)</li> <li>\u50f5\u5316\u7684</li> </ul>"},{"location":"docs/collector/deployment/gateway/","title":"\u7f51\u5173","text":"<p>\u7f51\u5173\u6536\u96c6\u5668\u90e8\u7f72\u6a21\u5f0f\u7531\u5e94\u7528\u7a0b\u5e8f(\u6216\u5176\u4ed6\u6536\u96c6\u5668)\u7ec4\u6210\uff0c\u8fd9\u4e9b\u5e94\u7528\u7a0b\u5e8f(\u6216\u5176\u4ed6\u6536\u96c6\u5668)\u5c06\u9065\u6d4b \u4fe1\u53f7\u53d1\u9001\u5230\u4f5c\u4e3a\u72ec\u7acb\u670d\u52a1\u8fd0\u884c\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u6536\u96c6\u5668\u5b9e\u4f8b\u63d0\u4f9b\u7684\u5355\u4e2a OTLP \u7aef\u70b9(\u4f8b\u5982 \uff0cKubernetes \u4e2d\u7684\u90e8\u7f72)\uff0c\u901a\u5e38\u662f\u6bcf\u4e2a\u96c6\u7fa4\u3001\u6bcf\u4e2a\u6570\u636e\u4e2d\u5fc3\u6216\u6bcf\u4e2a\u533a\u57df\u3002</p> <p>\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5f00\u7bb1\u5373\u7528\u7684\u8d1f\u8f7d\u5747\u8861\u5668\u6765\u5728\u6536\u96c6\u5668\u4e4b\u95f4\u5206\u914d\u8d1f\u8f7d:</p> <p></p> <p>\u5bf9\u4e8e\u9065\u6d4b\u6570\u636e\u5904\u7406\u7684\u5904\u7406\u5fc5\u987b\u5728\u7279\u5b9a\u6536\u96c6\u5668\u4e2d\u8fdb\u884c\u7684\u7528\u4f8b\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4e24\u5c42\u8bbe\u7f6e\uff0c\u5176\u4e2d\u6536\u96c6 \u5668\u7684\u7ba1\u9053\u5728\u7b2c\u4e00\u5c42\u914d\u7f6e\u4e86\u8ddf\u8e2a ID/\u670d\u52a1\u540d\u79f0\u611f\u77e5\u7684\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u7a0b\u5e8f\uff0c\u800c \u6536\u96c6\u5668\u5728\u7b2c\u4e8c\u5c42\u5904\u7406\u5411\u5916\u6269\u5c55\u3002\u4f8b\u5982\uff0c\u5728\u4f7f\u7528Tail Sampling \u5904\u7406 \u5668\u65f6\uff0c\u60a8\u5c06\u9700\u8981\u4f7f\u7528\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u5668\uff0c\u4ee5\u4fbf\u7ed9\u5b9a\u8ddf\u8e2a\u7684\u6240\u6709\u8de8\u5ea6 \u5230\u8fbe\u5e94\u7528\u5c3e\u90e8\u62bd\u6837\u7b56\u7565\u7684\u540c\u4e00\u6536\u96c6\u5668\u5b9e\u4f8b\u3002</p> <p>\u8ba9\u6211\u4eec\u6765\u770b\u770b\u8fd9\u6837\u4e00\u4e2a\u4f7f\u7528\u8d1f\u8f7d\u5747\u8861\u5bfc\u51fa\u5668\u7684\u4f8b\u5b50:</p> <p></p> <ol> <li>\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0cSDK \u88ab\u914d\u7f6e\u4e3a\u5c06 OTLP \u6570\u636e\u53d1\u9001\u5230\u4e2d\u5fc3\u4f4d\u7f6e\u3002</li> <li>\u4f7f\u7528\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u5668\u914d\u7f6e\u7684\u6536\u96c6\u5668\uff0c\u5b83\u5c06\u4fe1\u53f7\u5206\u53d1\u5230\u4e00\u7ec4\u6536\u96c6\u5668\u3002</li> <li>\u91c7\u96c6\u5668\u914d\u7f6e\u4e3a\u5c06\u9065\u6d4b\u6570\u636e\u53d1\u9001\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u540e\u7aef\u3002</li> </ol> <p>Warning</p> <p>\u76ee\u524d\uff0c\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u5668\u53ea\u652f\u6301<code>traces</code>\u7c7b\u578b\u7684\u7ba1\u9053\u3002</p>"},{"location":"docs/collector/deployment/gateway/#example","title":"Example","text":"<p>\u5bf9\u4e8e\u96c6\u4e2d\u5f0f\u6536\u96c6\u5668\u90e8\u7f72\u6a21\u5f0f\u7684\u5177\u4f53\u793a\u4f8b\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u4ed4\u7ec6\u7814\u7a76\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u5668\u3002\u5b83\u6709\u4e24\u4e2a \u4e3b\u8981\u914d\u7f6e\u5b57\u6bb5:</p> <ul> <li><code>resolver</code>\uff0c\u5b83\u51b3\u5b9a\u5728\u54ea\u91cc\u627e\u5230\u4e0b\u6e38\u6536\u96c6\u5668(\u6216:\u540e\u7aef)\u3002\u5982\u679c\u5728\u8fd9\u91cc\u4f7f\u7528 <code>static</code>\u5b50\u952e\uff0c   \u5219\u5fc5\u987b\u624b\u52a8\u679a\u4e3e\u6536\u96c6\u5668\u7684 url\u3002\u53e6\u4e00\u4e2a\u652f\u6301\u7684\u89e3\u6790\u5668\u662f DNS \u89e3\u6790\u5668\uff0c\u5b83\u5c06\u5b9a\u671f\u68c0\u67e5\u66f4\u65b0   \u548c\u89e3\u6790 IP \u5730\u5740\u3002\u5bf9\u4e8e\u8fd9\u79cd\u89e3\u6790\u5668\u7c7b\u578b\uff0c<code>hostname</code>\u5b50\u952e\u6307\u5b9a\u8981\u67e5\u8be2\u7684\u4e3b\u673a\u540d\uff0c\u4ee5\u4fbf\u83b7\u5f97   IP \u5730\u5740\u5217\u8868\u3002</li> <li>\u4f7f\u7528<code>routing_key</code>\u5b57\u6bb5\uff0c\u60a8\u53ef\u4ee5\u544a\u8bc9\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u5668\u5c06 spans \u8def\u7531\u5230\u7279\u5b9a\u7684\u4e0b\u6e38\u6536\u96c6\u5668   \u3002\u5982\u679c\u60a8\u5c06\u6b64\u5b57\u6bb5\u8bbe\u7f6e\u4e3a<code>traceID</code> (\u9ed8\u8ba4)\uff0c\u5219\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u7a0b\u5e8f\u5c06\u6839\u636e\u5176<code>traceID</code> \u5bfc   \u51fa spans\u3002\u5426\u5219\uff0c\u5982\u679c\u4f60\u4f7f\u7528<code>service</code>\u4f5c\u4e3a<code>routing_key</code>\u7684\u503c\uff0c\u5b83\u4f1a\u6839\u636e\u5b83\u4eec\u7684\u670d\u52a1\u540d   \u79f0\u5bfc\u51fa spans\uff0c\u8fd9\u5728\u4f7f\u7528\u50cfSpan Metrics \u8fde\u63a5\u5668\u8fd9\u6837\u7684\u8fde\u63a5   \u5668\u65f6\u5f88\u6709\u7528\uff0c\u6240\u4ee5\u4e00\u4e2a\u670d\u52a1\u7684\u6240\u6709 spans \u5c06\u88ab\u53d1\u9001\u5230\u76f8\u540c\u7684\u4e0b\u6e38\u6536\u96c6\u5668\u8fdb\u884c\u5ea6\u91cf\u6536\u96c6\uff0c   \u4fdd\u8bc1\u51c6\u786e\u7684\u805a\u5408\u3002</li> </ul> <p>\u670d\u52a1\u4e8e OTLP \u7aef\u70b9\u7684\u7b2c\u4e00\u5c42\u6536\u96c6\u5668\u5c06\u6309\u7167\u5982\u4e0b\u6240\u793a\u8fdb\u884c\u914d\u7f6e:</p> StaticDNSDNS with service <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  loadbalancing:\n    protocol:\n      otlp:\n        insecure: true\n    resolver:\n      static:\n        hostnames:\n          - collector-1.example.com:4317\n          - collector-2.example.com:5317\n          - collector-3.example.com\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [loadbalancing]\n</code></pre> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  loadbalancing:\n    protocol:\n      otlp:\n        insecure: true\n    resolver:\n      dns:\n        hostname: collectors.example.com\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [loadbalancing]\n</code></pre> <pre><code>receivers:\n  otlp:\n    protocols:\n      grpc:\n\nexporters:\n  loadbalancing:\n    routing_key: 'service'\n    protocol:\n      otlp:\n        insecure: true\n    resolver:\n      dns:\n        hostname: collectors.example.com\n        port: 5317\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [loadbalancing]\n</code></pre> <p>\u8d1f\u8f7d\u5e73\u8861\u5bfc\u51fa\u7a0b\u5e8f\u53d1\u51fa\u7684\u6307\u6807\u5305\u62ec<code>otelcol_loadbalancer_num_backends</code> \u548c <code>otelcol_loadbalancer_backend_latency</code>\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u6307\u6807\u76d1\u89c6 OTLP \u7aef\u70b9\u6536\u96c6\u5668\u7684 \u8fd0\u884c\u72b6\u51b5\u548c\u6027\u80fd\u3002</p>"},{"location":"docs/collector/deployment/gateway/#_1","title":"\u5229\u5f0a","text":"<p>\u4f18\u70b9:</p> <ul> <li>\u5173\u6ce8\u70b9\u5206\u79bb\uff0c\u4f8b\u5982\u96c6\u4e2d\u7ba1\u7406\u7684\u51ed\u636e</li> <li>\u96c6\u4e2d\u7b56\u7565\u7ba1\u7406(\u4f8b\u5982\uff0c\u8fc7\u6ee4\u67d0\u4e9b\u65e5\u5fd7\u6216\u91c7\u6837)</li> </ul> <p>\u7f3a\u70b9:</p> <ul> <li>\u5b83\u53c8\u591a\u4e86\u4e00\u4e2a\u9700\u8981\u7ef4\u62a4\u7684\u4e1c\u897f\uff0c\u800c\u4e14\u53ef\u80fd\u4f1a\u5931\u8d25(\u590d\u6742\u6027)</li> <li>\u589e\u52a0\u4e86\u7ea7\u8054\u6536\u96c6\u5668\u60c5\u51b5\u4e0b\u7684\u5ef6\u8fdf</li> <li>\u66f4\u9ad8\u7684\u6574\u4f53\u8d44\u6e90\u4f7f\u7528(\u6210\u672c)</li> </ul>"},{"location":"docs/collector/deployment/no-collector/","title":"\u6ca1\u6709\u6536\u96c6\u5668","text":"<p>\u6700\u7b80\u5355\u7684\u6a21\u5f0f\u662f\u6839\u672c\u4e0d\u4f7f\u7528\u6536\u96c6\u5668\u3002\u8be5\u6a21\u5f0f\u7531\u5e26\u6709 OpenTelemetry SDK \u7684\u5e94\u7528\u7a0b \u5e8finstrumented\u7ec4\u6210\uff0c\u8be5 SDK \u5c06\u9065\u6d4b\u4fe1\u53f7(\u8ddf\u8e2a\uff0c\u5ea6\u91cf\uff0c\u65e5\u5fd7)\u76f4\u63a5\u5bfc \u51fa\u5230\u540e\u7aef:</p> <p></p>"},{"location":"docs/collector/deployment/no-collector/#_1","title":"\u4f8b\u5b50","text":"<p>\u8bf7\u53c2\u9605\u7f16\u7a0b\u8bed\u8a00\u7684\u4ee3\u7801\u63d2\u88c5\u4e86\u89e3\u5982\u4f55\u5c06\u4fe1\u53f7\u4ece\u5e94\u7528\u7a0b\u5e8f\u76f4\u63a5\u5bfc\u51fa\u5230\u540e \u7aef\u5177\u4f53\u7684\u7aef\u5230\u7aef\u793a\u4f8b\u3002</p>"},{"location":"docs/collector/deployment/no-collector/#_2","title":"\u6743\u8861","text":"<p>\u4f18\u70b9:</p> <ul> <li>\u4f7f\u7528\u7b80\u5355(\u7279\u522b\u662f\u5728\u5f00\u53d1/\u6d4b\u8bd5\u73af\u5883\u4e2d)</li> <li>\u6ca1\u6709\u989d\u5916\u7684\u79fb\u52a8\u90e8\u4ef6\u9700\u8981\u64cd\u4f5c(\u5728\u751f\u4ea7\u73af\u5883\u4e2d)</li> </ul> <p>\u7f3a\u70b9:</p> <ul> <li>\u5982\u679c\u6536\u96c6\u3001\u5904\u7406\u6216\u6444\u53d6\u53d1\u751f\u53d8\u5316\uff0c\u5219\u9700\u8981\u66f4\u6539\u4ee3\u7801</li> <li>\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u548c\u540e\u7aef\u4e4b\u95f4\u7684\u5f3a\u8026\u5408</li> <li>\u6bcf\u79cd\u8bed\u8a00\u5b9e\u73b0\u7684\u5bfc\u51fa\u5668\u6570\u91cf\u6709\u9650</li> </ul>"},{"location":"docs/concepts/","title":"\u5f00\u653e\u9065\u6d4b\u6982\u5ff5","text":"<p>\u5728\u672c\u8282\u4e2d\uff0c\u60a8\u5c06\u4e86\u89e3OpenTelemetry\u9879\u76ee\u7684\u6570\u636e\u6e90\u548c\u5173\u952e\u7ec4\u4ef6\u3002\u8fd9\u5c06\u5e2e\u52a9\u60a8\u7406\u89e3OpenTelemetry\u662f\u5982\u4f55\u5de5\u4f5c\u7684\u3002</p>"},{"location":"docs/concepts/components/","title":"\u7ec4\u4ef6","text":"<p>OpenTelemetry \u76ee\u524d\u7531\u51e0\u4e2a\u4e3b\u8981\u7ec4\u4ef6\u7ec4\u6210:</p> <ul> <li>\u8de8\u8bed\u8a00\u89c4\u8303</li> <li>OpenTelemetry \u6536\u96c6\u5668</li> <li>\u6bcf\u79cd\u8bed\u8a00 sdk</li> <li>\u6bcf\u79cd\u8bed\u8a00\u7684\u5de5\u5177\u5e93</li> <li>\u6309\u8bed\u8a00\u81ea\u52a8\u68c0\u6d4b</li> <li>K8s \u64cd\u4f5c\u5668</li> </ul> <p>OpenTelemetry \u5141\u8bb8\u60a8\u66ff\u6362\u5bf9\u7279\u5b9a\u4e8e\u4f9b\u5e94\u5546\u7684 sdk \u548c\u5de5\u5177\u7684\u9700\u6c42\uff0c\u4ee5\u751f\u6210\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e \u3002</p>"},{"location":"docs/concepts/components/#_1","title":"\u89c4\u8303","text":"<p>\u63cf\u8ff0\u6240\u6709\u5b9e\u73b0\u7684\u8de8\u8bed\u8a00\u9700\u6c42\u548c\u671f\u671b\u3002\u9664\u4e86\u5bf9\u672f\u8bed\u7684\u5b9a\u4e49\u4e4b\u5916\uff0c\u8be5\u89c4\u8303\u8fd8\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u5185\u5bb9:</p> <ul> <li>API: \u5b9a\u4e49\u7528\u4e8e\u751f\u6210\u548c\u5173\u8054\u8ddf\u8e2a\u3001\u5ea6\u91cf\u548c\u65e5\u5fd7\u6570\u636e\u7684\u6570\u636e\u7c7b\u578b\u548c\u64cd\u4f5c\u3002</li> <li>SDK: \u5b9a\u4e49\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684 API \u5b9e\u73b0\u7684\u9700\u6c42\u3002\u8fd9\u91cc\u8fd8\u5b9a\u4e49\u4e86\u914d\u7f6e\u3001\u6570\u636e\u5904\u7406\u548c\u5bfc\u51fa\u6982\u5ff5   \u3002</li> <li>Data: \u5b9a\u4e49\u9065\u6d4b\u540e\u7aef\u53ef\u4ee5\u63d0\u4f9b\u652f\u6301\u7684 OpenTelemetry \u534f\u8bae(OTLP)\u548c\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684   \u8bed\u4e49\u7ea6\u5b9a\u3002</li> </ul> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u89c4\u8303.</p> <p>\u6b64\u5916\uff0cAPI \u6982\u5ff5\u7684\u5e7f\u6cdb\u6ce8\u91ca\u7684 protobuf \u63a5\u53e3\u6587\u4ef6\u53ef\u4ee5 \u5728\u539f\u578b\u5b58\u50a8\u5e93\u4e2d\u627e\u5230\u3002</p>"},{"location":"docs/concepts/components/#_2","title":"\u6536\u96c6\u5668","text":"<p>OpenTelemetry Collector \u662f\u4e00\u4e2a\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u4ee3\u7406\uff0c\u5b83\u53ef\u4ee5\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e \u3002\u5b83\u652f\u6301\u4ee5\u591a\u79cd\u683c\u5f0f\u63a5\u6536\u9065\u6d4b\u6570\u636e(\u4f8b\u5982\uff0cOTLP\u3001Jaeger\u3001Prometheus \u4ee5\u53ca\u8bb8\u591a\u5546\u4e1a/\u4e13\u6709 \u5de5\u5177)\uff0c\u5e76\u5c06\u6570\u636e\u53d1\u9001\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u540e\u7aef\u3002\u5b83\u8fd8\u652f\u6301\u5728\u9065\u6d4b\u6570\u636e\u5bfc\u51fa\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u5904\u7406\u548c\u8fc7 \u6ee4\u3002\u6536\u96c6\u5668\u8d21\u732e\u5305\u652f\u6301\u66f4\u591a\u7684\u6570\u636e\u683c\u5f0f\u548c\u4f9b\u5e94\u5546\u540e\u7aef\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u6536\u96c6\u5668.</p>"},{"location":"docs/concepts/components/#sdks","title":"\u8bed\u8a00 SDKs","text":"<p>OpenTelemetry \u4e5f\u6709\u8bed\u8a00 SDKs\uff0c\u5141\u8bb8\u60a8\u4f7f\u7528 OpenTelemetry API \u7528\u60a8\u9009\u62e9\u7684\u8bed\u8a00\u751f\u6210\u9065\u6d4b \u6570\u636e\uff0c\u5e76\u5c06\u8be5\u6570\u636e\u5bfc\u51fa\u5230\u9996\u9009\u540e\u7aef\u3002\u8fd9\u4e9b SDKs \u8fd8\u5141\u8bb8\u60a8\u4e3a\u901a\u7528\u5e93\u548c\u6846\u67b6\u5408\u5e76\u68c0\u6d4b\u5e93\uff0c\u60a8\u53ef \u4ee5\u4f7f\u7528\u8fd9\u4e9b\u5e93\u548c\u6846\u67b6\u8fde\u63a5\u5230\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u624b\u52a8\u68c0\u6d4b\u3002</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u68c0\u6d4b.</p>"},{"location":"docs/concepts/components/#_3","title":"\u5de5\u5177\u5e93","text":"<p>OpenTelemetry \u652f\u6301\u5927\u91cf\u7684\u7ec4\u4ef6\uff0c\u8fd9\u4e9b\u7ec4\u4ef6\u4ece\u53d7\u652f\u6301\u8bed\u8a00\u7684\u6d41\u884c\u5e93\u548c\u6846\u67b6\u4e2d\u751f\u6210\u76f8\u5173\u7684\u9065\u6d4b \u6570\u636e\u3002\u4f8b\u5982\uff0c\u6765\u81ea HTTP \u5e93\u7684\u5165\u7ad9\u548c\u51fa\u7ad9 HTTP \u8bf7\u6c42\u5c06\u751f\u6210\u5173\u4e8e\u8fd9\u4e9b\u8bf7\u6c42\u7684\u6570\u636e\u3002</p> <p>\u6211\u4eec\u7684\u957f\u671f\u76ee\u6807\u662f\u5c06\u6d41\u884c\u7684\u5e93\u7f16\u5199\u4e3a\u5f00\u7bb1\u5373\u7528\u7684\u53ef\u89c2\u5bdf\u5e93\uff0c\u8fd9\u6837\u5c31\u4e0d\u9700\u8981\u5f15\u5165\u5355\u72ec\u7684\u7ec4\u4ef6\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u68c0\u6d4b\u5e93.</p>"},{"location":"docs/concepts/components/#_4","title":"\u81ea\u52a8\u63d2\u88c5","text":"<p>\u5982\u679c\u9002\u7528\uff0cOpenTelemetry \u7684\u7279\u5b9a\u8bed\u8a00\u5b9e\u73b0\u5c06\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u800c\u65e0\u9700\u89e6 \u53ca\u60a8\u7684\u6e90\u4ee3\u7801\u3002\u867d\u7136\u5e95\u5c42\u673a\u5236\u53d6\u51b3\u4e8e\u8bed\u8a00\uff0c\u4f46\u81f3\u5c11\u8fd9\u4f1a\u5c06 OpenTelemetry API \u548c SDK \u529f\u80fd \u6dfb\u52a0\u5230\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u53ef\u80fd\u4f1a\u6dfb\u52a0\u4e00\u7ec4 Instrumentation Libraries \u548c\u5bfc\u51fa \u5668\u4f9d\u8d56\u9879\u3002</p> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1\u68c0\u6d4b.</p>"},{"location":"docs/concepts/components/#k8s-operator","title":"K8s operator","text":"<p>OpenTelemetry Operator \u662f Kubernetes Operator \u7684\u4e00\u4e2a\u5b9e\u73b0\u3002Operator \u4f7f\u7528 OpenTelemetry \u7ba1\u7406 OpenTelemetry \u6536\u96c6\u5668\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u52a8\u68c0\u6d4b\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605K8s Operator.</p>"},{"location":"docs/concepts/data-collection/","title":"\u6570\u636e\u6536\u96c6","text":"<p>OpenTelemetry\u9879\u76ee\u901a\u8fc7OpenTelemetry Collector\u4fc3\u8fdb\u4e86\u9065\u6d4b\u6570\u636e\u7684\u6536\u96c6\u3002 OpenTelemetry Collector\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u5b9e\u73b0\uff0c\u7528\u4e8e\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u3002 \u5b83\u6d88\u9664\u4e86\u8fd0\u884c\u3001\u64cd\u4f5c\u548c\u7ef4\u62a4\u591a\u4e2a\u4ee3\u7406/\u6536\u96c6\u5668\u4ee5\u652f\u6301\u5411\u4e00\u4e2a\u6216\u591a\u4e2a\u5f00\u6e90\u6216\u5546\u4e1a\u540e\u7aef\u53d1\u9001\u7684\u5f00\u6e90\u53ef\u89c2\u5bdf\u6027\u6570\u636e\u683c\u5f0f(\u4f8b\u5982Jaeger\u3001Prometheus\u7b49)\u7684\u9700\u8981\u3002 \u6b64\u5916\uff0cCollector\u8fd8\u4e3a\u6700\u7ec8\u7528\u6237\u63d0\u4f9b\u4e86\u5bf9\u5176\u6570\u636e\u7684\u63a7\u5236\u3002 Collector\u662f\u4eea\u5668\u5e93\u5bfc\u51fa\u5176\u9065\u6d4b\u6570\u636e\u7684\u9ed8\u8ba4\u4f4d\u7f6e\u3002</p> <p>\u6536\u96c6\u5668\u53ef\u4ee5\u4f5c\u4e3a\u53d1\u884c\u7248\u63d0\u4f9b\uff0c\u8bf7\u53c2\u9605\u8fd9\u91cc\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002</p>"},{"location":"docs/concepts/data-collection/#_1","title":"\u90e8\u7f72","text":"<p>\u5f00\u653e\u9065\u6d4b\u91c7\u96c6\u5668\u63d0\u4f9b\u5355\u4e00\u4e8c\u8fdb\u5236\u548c\u4e24\u79cd\u90e8\u7f72\u65b9\u6cd5:</p> <ul> <li>Agent: A Collector instance running with the application or on the same   host as the application (e.g. binary, sidecar, or daemonset).</li> <li>Gateway: One or more Collector instances running as a standalone service   (e.g. container or deployment) typically per cluster, data center or region.</li> </ul> <p>For information on how to use the Collector see the getting started documentation.</p>"},{"location":"docs/concepts/data-collection/#_2","title":"\u7ec4\u4ef6","text":"<p>The Collector is made up of the following components:</p> <ul> <li> <code>receivers</code>: How to get data into the Collector; these can be push or pull   based</li> <li> <code>processors</code>: What to do with received data</li> <li> <code>exporters</code>: Where to send received data; these can be push or pull based</li> </ul> <p>These components are enabled through <code>pipelines</code>. Multiple instances of components as well as pipelines can be defined via YAML configuration.</p> <p>For more information about these components see the configuration documentation.</p>"},{"location":"docs/concepts/data-collection/#_3","title":"\u5b58\u50a8\u5e93","text":"<p>The OpenTelemetry project provides two versions of the Collector:</p> <ul> <li>Core:   Foundational components such as configuration and generally applicable   receivers, processors, exporters, and extensions.</li> <li>Contrib:   All the components of core plus optional or possibly experimental components.   Offers support for popular open source projects including Jaeger, Prometheus,   and Fluent Bit. Also contains more specialized or vendor-specific receivers,   processors, exporters, and extensions.</li> </ul>"},{"location":"docs/concepts/distributions/","title":"\u5206\u53d1","text":"<p>OpenTelemetry \u9879\u76ee\u7531\u591a\u4e2a\u652f\u6301\u591a\u4e2a\u4fe1\u53f7\u7684\u7ec4\u4ef6 \u7ec4\u6210\u3002 OpenTelemetry \u7684\u53c2\u8003\u5b9e\u73b0\u5982\u4e0b:</p> <ul> <li>\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u5de5\u5177\u5e93</li> <li>\u6536\u96c6\u5668\u4e8c\u8fdb\u5236\u6587\u4ef6</li> </ul> <p>\u53ef\u4ee5\u4ece\u4efb\u4f55\u53c2\u8003\u5b9e\u73b0\u521b\u5efa\u4e00\u4e2a\u53d1\u884c\u7248\u3002</p>"},{"location":"docs/concepts/distributions/#_1","title":"\u4ec0\u4e48\u662f\u5206\u53d1?","text":"<p>\u5206\u53d1\u7248(\u4e0d\u8981\u4e0e fork \u6df7\u6dc6)\u662f OpenTelemetry \u7ec4\u4ef6\u7684\u5b9a\u5236\u7248\u672c\u3002\u53d1\u884c\u7248\u662f\u4e0a\u6e38 OpenTelemetry \u5b58\u50a8\u5e93\u7684\u5305\u88c5\u5668\uff0c\u5e26\u6709\u4e00\u4e9b\u5b9a\u5236\u3002\u53d1\u884c\u7248\u4e2d\u7684\u81ea\u5b9a\u4e49\u53ef\u80fd\u5305\u62ec:</p> <ul> <li>\u4e3a\u7279\u5b9a\u540e\u7aef\u6216\u4f9b\u5e94\u5546\u7b80\u5316\u4f7f\u7528\u6216\u81ea\u5b9a\u4e49\u4f7f\u7528\u7684\u811a\u672c</li> <li>\u66f4\u6539\u540e\u7aef\u3001\u4f9b\u5e94\u5546\u6216\u6700\u7ec8\u7528\u6237\u6240\u9700\u7684\u9ed8\u8ba4\u8bbe\u7f6e</li> <li>\u53ef\u80fd\u662f\u4f9b\u5e94\u5546\u6216\u6700\u7ec8\u7528\u6237\u7279\u5b9a\u7684\u9644\u52a0\u5305\u88c5\u9009\u9879</li> <li>\u6d4b\u8bd5\u3001\u6027\u80fd\u548c\u5b89\u5168\u8986\u76d6\u8d85\u51fa\u4e86 OpenTelemetry \u63d0\u4f9b\u7684\u8303\u56f4</li> <li>OpenTelemetry \u63d0\u4f9b\u7684\u529f\u80fd\u4e4b\u5916\u7684\u5176\u4ed6\u529f\u80fd</li> <li>OpenTelemetry \u63d0\u4f9b\u7684\u529f\u80fd\u66f4\u5c11</li> </ul> <p>\u5206\u53d1\u5c06\u5927\u81f4\u5206\u4e3a\u4ee5\u4e0b\u51e0\u7c7b:</p> <ul> <li>\"Pure\": \u8fd9\u4e9b\u53d1\u884c\u7248\u63d0\u4f9b\u4e0e\u4e0a\u6e38\u7248\u672c\u76f8\u540c\u7684\u529f\u80fd\uff0c\u5e76\u4e14 100%\u517c\u5bb9\u3002\u5b9a\u5236\u901a\u5e38\u662f\u4e3a\u4e86   \u4fbf\u4e8e\u4f7f\u7528\u6216\u6253\u5305\u3002\u8fd9\u4e9b\u5b9a\u5236\u53ef\u80fd\u662f\u7279\u5b9a\u4e8e\u540e\u7aef\u3001\u4f9b\u5e94\u5546\u6216\u6700\u7ec8\u7528\u6237\u7684\u3002</li> <li>\"Plus\": \u8fd9\u4e9b\u53d1\u884c\u7248\u63d0\u4f9b\u4e86\u4e0e\u4e0a\u6e38\u7248\u672c\u76f8\u540c\u7684\u529f\u80fd\u3002\u9664\u4e86\u5728\u7eaf\u53d1\u884c\u7248\u4e2d\u53d1\u73b0\u7684\u5b9a\u5236\u4e4b   \u5916\uff0c\u8fd8\u5305\u62ec\u5176\u4ed6\u7ec4\u4ef6\u3002\u8fd9\u65b9\u9762\u7684\u4f8b\u5b50\u5305\u62ec\u6ca1\u6709\u4e0a\u6eaf\u5230 OpenTelemetry \u9879\u76ee\u7684\u4eea\u5668\u5e93\u6216\u4f9b   \u5e94\u5546\u5bfc\u51fa\u7a0b\u5e8f\u3002</li> <li>\"Minus\": \u8fd9\u4e9b\u53d1\u884c\u7248\u63d0\u4f9b\u4e86\u6765\u81ea\u4e0a\u6e38\u7684\u4e00\u7ec4\u7b80\u5316\u7684\u529f\u80fd\u3002\u8fd9\u65b9\u9762\u7684\u4f8b\u5b50\u5305\u62ec\u79fb\u9664   OpenTelemetry Collector \u9879\u76ee\u4e2d\u7684\u4eea\u5668\u5e93\u6216\u63a5\u6536\u5668/\u5904\u7406\u5668/\u5bfc\u51fa\u5668/\u6269\u5c55\u3002\u63d0\u4f9b\u8fd9\u4e9b\u53d1   \u884c\u7248\u53ef\u80fd\u662f\u4e3a\u4e86\u589e\u52a0\u53ef\u652f\u6301\u6027\u548c\u5b89\u5168\u6027\u8003\u8651\u3002</li> </ul>"},{"location":"docs/concepts/distributions/#_2","title":"\u8c01\u4f1a\u521b\u9020\u4e00\u4e2a\u5206\u53d1?","text":"<p>\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u53d1\u884c\u7248\u3002\u4eca\u5929\uff0c\u4e00\u4e9b\u4f9b\u5e94\u5546\u63d0\u4f9b\u53d1\u884c\u7248\u3002 \u6b64\u5916\uff0c\u5982\u679c\u7ec8\u7aef\u7528\u6237\u5e0c\u671b\u5728Registry \u4e2d\u4f7f\u7528\u6ca1\u6709\u4e0a\u884c\u5230 OpenTelemetry \u9879\u76ee\u7684\u7ec4\u4ef6\uff0c\u4ed6\u4eec\u53ef\u80fd\u4f1a\u8003\u8651\u521b\u5efa\u4e00\u4e2a\u53d1\u884c\u7248\u3002</p>"},{"location":"docs/concepts/distributions/#_3","title":"\u8d21\u732e\u8fd8\u662f\u5206\u53d1?","text":"<p>\u5728\u4f60\u7ee7\u7eed\u9605\u8bfb\u5e76\u5b66\u4e60\u5982\u4f55\u521b\u5efa\u4f60\u81ea\u5df1\u7684\u53d1\u884c\u7248\u4e4b\u524d\uff0c\u95ee\u95ee\u4f60\u81ea\u5df1\uff0c\u4f60\u5728 OpenTelemetry \u7ec4 \u4ef6\u4e4b\u4e0a\u6dfb\u52a0\u7684\u4e1c\u897f\u662f\u5426\u5bf9\u6bcf\u4e2a\u4eba\u90fd\u6709\u76ca\uff0c\u56e0\u6b64\u5e94\u8be5\u5305\u542b\u5728\u53c2\u8003\u5b9e\u73b0\u4e2d:</p> <ul> <li>Can your scripts for \"ease of use\" be generalized?</li> <li>Can your changes to default settings be the better option for everyone?</li> <li>Are your additional packaging options really specific?</li> <li>Might your test, performance &amp; security coverage work with the reference   implementation as well?</li> <li>Have you checked with the community if your additional capabilities could be   part of the standard?</li> </ul>"},{"location":"docs/concepts/distributions/#_4","title":"\u521b\u5efa\u81ea\u5df1\u7684\u53d1\u884c\u7248","text":""},{"location":"docs/concepts/distributions/#collector","title":"Collector","text":"<p>A guide on how to create your own distribution is available in this blog post: \"Building your own OpenTelemetry Collector distribution\"</p> <p>If you are building your own distribution, the OpenTelemetry Collector Builder might be a good starting point.</p>"},{"location":"docs/concepts/distributions/#_5","title":"\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u5de5\u5177\u5e93","text":"<p>There are language specific extensibility mechanisms to customize the instrumentation libraries:</p> <ul> <li>Javaagent</li> </ul>"},{"location":"docs/concepts/distributions/#_6","title":"\u5173\u4e8e\u53d1\u884c\u7248\uff0c\u4f60\u5e94\u8be5\u77e5\u9053\u4e9b\u4ec0\u4e48","text":"<p>When using OpenTelemetry project collateral such as logo and name for your distribution, make sure that you are in line with the OpenTelemetry Marketing Guidelines for Contributing Organizations.</p> <p>The OpenTelemetry project does not certify distributions at this time. In the future, OpenTelemetry may certify distributions and partners similarly to the Kubernetes project. When evaluating a distribution, ensure using the distribution does not result in vendor lock-in.</p> <p>Any support for a distribution comes from the distribution authors and not the OpenTelemetry authors.</p>"},{"location":"docs/concepts/glossary/","title":"\u672f\u8bed\u8868","text":"<p>OpenTelemetry \u9879\u76ee\u4f7f\u7528\u7684\u672f\u8bed\u60a8\u53ef\u80fd\u4e0d\u719f\u6089\uff0c\u4e5f\u53ef\u80fd\u4e0d\u719f\u6089\u3002\u6b64\u5916\uff0c\u8be5\u9879\u76ee\u53ef\u80fd\u4ee5\u4e0d\u540c\u4e8e \u5176\u4ed6\u9879\u76ee\u7684\u65b9\u5f0f\u5b9a\u4e49\u672f\u8bed\u3002\u672c\u9875\u5305\u542b\u9879\u76ee\u4e2d\u4f7f\u7528\u7684\u672f\u8bed\u53ca\u5176\u542b\u4e49\u3002</p>"},{"location":"docs/concepts/glossary/#_1","title":"\u901a\u7528\u672f\u8bed","text":""},{"location":"docs/concepts/glossary/#aggregation","title":"Aggregation","text":"<p>\u5728\u7a0b\u5e8f\u6267\u884c\u671f\u95f4\u7684\u4e00\u6bb5\u65f6\u95f4\u5185\uff0c\u5c06\u591a\u4e2a\u6d4b\u91cf\u7ec4\u5408\u6210\u6709\u5173\u6d4b\u91cf\u7684\u7cbe\u786e\u6216\u4f30\u8ba1\u7edf\u8ba1\u4fe1\u606f\u7684\u8fc7\u7a0b\u3002 \u7531<code>Metric</code> <code>Data Source</code>\u4f7f\u7528\u3002</p>"},{"location":"docs/concepts/glossary/#api","title":"API","text":"<p>\u5e94\u7528\u7a0b\u5e8f\u7f16\u7a0b\u63a5\u53e3\u3002\u5728 OpenTelemetry \u9879\u76ee\u4e2d\uff0c\u7528\u4e8e\u5b9a\u4e49\u5982\u4f55\u6839 \u636e<code>Data Source</code>\u751f\u6210\u9065\u6d4b\u6570\u636e\u3002</p>"},{"location":"docs/concepts/glossary/#application","title":"Application","text":"<p>\u4e3a\u6700\u7ec8\u7528\u6237\u6216\u5176\u4ed6\u5e94\u7528\u7a0b\u5e8f\u8bbe\u8ba1\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u670d\u52a1\u3002</p>"},{"location":"docs/concepts/glossary/#apm","title":"APM","text":"<p>\u5e94\u7528\u7a0b\u5e8f\u6027\u80fd\u76d1\u89c6\u662f\u5173\u4e8e\u76d1\u89c6\u8f6f\u4ef6\u5e94\u7528\u7a0b\u5e8f\u53ca\u5176\u6027\u80fd(\u901f\u5ea6\u3001\u53ef\u9760\u6027\u3001\u53ef\u7528\u6027\u7b49)\uff0c\u4ee5\u68c0\u6d4b\u95ee \u9898\u3001\u8b66\u62a5\u548c\u5de5\u5177\uff0c\u4ee5\u627e\u5230\u6839\u672c\u539f\u56e0\u3002</p>"},{"location":"docs/concepts/glossary/#attribute","title":"Attribute","text":"<p>\u4e00\u4e2a\u952e\u503c\u5bf9\u3002\u7528\u4e8e\u9065\u6d4b\u4fe1\u53f7-\u4f8b\u5982\u5728<code>Traces</code>\u4e2d\u5c06\u6570\u636e\u9644\u52a0\u5230<code>Span</code>\uff0c \u6216\u5728<code>Metrics</code>\u4e2d\u3002\u53c2\u89c1\u5c5e\u6027\u89c4\u8303\u3002</p>"},{"location":"docs/concepts/glossary/#automatic-instrumentation","title":"Automatic Instrumentation","text":"<p>\u6307\u4e0d\u9700\u8981\u6700\u7ec8\u7528\u6237\u4fee\u6539\u5e94\u7528\u7a0b\u5e8f\u6e90\u4ee3\u7801\u7684\u9065\u6d4b\u6536\u96c6\u65b9\u6cd5\u3002\u65b9\u6cd5\u56e0\u7f16\u7a0b\u8bed\u8a00\u800c\u5f02\uff0c\u4f8b\u5982\u5b57\u8282\u7801 \u6ce8\u5165\u6216\u7334\u5b50\u8865\u4e01\u3002</p>"},{"location":"docs/concepts/glossary/#baggage","title":"Baggage","text":"<p>\u4e00\u79cd\u4f20\u64ad\u540d\u79f0/\u503c\u5bf9\u7684\u673a\u5236\uff0c\u4ee5\u5e2e\u52a9\u5728\u4e8b\u4ef6\u548c\u670d\u52a1\u4e4b\u95f4\u5efa\u7acb\u56e0\u679c\u5173\u7cfb\u3002\u53c2\u89c1\u884c\u674e\u89c4 \u683c\u3002</p>"},{"location":"docs/concepts/glossary/#client-library","title":"Client Library","text":"<p>\u67e5\u770b <code>\u63d2\u88c5\u5e93</code>.</p>"},{"location":"docs/concepts/glossary/#client-side-app","title":"Client-side App","text":"<p>\u4e00\u4e2a\u5e94\u7528\u7a0b\u5e8f\u7684\u7ec4\u4ef6\uff0c\u5b83\u4e0d\u5728\u79c1\u6709\u57fa\u7840\u8bbe\u65bd\u4e2d\u8fd0\u884c\uff0c\u901a\u5e38\u7531\u6700\u7ec8\u7528\u6237\u76f4\u63a5 \u4f7f\u7528\u3002\u5ba2\u6237\u7aef\u5e94\u7528\u7684\u4f8b\u5b50\u6709\u6d4f\u89c8\u5668\u5e94\u7528\u3001\u79fb\u52a8\u5e94\u7528\u548c\u8fd0\u884c\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u3002</p>"},{"location":"docs/concepts/glossary/#collector","title":"Collector","text":"<p>\u5173\u4e8e\u5982\u4f55\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u7684\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u5b9e\u73b0\u3002\u53ef\u4ee5\u4f5c\u4e3a\u4ee3\u7406\u6216\u7f51\u5173\u90e8\u7f72\u7684\u5355 \u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002</p> <p>\u4e5f\u79f0\u4e3a OpenTelemetry \u6536\u96c6\u5668\u3002\u66f4\u591a\u5173\u4e8e\u6536\u96c6\u5668\u7684\u4fe1\u606f[\u5728\u8fd9\u91cc][\u6536\u96c6\u5668]\u3002</p>"},{"location":"docs/concepts/glossary/#contrib","title":"Contrib","text":"<p>\u51e0\u4e2a\u63d2\u88c5\u5e93\u548c\u6536\u96c6\u5668\u63d0\u4f9b\u4e86\u4e00\u7ec4\u6838\u5fc3\u529f\u80fd\uff0c \u4ee5\u53ca\u4e00\u4e2a\u4e13\u7528\u7684\u8d21\u732e\u5e93\uff0c\u7528\u4e8e\u975e\u6838\u5fc3\u529f\u80fd\uff0c\u5305\u62ec\u4f9b\u5e94\u5546\u7684\u201c\u51fa\u53e3\u5668\u201d\u3002</p>"},{"location":"docs/concepts/glossary/#context-propagation","title":"Context Propagation","text":"<p>\u5141\u8bb8\u6240\u6709<code>Data Sources</code>\u5171\u4eab\u4e00\u4e2a\u5e95\u5c42\u4e0a\u4e0b\u6587\u673a\u5236\uff0c\u7528\u4e8e \u5728<code>Transaction</code>\u7684\u751f\u547d\u5468\u671f\u5185\u5b58\u50a8\u72b6\u6001\u548c\u8bbf\u95ee\u6570\u636e\u3002\u53c2\u89c1[\u4e0a\u4e0b\u6587\u4f20\u64ad\u89c4 \u8303][\u4e0a\u4e0b\u6587\u4f20\u64ad]\u3002</p>"},{"location":"docs/concepts/glossary/#dag","title":"DAG","text":"<p>\u6709\u5411\u65e0\u73af\u56fe.</p>"},{"location":"docs/concepts/glossary/#data-source","title":"Data Source","text":"<p>\u67e5\u770b <code>Signal</code></p>"},{"location":"docs/concepts/glossary/#dimension","title":"Dimension","text":"<p>\u67e5\u770b <code>Label</code>.</p>"},{"location":"docs/concepts/glossary/#distributed-tracing","title":"Distributed Tracing","text":"<p>\u8ddf\u8e2a\u5355\u4e2a<code>Request</code>\u7684\u8fdb\u7a0b\uff0c\u79f0\u4e3a\u8ddf\u8e2a\uff0c\u56e0\u4e3a\u5b83\u662f \u7531<code>Services</code>\u5904\u7406\u7684\uff0c\u7ec4 \u6210<code>Application</code>\u3002<code>Distributed Trace</code>\u8de8\u8d8a \u8fdb\u7a0b\u3001\u7f51\u7edc\u548c\u5b89\u5168\u8fb9\u754c\u3002</p> <p>\u66f4\u591a\u5173\u4e8e\u5206\u5e03\u5f0f\u8ddf\u8e2a\u7684\u4fe1\u606f\u5728\u8fd9\u91cc.</p>"},{"location":"docs/concepts/glossary/#event","title":"Event","text":"<p>\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8868\u793a\u4f9d\u8d56\u4e8e<code>Data Source</code>\u3002\u4f8b\u5982\uff0c<code>Spans</code>\u3002</p>"},{"location":"docs/concepts/glossary/#exporter","title":"Exporter","text":"<p>\u63d0\u4f9b\u5411\u6d88\u8d39\u8005\u53d1\u9001\u9065\u6d4b\u4fe1\u606f\u7684\u529f\u80fd\u3002 \u7531[<code>\u63d2\u88c5\u5e93</code>][spec-export-lib]\u548c<code>Collector</code>\u4f7f \u7528\u3002\u5bfc\u51fa\u5668\u53ef\u4ee5\u662f push-\uff0c\u4e5f\u53ef\u4ee5\u662f pull-based\u3002</p>"},{"location":"docs/concepts/glossary/#field","title":"Field","text":"<p>\u6dfb\u52a0\u5230<code>Log Records</code>\u7684\u540d\u79f0/\u503c\u5bf9(\u7c7b\u4f3c \u4e8e<code>Spans</code>\u7684<code>Attributes</code>\u548c<code>Metrics</code>\u7684<code>Labels</code>)\u3002 \u53c2\u89c1field spec\u3002</p>"},{"location":"docs/concepts/glossary/#grpc","title":"gRPC","text":"<p>\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u5f00\u6e90\u7684\u901a\u7528<code>RPC</code>\u6846\u67b6\u3002\u66f4\u591a\u5173\u4e8e gRPC \u7684\u4fe1 \u606f\u5728\u8fd9\u91cc\u3002</p>"},{"location":"docs/concepts/glossary/#http","title":"HTTP","text":"<p>\u8d85\u6587\u672c\u4f20\u8f93\u534f\u8bae\u7684\u7b80\u5199\u3002</p>"},{"location":"docs/concepts/glossary/#instrumented-library","title":"Instrumented Library","text":"<p>\u8868\u793a\u6536\u96c6\u9065\u6d4b\u4fe1\u53f7(<code>Traces</code>, <code>Metrics</code>, <code>Logs</code>) \u7684<code>\u5e93</code>\u3002 \u66f4\u591a\u3002</p>"},{"location":"docs/concepts/glossary/#instrumentation-library","title":"Instrumentation Library","text":"<p>\u8868\u793a\u4e3a\u7ed9\u5b9a\u7684<code>Instrumented Library</code>\u63d0\u4f9b\u68c0\u6d4b \u7684<code>\u5e93</code>\u3002 <code>Instrumented Library</code>\u548c<code>Instrumentation Library</code>\u53ef \u80fd\u662f\u76f8\u540c\u7684<code>\u5e93</code>\uff0c\u5982\u679c\u5b83\u6709\u5185\u7f6e\u7684 OpenTelemetry \u63d2\u88c5\u3002 \u66f4 \u591a\u3002</p>"},{"location":"docs/concepts/glossary/#json","title":"JSON","text":"<p>JavaScript \u5bf9\u8c61\u8868\u793a\u6cd5\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#label","title":"Label","text":"<p>\u67e5\u770b Attribute.</p>"},{"location":"docs/concepts/glossary/#language","title":"Language","text":"<p>\u7f16\u7a0b\u8bed\u8a00\u3002</p>"},{"location":"docs/concepts/glossary/#library","title":"Library","text":"<p>\u7531\u63a5\u53e3\u8c03\u7528\u7684\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u884c\u4e3a\u96c6\u5408\u3002</p>"},{"location":"docs/concepts/glossary/#log","title":"Log","text":"<p>\u6709\u65f6\u7528\u4e8e\u6307'\u65e5\u5fd7\u8bb0\u5f55'\u7684\u96c6\u5408\u3002\u53ef\u80fd\u4f1a\u6709\u6b67\u4e49\uff0c\u56e0\u4e3a\u4eba\u4eec\u6709\u65f6\u4e5f\u4f1a \u7528<code>Log</code>\u6765\u6307\u4ee3\u5355\u4e2a\u7684<code>Log Record</code>\uff0c\u56e0\u6b64\u8fd9\u4e2a\u672f\u8bed\u5e94\u8be5\u8c28\u614e\u4f7f\u7528 \uff0c\u5728\u53ef\u80fd\u4ea7\u751f\u6b67\u4e49\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5e94\u8be5\u4f7f\u7528\u989d\u5916\u7684\u9650\u5b9a\u8bcd(\u4f8b\u5982:\u201c\u65e5\u5fd7\u8bb0\u5f55\u201d)\u3002\u67e5\u770b\u66f4 \u591a</p>"},{"location":"docs/concepts/glossary/#log-record","title":"Log Record","text":"<p>['\u4e8b\u4ef6'](#e \u7684\u8bb0\u5f55\u3002\u901a\u5e38\uff0c\u8bb0\u5f55\u5305\u62ec\u4e00\u4e2a\u65f6\u95f4\u6233\uff0c\u8868\u660e<code>Event</code>\u53d1\u751f\u7684\u65f6\u95f4\uff0c \u4ee5\u53ca\u63cf\u8ff0\u53d1\u751f\u4e86\u4ec0\u4e48\uff0c\u53d1\u751f\u5728\u54ea\u91cc\u7b49\u5176\u4ed6\u6570\u636e\u3002\u67e5\u770b\u66f4\u591a</p>"},{"location":"docs/concepts/glossary/#metadata","title":"Metadata","text":"<p>A name/value pair added to telemetry data. OpenTelemetry calls this <code>Attributes</code> on <code>Spans</code>, <code>Labels</code> on <code>Metrics</code> and <code>Fields</code> on <code>Logs</code>.</p> <p>\u6dfb\u52a0\u5230\u9065\u6d4b\u6570\u636e\u4e2d\u7684\u540d\u79f0/\u503c\u5bf9\u3002 OpenTelemetry \u5728' span '\u4e0a\u8c03 \u7528' Attributes '\uff0c\u5728' Metrics '\u4e0a\u8c03 \u7528' Labels '\uff0c\u5728' Logs '\u4e0a\u8c03\u7528' Fields '\u3002</p>"},{"location":"docs/concepts/glossary/#metric","title":"Metric","text":"<p>\u8bb0\u5f55\u4e00\u4e2a\u6570\u636e\u70b9\uff0c\u65e0\u8bba\u662f\u539f\u59cb\u6d4b\u91cf\u6216\u9884\u5b9a\u4e49\u7684\u805a\u5408\uff0c\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u4e0e'\u5143\u6570\u636e'\u3002 \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#oc","title":"OC","text":"<p>' OpenCensus '\u7684\u7f29\u5199\u5f62\u5f0f\u3002</p>"},{"location":"docs/concepts/glossary/#opencensus","title":"OpenCensus","text":"<p>\u4e00\u7ec4\u9488\u5bf9\u5404\u79cd\u8bed\u8a00\u7684\u5e93\uff0c\u5141\u8bb8\u60a8\u6536\u96c6\u5e94\u7528\u7a0b\u5e8f\u6307\u6807\u548c\u5206\u5e03\u5f0f\u8ddf\u8e2a\uff0c\u7136\u540e\u5c06\u6570\u636e\u5b9e\u65f6\u4f20\u8f93\u5230\u60a8 \u9009\u62e9\u7684\u540e\u7aef\u3002 OpenTelemetry \u7684\u524d\u8eab\u3002 \u66f4 \u591a\u3002</p>"},{"location":"docs/concepts/glossary/#opentracing","title":"OpenTracing","text":"<p>\u7528\u4e8e\u5206\u5e03\u5f0f\u8ddf\u8e2a\u7684\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684 api \u548c\u5de5\u5177\u3002 OpenTelemetry \u7684\u524d\u8eab\u3002\u66f4 \u591a\u3002</p>"},{"location":"docs/concepts/glossary/#ot","title":"OT","text":"<p><code>OpenTracing</code>\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#otel","title":"OTel","text":"<p>OpenTelemetry\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#otelcol","title":"OTelCol","text":"<p>OpenTelemetry Collector\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#otlp","title":"OTLP","text":"<p>OpenTelemetry Protocol\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#processor","title":"Processor","text":"<p>\u4ece\u63a5\u6536\u6570\u636e\u5230\u5bfc\u51fa\u6570\u636e\u4e4b\u95f4\u7684\u64cd\u4f5c\u3002\u4f8b\u5982\uff0c\u6279\u5904\u7406\u3002 \u7531'Instrumentation Libraries'\u548cCollector\u4f7f \u7528\u3002</p>"},{"location":"docs/concepts/glossary/#propagators","title":"Propagators","text":"<p>\u7528\u4e8e\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316\u9065\u6d4b\u6570\u636e\u7684\u7279\u5b9a\u90e8\u5206\uff0c\u4f8b\u5982<code>Spans</code>\u4e2d\u7684 span \u4e0a\u4e0b\u6587 \u548c<code>Baggage</code>. \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#proto","title":"Proto","text":"<p>\u4e0e\u8bed\u8a00\u65e0\u5173\u7684\u63a5\u53e3\u7c7b\u578b\u3002 \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#receiver","title":"Receiver","text":"<p><code>Collector</code>\u7528\u6765\u5b9a\u4e49\u5982\u4f55\u63a5\u6536\u9065\u6d4b\u6570\u636e \u7684\u672f\u8bed\u3002\u63a5\u6536\u5668\u53ef\u4ee5\u662f\u63a8\u6216\u62c9\u4e3a\u57fa\u7840\u7684\u3002\u770b\u5230\u66f4\u591a\u3002</p>"},{"location":"docs/concepts/glossary/#request","title":"Request","text":"<p>\u67e5\u770b <code>Distributed Tracing</code>.</p>"},{"location":"docs/concepts/glossary/#resource","title":"Resource","text":"<p>\u6355\u83b7\u6709\u5173\u8bb0\u5f55\u9065\u6d4b\u7684\u5b9e\u4f53\u7684\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5728 Kubernetes \u4e0a\u7684\u5bb9\u5668\u4e2d\u8fd0\u884c\u7684\u4ea7\u751f\u9065\u6d4b\u7684\u8fdb\u7a0b \u6709\u4e00\u4e2a Pod \u540d\u79f0\uff0c\u5b83\u5728\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u4e2d\uff0c\u53ef\u80fd\u662f\u90e8\u7f72\u7684\u4e00\u90e8\u5206\uff0c\u4e5f\u6709\u4e00\u4e2a\u540d\u79f0\u3002\u6240\u6709\u8fd9\u4e09 \u4e2a\u5c5e\u6027\u90fd\u53ef\u4ee5\u5305\u542b\u5728<code>Resource</code>\u4e2d\uff0c\u5e76\u5e94\u7528\u4e8e\u4efb\u4f55\u6570\u636e\u6e90\u3002</p>"},{"location":"docs/concepts/glossary/#rest","title":"REST","text":"<p>Representational State Transfer\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#rpc","title":"RPC","text":"<p>Remote Procedure Call\u7684\u7b80\u5199.</p>"},{"location":"docs/concepts/glossary/#sampling","title":"Sampling","text":"<p>\u63a7\u5236\u5bfc\u51fa\u6570\u636e\u91cf\u7684\u673a\u5236\u3002\u6700\u5e38\u4e0e<code>Tracing</code> <code>Data Source</code>\u4e00 \u8d77\u4f7f\u7528. \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#sdk","title":"SDK","text":"<p>\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u5305\u7684\u7b80\u79f0\u3002\u6307\u9065\u6d4b SDK\uff0c\u8868\u793a\u5b9e\u73b0 OpenTelemetry <code>API</code>\u7684<code>Library</code></p>"},{"location":"docs/concepts/glossary/#semantic-conventions","title":"Semantic Conventions","text":"<p>\u5b9a\u4e49' Metadata '\u7684\u6807\u51c6\u540d\u79f0\u548c\u503c\uff0c\u4ee5\u4fbf\u63d0\u4f9b\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u9065\u6d4b\u6570\u636e\u3002</p>"},{"location":"docs/concepts/glossary/#service","title":"Service","text":"<p><code>Application</code>\u7684\u7ec4\u4ef6\u3002\u4e00\u4e2a<code>Service</code>\u7684\u591a\u4e2a\u5b9e\u4f8b\u901a\u5e38\u662f\u4e3a \u4e86\u9ad8\u53ef\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u800c\u90e8\u7f72\u7684\u3002\u4e00\u4e2a<code>Service</code>\u53ef\u4ee5\u90e8\u7f72\u5728\u591a\u4e2a\u4f4d\u7f6e\u3002</p>"},{"location":"docs/concepts/glossary/#signal","title":"Signal","text":"<p><code>Traces</code>, <code>Metrics</code> or <code>Logs</code>\u4e4b\u4e00\u3002\u66f4\u591a\u5173\u4e8e\u4fe1\u53f7\u5728 \u8fd9\u91cc\u3002</p>"},{"location":"docs/concepts/glossary/#span","title":"Span","text":"<p>\u8868\u793a<code>Trace</code>\u4e2d\u7684\u5355\u4e2a\u64cd\u4f5c\u3002\u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#span-link","title":"Span Link","text":"<p>\u8de8\u5ea6\u94fe\u63a5\u662f\u56e0\u679c\u76f8\u5173\u7684\u8de8\u5ea6\u4e4b\u95f4\u7684\u94fe\u63a5\u3002\u8be6\u60c5\u8bf7\u53c2 \u89c1\u8de8\u95f4\u94fe\u63a5 \u548c \u6307\u5b9a\u94fe\u63a5.\u3002</p>"},{"location":"docs/concepts/glossary/#specification","title":"Specification","text":"<p>\u63cf\u8ff0\u6240\u6709\u5b9e\u73b0\u7684\u8de8\u8bed\u8a00\u9700\u6c42\u548c\u671f\u671b\u3002\u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#status","title":"Status","text":"<p>\u64cd\u4f5c\u7684\u7ed3\u679c\u3002\u901a\u5e38\u7528\u4e8e\u6307\u793a\u662f\u5426\u53d1\u751f\u9519\u8bef\u3002 \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#tag","title":"Tag","text":"<p>\u67e5\u770b <code>Metadata</code>.</p>"},{"location":"docs/concepts/glossary/#trace","title":"Trace","text":"<p><code>Spans</code>\u7684<code>DAG</code> \uff0c\u5176\u4e2d<code>Spans</code>\u4e4b\u95f4\u7684\u8fb9\u5b9a\u4e49\u4e3a\u7236/\u5b50\u5173\u7cfb\u3002 \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#tracer","title":"Tracer","text":"<p>\u8d1f\u8d23\u521b\u5efa' span '. \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#transaction","title":"Transaction","text":"<p>\u67e5\u770b <code>Distributed Tracing</code>.</p>"},{"location":"docs/concepts/glossary/#zpages","title":"zPages","text":"<p>\u5728\u8fdb\u7a0b\u5185\u66ff\u4ee3\u5916\u90e8\u5bfc\u51fa\u7a0b\u5e8f\u3002\u5f53\u5305\u542b\u65f6\uff0c\u5b83\u4eec\u5728\u540e\u53f0\u6536\u96c6\u548c\u6c47\u603b\u8ddf\u8e2a\u548c\u5ea6\u91cf\u4fe1\u606f;\u5f53\u88ab\u8bf7\u6c42 \u65f6\uff0c\u8fd9\u4e9b\u6570\u636e\u88ab\u63d0\u4f9b\u7ed9\u7f51\u9875\u3002 \u67e5\u770b\u66f4\u591a.</p>"},{"location":"docs/concepts/glossary/#_2","title":"\u989d\u5916\u672f\u8bed","text":""},{"location":"docs/concepts/glossary/#traces","title":"Traces","text":""},{"location":"docs/concepts/glossary/#trace-api-terminology","title":"Trace API Terminology","text":""},{"location":"docs/concepts/glossary/#trace-sdk-terminology","title":"Trace SDK Terminology","text":""},{"location":"docs/concepts/glossary/#metrics","title":"Metrics","text":""},{"location":"docs/concepts/glossary/#metric-api-terminology","title":"Metric API Terminology","text":""},{"location":"docs/concepts/glossary/#metric-sdk-terminology","title":"Metric SDK Terminology","text":""},{"location":"docs/concepts/glossary/#logs","title":"Logs","text":""},{"location":"docs/concepts/glossary/#trace-context-fields","title":"Trace Context Fields","text":""},{"location":"docs/concepts/glossary/#severity-fields","title":"Severity Fields","text":""},{"location":"docs/concepts/glossary/#log-record-fields","title":"Log Record Fields","text":""},{"location":"docs/concepts/glossary/#semantic-conventions_1","title":"Semantic Conventions","text":""},{"location":"docs/concepts/glossary/#resource-conventions","title":"Resource Conventions","text":""},{"location":"docs/concepts/glossary/#span-conventions","title":"Span Conventions","text":""},{"location":"docs/concepts/glossary/#metric-conventions","title":"Metric Conventions","text":""},{"location":"docs/concepts/observability-primer/","title":"\u53ef\u89c2\u6d4b Primer","text":""},{"location":"docs/concepts/observability-primer/#_1","title":"\u4ec0\u4e48\u662f\u53ef\u89c2\u5bdf\u6027?","text":"<p>\u53ef\u89c2\u5bdf\u6027\u8ba9\u6211\u4eec\u5728\u4e0d\u77e5\u9053\u7cfb\u7edf\u5185\u90e8\u8fd0\u4f5c\u7684\u60c5\u51b5\u4e0b\u5bf9\u7cfb\u7edf\u63d0\u51fa\u95ee\u9898\uff0c\u4ece\u800c\u4ece\u5916\u90e8\u7406\u89e3\u7cfb\u7edf\u3002 \u6b64\u5916\uff0c\u5b83\u5141\u8bb8\u6211\u4eec\u8f7b\u677e\u5730\u6392\u9664\u6545\u969c\u5e76\u5904\u7406\u65b0\u95ee\u9898(\u5373\u201c\u672a\u77e5\u7684\u672a\u77e5\u201d)\uff0c\u5e76\u5e2e\u52a9\u6211\u4eec\u56de\u7b54\u201c\u4e3a\u4ec0\u4e48\u4f1a\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5?\u201d</p> <p>\u4e3a\u4e86\u80fd\u591f\u5bf9\u7cfb\u7edf\u63d0\u51fa\u8fd9\u4e9b\u95ee\u9898\uff0c\u5fc5\u987b\u5bf9\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u9002\u5f53\u7684\u68c0\u6d4b\u3002 \u4e5f\u5c31\u662f\u8bf4\uff0c\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u5fc5\u987b\u53d1\u51fa\u4fe1\u53f7\uff0c\u4f8b\u5982trace\uff0c metrics\u548clogs\u3002 \u5f53\u5f00\u53d1\u4eba\u5458\u4e0d\u9700\u8981\u6dfb\u52a0\u66f4\u591a\u7684\u68c0\u6d4b\u6765\u89e3\u51b3\u95ee\u9898\u65f6\uff0c\u5e94\u7528\u7a0b\u5e8f\u5c31\u88ab\u9002\u5f53\u5730\u68c0\u6d4b\u4e86\uff0c\u56e0\u4e3a\u4ed6\u4eec\u5df2\u7ecf\u62e5\u6709\u4e86\u6240\u9700\u7684\u6240\u6709\u4fe1\u606f\u3002</p> <p>OpenTelemetry\u662f\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u88ab\u68c0\u6d4b\u7684\u673a\u5236\uff0c\u4ee5\u5e2e\u52a9\u4f7f\u7cfb\u7edf\u53ef\u89c2\u5bdf\u3002</p>"},{"location":"docs/concepts/observability-primer/#_2","title":"\u53ef\u9760\u6027\u548c\u5ea6\u91cf","text":"<p>\u9065\u6d4b\u662f\u6307\u4ece\u7cfb\u7edf\u53d1\u51fa\u7684\u6709\u5173\u5176\u884c\u4e3a\u7684\u6570\u636e\u3002 \u6570\u636e\u53ef\u4ee5\u4ee5\u75d5\u8ff9\u3001\u6307\u6807\u548c\u65e5\u5fd7\u7684\u5f62\u5f0f\u51fa\u73b0\u3002</p> <p>\u53ef\u9760\u6027 \u56de\u7b54\u7684\u95ee\u9898\u662f:\u201c\u670d\u52a1\u662f\u5426\u5728\u505a\u7528\u6237\u671f\u671b\u5b83\u505a\u7684\u4e8b\u60c5?\u201d\u7cfb\u7edf\u53ef\u4ee5100%\u6b63\u5e38\u8fd0\u884c\uff0c\u4f46\u5982\u679c\u5f53\u7528\u6237\u70b9\u51fb\u201c\u6dfb\u52a0\u5230\u8d2d\u7269\u8f66\u201d\u5c06\u4e00\u6761\u9ed1\u8272\u88e4\u5b50\u6dfb\u52a0\u5230\u8d2d\u7269\u8f66\u65f6\uff0c\u7cfb\u7edf\u5374\u4e00\u76f4\u5728\u6dfb\u52a0\u4e00\u6761\u7ea2\u8272\u88e4\u5b50\uff0c\u90a3\u4e48\u7cfb\u7edf\u5c31\u4f1a\u88ab\u8ba4\u4e3a\u662f**\u4e0d**\u53ef\u9760\u7684\u3002</p> <p>\u6307\u6807 \u662f\u4e00\u6bb5\u65f6\u95f4\u5185\u5173\u4e8e\u57fa\u7840\u8bbe\u65bd\u6216\u5e94\u7528\u7a0b\u5e8f\u7684\u6570\u5b57\u6570\u636e\u7684\u805a\u5408\u3002 \u793a\u4f8b\u5305\u62ec:\u7cfb\u7edf\u9519\u8bef\u7387\u3001CPU\u5229\u7528\u7387\u3001\u7ed9\u5b9a\u670d\u52a1\u7684\u8bf7\u6c42\u7387\u3002</p> <p>SLI \uff0c\u5373\u670d\u52a1\u6c34\u5e73\u6307\u6807\uff0c\u8868\u793a\u5bf9\u670d\u52a1\u884c\u4e3a\u7684\u5ea6\u91cf\u3002 \u597d\u7684SLI\u4ece\u7528\u6237\u7684\u89d2\u5ea6\u6765\u8861\u91cf\u60a8\u7684\u670d\u52a1\u3002 \u4f8b\u5982\uff0cSLI\u53ef\u4ee5\u662f\u7f51\u9875\u52a0\u8f7d\u7684\u901f\u5ea6\u3002</p> <p>SLO \uff0c\u5373\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff0c\u662f\u5411\u7ec4\u7ec7/\u5176\u4ed6\u56e2\u961f\u4f20\u8fbe\u53ef\u9760\u6027\u7684\u624b\u6bb5\u3002 \u8fd9\u53ef\u4ee5\u901a\u8fc7\u5c06\u4e00\u4e2a\u6216\u591a\u4e2asli\u9644\u52a0\u5230\u4e1a\u52a1\u503c\u6765\u5b9e\u73b0\u3002</p>"},{"location":"docs/concepts/observability-primer/#_3","title":"\u7406\u89e3\u5206\u5e03\u5f0f\u8ddf\u8e2a","text":"<p>\u4e3a\u4e86\u7406\u89e3\u5206\u5e03\u5f0f\u8ddf\u8e2a\uff0c\u8ba9\u6211\u4eec\u4ece\u4e00\u4e9b\u57fa\u7840\u77e5\u8bc6\u5f00\u59cb\u3002</p>"},{"location":"docs/concepts/observability-primer/#_4","title":"\u65e5\u5fd7","text":"<p>\u65e5\u5fd7\u662f\u7531\u670d\u52a1\u6216\u5176\u4ed6\u7ec4\u4ef6\u53d1\u51fa\u7684\u5e26\u6709\u65f6\u95f4\u6233\u7684\u6d88\u606f\u3002 \u7136\u800c\uff0c\u4e0etraces\u4e0d\u540c\uff0c\u5b83\u4eec\u4e0d\u4e00\u5b9a\u4e0e\u4efb\u4f55\u7279\u5b9a\u7684\u7528\u6237\u8bf7\u6c42\u6216\u4e8b\u52a1\u76f8\u5173\u8054\u3002 \u5b83\u4eec\u5728\u8f6f\u4ef6\u4e2d\u51e0\u4e4e\u65e0\u5904\u4e0d\u5728\uff0c\u5e76\u4e14\u5728\u8fc7\u53bb\u88ab\u5f00\u53d1\u4eba\u5458\u548c\u64cd\u4f5c\u4eba\u5458\u4e25\u91cd\u4f9d\u8d56\uff0c\u4ee5\u5e2e\u52a9\u4ed6\u4eec\u7406\u89e3\u7cfb\u7edf\u884c\u4e3a\u3002</p> <p>\u793a\u4f8b\u65e5\u5fd7:</p> <pre><code>I, [2021-02-23T13:26:23.505892 #22473]  INFO -- : [6459ffe1-ea53-4044-aaa3-bf902868f730] Started GET \"/\" for ::1 at 2021-02-23 13:26:23 -0800\n</code></pre> <p>\u4e0d\u5e78\u7684\u662f\uff0c\u65e5\u5fd7\u5bf9\u4e8e\u8ddf\u8e2a\u4ee3\u7801\u6267\u884c\u5e76\u4e0d\u662f\u975e\u5e38\u6709\u7528\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4f8b\u5982\u4ece\u54ea\u91cc\u8c03\u7528\u5b83\u4eec\u3002</p> <p>\u5f53\u5b83\u4eec\u4f5c\u4e3aspan\u7684\u4e00\u90e8\u5206\u5305\u542b\u65f6\uff0c\u5b83\u4eec\u5c06\u53d8\u5f97\u66f4\u52a0\u6709\u7528\u3002</p>"},{"location":"docs/concepts/observability-primer/#spans","title":"Spans","text":"<p>span \u8868\u793a\u4e00\u4e2a\u5de5\u4f5c\u6216\u64cd\u4f5c\u5355\u5143\u3002 \u5b83\u8ddf\u8e2a\u8bf7\u6c42\u6240\u505a\u7684\u7279\u5b9a\u64cd\u4f5c\uff0c\u63cf\u7ed8\u51fa\u5728\u6267\u884c\u8be5\u64cd\u4f5c\u671f\u95f4\u53d1\u751f\u7684\u60c5\u51b5\u3002</p> <p>span\u5305\u542b\u540d\u79f0\u3001\u4e0e\u65f6\u95f4\u76f8\u5173\u7684\u6570\u636e\u3001\u7ed3\u6784\u5316\u65e5\u5fd7\u6d88\u606f\u548c\u5176\u4ed6\u5143\u6570\u636e(\u5373\u5c5e\u6027)\uff0c\u4ee5\u63d0\u4f9b\u5173\u4e8e\u5b83\u6240\u8ddf\u8e2a\u7684\u64cd\u4f5c\u7684\u4fe1\u606f\u3002</p>"},{"location":"docs/concepts/observability-primer/#span","title":"Span \u5c5e\u6027","text":"<p>\u4e0b\u8868\u5305\u542b\u4e86span\u5c5e\u6027\u7684\u793a\u4f8b:</p> Key Value net.transport <code>IP.TCP</code> net.peer.ip <code>10.244.0.1</code> net.peer.port <code>10243</code> net.host.name <code>localhost</code> http.method <code>GET</code> http.target <code>/cart</code> http.server_name <code>frontend</code> http.route <code>/cart</code> http.scheme <code>http</code> http.host <code>localhost</code> http.flavor <code>1.1</code> http.status_code <code>200</code> http.user_agent <code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36</code> <p>For more on spans and how they pertain to OTel, see Spans.</p>"},{"location":"docs/concepts/observability-primer/#_5","title":"\u5206\u5e03\u5f0f\u8ddf\u8e2a","text":"<p>\u5206\u5e03\u5f0f\u8ddf\u8e2a\uff0c\u901a\u5e38\u79f0\u4e3a\u8ddf\u8e2a\uff0c\u8bb0\u5f55\u8bf7\u6c42(\u7531\u5e94\u7528\u7a0b\u5e8f\u6216\u6700\u7ec8\u7528\u6237\u53d1\u51fa)\u901a\u8fc7\u591a\u670d\u52a1\u67b6\u6784(\u5982\u5fae\u670d\u52a1\u548c\u65e0\u670d\u52a1\u5668\u5e94\u7528\u7a0b\u5e8f)\u4f20\u64ad\u65f6\u6240\u91c7\u53d6\u7684\u8def\u5f84\u3002</p> <p>\u5982\u679c\u4e0d\u8fdb\u884c\u8ddf\u8e2a\uff0c\u5c31\u5f88\u96be\u786e\u5b9a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u6027\u80fd\u95ee\u9898\u7684\u539f\u56e0\u3002</p> <p>\u5b83\u63d0\u9ad8\u4e86\u5e94\u7528\u7a0b\u5e8f\u6216\u7cfb\u7edf\u8fd0\u884c\u72b6\u51b5\u7684\u53ef\u89c1\u6027\uff0c\u5e76\u5141\u8bb8\u6211\u4eec\u8c03\u8bd5\u96be\u4ee5\u5728\u672c\u5730\u91cd\u73b0\u7684\u884c\u4e3a\u3002 \u8ddf\u8e2a\u5bf9\u4e8e\u5206\u5e03\u5f0f\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u901a\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u6216\u8005\u8fc7\u4e8e\u590d\u6742\u800c\u65e0\u6cd5\u5728\u672c\u5730\u91cd\u73b0\u3002</p> <p>\u8ddf\u8e2a\u901a\u8fc7\u5206\u89e3\u8bf7\u6c42\u6d41\u7ecf\u5206\u5e03\u5f0f\u7cfb\u7edf\u65f6\u6240\u53d1\u751f\u7684\u4e8b\u60c5\uff0c\u4f7f\u8c03\u8bd5\u548c\u7406\u89e3\u5206\u5e03\u5f0f\u7cfb\u7edf\u53d8\u5f97\u4e0d\u90a3\u4e48\u4ee4\u4eba\u751f\u754f\u3002</p> <p>\u8ff9\u7ebf\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u8de8\u5ea6\u7ec4\u6210\u3002\u7b2c\u4e00\u4e2a\u8de8\u5ea6\u8868\u793a\u6839\u8de8\u5ea6\u3002 \u6bcf\u4e2a\u6839\u8de8\u5ea6\u4ee3\u8868\u4e00\u4e2a\u4ece\u5934\u5230\u5c3e\u7684\u8bf7\u6c42\u3002\u7236\u7c7b\u4e0b\u9762\u7684\u8de8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u6df1\u5165\u7684\u4e0a\u4e0b\u6587\uff0c\u8bf4\u660e\u5728\u8bf7\u6c42\u671f\u95f4\u53d1\u751f\u4e86\u4ec0\u4e48(\u6216\u7ec4\u6210\u8bf7\u6c42\u7684\u6b65\u9aa4)\u3002</p> <p>\u8bb8\u591a\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\u5c06\u8f68\u8ff9\u53ef\u89c6\u5316\u4e3a\u7011\u5e03\u56fe\uff0c\u5982\u4e0b\u56fe\u6240\u793a:</p> <p></p> <p>\u7011\u5e03\u56fe\u663e\u793a\u4e86\u6839\u8de8\u5ea6\u4e0e\u5176\u5b50\u8de8\u5ea6\u4e4b\u95f4\u7684\u7236\u5b50\u5173\u7cfb\u3002 \u5f53\u4e00\u4e2aspan\u5c01\u88c5\u53e6\u4e00\u4e2aspan\u65f6\uff0c\u8fd9\u4e5f\u8868\u793a\u5d4c\u5957\u5173\u7cfb\u3002</p> <p>\u6709\u5173trace\u53ca\u5176\u4e0eOTel\u7684\u5173\u7cfb\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1trace.</p>"},{"location":"docs/concepts/semantic-conventions/","title":"\u8bed\u4e49\u7ea6\u5b9a","text":"<p>OpenTelemetry\u5b9a\u4e49\u8bed\u4e49\u7ea6\u5b9a(\u6709\u65f6\u79f0\u4e3a\u8bed\u4e49\u5c5e\u6027)\uff0c\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u64cd\u4f5c\u548c\u6570\u636e\u6307\u5b9a\u901a\u7528\u540d\u79f0\u3002 \u4f7f\u7528\u8bed\u4e49\u7ea6\u5b9a\u7684\u597d\u5904\u662f\u9075\u5faa\u4e00\u4e2a\u901a\u7528\u7684\u547d\u540d\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u53ef\u4ee5\u8de8\u4ee3\u7801\u5e93\u3001\u5e93\u548c\u5e73\u53f0\u8fdb\u884c\u6807\u51c6\u5316\u3002</p> <p>\u76ee\u524d\uff0c\u8bed\u4e49\u7ea6\u5b9a\u53ef\u7528\u4e8e\u8ddf\u8e2a\u3001\u5ea6\u91cf\u548c\u8d44\u6e90:</p> <ul> <li>\u8ddf\u8e2a\u8bed\u4e49\u7ea6\u5b9a</li> <li>\u5ea6\u91cf\u8bed\u4e49\u7ea6\u5b9a</li> <li>\u8d44\u6e90\u8bed\u4e49\u7ea6\u5b9a</li> </ul>"},{"location":"docs/concepts/instrumentation/","title":"\u63d2\u88c5","text":"<p>\u4e3a\u4e86\u4f7f\u7cfb\u7edf\u53ef\u89c2\u5bdf\uff0c\u5b83\u5fc5\u987b \u4eea\u5668\u5316 :\u4e5f\u5c31\u662f\u8bf4\uff0c\u6765\u81ea\u7cfb\u7edf\u7ec4\u4ef6\u7684\u4ee3\u7801\u5fc5\u987b\u53d1 \u51fa\u8ddf\u8e2a\uff0c\u5ea6\u91cf\u548c\u65e5\u5fd7\u3002</p> <p>\u4e0d\u9700\u8981\u4fee\u6539\u6e90\u4ee3\u7801\uff0c\u60a8\u5c31\u53ef\u4ee5\u4f7f\u7528automatic instrumentation\u4ece\u5e94\u7528\u7a0b\u5e8f \u6536\u96c6\u9065\u6d4b\u6570\u636e\u3002\u5982\u679c\u60a8\u4ee5\u524d\u4f7f\u7528 APM \u4ee3\u7406\u4ece\u5e94\u7528\u7a0b\u5e8f\u4e2d\u63d0\u53d6\u9065\u6d4b\u6570\u636e\uff0c\u90a3\u4e48\u81ea\u52a8\u68c0\u6d4b\u5c06\u4e3a \u60a8\u63d0\u4f9b\u7c7b\u4f3c\u7684\u5f00\u7bb1\u5373\u7528\u4f53\u9a8c\u3002</p> <p>\u4e3a\u4e86\u66f4\u65b9\u4fbf\u5730\u68c0\u6d4b\u5e94\u7528\u7a0b\u5e8f\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u5bf9 OpenTelemetry api \u8fdb\u884c\u7f16\u7801 \u6765\u624b\u52a8\u68c0\u6d4b\u5e94\u7528\u7a0b\u5e8f\u3002</p> <p>\u4e3a\u6b64\uff0c\u4f60\u4e0d\u9700\u8981\u68c0\u6d4b\u5e94\u7528\u7a0b\u5e8f\u4e2d\u4f7f\u7528\u7684\u6240\u6709\u4f9d\u8d56\u9879:</p> <ul> <li>\u901a\u8fc7\u76f4\u63a5\u8c03\u7528 OpenTelemetry API\uff0c\u4f60\u7684\u4e00\u4e9b\u5e93\u5c06\u662f\u5f00\u7bb1\u5373\u7528\u7684\u53ef\u89c2\u5bdf\u7684\u3002\u8fd9\u4e9b\u5e93\u6709\u65f6\u88ab   \u79f0\u4e3a \u672c\u673a\u63d2\u88c5\u5e93\u3002</li> <li>\u5bf9\u4e8e\u6ca1\u6709\u8fd9\u79cd\u96c6\u6210\u7684\u5e93\uff0cOpenTelemetry \u9879\u76ee\u63d0\u4f9b\u4e86\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684[\u4eea\u5668\u5e93][]</li> </ul> <p>\u8bf7\u6ce8\u610f\uff0c\u5bf9\u4e8e\u5927\u591a\u6570\u8bed\u8a00\uff0c\u53ef\u4ee5\u540c\u65f6\u4f7f\u7528\u624b\u52a8\u548c\u81ea\u52a8\u63d2\u88c5:\u81ea\u52a8\u63d2\u88c5\u5c06\u5141\u8bb8\u60a8\u5feb\u901f\u4e86\u89e3\u5e94\u7528 \u7a0b\u5e8f\uff0c\u800c\u624b\u52a8\u63d2\u88c5\u5c06\u4f7f\u60a8\u80fd\u591f\u5c06\u7c92\u5ea6\u53ef\u89c2\u5bdf\u6027\u5d4c\u5165\u5230\u4ee3\u7801\u4e2d\u3002</p> <p>manual\u548cautomatic\u68c0\u6d4b\u7684\u786e\u5207\u5b89\u88c5\u673a\u5236\u56e0\u60a8\u6240\u4f7f\u7528\u7684\u5f00\u53d1\u8bed\u8a00 \u800c\u5f02\uff0c\u4f46\u4e0b\u9762\u51e0\u8282\u5c06\u4ecb\u7ecd\u4e00\u4e9b\u76f8\u4f3c\u4e4b\u5904\u3002</p>"},{"location":"docs/concepts/instrumentation/automatic/","title":"\u81ea\u52a8\u63d2\u88c5","text":"<p>\u5982\u679c\u9002\u7528\uff0cOpenTelemetry \u7684\u7279\u5b9a\u8bed\u8a00\u5b9e\u73b0\u5c06\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u800c\u65e0\u9700\u89e6 \u53ca\u60a8\u7684\u6e90\u4ee3\u7801\u3002\u867d\u7136\u5e95\u5c42\u673a\u5236\u53d6\u51b3\u4e8e\u8bed\u8a00\uff0c\u4f46\u81f3\u5c11\u8fd9\u4f1a\u5c06 OpenTelemetry API \u548c SDK \u529f\u80fd \u6dfb\u52a0\u5230\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u53ef\u80fd\u4f1a\u6dfb\u52a0\u4e00\u7ec4\u5de5\u5177\u5e93\u548c\u5bfc\u51fa\u5668\u4f9d\u8d56\u9879\u3002</p> <p>\u53ef\u4ee5\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u548c\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u65b9\u5f0f(\u5982 Java \u4e2d\u7684\u7cfb\u7edf\u5c5e\u6027)\u8fdb\u884c\u914d\u7f6e\u3002\u81f3\u5c11\uff0c\u5fc5\u987b\u914d\u7f6e \u670d\u52a1\u540d\u79f0\u6765\u6807\u8bc6\u88ab\u68c0\u6d4b\u7684\u670d\u52a1\u3002\u5404\u79cd\u5176\u4ed6\u914d\u7f6e\u9009\u9879\u53ef\u7528\uff0c\u53ef\u80fd\u5305\u62ec:</p> <ul> <li>\u7279\u5b9a\u4e8e\u6570\u636e\u6e90\u7684\u914d\u7f6e</li> <li>\u5bfc\u51fa\u5668\u914d\u7f6e</li> <li>\u4f20\u64ad\u5668\u914d\u7f6e</li> <li>\u8d44\u6e90\u914d\u7f6e</li> </ul> <p>\u81ea\u52a8\u63d2\u88c5\u53ef\u7528\u4e8e\u4ee5\u4e0b\u8bed\u8a00:</p> <ul> <li>.NET</li> <li>Java</li> <li>JavaScript</li> <li>PHP</li> <li>Python</li> </ul>"},{"location":"docs/concepts/instrumentation/libraries/","title":"\u63d2\u88c5\u5e93","text":"<p>OpenTelemetry \u4e3a\u8bb8\u591a\u5e93\u63d0\u4f9b\u4e86\u63d2\u88c5\u5e93\uff0c\u8fd9\u901a\u5e38\u662f\u901a\u8fc7\u5e93\u94a9\u5b50\u6216\u7334\u5b50\u8865\u4e01\u5e93\u4ee3\u7801\u5b8c\u6210\u7684 \u3002</p> <p>\u4f7f\u7528 OpenTelemetry \u7684\u672c\u673a\u5e93\u63d2\u88c5\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89c2\u5bdf\u6027\u548c\u5f00\u53d1\u4f53\u9a8c\uff0c\u6d88\u9664\u4e86\u5e93\u66b4 \u9732\u548c\u6587\u6863\u6302\u94a9\u7684\u9700\u8981:</p> <ul> <li>\u81ea\u5b9a\u4e49\u65e5\u5fd7\u94a9\u5b50\u53ef\u4ee5\u88ab\u5e38\u89c1\u7684\u548c\u6613\u4e8e\u4f7f\u7528\u7684 OpenTelemetry api \u53d6\u4ee3\uff0c\u7528\u6237\u5c06\u53ea\u4e0e   OpenTelemetry \u4ea4\u4e92</li> <li>\u6765\u81ea\u5e93\u548c\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u8ddf\u8e2a\u3001\u65e5\u5fd7\u3001\u6307\u6807\u662f\u76f8\u5173\u548c\u4e00\u81f4\u7684</li> <li>\u901a\u7528\u7ea6\u5b9a\u5141\u8bb8\u7528\u6237\u5728\u76f8\u540c\u7684\u6280\u672f\u548c\u8de8\u5e93\u548c\u8bed\u8a00\u4e2d\u83b7\u5f97\u76f8\u4f3c\u548c\u4e00\u81f4\u7684\u9065\u6d4b</li> <li>\u9065\u6d4b\u4fe1\u53f7\u53ef\u4ee5\u4f7f\u7528\u5404\u79cd\u8bb0\u5f55\u826f\u597d\u7684 OpenTelemetry \u6269\u5c55\u70b9\u5bf9\u5404\u79cd\u6d88\u8d39\u573a\u666f\u8fdb\u884c\u5fae\u8c03(\u8fc7\u6ee4   \u3001\u5904\u7406\u3001\u805a\u5408)\u3002</li> </ul>"},{"location":"docs/concepts/instrumentation/libraries/#_1","title":"\u8bed\u4e49\u7ea6\u5b9a","text":"<p>\u67e5\u770b\u53ef\u7528\u7684\u8bed\u4e49\u7ea6\u5b9a\uff0c\u6db5\u76d6 web \u6846\u67b6\u3001RPC \u5ba2\u6237\u7aef\u3001\u6570\u636e\u5e93\u3001\u6d88\u606f\u4f20\u9012\u5ba2\u6237\u7aef\u3001\u57fa\u7840\u8bbe\u65bd\u7b49!</p> <p>\u5982\u679c\u60a8\u7684\u5e93\u662f\u5176\u4e2d\u4e4b\u4e00-\u9075\u5faa\u60ef\u4f8b\uff0c\u5b83\u4eec\u662f\u4e8b\u5b9e\u7684\u4e3b\u8981\u6765\u6e90\uff0c\u5e76\u544a\u8bc9\u54ea\u4e9b\u4fe1\u606f\u5e94\u8be5\u5305\u542b\u5728 spans \u4e2d\u3002\u7ea6\u5b9a\u4f7f\u68c0\u6d4b\u4fdd\u6301\u4e00\u81f4:\u4f7f\u7528\u9065\u6d4b\u6280\u672f\u7684\u7528\u6237\u4e0d\u5fc5\u5b66\u4e60\u5e93\u7684\u7ec6\u8282\uff0c\u800c\u53ef\u89c2\u5bdf\u6027\u4f9b\u5e94 \u5546\u53ef\u4ee5\u4e3a\u5404\u79cd\u5404\u6837\u7684\u6280\u672f(\u4f8b\u5982\u6570\u636e\u5e93\u6216\u6d88\u606f\u4f20\u9012\u7cfb\u7edf)\u6784\u5efa\u4f53\u9a8c\u3002\u5f53\u5e93\u9075\u5faa\u7ea6\u5b9a\u65f6\uff0c\u65e0\u9700\u7528 \u6237\u8f93\u5165\u6216\u914d\u7f6e\uff0c\u8bb8\u591a\u573a\u666f\u5c31\u53ef\u4ee5\u5f00\u7bb1\u5373\u7528\u3002</p> <p>\u5982\u679c\u60a8\u6709\u4efb\u4f55\u53cd\u9988\u6216\u60f3\u8981\u6dfb\u52a0\u4e00\u4e2a\u65b0\u7684\u4f1a\u8bae-\u8bf7\u6765\u8d21\u732e! Instrumentation Slack\u6216Specification repo\u662f \u4e00\u4e2a\u5f88\u597d\u7684\u5f00\u59cb!</p>"},{"location":"docs/concepts/instrumentation/libraries/#_2","title":"\u5f53 \u4e0d \u4eea\u5668","text":"<p>\u6709\u4e9b\u5e93\u662f\u5305\u88c5\u7f51\u7edc\u8c03\u7528\u7684\u7626\u5ba2\u6237\u673a\u3002 OpenTelemetry \u5f88\u53ef\u80fd\u6709\u4e00\u4e2a\u7528\u4e8e\u5e95\u5c42 RPC \u5ba2\u6237\u7aef\u7684 \u5de5\u5177\u5e93(\u67e5\u770bregistry)\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u6ca1\u6709\u5fc5\u8981\u68c0\u6d4b\u5305\u88c5 \u5668\u5e93\u3002</p> <p>\u5982\u679c:</p> <ul> <li>\u60a8\u7684\u5e93\u662f\u6587\u6863\u5316\u6216\u81ea\u89e3\u91ca api \u4e4b\u4e0a\u7684\u7626\u4ee3\u7406</li> <li>\u548c OpenTelemetry \u6709\u7528\u4e8e\u5e95\u5c42\u7f51\u7edc\u8c03\u7528\u7684\u5de5\u5177</li> <li>\u548c \u60a8\u7684\u5e93\u4e0d\u5e94\u8be5\u9075\u5faa\u4efb\u4f55\u60ef\u4f8b\u6765\u4e30\u5bcc\u9065\u6d4b\u6280\u672f</li> </ul> <p>\u5982\u679c\u4f60\u6709\u7591\u95ee-\u4e0d\u8981\u4eea\u5668-\u4f60\u53ef\u4ee5\u5728\u4f60\u770b\u5230\u9700\u8981\u7684\u65f6\u5019\u518d\u505a\u3002</p> <p>\u5982\u679c\u60a8\u9009\u62e9\u4e0d\u8fdb\u884c\u68c0\u6d4b\uff0c\u90a3\u4e48\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u4e3a\u60a8\u7684\u5185\u90e8 RPC \u5ba2\u6237\u7aef\u5b9e\u4f8b\u914d\u7f6e OpenTelemetry \u5904\u7406\u7a0b\u5e8f\u53ef\u80fd\u4ecd\u7136\u662f\u6709\u7528\u7684\u3002\u5b83\u5728\u4e0d\u652f\u6301\u5168\u81ea\u52a8\u63d2\u88c5\u7684\u8bed\u8a00\u4e2d\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684 \uff0c\u4f46\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u4ecd\u7136\u5f88\u6709\u7528\u3002</p> <p>\u5982\u679c\u60a8\u51b3\u5b9a\u8fd9\u6837\u505a\uff0c\u672c\u6587\u7684\u5176\u4f59\u90e8\u5206\u5c06\u6307\u5bfc\u60a8\u4f7f\u7528\u4ec0\u4e48\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u3002</p>"},{"location":"docs/concepts/instrumentation/libraries/#opentelemetry-api","title":"OpenTelemetry API","text":"<p>\u7b2c\u4e00\u6b65\u662f\u4f9d\u8d56\u4e8e OpenTelemetry API \u5305\u3002</p> <p>OpenTelemetry \u6709\u4e24\u4e2a\u4e3b\u8981\u6a21\u5757\u2014\u2014API \u548c SDK\u3002 OpenTelemetry API \u662f\u4e00\u7ec4\u62bd\u8c61\u548c\u975e\u64cd\u4f5c\u5b9e\u73b0\u3002\u9664\u975e\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u5bfc\u5165 OpenTelemetry SDK\uff0c\u5426\u5219\u60a8\u7684\u68c0\u6d4b\u5de5\u5177\u4e0d\u4f1a\u505a\u4efb\u4f55\u4e8b\u60c5\uff0c\u4e5f\u4e0d\u4f1a\u5f71\u54cd\u5e94\u7528\u7a0b\u5e8f\u7684\u6027\u80fd\u3002</p> <p>\u5e93\u5e94\u8be5\u53ea\u4f7f\u7528 OpenTelemetry API\u3002</p> <p>\u4f60\u53ef\u80fd\u6709\u7406\u7531\u62c5\u5fc3\u6dfb\u52a0\u65b0\u7684\u4f9d\u8d56\uff0c\u8fd9\u91cc\u6709\u4e00\u4e9b\u6ce8\u610f\u4e8b\u9879\u53ef\u4ee5\u5e2e\u52a9\u4f60\u51b3\u5b9a\u5982\u4f55\u51cf\u5c11\u4f9d\u8d56\u5730\u72f1:</p> <ul> <li>OpenTelemetry Trace API \u5728 2021 \u5e74\u521d\u8fbe\u5230\u7a33\u5b9a\uff0c\u5b83\u9075\u5faa\u8bed\u4e49\u7248\u672c\u63a7\u5236   2.0\u548c\u6211\u4eec\u8ba4\u771f\u5bf9\u5f85 API \u7a33\u5b9a\u6027\u3002</li> <li>\u5f53\u4f7f\u7528\u4f9d\u8d56\u65f6\uff0c\u8bf7\u4f7f\u7528\u6700\u65e9\u7684\u7a33\u5b9a OpenTelemetry API(1.0.*)\u5e76\u907f\u514d\u66f4\u65b0\u5b83\uff0c\u9664\u975e\u60a8\u5fc5   \u987b\u4f7f\u7528\u65b0\u529f\u80fd\u3002</li> <li>\u5f53\u60a8\u7684\u5de5\u5177\u7a33\u5b9a\u4e0b\u6765\u65f6\uff0c\u8bf7\u8003\u8651\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u5355\u72ec\u7684\u5305\u53d1\u5e03\uff0c\u8fd9\u6837\u5c31\u4e0d\u4f1a\u7ed9\u4e0d\u4f7f\u7528\u5b83\u7684\u7528   \u6237\u5e26\u6765\u95ee\u9898\u3002\u60a8\u53ef\u4ee5\u5c06\u5176\u4fdd\u7559\u5728\u60a8\u7684 repo \u4e2d\uff0c\u6216   \u8005\u5c06\u5176\u6dfb\u52a0\u5230 OpenTelemetry\uff0c   \u8fd9\u6837\u5b83\u5c06\u4e0e\u5176\u4ed6\u4eea\u5668\u5305\u4e00\u8d77\u53d1\u5e03\u3002</li> <li>\u8bed\u4e49\u7ea6\u5b9a\u662f\u7a33\u5b9a\u7684\uff0c\u4f46\u53d7\u5236\u4e8e\u6f14\u53d8:\u867d\u7136\u8fd9\u4e0d\u4f1a\u5bfc\u81f4\u4efb\u4f55\u529f\u80fd\u95ee\u9898\uff0c\u4f46\u60a8\u53ef\u80fd\u9700\u8981\u6bcf   \u9694\u4e00\u6bb5\u65f6\u95f4\u66f4\u65b0\u60a8\u7684\u5de5\u5177\u3002\u5c06\u5176\u653e\u5728\u9884\u89c8\u63d2\u4ef6\u6216 opentelement_contrib_repo \u4e2d\u53ef\u80fd\u6709   \u52a9\u4e8e\u4fdd\u6301\u60ef\u4f8b\u7684\u6700\u65b0\uff0c\u800c\u4e0d\u4f1a\u7834\u574f\u7528\u6237\u7684\u66f4\u6539\u3002</li> </ul>"},{"location":"docs/concepts/instrumentation/libraries/#_3","title":"\u83b7\u53d6\u8ffd\u8e2a\u5668","text":"<p>\u6240\u6709\u5e94\u7528\u7a0b\u5e8f\u914d\u7f6e\u90fd\u901a\u8fc7 Tracer API \u5bf9\u5e93\u9690\u85cf\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5e93\u5e94\u8be5 \u4eceglobal <code>TracerProvider</code>\u83b7\u53d6\u8ddf\u8e2a\u5668 \u3002</p> <pre><code>private static final Tracer tracer = GlobalOpenTelemetry.getTracer(\"demo-db-client\", \"0.1.0-beta1\");\n</code></pre> <p>\u5bf9\u4e8e\u5e93\u6765\u8bf4\uff0c\u6709\u4e00\u4e2a\u5141\u8bb8\u5e94\u7528\u7a0b\u5e8f\u663e\u5f0f\u4f20\u9012<code>TracerProvider</code>\u5b9e\u4f8b\u7684 API \u662f\u5f88\u6709\u7528\u7684\uff0c\u8fd9 \u6837\u53ef\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u4f9d\u8d56\u6ce8\u5165\u5e76\u7b80\u5316\u6d4b\u8bd5\u3002</p> <p>\u5728\u83b7\u5f97\u8ddf\u8e2a\u7a0b\u5e8f\u65f6\uff0c\u63d0\u4f9b\u60a8\u7684\u5e93(\u6216\u8ddf\u8e2a\u63d2\u4ef6)\u540d\u79f0\u548c\u7248\u672c\u2014\u2014\u5b83\u4eec\u663e\u793a\u5728\u9065\u6d4b\u6570\u636e\u4e0a\uff0c\u5e2e\u52a9\u7528 \u6237\u5904\u7406\u548c\u8fc7\u6ee4\u9065\u6d4b\u6570\u636e\uff0c\u4e86\u89e3\u5b83\u7684\u6765\u6e90\uff0c\u5e76\u8c03\u8bd5/\u62a5\u544a\u4efb\u4f55\u4eea\u8868\u95ee\u9898\u3002</p>"},{"location":"docs/concepts/instrumentation/libraries/#_4","title":"\u4eea\u5668\u4eea\u8868","text":""},{"location":"docs/concepts/instrumentation/libraries/#api","title":"\u516c\u5171 api","text":"<p>\u516c\u5171 API \u662f\u5f88\u597d\u7684\u8ddf\u8e2a\u5bf9\u8c61:\u4e3a\u516c\u5171 API \u8c03\u7528\u521b\u5efa\u7684\u8303\u56f4\u5141\u8bb8\u7528\u6237\u5c06\u9065\u6d4b\u6620\u5c04\u5230\u5e94\u7528\u7a0b\u5e8f \u4ee3\u7801\uff0c\u4e86\u89e3\u5e93\u8c03\u7528\u7684\u6301\u7eed\u65f6\u95f4\u548c\u7ed3\u679c\u3002\u8c03\u7528 trace:</p> <ul> <li>\u5185\u90e8\u8fdb\u884c\u7f51\u7edc\u8c03\u7528\u7684\u516c\u5171\u65b9\u6cd5\u6216\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\u4e14\u53ef\u80fd\u5931\u8d25\u7684\u672c\u5730\u64cd\u4f5c(\u4f8b\u5982 IO)</li> <li>\u5904\u7406\u8bf7\u6c42\u6216\u6d88\u606f\u7684\u5904\u7406\u7a0b\u5e8f</li> </ul> <p>\u63d2\u88c5\u7684\u4f8b\u5b50:</p> <pre><code>private static final Tracer tracer = GlobalOpenTelemetry.getTracer(\"demo-db-client\", \"0.1.0-beta1\");\nprivate Response selectWithTracing(Query query) {\n// check out conventions for guidance on span names and attributes\nSpan span = tracer.spanBuilder(String.format(\"SELECT %s.%s\", dbName, collectionName))\n.setSpanKind(SpanKind.CLIENT)\n.setAttribute(\"db.name\", dbName)\n...\n.startSpan();\n// makes span active and allows correlating logs and nest spans\ntry (Scope unused = span.makeCurrent()) {\nResponse response = query.runWithRetries();\nif (response.isSuccessful()) {\nspan.setStatus(StatusCode.OK);\n}\nif (span.isRecording()) {\n// populate response attributes for response codes and other information\n}\n} catch (Exception e) {\nspan.recordException(e);\nspan.setStatus(StatusCode.ERROR, e.getClass().getSimpleName());\nthrow e;\n} finally {\nspan.end();\n}\n}\n</code></pre> <p>\u6309\u7167\u7ea6\u5b9a\u586b\u5145\u5c5e\u6027!\u5982\u6ca1\u6709\u9002\u7528\u7684\u89c4\u5b9a\uff0c\u8bf7\u53c2 \u9605\u4e00\u822c\u60ef\u4f8b.</p>"},{"location":"docs/concepts/instrumentation/libraries/#spans","title":"\u5d4c\u5957\u7f51\u7edc\u548c\u5176\u4ed6 spans","text":"<p>Network calls are usually traced with OpenTelemetry auto-instrumentations through corresponding client implementation.</p> <p></p> <p>If OpenTelemetry does not support tracing your network client, use your best judgement, here are some considerations to help:</p> <ul> <li>Would tracing network calls improve observability for users or your ability to   support them?</li> <li>Is your library a wrapper on top of public, documented RPC API? Would users   need to get support from the underlying service in case of issues?</li> <li>instrument the library and make sure to trace individual network tries</li> <li>Would tracing those calls with spans be very verbose? or would it noticeably   impact performance?</li> <li>use logs with verbosity or span events: logs can be correlated to parent     (public API calls), while span events should be set on public API span.</li> <li>if they have to be spans (to carry and propagate unique trace context), put     them behind a configuration option and disable them by default.</li> </ul> <p>If OpenTelemetry already supports tracing your network calls, you probably don't want to duplicate it. There may be some exceptions:</p> <ul> <li>to support users without auto-instrumentation (which may not work in certain   environments or users may have concerns with monkey-patching)</li> <li>to enable custom (legacy) correlation and context propagation protocols with   underlying service</li> <li>enrich RPC spans with absolutely essential library/service-specific   information not covered by auto-instrumentation</li> </ul> <p>WARNING: Generic solution to avoid duplication is under construction \ud83d\udea7.</p>"},{"location":"docs/concepts/instrumentation/libraries/#events","title":"Events","text":"<p>Traces are one kind of signal that your apps can emit. Events (or logs) and traces complement, not duplicate, each other. Whenever you have something that should have a verbosity, logs are a better choice than traces.</p> <p>Chances are that your app uses logging or some similar module already. Your module might already have OpenTelemetry integration -- to find out, see the registry. Integrations usually stamp active trace context on all logs, so users can correlate them.</p> <p>If your language and ecosystem don't have common logging support, use span events to share additional app details. Events maybe more convenient if you want to add attributes as well.</p> <p>As a rule of thumb, use events or logs for verbose data instead of spans. Always attach events to the span instance that your instrumentation created. Avoid using the active span if you can, since you don't control what it refers to.</p>"},{"location":"docs/concepts/instrumentation/libraries/#_5","title":"\u4e0a\u4e0b\u6587\u4f20\u64ad","text":""},{"location":"docs/concepts/instrumentation/libraries/#_6","title":"\u63d0\u53d6\u4e0a\u4e0b\u6587","text":"<p>If you work on a library or a service that receives upstream calls, e.g. a web framework or a messaging consumer, you should extract context from the incoming request/message. OpenTelemetry provides the <code>Propagator</code> API, which hides specific propagation standards and reads the trace <code>Context</code> from the wire. In case of a single response, there is just one context on the wire, which becomes the parent of the new span the library creates.</p> <p>After you create a span, you should pass new trace context to the application code (callback or handler), by making the span active; if possible, you should do this explicitly.</p> <pre><code>// extract the context\nContext extractedContext = propagator.extract(Context.current(), httpExchange, getter);\nSpan span = tracer.spanBuilder(\"receive\")\n.setSpanKind(SpanKind.SERVER)\n.setParent(extractedContext)\n.startSpan();\n// make span active so any nested telemetry is correlated\ntry (Scope unused = span.makeCurrent()) {\nuserCode();\n} catch (Exception e) {\nspan.recordException(e);\nspan.setStatus(StatusCode.ERROR);\nthrow e;\n} finally {\nspan.end();\n}\n</code></pre> <p>Here're the full examples of context extraction in Java, check out OpenTelemetry documentation in your language.</p> <p>In the case of a messaging system, you may receive more than one message at once. Received messages become links on the span you create. Refer to messaging conventions for details (WARNING: messaging conventions are under constructions \ud83d\udea7).</p>"},{"location":"docs/concepts/instrumentation/libraries/#_7","title":"\u6ce8\u5165\u4e0a\u4e0b\u6587","text":"<p>When you make an outbound call, you will usually want to propagate context to the downstream service. In this case, you should create a new span to trace the outgoing call and use <code>Propagator</code> API to inject context into the message. There may be other cases where you might want to inject context, e.g. when creating messages for async processing.</p> <pre><code>Span span = tracer.spanBuilder(\"send\")\n.setSpanKind(SpanKind.CLIENT)\n.startSpan();\n// make span active so any nested telemetry is correlated\n// even network calls might have nested layers of spans, logs or events\ntry (Scope unused = span.makeCurrent()) {\n// inject the context\npropagator.inject(Context.current(), transportLayer, setter);\nsend();\n} catch (Exception e) {\nspan.recordException(e);\nspan.setStatus(StatusCode.ERROR);\nthrow e;\n} finally {\nspan.end();\n}\n</code></pre> <p>Here's the full example of context injection in Java.</p> <p>There might be some exceptions:</p> <ul> <li>downstream service does not support metadata or prohibits unknown fields</li> <li>downstream service does not define correlation protocols. Is it possible that   some future service version will support compatible context propagation?   Inject it!</li> <li>downstream service supports custom correlation protocol.</li> <li>best effort with custom propagator: use OpenTelemetry trace context if     compatible.</li> <li>or generate and stamp custom correlation ids on the span.</li> </ul>"},{"location":"docs/concepts/instrumentation/libraries/#_8","title":"\u8fdb\u7a0b\u5185\u7684","text":"<ul> <li>Make your spans active (aka current): it enables correlating spans with   logs and any nested auto-instrumentations.</li> <li>If the library has a notion of context, support optional explicit trace   context propagation in addition to active spans</li> <li>put spans (trace context) created by library in the context explicitly,     document how to access it</li> <li>allow users to pass trace context in your context</li> <li>Within the library, propagate trace context explicitly - active spans may   change during callbacks!</li> <li>capture active context from users on the public API surface as soon as you     can, use it as a parent context for your spans</li> <li>pass context around and stamp attributes, exceptions, events on explicitly     propagated instances</li> <li>this is essential if you start threads explicitly, do background processing     or other things that can break due to async context flow limitations in your     language</li> </ul>"},{"location":"docs/concepts/instrumentation/libraries/#misc","title":"Misc","text":""},{"location":"docs/concepts/instrumentation/libraries/#_9","title":"\u8bbe\u5907\u6ce8\u518c","text":"<p>Please add your instrumentation library to the OpenTelemetry registry, so users can find it.</p>"},{"location":"docs/concepts/instrumentation/libraries/#_10","title":"\u8868\u6f14","text":"<p>OpenTelemetry API is no-op and very performant when there is no SDK in the application. When OpenTelemetry SDK is configured, it consumes bound resources.</p> <p>Real-life applications, especially on the high scale, would frequently have head-based sampling configured. Sampled-out spans are cheap and you can check if the span is recording, to avoid extra allocations and potentially expensive calculations, while populating attributes.</p> <pre><code>// some attributes are important for sampling, they should be provided at creation time\nSpan span = tracer.spanBuilder(String.format(\"SELECT %s.%s\", dbName, collectionName))\n.setSpanKind(SpanKind.CLIENT)\n.setAttribute(\"db.name\", dbName)\n...\n.startSpan();\n// other attributes, especially those that are expensive to calculate\n// should be added if span is recording\nif (span.isRecording()) {\nspan.setAttribute(\"db.statement\", sanitize(query.statement()))\n}\n</code></pre>"},{"location":"docs/concepts/instrumentation/libraries/#_11","title":"\u9519\u8bef\u5904\u7406","text":"<p>OpenTelemetry API is forgiving at runtime - does not fail on invalid arguments, never throws, and swallows exceptions. This way instrumentation issues do not affect application logic. Test the instrumentation to notice issues OpenTelemetry hides at runtime.</p>"},{"location":"docs/concepts/instrumentation/libraries/#_12","title":"\u6d4b\u8bd5","text":"<p>Since OpenTelemetry has variety of auto-instrumentations, it's useful to try how your instrumentation interacts with other telemetry: incoming requests, outgoing requests, logs, etc. Use a typical application, with popular frameworks and libraries and all tracing enabled when trying out your instrumentation. Check out how libraries similar to yours show up.</p> <p>For unit testing, you can usually mock or fake <code>SpanProcessor</code> and <code>SpanExporter</code>.</p> <pre><code>@Test\npublic void checkInstrumentation() {\nSpanExporter exporter = new TestExporter();\nTracer tracer = OpenTelemetrySdk.builder()\n.setTracerProvider(SdkTracerProvider.builder()\n.addSpanProcessor(SimpleSpanProcessor.create(exporter)).build()).build()\n.getTracer(\"test\");\n// run test ...\nvalidateSpans(exporter.exportedSpans);\n}\nclass TestExporter implements SpanExporter {\npublic final List&lt;SpanData&gt; exportedSpans = Collections.synchronizedList(new ArrayList&lt;&gt;());\n@Override\npublic CompletableResultCode export(Collection&lt;SpanData&gt; spans) {\nexportedSpans.addAll(spans);\nreturn CompletableResultCode.ofSuccess();\n}\n...\n}\n</code></pre>"},{"location":"docs/concepts/instrumentation/manual/","title":"\u624b\u52a8\u63d2\u88c5","text":""},{"location":"docs/concepts/instrumentation/manual/#opentelemetry-api-sdk","title":"\u5bfc\u5165 OpenTelemetry API \u548c SDK","text":"<p>\u9996\u5148\u9700\u8981\u5c06 OpenTelemetry \u5bfc\u5165\u5230\u670d\u52a1\u4ee3\u7801\u4e2d\u3002\u5982\u679c\u60a8\u6b63\u5728\u5f00\u53d1\u4e00\u4e2a\u5e93\u6216\u5176\u4ed6\u6253\u7b97\u7531\u53ef\u8fd0 \u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u4f7f\u7528\u7684\u7ec4\u4ef6\uff0c\u90a3\u4e48\u60a8\u53ea\u9700\u8981\u4f9d\u8d56\u4e8e API\u3002\u5982\u679c\u60a8\u7684\u5de5\u4ef6\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u6d41\u7a0b\u6216\u670d \u52a1\uff0c\u90a3\u4e48\u60a8\u5c06\u4f9d\u8d56\u4e8e API \u548c SDK\u3002\u6709\u5173 OpenTelemetry API \u548c SDK \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2 \u89c1specification\u3002</p>"},{"location":"docs/concepts/instrumentation/manual/#opentelemetry-api","title":"\u914d\u7f6e OpenTelemetry API","text":"<p>\u4e3a\u4e86\u521b\u5efa\u8ddf\u8e2a\u6216\u5ea6\u91cf\uff0c\u60a8\u9700\u8981\u9996\u5148\u521b\u5efa\u8ddf\u8e2a\u7a0b\u5e8f\u548c/\u6216\u5ea6\u91cf\u63d0\u4f9b\u7a0b\u5e8f\u3002\u901a\u5e38\uff0c\u6211\u4eec\u5efa\u8bae SDK \u5e94\u8be5\u4e3a\u8fd9\u4e9b\u5bf9\u8c61\u63d0\u4f9b\u5355\u4e2a\u9ed8\u8ba4\u63d0\u4f9b\u7a0b\u5e8f\u3002\u7136\u540e\uff0c\u60a8\u5c06\u4ece\u8be5\u63d0\u4f9b\u7a0b\u5e8f\u83b7\u5f97\u8ddf\u8e2a\u7a0b\u5e8f\u6216\u63d2\u88c5\u5b9e\u4f8b \uff0c\u5e76\u4e3a\u5176\u63d0\u4f9b\u540d\u79f0\u548c\u7248\u672c\u3002\u60a8\u5728\u8fd9\u91cc\u9009\u62e9\u7684\u540d\u79f0\u5e94\u8be5\u786e\u5b9a\u8981\u68c0\u6d4b\u7684\u786e\u5207\u5185\u5bb9\u2014\u2014\u4f8b\u5982\uff0c\u5982\u679c\u60a8 \u6b63\u5728\u7f16\u5199\u4e00\u4e2a\u5e93\uff0c\u90a3\u4e48\u60a8\u5e94\u8be5\u4ee5\u60a8\u7684\u5e93(\u4f8b\u5982<code>com.legitimatebusiness.myLibrary</code>)\u6765\u547d\u540d \u5b83\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u540d\u79f0\u5c06\u547d\u540d\u751f\u6210\u7684\u6240\u6709\u8de8\u5ea6\u6216\u5ea6\u91cf\u4e8b\u4ef6\u3002\u8fd8\u5efa\u8bae\u60a8\u63d0\u4f9b\u4e0e\u5e93\u6216\u670d\u52a1\u7684\u5f53\u524d\u7248\u672c \u5bf9\u5e94\u7684\u7248\u672c\u5b57\u7b26\u4e32(\u5373' semver:1.0.0 ')\u3002</p>"},{"location":"docs/concepts/instrumentation/manual/#opentelemetry-sdk","title":"\u914d\u7f6e OpenTelemetry SDK","text":"<p>\u5982\u679c\u60a8\u6b63\u5728\u6784\u5efa\u4e00\u4e2a\u670d\u52a1\u6d41\u7a0b\uff0c\u60a8\u8fd8\u9700\u8981\u4e3a SDK \u914d\u7f6e\u9002\u5f53\u7684\u9009\u9879\uff0c\u4ee5\u4fbf\u5c06\u9065\u6d4b\u6570\u636e\u5bfc\u51fa\u5230 \u67d0\u4e2a\u5206\u6790\u540e\u7aef\u3002\u6211\u4eec\u5efa\u8bae\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6216\u5176\u4ed6\u673a\u5236\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u5904\u7406\u6b64\u914d\u7f6e\u3002\u60a8\u53ef\u80fd\u8fd8\u5e0c\u671b\u5229 \u7528\u4e0d\u540c\u8bed\u8a00\u7684\u8c03\u4f18\u9009\u9879\u3002</p>"},{"location":"docs/concepts/instrumentation/manual/#_1","title":"\u521b\u5efa\u9065\u6d4b\u6570\u636e","text":"<p>\u914d\u7f6e\u597d API \u548c SDK \u4e4b\u540e\uff0c\u60a8\u5c31\u53ef\u4ee5\u901a\u8fc7\u4ece\u63d0\u4f9b\u7a0b\u5e8f\u83b7\u5f97\u7684\u8ddf\u8e2a\u5668\u548c\u5ea6\u91cf\u5bf9\u8c61\u81ea\u7531\u5730\u521b\u5efa\u8ddf \u8e2a\u548c\u5ea6\u91cf\u4e8b\u4ef6\u3002\u4e3a\u4f60\u7684\u4f9d\u8d56\u9879\u4f7f\u7528\u5de5\u5177\u5e93\u2014\u2014\u67e5\u770bregistry\u6216\u4f60\u7684 \u8bed\u8a00\u7684\u5b58\u50a8\u5e93\uff0c\u4e86\u89e3\u66f4\u591a\u76f8\u5173\u4fe1\u606f\u3002</p>"},{"location":"docs/concepts/instrumentation/manual/#_2","title":"\u51fa\u53e3\u6570\u636e","text":"<p>\u4e00\u65e6\u60a8\u521b\u5efa\u4e86\u9065\u6d4b\u6570\u636e\uff0c\u60a8\u5c06\u5e0c\u671b\u5c06\u5176\u53d1\u9001\u5230\u67d0\u4e2a\u5730\u65b9\u3002 OpenTelemetry \u652f\u6301\u5c06\u6570\u636e\u4ece\u8fdb \u7a0b\u5bfc\u51fa\u5230\u5206\u6790\u540e\u7aef\u7684\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u8981\u4e48\u76f4\u63a5\u4ece\u8fdb\u7a0b\u5bfc\u51fa\uff0c\u8981\u4e48\u901a \u8fc7OpenTelemetry Collector\u8fdb\u884c\u4ee3\u7406\u3002</p> <p>\u8fdb\u7a0b\u5185\u5bfc\u51fa\u9700\u8981\u60a8\u5bfc\u5165\u5e76\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u6216\u591a\u4e2a exporters\uff0c\u8fd9\u4e9b\u5e93\u5c06 OpenTelemetry \u7684\u5185 \u5b58\u8de8\u5ea6\u548c\u5ea6\u91cf\u5bf9\u8c61\u8f6c\u6362\u4e3a\u9002\u5408 Jaeger \u6216 Prometheus \u7b49\u9065\u6d4b\u5206\u6790\u5de5\u5177\u7684\u683c\u5f0f\u3002\u6b64\u5916 \uff0cOpenTelemetry \u8fd8\u652f\u6301\u4e00\u79cd\u540d\u4e3a\u201cOTLP\u201d\u7684\u6709\u7ebf\u534f\u8bae\uff0c\u6240\u6709 OpenTelemetry sdk \u90fd\u652f\u6301\u8be5 \u534f\u8bae\u3002\u8be5\u534f\u8bae\u53ef\u7528\u4e8e\u5411 OpenTelemetry Collector \u53d1\u9001\u6570\u636e\uff0cOpenTelemetry Collector \u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u4e8c\u8fdb\u5236\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u670d\u52a1\u5b9e\u4f8b\u7684\u4ee3\u7406\u6216\u4fa7\u8f66\u8fd0\u884c\uff0c\u4e5f\u53ef\u4ee5\u5728\u5355\u72ec\u7684\u4e3b\u673a\u4e0a\u8fd0 \u884c\u3002\u7136\u540e\u53ef\u4ee5\u914d\u7f6e Collector \u6765\u8f6c\u53d1\u548c\u5bfc\u51fa\u8be5\u6570\u636e\u5230\u60a8\u9009\u62e9\u7684\u5206\u6790\u5de5\u5177\u3002</p> <p>\u9664\u4e86\u50cf Jaeger \u6216 Prometheus \u8fd9\u6837\u7684\u5f00\u6e90\u5de5\u5177\u4e4b\u5916\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u516c\u53f8\u652f\u6301\u4ece OpenTelemetry \u83b7\u53d6\u9065\u6d4b\u6570\u636e\u3002\u8be6\u60c5\u8bf7\u53c2\u89c1vendor\u3002</p>"},{"location":"docs/concepts/sampling/","title":"\u91c7\u6837","text":"<p>\u4f7f\u7528\u5206\u5e03\u5f0f\u8ddf\u8e2a\uff0c\u60a8\u53ef\u4ee5\u89c2\u5bdf\u8bf7\u6c42\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u4ece\u4e00\u4e2a\u670d\u52a1\u79fb\u52a8\u5230\u53e6\u4e00\u4e2a\u670d\u52a1\u7684\u8fc7\u7a0b\u3002 \u5b83\u975e\u5e38\u5b9e\u7528\uff0c\u539f\u56e0\u6709\u5f88\u591a\uff0c\u6bd4\u5982\u7406\u89e3\u60a8\u7684\u670d\u52a1\u8fde\u63a5\u548c\u8bca\u65ad\u5ef6\u8fdf\u95ee\u9898\uff0c\u8fd8\u6709\u8bb8\u591a\u5176\u4ed6\u597d\u5904\u3002</p> <p>However, if the majority of all your requests are successful 200s and finish without unacceptable latency or errors, do you really need all that data? Here\u2019s the thing\u2014you don\u2019t always need a ton of data to find the right insights. You just need the right sampling of data.</p> <p></p> <p>The idea behind sampling is to control the spans you send to your observability backend, resulting in lower ingest costs. Different organizations will have their own reasons for not just why they want to sample, but also what they want to sample. You might want to customize your sampling strategy to:</p> <ul> <li>Manage costs: If you have a high volume of telemetry, you risk incurring   heavy charges from a telemetry backend vendor or cloud provider to export and   store every span.</li> <li>Focus on interesting traces: For example, your frontend team may only want   to see traces with specific user attributes.</li> <li>Filter out noise: For example, you may want to filter out health checks.</li> </ul>"},{"location":"docs/concepts/sampling/#_1","title":"\u672f\u8bed","text":"<p>It's important to use consistent terminology when discussing sampling. A trace or span is considered \"sampled\" or \"not sampled\":</p> <ul> <li>Sampled: A trace or span is processed and exported. Because it is chosen   by the sampler as a representative of the population, it is considered   \"sampled\".</li> <li>Not sampled: A trace or span is not processed or exported. Because it is   not chosen by the sampler, it is considered \"not sampled\".</li> </ul> <p>Sometimes, the definitions of these terms get mixed up. You may find someone state that they are \"sampling out data\" or that data not processed or exported is considered \"sampled\". These are incorrect statements.</p>"},{"location":"docs/concepts/sampling/#_2","title":"\u5934\u62bd\u6837","text":"<p>Head sampling is a sampling technique used to make a sampling decision as early as possible. A decision to sample or drop a span or trace is not made by inspecting the trace as a whole.</p> <p>For example, the most common form of head sampling is Consistent Probability Sampling. It may also be referred to as Deterministic Sampling. In this case, a sampling decision is made based on the trace ID and a desired percentage of traces to sample. This ensures that whole traces are sampled - no missing spans - at a consistent rate, such as 5% of all traces.</p> <p>The upsides to head sampling are:</p> <ul> <li>Easy to understand</li> <li>Easy to configure</li> <li>Efficient</li> <li>Can be done at any point in the trace collection pipeline</li> </ul> <p>The primary downside to head sampling is that it is not possible make a sampling decision based on data in the entire trace. This means that head sampling is effective as a blunt instrument, but is wholly insufficient for sampling strategies that must take whole-system information into account. For example, it is not possible to use head sampling to ensure that all traces with an error within them are sampled. For this, you need Tail Sampling.</p>"},{"location":"docs/concepts/sampling/#_3","title":"\u5c3e\u5df4\u62bd\u6837","text":"<p>Tail sampling is where the decision to sample a trace takes place by considering all or most of the spans within the trace. Tail Sampling gives you the option to sample your traces based on specific criteria derived from different parts of a trace, which isn\u2019t an option with Head Sampling.</p> <p></p> <p>Some examples of how you can use Tail Sampling include:</p> <ul> <li>Always sampling traces that contain an error</li> <li>Sampling traces based on overall latency</li> <li>Sampling traces based on the presence or value of specific attributes on one   or more spans in a trace; for example, sampling more traces originating from a   newly deployed service</li> <li>Applying different sampling rates to traces based on certain criteria</li> </ul> <p>As you can see, tail sampling allows for a much higher degree of sophistication. For larger systems that must sample telemetry, it is almost always necessary to use Tail Sampling to balance data volume with usefulness of that data.</p> <p>There are three primary downsides to tail sampling today:</p> <ul> <li>Tail sampling can be difficult to implement. Depending on the kind of sampling   techniques available to you, it is not always a \"set and forget\" kind of   thing. As your systems change, so too will your sampling strategies. For a   large and sophisticated distributed system, rules that implement sampling   strategies can also be large and sophisticated.</li> <li>Tail sampling can be difficult to operate. The component(s) that implement   tail sampling must be stateful systems that can accept and store a large   amount of data. Depending on traffic patterns, this can require dozens or even   hundreds of nodes that all utilize resources differently. Furthermore, a tail   sampler may need to \"fall back\" to less computationally-intensive sampling   techniques if it is unable to keep up with the volume of data it is receiving.   Because of these factors, it is critical to monitor tail sampling components   to ensure that they have the resources they need to make the correct sampling   decisions.</li> <li>Tail samplers often end up being in the domain of vendor-specific technology   today. If you're using a paid vendor for Observability, the most effective   tail sampling options available to you may be limited to what the vendor   offers.</li> </ul> <p>Finally, for some systems, tail sampling may be used in conjunction with Head Sampling. For example, a set of services that produce an extremely high volume of trace data may first use head sampling to only sample a small percentage of traces, and then later in the telemetry pipeline use tail sampling to make more sophisticated sampling decisions before exporting to a backend. This is often done in the interest of protecting the telemetry pipeline from being overloaded.</p>"},{"location":"docs/concepts/sdk-configuration/","title":"SDK \u914d\u7f6e","text":"<p>OpenTelemetry sdk\u652f\u6301\u6bcf\u79cd\u8bed\u8a00\u548c\u73af\u5883\u53d8\u91cf\u7684\u914d\u7f6e\u3002 \u4e0b\u9762\u7684\u9875\u9762\u63cf\u8ff0\u4e86\u53ef\u7528\u4e8e\u914d\u7f6eSDK\u7684\u73af\u5883\u53d8\u91cf\u3002 \u4f7f\u7528\u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u7684\u503c\u8986\u76d6\u5728\u4f7f\u7528SDK api\u7684\u4ee3\u7801\u4e2d\u5b8c\u6210\u7684\u7b49\u6548\u914d\u7f6e\u3002</p>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/","title":"\u901a\u7528SDK\u914d\u7f6e","text":""},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_service_name","title":"<code>OTEL_SERVICE_NAME</code>","text":"<p>\u8bbe\u7f6e' service.name '\u8d44\u6e90\u5c5e\u6027\u7684\u503c\u3002</p> <p>Default value: <code>\"unknown_service\"</code></p> <p>If <code>service.name</code> is also provided in <code>OTEL_RESOURCE_ATTRIBUTES</code>, then <code>OTEL_SERVICE_NAME</code> takes precedence.</p> <p>Example:</p> <p><code>export OTEL_SERVICE_NAME=\"your-service-name\"</code></p>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_resource_attributes","title":"<code>OTEL_RESOURCE_ATTRIBUTES</code>","text":"<p>Key-value pairs to be used as resource attributes. See Resource SDK for more details.</p> <p>Default value: Empty.</p> <p>See Resource semantic conventions for semantic conventions to follow for common resource types.</p> <p>Example:</p> <p><code>export OTEL_RESOURCE_ATTRIBUTES=\"key1=value1,key2=value2\"</code></p>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_traces_sampler","title":"<code>OTEL_TRACES_SAMPLER</code>","text":"<p>Specifies the Sampler used to sample traces by the SDK.</p> <p>Default value: <code>\"parentbased_always_on\"</code></p> <p>Example:</p> <p><code>export OTEL_TRACES_SAMPLER=\"traceidratio\"</code></p> <p>Accepted values for <code>OTEL_TRACES_SAMPLER</code> are:</p> <ul> <li><code>\"always_on\"</code>: <code>AlwaysOnSampler</code></li> <li><code>\"always_off\"</code>: <code>AlwaysOffSampler</code></li> <li><code>\"traceidratio\"</code>: <code>TraceIdRatioBased</code></li> <li><code>\"parentbased_always_on\"</code>: <code>ParentBased(root=AlwaysOnSampler)</code></li> <li><code>\"parentbased_always_off\"</code>: <code>ParentBased(root=AlwaysOffSampler)</code></li> <li><code>\"parentbased_traceidratio\"</code>: <code>ParentBased(root=TraceIdRatioBased)</code></li> <li><code>\"parentbased_jaeger_remote\"</code>: <code>ParentBased(root=JaegerRemoteSampler)</code></li> <li><code>\"jaeger_remote\"</code>: <code>JaegerRemoteSampler</code></li> <li><code>\"xray\"</code>:   AWS X-Ray Centralized Sampling   (third party)</li> </ul>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_traces_sampler_arg","title":"<code>OTEL_TRACES_SAMPLER_ARG</code>","text":"<p>Specifies arguments, if applicable, to the sampler defined in by <code>OTEL_TRACES_SAMPLER</code>. The specified value will only be used if <code>OTEL_TRACES_SAMPLER</code> is set. Each Sampler type defines its own expected input, if any. Invalid or unrecognized input is logged as an error.</p> <p>Default value: Empty.</p> <p>Example:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"traceidratio\"\nexport OTEL_TRACES_SAMPLER_ARG=\"0.5\"\n</code></pre> <p>Depending on the value of <code>OTEL_TRACES_SAMPLER</code>, <code>OTEL_TRACES_SAMPLER_ARG</code> may be set as follows:</p> <ul> <li>For <code>traceidratio</code> and <code>parentbased_traceidratio</code> samplers: Sampling   probability, a number in the [0..1] range, e.g. \"0.25\". Default is 1.0 if   unset.</li> <li>For <code>jaeger_remote</code> and <code>parentbased_jaeger_remote</code>: The value is a comma   separated list:</li> <li>Example:     <code>\"endpoint=http://localhost:14250,pollingIntervalMs=5000,initialSamplingRate=0.25\"</code></li> <li><code>endpoint</code>: the endpoint in form of <code>scheme://host:port</code> of gRPC server that     serves the sampling strategy for the service     (sampling.proto).</li> <li><code>pollingIntervalMs</code>: in milliseconds indicating how often the sampler will     poll the backend for updates to sampling strategy.</li> <li><code>initialSamplingRate</code>: in the [0..1] range, which is used as the sampling     probability when the backend cannot be reached to retrieve a sampling     strategy. This value stops having an effect once a sampling strategy is     retrieved successfully, as the remote strategy will be used until a new     update is retrieved.</li> </ul>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_propagators","title":"<code>OTEL_PROPAGATORS</code>","text":"<p>Specifies Propagators to be used in a comma-separated list.</p> <p>Default value: `\"tracecontext,baggage\"</p> <p>Example:</p> <p><code>export OTEL_PROPAGATORS=\"b3\"</code></p> <p>Accepted values for <code>OTEL_PROPAGATORS</code> are:</p> <ul> <li><code>\"tracecontext\"</code>: W3C Trace Context</li> <li><code>\"baggage\"</code>: W3C Baggage</li> <li><code>\"b3\"</code>: B3 Single</li> <li><code>\"b3multi\"</code>:   B3 Multi</li> <li><code>\"jaeger\"</code>:   Jaeger</li> <li><code>\"xray\"</code>:   AWS X-Ray   (third party)</li> <li><code>\"ottrace\"</code>:   OT Trace (third   party)</li> <li><code>\"none\"</code>: No automatically configured propagator.</li> </ul>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_traces_exporter","title":"<code>OTEL_TRACES_EXPORTER</code>","text":"<p>Specifies which exporter is used for traces.</p> <p>Default value: <code>\"otlp\"</code></p> <p>Example:</p> <p><code>export OTEL_TRACES_EXPORTER=\"jaeger\"</code></p> <p>Accepted values for are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"jaeger\"</code>: export in Jaeger data model</li> <li><code>\"zipkin\"</code>: Zipkin</li> <li><code>\"none\"</code>: No automatically configured exporter for traces.</li> </ul>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_metrics_exporter","title":"<code>OTEL_METRICS_EXPORTER</code>","text":"<p>Specifies which exporter is used for metrics.</p> <p>Default value: <code>\"otlp\"</code></p> <p>Example:</p> <p><code>export OTEL_METRICS_EXPORTER=\"prometheus\"</code></p> <p>Accepted values for <code>OTEL_METRICS_EXPORTER</code> are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"prometheus\"</code>:   Prometheus</li> <li><code>\"none\"</code>: No automatically configured exporter for metrics.</li> </ul>"},{"location":"docs/concepts/sdk-configuration/general-sdk-configuration/#otel_logs_exporter","title":"<code>OTEL_LOGS_EXPORTER</code>","text":"<p>Specifies which exporter is used for logs.</p> <p>Default value: <code>\"otlp\"</code></p> <p>Example:</p> <p><code>export OTEL_LOGS_EXPORTER=\"otlp\"</code></p> <p>Accepted values for <code>OTEL_LOGS_EXPORTER</code> are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"none\"</code>: No automatically configured exporter for logs.</li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/","title":"OTLP\u5bfc\u51fa\u914d\u7f6e","text":""},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#endpoint-configuration","title":"Endpoint Configuration","text":"<p>The following environment variables let you configure an OTLP/gRPC or OTLP/HTTP endpoint for your traces, metrics, and logs.</p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_endpoint","title":"<code>OTEL_EXPORTER_OTLP_ENDPOINT</code>","text":"<p>A base endpoint URL for any signal type, with an optionally-specified port number. Helpful for when you're sending more than one signal to the same endpoint and want one environment variable to control the endpoint.</p> <p>Default value:</p> <ul> <li>gRPC: <code>\"http://localhost:4317\"</code></li> <li>HTTP: <code>\"http://localhost:4318\"</code></li> </ul> <p>Example:</p> <ul> <li>gRPC: <code>export OTEL_EXPORTER_OTLP_ENDPOINT=\"my-api-endpoint:443\"</code></li> <li>HTTP: <code>export OTEL_EXPORTER_OTLP_ENDPOINT=\"http://my-api-endpoint/\"</code></li> </ul> <p>For OTLP/HTTP, exporters in the SDK construct signal-specific URLs when this environment variable is set. This means that if you're sending traces, metrics, and logs, the following URLs are constructed from the example above:</p> <ul> <li>Traces: <code>\"http://my-api-endpoint/v1/traces\"</code></li> <li>Metrics: <code>\"http://my-api-endpoint/v1/metrics\"</code></li> <li>Logs: <code>\"http://my-api-endpoint/v1/logs\"</code></li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_traces_endpoint","title":"<code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code>","text":"<p>Endpoint URL for trace data only, with an optionally-specified port number. Must end with <code>v1/traces</code> if using OTLP/HTTP.</p> <p>Default value:</p> <ul> <li>gRPC: <code>\"http://localhost:4317\"</code></li> <li>HTTP: <code>\"http://localhost:4318/v1/traces\"</code></li> </ul> <p>Example:</p> <ul> <li>gRPC: <code>export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=\"my-api-endpoint:443\"</code></li> <li>HTTP:   <code>export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=\"http://my-api-endpoint/v1/traces\"</code></li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_metrics_endpoint","title":"<code>OTEL_EXPORTER_OTLP_METRICS_ENDPOINT</code>","text":"<p>Endpoint URL for metric data only, with an optionally-specified port number. Must end with <code>v1/metrics</code> if using OTLP/HTTP.</p> <p>Default value:</p> <ul> <li>gRPC: <code>\"http://localhost:4317\"</code></li> <li>HTTP: <code>\"http://localhost:4318/v1/metrics\"</code></li> </ul> <p>Example:</p> <ul> <li>gRPC: <code>export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=\"my-api-endpoint:443\"</code></li> <li>HTTP:   <code>export OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=\"http://my-api-endpoint/v1/metrics\"</code></li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_logs_endpoint","title":"<code>OTEL_EXPORTER_OTLP_LOGS_ENDPOINT</code>","text":"<p>Endpoint URL for log data only, with an optionally-specified port number. Must end with <code>v1/logs</code> if using OTLP/HTTP.</p> <p>Default value:</p> <ul> <li>gRPC: <code>\"http://localhost:4317\"</code></li> <li>HTTP: <code>\"http://localhost:4318/v1/logs\"</code></li> </ul> <p>Example:</p> <ul> <li>gRPC: <code>export OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=\"my-api-endpoint:443\"</code></li> <li>HTTP:   <code>export OTEL_EXPORTER_OTLP_LOGS_ENDPOINT=\"http://my-api-endpoint/v1/logs\"</code></li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#header-configuration","title":"Header configuration","text":"<p>The following environment variables let you configure additional headers as a list of key-value pairs to add in outgoing gRPC or HTTP requests.</p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_headers","title":"<code>OTEL_EXPORTER_OTLP_HEADERS</code>","text":"<p>A list of headers to apply to all outgoing data (traces, metrics, and logs).</p> <p>Default value: N/A</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_HEADERS=\"api-key=key,other-config-value=value\"</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_traces_headers","title":"<code>OTEL_EXPORTER_OTLP_TRACES_HEADERS</code>","text":"<p>A list of headers to apply to all outgoing traces.</p> <p>Default value: N/A</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_TRACES_HEADERS=\"api-key=key,other-config-value=value\"</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_metrics_headers","title":"<code>OTEL_EXPORTER_OTLP_METRICS_HEADERS</code>","text":"<p>A list of headers to apply to all outgoing metrics.</p> <p>Default value: N/A</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_METRICS_HEADERS=\"api-key=key,other-config-value=value\"</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_logs_headers","title":"<code>OTEL_EXPORTER_OTLP_LOGS_HEADERS</code>","text":"<p>A list of headers to apply to all outgoing logs.</p> <p>Default value: N/A</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_LOGS_HEADERS=\"api-key=key,other-config-value=value\"</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#timeout-configuration","title":"Timeout Configuration","text":"<p>The following environment variables configure the maximum time (in milliseconds) an OTLP Exporter will wait before transmitting the net batch of data.</p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_timeout","title":"<code>OTEL_EXPORTER_OTLP_TIMEOUT</code>","text":"<p>The timeout value for all outgoing data (traces, metrics, and logs) in milliseconds.</p> <p>Default value: <code>10000</code> (10s)</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_TIMEOUT=500</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_traces_timeout","title":"<code>OTEL_EXPORTER_OTLP_TRACES_TIMEOUT</code>","text":"<p>The timeout value for all outgoing traces in milliseconds.</p> <p>Default value: 10000 (10s)</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_TRACES_TIMEOUT=500</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_metrics_timeout","title":"<code>OTEL_EXPORTER_OTLP_METRICS_TIMEOUT</code>","text":"<p>The timeout value for all outgoing metrics in milliseconds.</p> <p>Default value: 10000 (10s)</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_METRICS_TIMEOUT=500</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_logs_timeout","title":"<code>OTEL_EXPORTER_OTLP_LOGS_TIMEOUT</code>","text":"<p>The timeout value for all outgoing logs in milliseconds.</p> <p>Default value: 10000 (10s)</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_LOGS_TIMEOUT=500</code></p>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_protocol","title":"<code>OTEL_EXPORTER_OTLP_PROTOCOL</code>","text":"<p>Specifies the OTLP transport protocol to be used for all telemetry data.</p> <p>Default value: SDK-dependent, but will typically be either <code>http/protobuf</code> or <code>grpc</code>.</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_PROTOCOL=grpc</code></p> <p>Valid values are:</p> <ul> <li><code>grpc</code> to use OTLP/gRPC</li> <li><code>http/protobuf</code> to use OTLP/HTTP + protobuf</li> <li><code>http/json</code> to use OTLP/HTTP + json</li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_traces_protocol","title":"<code>OTEL_EXPORTER_OTLP_TRACES_PROTOCOL</code>","text":"<p>Specifies the OTLP transport protocol to be used for trace data.</p> <p>Default value: SDK-dependent, but will typically be either <code>http/protobuf</code> or <code>grpc</code>.</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_TRACES_PROTOCOL=grpc</code></p> <p>Valid values are:</p> <ul> <li><code>grpc</code> to use OTLP/gRPC</li> <li><code>http/protobuf</code> to use OTLP/HTTP + protobuf</li> <li><code>http/json</code> to use OTLP/HTTP + json</li> </ul>"},{"location":"docs/concepts/sdk-configuration/otlp-exporter-configuration/#otel_exporter_otlp_metrics_protocol","title":"<code>OTEL_EXPORTER_OTLP_METRICS_PROTOCOL</code>","text":"<p>Specifies the OTLP transport protocol to be used for metrics data.</p> <p>Default value: SDK-dependent, but will typically be either <code>http/protobuf</code> or <code>grpc</code>.</p> <p>Example: <code>export OTEL_EXPORTER_OTLP_METRICS_PROTOCOL=grpc</code></p> <p>Valid values are:</p> <ul> <li><code>grpc</code> to use OTLP/gRPC</li> <li><code>http/protobuf</code> to use OTLP/HTTP + protobuf</li> <li><code>http/json</code> to use OTLP/HTTP + json</li> </ul>"},{"location":"docs/concepts/signals/","title":"\u4fe1\u53f7","text":"<p>\u5728\u5f00\u653e\u5f0f\u9065\u6d4b\u4e2d\uff0c\u4e00\u4e2a \u4fe1\u53f7 \u8868\u793a\u9065\u6d4b\u7684\u4e00\u4e2a\u7c7b\u522b\u3002 \u672c\u8282\u5c06\u7b80\u8981\u4ecb\u7ecd\u5f53\u524d\u652f\u6301\u7684\u4fe1\u53f7\u3002</p>"},{"location":"docs/concepts/signals/baggage/","title":"Baggage","text":"<p>\u5728 OpenTelemetry \u4e2d\uff0cBaggage \u662f\u5728 spans \u4e4b\u95f4\u4f20\u9012\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u5b83\u662f\u4e00\u4e2a\u952e\u503c\u5b58\u50a8\uff0c \u4f4d\u4e8e\u8ddf\u8e2a\u4e2d\u7684 span \u4e0a\u4e0b\u6587\u65c1\u8fb9\uff0c\u4f7f\u8be5\u8ddf\u8e2a\u4e2d\u521b\u5efa\u7684\u4efb\u4f55 span \u90fd\u53ef\u4ee5\u4f7f\u7528\u503c\u3002</p> <p>\u4f8b\u5982\uff0c\u5047\u8bbe\u60a8\u5e0c\u671b\u5728\u8ddf\u8e2a\u4e2d\u7684\u6bcf\u4e2a span \u4e0a\u90fd\u6709\u4e00\u4e2a <code>CustomerId</code> \u5c5e\u6027\uff0c\u5176\u4e2d\u6d89\u53ca\u591a\u4e2a\u670d \u52a1;\u7136\u800c\uff0c<code>CustomerId</code> \u53ea\u5728\u4e00\u4e2a\u7279\u5b9a\u7684\u670d\u52a1\u4e2d\u53ef\u7528\u3002\u4e3a\u4e86\u5b9e\u73b0\u60a8\u7684\u76ee\u6807\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u5f00\u653e \u9065\u6d4b Baggage \u5728\u60a8\u7684\u7cfb\u7edf\u4e2d\u4f20\u64ad\u8fd9\u4e2a\u503c\u3002</p> <p>OpenTelemetry \u4f7f\u7528Context Propagation\u6765\u4f20\u9012 Baggage\uff0c\u6bcf\u4e2a\u4e0d\u540c\u7684\u5e93\u5b9e\u73b0\u90fd\u6709\u4f20\u64ad \u5668\u6765\u89e3\u6790\u5e76\u4f7f Baggage \u53ef\u7528\uff0c\u800c\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u5b9e\u73b0\u5b83\u3002</p> <p></p>"},{"location":"docs/concepts/signals/baggage/#baggage","title":"Baggage \u4e3a\u4ec0\u4e48\u5b58\u5728?","text":"<p>Baggage \u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u5f0f\u6765\u5b58\u50a8\u548c\u8de8\u8ddf\u8e2a\u548c\u5176\u4ed6\u4fe1\u53f7\u4f20\u64ad\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u60a8\u53ef\u80fd\u5e0c\u671b\u5c06 \u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u4fe1\u606f\u9644\u52a0\u5230\u4e00\u4e2a span \u4e0a\uff0c\u5e76\u5728\u5f88\u4e45\u4ee5\u540e\u68c0\u7d22\u8be5\u4fe1\u606f\uff0c\u7136\u540e\u5c06\u5176\u7528\u4e8e\u53e6\u4e00\u4e2a span\u3002\u7136\u800c\uff0cOpenTelemetry \u4e2d\u7684 span \u5728\u521b\u5efa\u540e\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u4ee5\u540e\u9700\u8981\u5b83\u4eec\u7684 \u4fe1\u606f\u4e4b\u524d\u5bfc\u51fa\u3002\u901a\u8fc7\u63d0\u4f9b\u5b58\u50a8\u548c\u68c0\u7d22\u4fe1\u606f\u7684\u4f4d\u7f6e\uff0cBaggage \u5141\u8bb8\u60a8\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</p>"},{"location":"docs/concepts/signals/baggage/#baggage_1","title":"Baggage \u5e94\u8be5\u7528\u6765\u505a\u4ec0\u4e48?","text":"<p>OTel Baggage \u5e94\u8be5\u7528\u4e8e\u90a3\u4e9b\u4f60\u4e0d\u4ecb\u610f\u66b4\u9732\u7ed9\u4efb\u4f55\u68c0\u67e5\u4f60\u7f51\u7edc\u6d41\u91cf\u7684\u4eba\u7684\u6570\u636e\u3002\u8fd9\u662f\u56e0\u4e3a\u5b83 \u4e0e\u5f53\u524d\u4e0a\u4e0b\u6587\u4e00\u8d77\u5b58\u50a8\u5728 HTTP \u6807\u5934\u4e2d\u3002\u5982\u679c\u60a8\u7684\u76f8\u5173\u7f51\u7edc\u6d41\u91cf\u5b8c\u5168\u5728\u60a8\u81ea\u5df1\u7684\u7f51\u7edc\u4e2d\uff0c\u5219 \u6b64\u8b66\u544a\u53ef\u80fd\u4e0d\u9002\u7528\u3002</p> <p>\u5e38\u89c1\u7684\u7528\u4f8b\u5305\u62ec\u53ea\u80fd\u5728\u5806\u6808\u4e2d\u66f4\u4e0a\u5c42\u8bbf\u95ee\u7684\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u8fd9\u53ef\u4ee5\u5305\u62ec\u5e10\u6237\u6807\u8bc6\u3001\u7528\u6237 id\u3001 \u4ea7\u54c1 id \u548c\u539f\u59cb ip \u7b49\u5185\u5bb9\u3002\u5c06\u8fd9\u4e9b\u4fe1\u606f\u4f20\u9012\u5230\u4f60\u7684\u5806\u6808\u4e2d\uff0c\u4f60\u5c31\u53ef\u4ee5\u5c06\u5b83\u4eec\u6dfb\u52a0\u5230\u4e0b\u6e38\u670d \u52a1\u7684 span \u4e2d\uff0c\u8fd9\u6837\u5f53\u4f60\u5728\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\u8fdb\u884c\u641c\u7d22\u65f6\uff0c\u5c31\u53ef\u4ee5\u66f4\u5bb9\u6613\u5730\u8fdb\u884c\u8fc7\u6ee4\u3002</p> <p>\u6ca1\u6709\u5185\u7f6e\u7684\u5b8c\u6574\u6027\u68c0\u67e5\u6765\u786e\u4fdd Baggage \u9879\u76ee\u662f\u60a8\u7684\uff0c\u56e0\u6b64\u5728\u68c0\u7d22\u5b83\u4eec\u65f6\u8981\u5c0f\u5fc3\u3002</p> <p></p>"},{"location":"docs/concepts/signals/baggage/#baggage-span","title":"Baggage \u4e0e Span \u5c5e\u6027\u4e0d\u540c","text":"<p>\u5173\u4e8e<code>Baggage</code>\u9700\u8981\u6ce8\u610f\u7684\u91cd\u8981\u4e00\u70b9\u662f\uff0c\u5b83\u4e0d\u662fSpan Attributes\u7684\u5b50\u96c6\u3002\u5f53\u60a8\u5c06\u67d0\u4e9b\u5185\u5bb9 \u6dfb\u52a0\u4e3a Baggage \u65f6\uff0c\u5b83\u4e0d\u4f1a\u81ea\u52a8\u7ed3\u675f\u5728\u5b50\u7cfb\u7edf\u7684\u8de8\u7684 Attributes \u4e2d\u3002\u60a8\u5fc5\u987b\u663e\u5f0f\u5730\u4ece Baggage \u4e2d\u53d6\u51fa\u4e00\u4e9b\u4e1c\u897f\uff0c\u5e76\u5c06\u5176\u9644\u52a0\u4e3a Attributes\u3002</p> <p>\u4f8b\u5982\uff0c\u5728.net \u4e2d\u4f60\u53ef\u4ee5\u8fd9\u6837\u505a:</p> <pre><code>var accountId = Baggage.GetBaggage(\"AccountId\");\nActivity.Current?.SetTag(\"AccountId\", accountId);\n</code></pre> <p>\u6b32\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2 \u9605Baggage \u89c4\u683c.</p>"},{"location":"docs/concepts/signals/logs/","title":"Logs","text":"<p>log \u662f\u5e26\u6709\u65f6\u95f4\u6233\u7684\u6587\u672c\u8bb0\u5f55\uff0c\u53ef\u4ee5\u662f\u7ed3\u6784\u5316\u7684(\u63a8\u8350)\uff0c\u4e5f\u53ef\u4ee5\u662f\u975e\u7ed3\u6784\u5316\u7684\uff0c\u5e26\u6709\u5143 \u6570\u636e\u3002\u867d\u7136\u65e5\u5fd7\u662f\u72ec\u7acb\u7684\u6570\u636e\u6e90\uff0c\u4f46\u5b83\u4eec\u4e5f\u53ef\u4ee5\u9644\u52a0\u5230\u8de8\u5ea6\u4e0a\u3002\u5728 OpenTelemetry \u4e2d\uff0c\u4efb \u4f55\u4e0d\u5c5e\u4e8e\u5206\u5e03\u5f0f\u8ddf\u8e2a\u6216\u5ea6\u91cf\u7684\u6570\u636e\u90fd\u662f\u65e5\u5fd7\u3002\u4f8b\u5982\uff0cevents \u662f\u4e00\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u65e5\u5fd7\u3002\u65e5 \u5fd7\u901a\u5e38\u7528\u4e8e\u786e\u5b9a\u95ee\u9898\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u4e14\u901a\u5e38\u5305\u542b\u5173\u4e8e\u8c01\u66f4\u6539\u4e86\u4ec0\u4e48\u4ee5\u53ca\u66f4\u6539\u7684\u7ed3\u679c\u7684\u4fe1\u606f\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u65e5\u5fd7\u89c4\u8303.</p>"},{"location":"docs/concepts/signals/metrics/","title":"Metrics","text":"<p>metric \u662f\u5173\u4e8e\u670d\u52a1\u7684\u5ea6\u91cf\uff0c\u5728\u8fd0\u884c\u65f6\u6355\u83b7\u3002 \u4ece\u903b\u8f91\u4e0a\u8bb2\uff0c\u6355\u83b7\u8fd9\u4e9b\u5ea6\u91cf\u4e4b\u4e00\u7684\u65f6\u523b\u79f0\u4e3a \u5ea6\u91cf\u4e8b\u4ef6 \uff0c\u5b83\u4e0d\u4ec5\u5305\u62ec\u5ea6\u91cf\u672c\u8eab\uff0c\u8fd8\u5305\u62ec\u6355\u83b7\u5ea6\u91cf\u7684\u65f6\u95f4\u548c\u76f8\u5173\u7684\u5143\u6570\u636e\u3002</p> <p>\u5e94\u7528\u7a0b\u5e8f\u548c\u8bf7\u6c42\u5ea6\u91cf\u662f\u53ef\u7528\u6027\u548c\u6027\u80fd\u7684\u91cd\u8981\u6307\u6807\u3002 \u81ea\u5b9a\u4e49\u5ea6\u91cf\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u53ef\u7528\u6027\u6307\u6807\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u6216\u4e1a\u52a1\u3002 \u6536\u96c6\u7684\u6570\u636e\u53ef\u7528\u4e8e\u53d1\u51fa\u505c\u673a\u8b66\u62a5\u6216\u89e6\u53d1\u8c03\u5ea6\u51b3\u7b56\uff0c\u4ee5\u4fbf\u5728\u9ad8\u9700\u6c42\u65f6\u81ea\u52a8\u6269\u5c55\u90e8\u7f72\u3002</p> <p>OpenTelemetry\u5b9a\u4e49\u4e86\u516d\u79cd \u5ea6\u91cf\u5de5\u5177\uff0c\u53ef\u4ee5\u901a\u8fc7OpenTelemetry API\u521b\u5efa:</p> <ul> <li>Counter: \u4e00\u4e2a\u968f\u7740\u65f6\u95f4\u79ef\u7d2f\u7684\u503c- - \u4f60\u53ef\u4ee5\u628a\u5b83\u60f3\u8c61\u6210\u6c7d\u8f66\u4e0a\u7684\u91cc\u7a0b\u8868;\u5b83\u53ea\u4f1a\u4e0a\u5347\u3002</li> <li>Asynchronous Counter: \u4e0e**Counter**\u76f8\u540c\uff0c\u4f46\u5bf9\u6bcf\u4e2a\u5bfc\u51fa\u53ea\u6536\u96c6\u4e00\u6b21\u3002   \u5982\u679c\u60a8\u4e0d\u80fd\u8bbf\u95ee\u8fde\u7eed\u589e\u91cf\uff0c\u800c\u53ea\u80fd\u8bbf\u95ee\u805a\u5408\u503c\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u3002</li> <li>UpDownCounter: \u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u79ef\u7d2f\u7684\u503c\uff0c\u4f46\u4e5f\u53ef\u4ee5\u518d\u6b21\u4e0b\u964d\u3002   \u4f8b\u5982\u961f\u5217\u957f\u5ea6\uff0c\u5b83\u5c06\u968f\u7740\u961f\u5217\u4e2d\u5de5\u4f5c\u9879\u7684\u6570\u91cf\u800c\u589e\u52a0\u6216\u51cf\u5c11\u3002</li> <li>Asynchronous UpDownCounter: \u4e0e**UpDownCounter**\u76f8\u540c\uff0c\u4f46\u6bcf\u4e2a\u5bfc\u51fa\u53ea\u6536\u96c6\u4e00\u6b21\u3002   \u5982\u679c\u60a8\u65e0\u6cd5\u8bbf\u95ee\u6301\u7eed\u66f4\u6539\uff0c\u800c\u53ea\u80fd\u8bbf\u95ee\u805a\u5408\u503c(\u4f8b\u5982\uff0c\u5f53\u524d\u961f\u5217\u5927\u5c0f)\uff0c\u5219\u53ef\u4ee5\u4f7f\u7528\u3002</li> <li>(Asynchronous) Gauge: \u6d4b\u91cf\u8bfb\u53d6\u65f6\u7684\u7535\u6d41\u503c\u3002   \u4e00\u4e2a\u4f8b\u5b50\u5c31\u662f\u6c7d\u8f66\u4e0a\u7684\u71c3\u6cb9\u8868\u3002\u4eea\u8868*\u603b\u662f*\u5f02\u6b65\u7684\u3002</li> <li>Histogram: \u76f4\u65b9\u56fe\u662f\u503c\u7684\u5ba2\u6237\u7aef\u805a\u5408\uff0c\u4f8b\u5982\uff0c\u8bf7\u6c42\u5ef6\u8fdf\u3002   \u5982\u679c\u60a8\u6709\u5f88\u591a\u503c\uff0c\u5e76\u4e14\u5bf9\u6bcf\u4e2a\u5355\u72ec\u7684\u503c\u4e0d\u611f\u5174\u8da3\uff0c\u800c\u662f\u5bf9\u8fd9\u4e9b\u503c\u7684\u7edf\u8ba1\u6570\u636e\u611f\u5174\u8da3(\u4f8b\u5982\uff0c\u6709\u591a\u5c11\u8bf7\u6c42\u82b1\u8d39\u7684\u65f6\u95f4\u5c11\u4e8e15 ?)\uff0c\u76f4\u65b9\u56fe\u53ef\u80fd\u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u9009\u62e9\u3002</li> </ul> <p>\u9664\u4e86\u5ea6\u91cf\u5de5\u5177\u4e4b\u5916\uff0caggregations \u7684\u6982\u5ff5\u4e5f\u662f\u4e00\u4e2a\u9700\u8981\u7406\u89e3\u7684\u91cd\u8981\u6982\u5ff5\u3002 \u805a\u5408\u662f\u4e00\u79cd\u6280\u672f\uff0c\u901a\u8fc7\u8fd9\u79cd\u6280\u672f\uff0c\u53ef\u4ee5\u5c06\u5927\u91cf\u5ea6\u91cf\u7ec4\u5408\u6210\u5173\u4e8e\u67d0\u4e2a\u65f6\u95f4\u7a97\u53e3\u5185\u53d1\u751f\u7684\u5ea6\u91cf\u4e8b\u4ef6\u7684\u7cbe\u786e\u6216\u4f30\u8ba1\u7edf\u8ba1\u4fe1\u606f\u3002 OTLP\u534f\u8bae\u4f20\u8f93\u8fd9\u6837\u7684\u805a\u5408\u5ea6\u91cf\u3002 OpenTelemetry API\u4e3a\u6bcf\u4e2a\u4eea\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ed8\u8ba4\u7684\u805a\u5408\uff0c\u53ef\u4ee5\u4f7f\u7528Views API\u8986\u76d6\u5b83\u3002 OpenTelemetry\u9879\u76ee\u65e8\u5728\u63d0\u4f9b\u7531\u53ef\u89c6\u5316\u5668\u548c\u9065\u6d4b\u540e\u7aef\u652f\u6301\u7684\u9ed8\u8ba4\u805a\u5408\u3002</p> <p>\u4e0e\u65e8\u5728\u6355\u83b7\u8bf7\u6c42\u751f\u547d\u5468\u671f\u5e76\u4e3a\u8bf7\u6c42\u7684\u5404\u4e2a\u90e8\u5206\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7684\u8bf7\u6c42\u8ddf\u8e2a\u4e0d\u540c\uff0c\u5ea6\u91cf\u65e8\u5728\u63d0\u4f9b\u6c47\u603b\u7684\u7edf\u8ba1\u4fe1\u606f\u3002 \u5ea6\u91cf\u7684\u4e00\u4e9b\u7528\u4f8b\u5305\u62ec:</p> <ul> <li>\u62a5\u544a\u4e1a\u52a1\u6309\u534f\u8bae\u7c7b\u578b\u8bfb\u51fa\u7684\u603b\u5b57\u8282\u6570\u3002</li> <li>\u62a5\u544a\u8bfb\u53d6\u7684\u603b\u5b57\u8282\u6570\u548c\u6bcf\u4e2a\u8bf7\u6c42\u7684\u5b57\u8282\u6570\u3002</li> <li>\u62a5\u544a\u7cfb\u7edf\u8c03\u7528\u7684\u6301\u7eed\u65f6\u95f4\u3002</li> <li>\u62a5\u544a\u8bf7\u6c42\u5927\u5c0f\uff0c\u4ee5\u786e\u5b9a\u8d8b\u52bf\u3002</li> <li>\u62a5\u544a\u8fdb\u7a0b\u7684CPU\u6216\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002</li> <li>\u62a5\u544a\u8d26\u6237\u7684\u5e73\u5747\u4f59\u989d\u503c\u3002</li> <li>\u62a5\u544a\u5f53\u524d\u6b63\u5728\u5904\u7406\u7684\u6d3b\u52a8\u8bf7\u6c42\u3002</li> </ul> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5ea6\u91cf\u89c4\u8303.</p>"},{"location":"docs/concepts/signals/traces/","title":"Traces","text":"<p>Traces\u5411\u6211\u4eec\u5c55\u793a\u4e86\u5f53\u5411\u5e94\u7528\u7a0b\u5e8f\u53d1\u51fa\u8bf7\u6c42\u65f6\u53d1\u751f\u7684\u60c5\u51b5\u3002 \u65e0\u8bba\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u662f\u5177\u6709\u5355\u4e2a\u6570\u636e\u5e93\u7684\u5355\u4f53\u8fd8\u662f\u590d\u6742\u7684\u670d\u52a1\u7f51\u683c\uff0c\u8ddf\u8e2a\u5bf9\u4e8e\u7406\u89e3\u8bf7\u6c42\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\u7684\u5b8c\u6574\u201c\u8def\u5f84\u201d\u90fd\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002</p> <p>\u8003\u8651\u4ee5\u4e0b\u8ddf\u8e2a\u4e09\u4e2a\u5de5\u4f5c\u5355\u5143\u7684\u793a\u4f8b:</p> <pre><code>{\n\"name\": \"Hello-Greetings\",\n\"context\": {\n\"trace_id\": \"0x5b8aa5a2d2c872e8321cf37308d69df2\",\n\"span_id\": \"0x5fb397be34d26b51\",\n},\n\"parent_id\": \"0x051581bf3cb55c13\",\n\"start_time\": \"2022-04-29T18:52:58.114304Z\",\n\"end_time\": \"2022-04-29T22:52:58.114561Z\",\n\"attributes\": {\n\"http.route\": \"some_route1\"\n},\n\"events\": [\n{\n\"name\": \"hey there!\",\n\"timestamp\": \"2022-04-29T18:52:58.114561Z\",\n\"attributes\": {\n\"event_attributes\": 1\n}\n},\n{\n\"name\": \"bye now!\",\n\"timestamp\": \"2022-04-29T18:52:58.114585Z\",\n\"attributes\": {\n\"event_attributes\": 1\n}\n}\n],\n}\n{\n\"name\": \"Hello-Salutations\",\n\"context\": {\n\"trace_id\": \"0x5b8aa5a2d2c872e8321cf37308d69df2\",\n\"span_id\": \"0x93564f51e1abe1c2\",\n},\n\"parent_id\": \"0x051581bf3cb55c13\",\n\"start_time\": \"2022-04-29T18:52:58.114492Z\",\n\"end_time\": \"2022-04-29T18:52:58.114631Z\",\n\"attributes\": {\n\"http.route\": \"some_route2\"\n},\n\"events\": [\n{\n\"name\": \"hey there!\",\n\"timestamp\": \"2022-04-29T18:52:58.114561Z\",\n\"attributes\": {\n\"event_attributes\": 1\n}\n}\n],\n}\n{\n\"name\": \"Hello\",\n\"context\": {\n\"trace_id\": \"0x5b8aa5a2d2c872e8321cf37308d69df2\",\n\"span_id\": \"0x051581bf3cb55c13\",\n},\n\"parent_id\": null,\n\"start_time\": \"2022-04-29T18:52:58.114201Z\",\n\"end_time\": \"2022-04-29T18:52:58.114687Z\",\n\"attributes\": {\n\"http.route\": \"some_route3\"\n},\n\"events\": [\n{\n\"name\": \"Guten Tag!\",\n\"timestamp\": \"2022-04-29T18:52:58.114561Z\",\n\"attributes\": {\n\"event_attributes\": 1\n}\n}\n],\n}\n</code></pre> <p>\u8fd9\u4e2a\u793a\u4f8b\u8ddf\u8e2a\u8f93\u51fa\u6709\u4e09\u4e2a\u4e0d\u540c\u7684\u7c7b\u4f3c\u65e5\u5fd7\u7684\u9879\u76ee\uff0c\u79f0\u4e3aspan\uff0c\u5206\u522b\u547d\u540d\u4e3a\u201cHello-greetings\u201d\u3001\u201cHello-salutations\u201d\u548c\u201cHello\u201d\u3002 \u56e0\u4e3a\u6bcf\u4e2a\u8bf7\u6c42\u7684\u4e0a\u4e0b\u6587\u90fd\u6709\u76f8\u540c\u7684\u201ctrace_id\u201d\uff0c\u6240\u4ee5\u5b83\u4eec\u88ab\u8ba4\u4e3a\u662f\u540c\u4e00\u4e2aTrace\u7684\u4e00\u90e8\u5206\u3002</p> <p>\u60a8\u5c06\u6ce8\u610f\u5230\u7684\u53e6\u4e00\u4ef6\u4e8b\u662f\uff0c\u8fd9\u4e2a\u793a\u4f8bTrace\u7684\u6bcf\u4e2aSpan\u770b\u8d77\u6765\u90fd\u50cf\u4e00\u4e2a\u7ed3\u6784\u5316\u65e5\u5fd7\u3002 \u90a3\u662f\u56e0\u4e3a\u5b83\u786e\u5b9e\u662f! \u8003\u8651trace\u7684\u4e00\u79cd\u65b9\u6cd5\u662f\uff0c\u5b83\u4eec\u662f\u5305\u542b\u4e0a\u4e0b\u6587\u3001\u76f8\u5173\u6027\u3001\u5c42\u6b21\u7ed3\u6784\u7b49\u5185\u5bb9\u7684\u7ed3\u6784\u5316\u65e5\u5fd7\u7684\u96c6\u5408\u3002 \u4f46\u662f\uff0c\u8fd9\u4e9b\u201c\u7ed3\u6784\u5316\u65e5\u5fd7\u201d\u53ef\u4ee5\u6765\u81ea\u4e0d\u540c\u7684\u8fdb\u7a0b\u3001\u670d\u52a1\u3001\u865a\u62df\u673a\u3001\u6570\u636e\u4e2d\u5fc3\u7b49\u3002 \u8fd9\u4f7f\u5f97\u8ddf\u8e2a\u80fd\u591f\u8868\u793a\u4efb\u4f55\u7cfb\u7edf\u7684\u7aef\u5230\u7aef\u89c6\u56fe\u3002</p> <p>\u4e3a\u4e86\u7406\u89e3\u5f00\u653e\u9065\u6d4b\u4e2d\u7684\u8ddf\u8e2a\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff0c\u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u5c06\u5728\u68c0\u6d4b\u4ee3\u7801\u4e2d\u53d1\u6325\u4f5c\u7528\u7684\u7ec4\u4ef6\u5217\u8868:</p> <ul> <li>Tracer</li> <li>Tracer Provider</li> <li>Trace Exporter</li> <li>Trace Context</li> </ul>"},{"location":"docs/concepts/signals/traces/#tracer-provider","title":"Tracer Provider","text":"<p>Tracer Provider(\u6709\u65f6\u79f0\u4e3a\u201cTracerProvider\u201d)\u662f\u201cTracer\u201d\u7684\u5de5\u5382\u3002 \u5728\u5927\u591a\u6570\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u8ddf\u8e2a\u7a0b\u5e8f\u63d0\u4f9b\u7a0b\u5e8f\u521d\u59cb\u5316\u4e00\u6b21\uff0c\u5176\u751f\u547d\u5468\u671f\u4e0e\u5e94\u7528\u7a0b\u5e8f\u7684\u751f\u547d\u5468\u671f\u76f8\u5339\u914d\u3002 \u8ddf\u8e2a\u7a0b\u5e8f\u63d0\u4f9b\u7a0b\u5e8f\u521d\u59cb\u5316\u8fd8\u5305\u62ec\u8d44\u6e90\u548c\u5bfc\u51fa\u7a0b\u5e8f\u521d\u59cb\u5316\u3002 \u8fd9\u901a\u5e38\u662f\u4f7f\u7528OpenTelemetry\u8fdb\u884c\u8ddf\u8e2a\u7684\u7b2c\u4e00\u6b65\u3002 \u5728\u67d0\u4e9b\u8bed\u8a00sdk\u4e2d\uff0c\u5df2\u7ecf\u4e3a\u60a8\u521d\u59cb\u5316\u4e86\u5168\u5c40\u8ddf\u8e2a\u7a0b\u5e8f\u63d0\u4f9b\u7a0b\u5e8f\u3002</p>"},{"location":"docs/concepts/signals/traces/#tracer","title":"Tracer","text":"<p>\u8ddf\u8e2a\u5668\u521b\u5efa\u7684\u8303\u56f4\u5305\u542b\u6709\u5173\u7ed9\u5b9a\u64cd\u4f5c(\u4f8b\u5982\u670d\u52a1\u4e2d\u7684\u8bf7\u6c42)\u6b63\u5728\u53d1\u751f\u7684\u4e8b\u60c5\u7684\u66f4\u591a\u4fe1\u606f\u3002 \u8ddf\u8e2a\u7a0b\u5e8f\u662f\u4ece\u8ddf\u8e2a\u7a0b\u5e8f\u63d0\u4f9b\u7a0b\u5e8f\u521b\u5efa\u7684\u3002</p>"},{"location":"docs/concepts/signals/traces/#trace-exporters","title":"Trace Exporters","text":"<p>\u8ddf\u8e2a\u51fa\u53e3\u5546\u5c06\u8ddf\u8e2a\u53d1\u9001\u7ed9\u6d88\u8d39\u8005\u3002 \u8fd9\u4e2a\u6d88\u8d39\u8005\u53ef\u4ee5\u662f\u8c03\u8bd5\u548c\u5f00\u53d1\u65f6\u95f4\u7684\u6807\u51c6\u8f93\u51fa\u3001OpenTelemetry Collector\uff0c\u6216\u8005\u60a8\u9009\u62e9\u7684\u4efb\u4f55\u5f00\u6e90\u6216\u4f9b\u5e94\u5546\u540e\u7aef\u3002</p>"},{"location":"docs/concepts/signals/traces/#context-propagation","title":"Context Propagation","text":"<p>\u4e0a\u4e0b\u6587\u4f20\u64ad\u662f\u652f\u6301\u5206\u5e03\u5f0f\u8ddf\u8e2a\u7684\u6838\u5fc3\u6982\u5ff5\u3002 \u4f7f\u7528\u4e0a\u4e0b\u6587\u4f20\u64ad\uff0c\u65e0\u8bba\u5728\u54ea\u91cc\u751f\u6210span\uff0c\u90fd\u53ef\u4ee5\u5c06span\u76f8\u4e92\u5173\u8054\u5e76\u7ec4\u88c5\u5230\u8ddf\u8e2a\u4e2d\u3002 \u6211\u4eec\u901a\u8fc7\u4e24\u4e2a\u5b50\u6982\u5ff5\u5b9a\u4e49\u4e0a\u4e0b\u6587\u4f20\u64ad:\u4e0a\u4e0b\u6587\u548c\u4f20\u64ad\u3002</p> <p>\u4e0a\u4e0b\u6587 \u662f\u4e00\u4e2a\u5bf9\u8c61\uff0c\u5b83\u5305\u542b\u53d1\u9001\u548c\u63a5\u6536\u670d\u52a1\u7684\u4fe1\u606f\uff0c\u4ee5\u4fbf\u5c06\u4e00\u4e2a\u8de8\u5ea6\u4e0e\u53e6\u4e00\u4e2a\u8de8\u5ea6\u76f8\u5173\u8054\uff0c\u5e76\u5c06\u5176\u4e0e\u6574\u4e2a\u8ddf\u8e2a\u76f8\u5173\u8054\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u670d\u52a1A\u8c03\u7528\u670d\u52a1B\uff0c\u90a3\u4e48\u6765\u81ea\u670d\u52a1A\u7684\u4e00\u4e2aspan(\u5176ID\u5728\u4e0a\u4e0b\u6587\u4e2d)\u5c06\u88ab\u7528\u4f5c\u5728\u670d\u52a1B\u4e2d\u521b\u5efa\u7684\u4e0b\u4e00\u4e2aspan\u7684\u7236span\u3002</p> <p>\u4f20\u64ad \u662f\u5728\u670d\u52a1\u548c\u8fdb\u7a0b\u4e4b\u95f4\u79fb\u52a8\u4e0a\u4e0b\u6587\u7684\u673a\u5236\u3002 \u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u5b83\u7ec4\u88c5\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u8ddf\u8e2a\u3002 \u5b83\u5e8f\u5217\u5316\u6216\u53cd\u5e8f\u5217\u5316Span Context\uff0c\u5e76\u63d0\u4f9b\u8981\u4ece\u4e00\u4e2a\u670d\u52a1\u4f20\u64ad\u5230\u53e6\u4e00\u4e2a\u670d\u52a1\u7684\u76f8\u5173\u8ddf\u8e2a\u4fe1\u606f\u3002 \u6211\u4eec\u73b0\u5728\u6709\u4e86\u6211\u4eec\u6240\u8bf4\u7684: \u8ddf\u8e2a\u4e0a\u4e0b\u6587 \u3002</p> <p>\u4e0a\u4e0b\u6587\u662f\u4e00\u4e2a\u62bd\u8c61\u7684\u6982\u5ff5\u2014\u2014\u5b83\u9700\u8981\u4e00\u4e2a\u5177\u4f53\u7684\u5b9e\u73b0\u624d\u80fd\u771f\u6b63\u6709\u7528\u3002 OpenTelemetry\u652f\u6301\u51e0\u79cd\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u683c\u5f0f\u3002 OpenTelemetry\u8ddf\u8e2a\u4e2d\u4f7f\u7528\u7684\u9ed8\u8ba4\u683c\u5f0f\u662fW3C  <code>TraceContext</code>\u3002 \u6bcf\u4e2aContext\u5bf9\u8c61\u90fd\u4e0e\u4e00\u4e2a\u8de8\u5ea6\u76f8\u5173\u8054\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u8de8\u5ea6\u4e0a\u8fdb\u884c\u8bbf\u95ee\u3002 \u53c2\u89c1Span Context\u3002</p> <p>\u901a\u8fc7\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u4f20\u64ad\uff0c\u60a8\u73b0\u5728\u53ef\u4ee5\u7ec4\u88c5\u8ddf\u8e2a\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u8ddf\u8e2a\u89c4\u8303</p>"},{"location":"docs/concepts/signals/traces/#spans","title":"Spans","text":"<p>span\u8868\u793a\u4e00\u4e2a\u5de5\u4f5c\u6216\u64cd\u4f5c\u5355\u5143\u3002 \u8de8\u5ea6\u662ftrace\u7684\u6784\u5efa\u5757\u3002 \u5728OpenTelemetry\u4e2d\uff0c\u5b83\u4eec\u5305\u62ec\u4ee5\u4e0b\u4fe1\u606f:</p> <ul> <li>Name</li> <li>Parent span ID (empty for root spans)</li> <li>Start and End Timestamps</li> <li>Span Context</li> <li>Attributes</li> <li>Span Events</li> <li>Span Links</li> <li>Span Status</li> </ul> <p>Sample span:</p> <pre><code>{\n\"trace_id\": \"7bba9f33312b3dbb8b2c2c62bb7abe2d\",\n\"parent_id\": \"\",\n\"span_id\": \"086e83747d0e381e\",\n\"name\": \"/v1/sys/health\",\n\"start_time\": \"2021-10-22 16:04:01.209458162 +0000 UTC\",\n\"end_time\": \"2021-10-22 16:04:01.209514132 +0000 UTC\",\n\"status_code\": \"STATUS_CODE_OK\",\n\"status_message\": \"\",\n\"attributes\": {\n\"net.transport\": \"IP.TCP\",\n\"net.peer.ip\": \"172.17.0.1\",\n\"net.peer.port\": \"51820\",\n\"net.host.ip\": \"10.177.2.152\",\n\"net.host.port\": \"26040\",\n\"http.method\": \"GET\",\n\"http.target\": \"/v1/sys/health\",\n\"http.server_name\": \"mortar-gateway\",\n\"http.route\": \"/v1/sys/health\",\n\"http.user_agent\": \"Consul Health Check\",\n\"http.scheme\": \"http\",\n\"http.host\": \"10.177.2.152:26040\",\n\"http.flavor\": \"1.1\"\n},\n\"events\": [\n{\n\"name\": \"\",\n\"message\": \"OK\",\n\"timestamp\": \"2021-10-22 16:04:01.209512872 +0000 UTC\"\n}\n]\n}\n</code></pre> <p>span\u53ef\u4ee5\u5d4c\u5957\uff0c\u6b63\u5982\u7236span ID\u7684\u5b58\u5728\u6240\u6697\u793a\u7684\u90a3\u6837:\u5b50span\u8868\u793a\u5b50\u64cd\u4f5c\u3002 \u8fd9\u5141\u8bb8span\u66f4\u51c6\u786e\u5730\u6355\u83b7\u5e94\u7528\u7a0b\u5e8f\u4e2d\u5b8c\u6210\u7684\u5de5\u4f5c\u3002</p>"},{"location":"docs/concepts/signals/traces/#span","title":"Span \u4e0a\u4e0b\u6587","text":"<p>Span context\u662f\u4e00\u4e2a\u4e0d\u53ef\u53d8\u5bf9\u8c61\uff0c\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9:</p> <ul> <li>Trace ID\u8868\u793a\u8be5span\u662f\u5176\u4e2d\u4e00\u90e8\u5206\u7684\u8ddf\u8e2a</li> <li>span\u7684span ID</li> <li>\u8ddf\u8e2a\u6807\u5fd7\uff0c\u4e00\u79cd\u4e8c\u8fdb\u5236\u7f16\u7801\uff0c\u5305\u542b\u6709\u5173\u8ddf\u8e2a\u7684\u4fe1\u606f</li> <li>\u8ddf\u8e2a\u72b6\u6001\uff0c\u53ef\u4ee5\u643a\u5e26\u7279\u5b9a\u4e8e\u4f9b\u5e94\u5546\u7684\u8ddf\u8e2a\u4fe1\u606f\u7684\u952e\u503c\u5bf9\u5217\u8868</li> </ul> <p>Span\u4e0a\u4e0b\u6587\u662fSpan\u7684\u4e00\u90e8\u5206\uff0c\u5b83\u4e0e\u5206\u5e03\u5f0f\u4e0a\u4e0b\u6587\u548c\u5305\u88b1\u4e00\u8d77\u88ab\u5e8f\u5217\u5316\u548c\u4f20\u64ad\u3002</p> <p>\u56e0\u4e3aSpan Context\u5305\u542bTrace ID\uff0c\u6240\u4ee5\u5728\u521b\u5efaSpan Links\u65f6\u4f7f\u7528\u5b83\u3002</p>"},{"location":"docs/concepts/signals/traces/#_1","title":"\u5c5e\u6027","text":"<p>\u5c5e\u6027\u662f\u5305\u542b\u5143\u6570\u636e\u7684\u952e\u503c\u5bf9\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u5143\u6570\u636e\u5bf9Span\u8fdb\u884c\u6ce8\u91ca\uff0c\u4ee5\u643a\u5e26\u6709\u5173\u5b83\u6b63\u5728\u8ddf\u8e2a\u7684\u64cd\u4f5c\u7684\u4fe1\u606f\u3002</p> <p>\u4f8b\u5982\uff0c\u5982\u679cspan\u8ddf\u8e2a\u5728\u7535\u5b50\u5546\u52a1\u7cfb\u7edf\u4e2d\u5411\u7528\u6237\u7684\u8d2d\u7269\u8f66\u4e2d\u6dfb\u52a0\u5546\u54c1\u7684\u64cd\u4f5c\uff0c\u5219\u53ef\u4ee5\u6355\u83b7\u7528\u6237\u7684ID\u3001\u8981\u6dfb\u52a0\u5230\u8d2d\u7269\u8f66\u4e2d\u7684\u5546\u54c1\u7684ID\u548c\u8d2d\u7269\u8f66ID\u3002</p> <p>\u6bcf\u79cd\u8bed\u8a00SDK\u5b9e\u73b0\u7684\u5c5e\u6027\u6709\u4ee5\u4e0b\u89c4\u5219:</p> <ul> <li>\u952e\u5fc5\u987b\u4e3a\u975e\u7a7a\u5b57\u7b26\u4e32\u503c</li> <li>\u53d6\u503c\u5fc5\u987b\u4e3a\u975e\u7a7a\u5b57\u7b26\u4e32\u3001\u5e03\u5c14\u503c\u3001\u6d6e\u70b9\u6570\u3001\u6574\u6570\u6216\u8fd9\u4e9b\u503c\u7684\u6570\u7ec4</li> </ul> <p>\u6b64\u5916\uff0c\u8fd8\u6709\u8bed\u4e49\u5c5e\u6027\uff0c\u5b83\u4eec\u662f\u5df2\u77e5\u7684\u5143\u6570\u636e\u547d\u540d\u7ea6\u5b9a\uff0c\u901a\u5e38\u51fa\u73b0\u5728\u516c\u5171\u64cd\u4f5c\u4e2d\u3002 \u5c3d\u53ef\u80fd\u4f7f\u7528\u8bed\u4e49\u5c5e\u6027\u547d\u540d\u662f\u5f88\u6709\u5e2e\u52a9\u7684\uff0c\u8fd9\u6837\u53ef\u4ee5\u8de8\u7cfb\u7edf\u6807\u51c6\u5316\u5e38\u89c1\u7c7b\u578b\u7684\u5143\u6570\u636e\u3002</p>"},{"location":"docs/concepts/signals/traces/#span_1","title":"Span \u4e8b\u4ef6","text":"<p>\u53ef\u4ee5\u5c06Span\u4e8b\u4ef6\u770b\u4f5c\u662fSpan\u4e0a\u7684\u7ed3\u6784\u5316\u65e5\u5fd7\u6d88\u606f(\u6216\u6ce8\u91ca)\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u793aSpan\u6301\u7eed\u671f\u95f4\u6709\u610f\u4e49\u7684\u5355\u4e00\u65f6\u95f4\u70b9\u3002</p> <p>\u4f8b\u5982\uff0c\u8003\u8651web\u6d4f\u89c8\u5668\u4e2d\u7684\u4e24\u79cd\u573a\u666f:</p> <ol> <li>\u8ddf\u8e2a\u9875\u9762\u52a0\u8f7d</li> <li>\u8868\u793a\u9875\u9762\u4f55\u65f6\u5177\u6709\u4ea4\u4e92\u6027</li> </ol> <p>Span\u6700\u9002\u5408\u7528\u4e8e\u7b2c\u4e00\u79cd\u573a\u666f\uff0c\u56e0\u4e3a\u5b83\u662f\u4e00\u4e2a\u6709\u5f00\u59cb\u548c\u7ed3\u675f\u7684\u64cd\u4f5c\u3002</p> <p>Span Event\u6700\u9002\u5408\u7528\u4e8e\u8ddf\u8e2a\u7b2c\u4e8c\u4e2a\u573a\u666f\uff0c\u56e0\u4e3a\u5b83\u4ee3\u8868\u4e86\u4e00\u4e2a\u6709\u610f\u4e49\u7684\u3001\u5947\u5f02\u7684\u65f6\u95f4\u70b9\u3002</p>"},{"location":"docs/concepts/signals/traces/#span_2","title":"Span \u94fe\u63a5","text":"<p>\u5b58\u5728\u94fe\u63a5\uff0c\u4ee5\u4fbf\u60a8\u53ef\u4ee5\u5c06\u4e00\u4e2a\u8de8\u5ea6\u4e0e\u4e00\u4e2a\u6216\u591a\u4e2a\u8de8\u5ea6\u5173\u8054\u8d77\u6765\uff0c\u8fd9\u610f\u5473\u7740\u5b58\u5728\u56e0\u679c\u5173\u7cfb\u3002 \u4f8b\u5982\uff0c\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5176\u4e2d\u4e00\u4e9b\u64cd\u4f5c\u7531\u8ddf\u8e2a\u5668\u8ddf\u8e2a\u3002</p> <p>\u4e3a\u4e86\u54cd\u5e94\u5176\u4e2d\u7684\u4e00\u4e9b\u64cd\u4f5c\uff0c\u4e00\u4e2a\u989d\u5916\u7684\u64cd\u4f5c\u6392\u961f\u7b49\u5f85\u6267\u884c\uff0c\u4f46\u662f\u5b83\u7684\u6267\u884c\u662f\u5f02\u6b65\u7684\u3002 \u6211\u4eec\u4e5f\u53ef\u4ee5\u7528trace\u6765\u8ddf\u8e2a\u8fd9\u4e2a\u540e\u7eed\u64cd\u4f5c\u3002</p> <p>\u6211\u4eec\u5e0c\u671b\u5c06\u540e\u7eed\u64cd\u4f5c\u7684\u8ddf\u8e2a\u4e0e\u7b2c\u4e00\u4e2a\u8ddf\u8e2a\u76f8\u5173\u8054\uff0c\u4f46\u662f\u6211\u4eec\u65e0\u6cd5\u9884\u6d4b\u540e\u7eed\u64cd\u4f5c\u4f55\u65f6\u5f00\u59cb\u3002 \u6211\u4eec\u9700\u8981\u5c06\u8fd9\u4e24\u6761\u8f68\u8ff9\u5173\u8054\u8d77\u6765\uff0c\u56e0\u6b64\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a\u8de8\u5ea6\u94fe\u63a5\u3002</p> <p>\u60a8\u53ef\u4ee5\u5c06\u7b2c\u4e00\u4e2a\u8ddf\u8e2a\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u8de8\u5ea6\u94fe\u63a5\u5230\u7b2c\u4e8c\u4e2a\u8ddf\u8e2a\u4e2d\u7684\u7b2c\u4e00\u4e2a\u8de8\u5ea6\u3002 \u73b0\u5728\uff0c\u5b83\u4eec\u4e92\u4e3a\u56e0\u679c\u5173\u7cfb\u3002</p> <p>\u94fe\u63a5\u662f\u53ef\u9009\u7684\uff0c\u4f46\u5b83\u662f\u5c06\u8ddf\u8e2a\u8de8\u5ea6\u76f8\u4e92\u5173\u8054\u7684\u597d\u65b9\u6cd5\u3002</p>"},{"location":"docs/concepts/signals/traces/#span_3","title":"Span \u72b6\u6001","text":"<p>\u4e00\u4e2a\u72b6\u6001\u5c06\u88ab\u9644\u52a0\u5230\u4e00\u4e2aSpan\u4e0a\u3002 \u901a\u5e38\uff0c\u5f53\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u4e2d\u5b58\u5728\u5df2\u77e5\u9519\u8bef(\u4f8b\u5982\u5f02\u5e38)\u65f6\uff0c\u60a8\u5c06\u8bbe\u7f6e\u4e00\u4e2aspan\u72b6\u6001\u3002 Span Status\u5c06\u88ab\u6807\u8bb0\u4e3a\u4ee5\u4e0b\u503c\u4e4b\u4e00:</p> <ul> <li><code>Unset</code></li> <li><code>Ok</code></li> <li><code>Error</code></li> </ul> <p>\u5904\u7406\u5f02\u5e38\u65f6\uff0c\u53ef\u4ee5\u5c06Span\u72b6\u6001\u8bbe\u7f6e\u4e3aError\u3002 \u5426\u5219\uff0cSpan\u72b6\u6001\u4e3aUnset\u72b6\u6001\u3002 \u901a\u8fc7\u5c06Span\u72b6\u6001\u8bbe\u7f6e\u4e3aUnset\uff0c\u5904\u7406Span\u7684\u540e\u7aef\u73b0\u5728\u53ef\u4ee5\u5206\u914d\u6700\u7ec8\u72b6\u6001\u3002</p>"},{"location":"docs/concepts/signals/traces/#span_4","title":"Span \u7c7b\u578b","text":"<p>\u5f53\u521b\u5efa\u4e00\u4e2aspan\u65f6\uff0c\u5b83\u662f\u201cClient\u201d\u3001\u201cServer\u201d\u3001\u201cInternal\u201d\u3001\u201cProducer\u201d\u6216\u201cConsumer\u201d\u4e2d\u7684\u4e00\u4e2a\u3002 \u6b64\u8de8\u5ea6\u7c7b\u578b\u4e3a\u8ddf\u8e2a\u540e\u7aef\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u7ec4\u88c5\u8ddf\u8e2a\u7684\u63d0\u793a\u3002 \u6839\u636eOpenTelemetry\u89c4\u8303\uff0c\u670d\u52a1\u5668\u8de8\u5ea6\u7684\u7236\u8282\u70b9\u901a\u5e38\u662f\u8fdc\u7a0b\u5ba2\u6237\u7aef\u8de8\u5ea6\uff0c\u800c\u5ba2\u6237\u7aef\u8de8\u5ea6\u7684\u5b50\u8282\u70b9\u901a\u5e38\u662f\u670d\u52a1\u5668\u8de8\u5ea6\u3002 \u7c7b\u4f3c\u5730\uff0c\u6d88\u8d39\u8005span\u7684\u7236\u7c7b\u59cb\u7ec8\u662f\u751f\u4ea7\u8005\uff0c\u751f\u4ea7\u8005span\u7684\u5b50\u7c7b\u59cb\u7ec8\u662f\u6d88\u8d39\u8005\u3002 \u5982\u679c\u6ca1\u6709\u63d0\u4f9b\uff0c\u5219\u5047\u5b9aspan\u7c7b\u578b\u662f\u5185\u90e8\u7684\u3002</p> <p>\u6709\u5173SpanKind\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1SpanKind.</p>"},{"location":"docs/concepts/signals/traces/#_2","title":"\u5ba2\u6237\u7aef","text":"<p>\u5ba2\u6237\u7aef\u8303\u56f4\u8868\u793a\u540c\u6b65\u4f20\u51fa\u8fdc\u7a0b\u8c03\u7528\uff0c\u4f8b\u5982\u4f20\u51faHTTP\u8bf7\u6c42\u6216\u6570\u636e\u5e93\u8c03\u7528\u3002 \u8bf7\u6ce8\u610f\uff0c\u5728\u8fd9\u4e2a\u4e0a\u4e0b\u6587\u4e2d\uff0c\u201c\u540c\u6b65\u201d\u4e0d\u662f\u6307\u201casync/await\u201d\uff0c\u800c\u662f\u6307\u5b83\u6ca1\u6709\u6392\u961f\u7b49\u5f85\u7a0d\u540e\u7684\u5904\u7406\u3002</p>"},{"location":"docs/concepts/signals/traces/#_3","title":"\u670d\u52a1\u5668","text":"<p>\u670d\u52a1\u5668\u8303\u56f4\u8868\u793a\u540c\u6b65\u4f20\u5165\u7684\u8fdc\u7a0b\u8c03\u7528\uff0c\u4f8b\u5982\u4f20\u5165\u7684HTTP\u8bf7\u6c42\u6216\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\u3002</p>"},{"location":"docs/concepts/signals/traces/#internal","title":"Internal","text":"<p>\u5185\u90e8Span\u8868\u793a\u4e0d\u8de8\u8d8a\u6d41\u7a0b\u8fb9\u754c\u7684\u64cd\u4f5c\u3002 \u63d2\u88c5\u51fd\u6570\u8c03\u7528\u6216\u5feb\u901f\u4e2d\u95f4\u4ef6\u4e4b\u7c7b\u7684\u4e8b\u60c5\u53ef\u80fd\u4f1a\u4f7f\u7528\u5185\u90e8Span\u3002</p>"},{"location":"docs/concepts/signals/traces/#producer","title":"Producer","text":"<p>\u751f\u4ea7\u8005Span\u8868\u793a\u7a0d\u540e\u53ef\u80fd\u5f02\u6b65\u5904\u7406\u7684\u4f5c\u4e1a\u7684\u521b\u5efa\u3002 \u5b83\u53ef\u4ee5\u662f\u8fdc\u7a0b\u4f5c\u4e1a\uff0c\u4f8b\u5982\u63d2\u5165\u4f5c\u4e1a\u961f\u5217\u7684\u4f5c\u4e1a\uff0c\u4e5f\u53ef\u4ee5\u662f\u7531\u4e8b\u4ef6\u4fa6\u542c\u5668\u5904\u7406\u7684\u672c\u5730\u4f5c\u4e1a\u3002</p>"},{"location":"docs/concepts/signals/traces/#consumer","title":"Consumer","text":"<p>\u6d88\u8d39\u8005Span\u8868\u793a\u7531\u751f\u4ea7\u8005\u521b\u5efa\u7684\u4f5c\u4e1a\u7684\u5904\u7406\uff0c\u5e76\u4e14\u53ef\u80fd\u5728\u751f\u4ea7\u8005Span\u5df2\u7ecf\u7ed3\u675f\u5f88\u4e45\u4e4b\u540e\u624d\u5f00\u59cb\u3002</p>"},{"location":"docs/demo/","title":"OpenTelemetry \u6f14\u793a\u6587\u6863","text":"<p>\u6b22\u8fce\u6765\u5230OpenTelemetry Demo\u6587\u6863\uff0c\u5b83\u6db5\u76d6\u4e86\u5982\u4f55\u5b89\u88c5\u548c\u8fd0\u884c\u6f14\u793a\uff0c\u4ee5\u53ca\u4e00\u4e9b\u53ef\u4ee5\u7528\u6765\u67e5\u770bOpenTelemetry\u7684\u573a\u666f\u3002</p>"},{"location":"docs/demo/#demo","title":"\u8fd0\u884cDemo","text":"<p>\u60f3\u8981\u90e8\u7f72\u6f14\u793a\u5e76\u67e5\u770b\u5b9e\u9645\u60c5\u51b5\u5417?\u4ece\u8fd9\u91cc\u5f00\u59cb\u3002</p> <ul> <li>Docker</li> <li>Kubernetes</li> </ul>"},{"location":"docs/demo/#_1","title":"\u8bed\u8a00\u7279\u6027\u53c2\u8003","text":"<p>\u60f3\u8981\u4e86\u89e3\u7279\u5b9a\u8bed\u8a00\u7684\u68c0\u6d4b\u662f\u5982\u4f55\u5de5\u4f5c\u7684?\u4ece\u8fd9\u91cc\u5f00\u59cb\u3002</p> Language Automatic Instrumentation Instrumentation Libraries Manual Instrumentation .NET Cart Service Cart Service C++ Currency Service Erlang/Elixir Feature Flag Service Feature Flag Service Go Accounting Service, Checkout Service, Product Catalog Service Checkout Service, Product Catalog Service Java Ad Service Ad Service JavaScript Frontend Frontend, Payment Service Kotlin Fraud Detection Service PHP Quote Service Quote Service Python Recommendation Service Recommendation Service Ruby Email Service Email Service Rust Shipping Service Shipping Service"},{"location":"docs/demo/#_2","title":"\u670d\u52a1\u6587\u6863","text":"<p>Specific information about how OpenTelemetry is deployed in each service can be found here:</p> <ul> <li>Ad Service</li> <li>Cart Service</li> <li>Checkout Service</li> <li>Email Service</li> <li>Feature Flag Service</li> <li>Frontend</li> <li>Load Generator</li> <li>Payment Service</li> <li>Product Catalog Service</li> <li>Quote Service</li> <li>Recommendation Service</li> <li>Shipping Service</li> </ul>"},{"location":"docs/demo/#_3","title":"\u573a\u666f","text":"<p>How can you solve problems with OpenTelemetry? These scenarios walk you through some pre-configured problems and show you how to interpret OpenTelemetry data to solve them.</p> <p>We'll be adding more scenarios over time.</p> <ul> <li>Generate a Product Catalog error for <code>GetProduct</code> requests   with product id: <code>OLJCESPC7Z</code> using the Feature Flag service</li> <li>Discover a memory leak and diagnose it using metrics and traces.   Read more</li> </ul>"},{"location":"docs/demo/#_4","title":"\u53c2\u8003","text":"<p>Project reference documentation, like requirements and feature matrices.</p> <ul> <li>Architecture</li> <li>Development</li> <li>Feature Flags Reference</li> <li>Metric Feature Matrix</li> <li>Requirements</li> <li>Screenshots</li> <li>Service Roles Table</li> <li>Span Attributes Reference</li> <li>Tests</li> <li>Trace Feature Matrix</li> </ul>"},{"location":"docs/demo/architecture/","title":"\u6f14\u793a\u67b6\u6784","text":"<p>OpenTelemetry\u6f14\u793a \u662f\u7531\u7528\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7f16\u5199\u7684\u5fae\u670d\u52a1\u7ec4\u6210\u7684\uff0c\u8fd9\u4e9b\u5fae\u670d\u52a1\u901a\u8fc7gRPC\u548cHTTP\u76f8\u4e92\u901a\u4fe1;\u4ee5\u53ca\u4f7f\u7528\u8757\u866b\u4f2a\u9020\u7528\u6237\u6d41\u91cf\u7684\u8d1f\u8f7d\u751f\u6210\u5668\u3002</p> <pre><code>graph TD\nsubgraph Service Diagram\naccountingservice(Accounting Service):::golang\nadservice(Ad Service):::java\ncache[(Cache&lt;br/&gt;&amp;#40redis&amp;#41)]\ncartservice(Cart Service):::dotnet\ncheckoutservice(Checkout Service):::golang\ncurrencyservice(Currency Service):::cpp\nemailservice(Email Service):::ruby\nfrauddetectionservice(Fraud Detection Service):::kotlin\nfrontend(Frontend):::typescript\nfrontendproxy(Frontend Proxy &lt;br/&gt;&amp;#40Envoy&amp;#41):::cpp\nloadgenerator([Load Generator]):::python\npaymentservice(Payment Service):::javascript\nproductcatalogservice(Product Catalog Service):::golang\nquoteservice(Quote Service):::php\nrecommendationservice(Recommendation Service):::python\nshippingservice(Shipping Service):::rust\nfeatureflagservice(Feature Flag Service):::erlang\nfeatureflagstore[(Feature Flag Store&lt;br/&gt;&amp;#40PostgreSQL DB&amp;#41)]\nqueue[(queue&lt;br/&gt;&amp;#40Kafka&amp;#41)]\n\nInternet --&gt;|HTTP| frontendproxy\nfrontendproxy --&gt;|HTTP| frontend\nfrontendproxy --&gt;|HTTP| featureflagservice\nloadgenerator --&gt;|HTTP| frontend\n\naccountingservice --&gt;|TCP| queue\n\ncheckoutservice ---&gt;|gRPC| cartservice --&gt; cache\ncheckoutservice ---&gt;|gRPC| productcatalogservice\ncheckoutservice ---&gt;|gRPC| currencyservice\ncheckoutservice ---&gt;|HTTP| emailservice\ncheckoutservice ---&gt;|gRPC| paymentservice\ncheckoutservice --&gt;|gRPC| shippingservice\ncheckoutservice ----&gt;|TCP| queue\n\nfrontend --&gt;|gRPC| adservice\nfrontend --&gt;|gRPC| cartservice\nfrontend --&gt;|gRPC| productcatalogservice\nfrontend --&gt;|gRPC| checkoutservice\nfrontend --&gt;|gRPC| currencyservice\nfrontend --&gt;|gRPC| recommendationservice --&gt;|gRPC| productcatalogservice\nfrontend --&gt;|gRPC| shippingservice --&gt;|HTTP| quoteservice\n\nfrauddetectionservice --&gt;|TCP| queue\n\nadservice --&gt;|gRPC| featureflagservice\n\nproductcatalogservice --&gt;|gRPC| featureflagservice\n\nrecommendationservice --&gt;|gRPC| featureflagservice\n\nshippingservice --&gt;|gRPC| featureflagservice\n\nfeatureflagservice --&gt; featureflagstore\n\nend\n\nclassDef dotnet fill:#178600,color:white;\nclassDef cpp fill:#f34b7d,color:white;\nclassDef erlang fill:#b83998,color:white;\nclassDef golang fill:#00add8,color:black;\nclassDef java fill:#b07219,color:white;\nclassDef javascript fill:#f1e05a,color:black;\nclassDef kotlin fill:#560ba1,color:white;\nclassDef php fill:#4f5d95,color:white;\nclassDef python fill:#3572A5,color:white;\nclassDef ruby fill:#701516,color:white;\nclassDef rust fill:#dea584,color:black;\nclassDef typescript fill:#e98516,color:black;</code></pre> <pre><code>graph TD\nsubgraph Service Legend\n  dotnetsvc(.NET):::dotnet\n  cppsvc(C++):::cpp\n  erlangsvc(Erlang/Elixir):::erlang\n  golangsvc(Go):::golang\n  javasvc(Java):::java\n  javascriptsvc(JavaScript):::javascript\n  kotlinsvc(Kotlin):::kotlin\n  phpsvc(PHP):::php\n  pythonsvc(Python):::python\n  rubysvc(Ruby):::ruby\n  rustsvc(Rust):::rust\n  typescriptsvc(TypeScript):::typescript\nend\n\nclassDef dotnet fill:#178600,color:white;\nclassDef cpp fill:#f34b7d,color:white;\nclassDef erlang fill:#b83998,color:white;\nclassDef golang fill:#00add8,color:black;\nclassDef java fill:#b07219,color:white;\nclassDef javascript fill:#f1e05a,color:black;\nclassDef kotlin fill:#560ba1,color:white;\nclassDef php fill:#4f5d95,color:white;\nclassDef python fill:#3572A5,color:white;\nclassDef ruby fill:#701516,color:white;\nclassDef rust fill:#dea584,color:black;\nclassDef typescript fill:#e98516,color:black;</code></pre> <p>\u6309\u7167\u8fd9\u4e9b\u94fe\u63a5\u67e5\u770b\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f\u7684metric\u548ctrace\u68c0\u6d4b\u7684\u5f53\u524d\u72b6\u6001\u3002</p> <p>\u6536\u96c6\u5668\u5728otelcol-config.yml\u4e2d\u914d\u7f6e\uff0c\u5176\u4ed6\u5bfc\u51fa\u5668\u53ef\u4ee5\u5728\u8fd9\u91cc\u914d\u7f6e\u3002</p> <pre><code>graph TB\nsubgraph tdf[Telemetry Data Flow]\n    subgraph subgraph_padding [ ]\n        style subgraph_padding fill:none,stroke:none;\n        %% padding to stop the titles clashing\n        subgraph od[Open Telemetry Demo]\n        ms(Microservice)\n        end\n\n        ms -.-&gt;|\"OTLP&lt;br/&gt;gRPC\"| oc-grpc\n        ms -.-&gt;|\"OTLP&lt;br/&gt;HTTP POST\"| oc-http\n\n        subgraph oc[OTel Collector]\n            style oc fill:#97aef3,color:black;\n            oc-grpc[/\"OTLP Receiver&lt;br/&gt;listening on&lt;br/&gt;grpc://localhost:4317/\"/]\n            oc-http[/\"OTLP Receiver&lt;br/&gt;listening on &lt;br/&gt;http://localhost:4318/&lt;br/&gt;https://localhost:4318/\"/]\n            oc-proc(Processors)\n            oc-prom[/\"Prometheus Exporter&lt;br/&gt;listening on&lt;br/&gt;http://localhost:9464/\"/]\n            oc-jag[/\"Jaeger Exporter\"/]\n\n            oc-grpc --&gt; oc-proc\n            oc-http --&gt; oc-proc\n\n            oc-proc --&gt; oc-prom\n            oc-proc --&gt; oc-jag\n        end\n\n        oc-prom --&gt;|\"http://localhost:9464/metrics\"| pr-sc\n        oc-jag --&gt;|gRPC| ja-col\n\n        subgraph pr[Prometheus]\n            style pr fill:#e75128,color:black;\n            pr-sc[/\"Prometheus Scraper&lt;br/&gt;polling every 5 seconds\"/]\n            pr-tsdb[(Prometheus TSDB)]\n            pr-http[/\"Prometheus HTTP&lt;br/&gt;listening on&lt;br/&gt;http://localhost:9090\"/]\n\n            pr-sc --&gt; pr-tsdb\n            pr-tsdb --&gt; pr-http\n        end\n\n        pr-b{{\"Browser&lt;br/&gt;Prometheus UI\"}}\n        pr-http ----&gt;|\"http://localhost:9090/graph\"| pr-b\n\n        subgraph ja[Jaeger]\n            style ja fill:#60d0e4,color:black;\n            ja-col[/\"Jaeger Collector&lt;br/&gt;listening on&lt;br/&gt;grpc://jaeger:4317/\"/]\n            ja-db[(Jaeger DB)]\n            ja-http[/\"Jaeger HTTP&lt;br/&gt;listening on&lt;br/&gt;http://localhost:16686\"/]\n\n            ja-col --&gt; ja-db\n            ja-db --&gt; ja-http\n        end\n\n        subgraph gr[Grafana]\n            style gr fill:#f8b91e,color:black;\n            gr-srv[\"Grafana Server\"]\n            gr-http[/\"Grafana HTTP&lt;br/&gt;listening on&lt;br/&gt;http://localhost:3000\"/]\n\n            gr-srv --&gt; gr-http\n        end\n\n        pr-http --&gt; |\"http://localhost:9090/api\"| gr-srv\n        ja-http --&gt; |\"http://localhost:16686/api\"| gr-srv\n\n        ja-b{{\"Browser&lt;br/&gt;Jaeger UI\"}}\n        ja-http ----&gt;|\"http://localhost:16686/search\"| ja-b\n\n        gr-b{{\"Browser&lt;br/&gt;Grafana UI\"}}\n        gr-http --&gt;|\"http://localhost:3000/dashboard\"| gr-b\n    end\nend</code></pre> <p>\u5728<code>/pb/</code>\u76ee\u5f55\u4e2d\u627e\u5230<code>\u534f\u8bae\u7f13\u51b2\u533a\u5b9a\u4e49</code>\u3002</p>"},{"location":"docs/demo/development/","title":"\u5f00\u53d1","text":"<p>Development for this demo requires tooling in several programming languages. Minimum required versions will be noted where possible, but it is recommended to update to the latest version for all tooling. The OpenTelemetry demo team will attempt to keep the services in this repository up to date with the latest version for dependencies and tooling when possible.</p>"},{"location":"docs/demo/development/#generate-protobuf-files","title":"Generate protobuf files","text":"<p>The <code>make generate-protobuf</code> command is provided to generate protobuf files for all services. This can be used to compile code locally (without Docker) and receive hints from IDEs such as IntelliJ or VS Code. It may be necessary to run <code>npm install</code> within the frontend source folder before generating the files.</p>"},{"location":"docs/demo/development/#development-tooling-requirements","title":"Development tooling requirements","text":""},{"location":"docs/demo/development/#net","title":".NET","text":"<ul> <li>.NET 6.0+</li> </ul>"},{"location":"docs/demo/development/#c","title":"C++","text":"<ul> <li>build-essential</li> <li>cmake</li> <li>libcurl4-openssl-dev</li> <li>libprotobuf-dev</li> <li>nlohmann-json3-dev</li> <li>pkg-config</li> <li>protobuf-compiler</li> </ul>"},{"location":"docs/demo/development/#elixir","title":"Elixir","text":"<ul> <li>Erlang/OTP 23+</li> <li>Elixir 1.13+</li> <li>Rebar3 3.20+</li> </ul>"},{"location":"docs/demo/development/#go","title":"Go","text":"<ul> <li>Go 1.19+</li> <li>protoc-gen-go</li> <li>protoc-gen-go-grpc</li> </ul>"},{"location":"docs/demo/development/#java","title":"Java","text":"<ul> <li>JDK 17+</li> <li>Gradle 7+</li> </ul>"},{"location":"docs/demo/development/#javascript","title":"JavaScript","text":"<ul> <li>Node.js 16+</li> </ul>"},{"location":"docs/demo/development/#php","title":"PHP","text":"<ul> <li>PHP 8.1+</li> <li>Composer 2.4+</li> </ul>"},{"location":"docs/demo/development/#python","title":"Python","text":"<ul> <li>Python 3.10</li> <li>grpcio-tools 1.48+</li> </ul>"},{"location":"docs/demo/development/#ruby","title":"Ruby","text":"<ul> <li>Ruby 3.1+</li> </ul>"},{"location":"docs/demo/development/#rust","title":"Rust","text":"<ul> <li>Rust 1.61+</li> <li>protoc 3.21+</li> <li>protobuf-dev</li> </ul>"},{"location":"docs/demo/docker-deployment/","title":"Docker \u90e8\u7f72","text":""},{"location":"docs/demo/docker-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose   v2.0.0+</li> <li>4 GB of RAM for the application</li> </ul>"},{"location":"docs/demo/docker-deployment/#get-and-run-the-demo","title":"Get and run the demo","text":"<ol> <li> <p>Clone the Webstore Demo repository:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-demo.git\n</code></pre> </li> <li> <p>Change to the demo folder:</p> <pre><code>cd opentelemetry-demo/\n</code></pre> </li> <li> <p>Run docker compose1 to start the demo:</p> <pre><code>docker compose up --no-build\n</code></pre> <p>Notes:</p> <ul> <li>The <code>--no-build</code> flag is used to fetch released docker images from   ghcr instead of building from   source. Removing the <code>--no-build</code> command line option will rebuild all   images from source. It may take more than 20 minutes to build if the   flag is omitted.</li> <li>If you're running on Apple Silicon, run <code>docker compose build</code>1 in   order to create local images vs. pulling them from the repository.</li> </ul> </li> </ol>"},{"location":"docs/demo/docker-deployment/#verify-the-webstore-and-telemetry","title":"Verify the Webstore and Telemetry","text":"<p>Once the images are built and containers are started you can access:</p> <ul> <li>Webstore: http://localhost:8080/</li> <li>Grafana: http://localhost:8080/grafana/</li> <li>Feature Flags UI: http://localhost:8080/feature/</li> <li>Load Generator UI: http://localhost:8080/loadgen/</li> <li>Jaeger UI: http://localhost:8080/jaeger/ui/</li> </ul>"},{"location":"docs/demo/docker-deployment/#bring-your-own-backend","title":"Bring your own backend","text":"<p>Likely you want to use the Webstore as a demo application for an observability backend you already have (e.g., an existing instance of Jaeger, Zipkin, or one of the vendor of your choice.</p> <p>OpenTelemetry Collector can be used to export telemetry data to multiple backends. By default, the collector in the demo application will merge the configuration from two files:</p> <ul> <li><code>otelcol-config.yml</code></li> <li><code>otelcol-config-extras.yml</code></li> </ul> <p>To add your backend, open the file src/otelcollector/otelcol-config-extras.yml with an editor.</p> <ul> <li>Start by adding a new exporter. For example, if your backend supports OTLP   over HTTP, add the following:</li> </ul> <pre><code>exporters:\notlphttp/example:\nendpoint: &lt;your-endpoint-url&gt;\n</code></pre> <ul> <li>Then add a new pipeline with your new exporter:</li> </ul> <pre><code>service:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlphttp/example]\n</code></pre> <p>Vendor backends might require you to add additional parameters for authentication, please check their documentation. Some backends require different exporters, you may find them and their documentation available at opentelemetry-collector-contrib/exporter.</p> <p>After updating the <code>otelcol-config-extras.yml</code>, start the demo by running <code>docker compose up</code>1. After a while, you should see the traces flowing into your backend as well.</p> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"docs/demo/feature-flags/","title":"\u529f\u80fd\u6807\u5fd7","text":"<p>This demo comes with several feature flags which can control failure conditions in specific services. By default the flags are disabled. Using the Feature Flags UI http://localhost:8080/feature you will be able to control the status of these feature flags.</p> Feature Flag Service(s) Description <code>adServiceFailure</code> Ad Service Generate an error for <code>GetAds</code> 1/10th of the time <code>cartServiceFailure</code> Cart Service Generate an error for <code>EmptyCart</code> 1/10th of the time <code>productCatalogFailure</code> Product Catalog Generate an error for <code>GetProduct</code> requests with product id: <code>OLJCESPC7Z</code> <code>recommendationCache</code> Recommendation Create a memory leak due to an exponentially growing cache. 1.4x growth, 50% of requests trigger growth."},{"location":"docs/demo/features/","title":"\u6f14\u793a\u529f\u80fd","text":"<ul> <li>Kubernetes: the app is designed to run on   Kubernetes (both locally, as well as on the cloud) using a Helm chart.</li> <li>Docker: this forked sample can also be executed   only with Docker.</li> <li>gRPC: microservices use a high volume of gRPC calls to   communicate to each other.</li> <li>HTTP: microservices use   HTTP where gRPC is unavailable or not well supported.</li> <li>OpenTelemetry Traces: all services are   instrumented using OpenTelemetry available instrumentation libraries.</li> <li>OpenTelemetry Metrics: Select services   are instrumented using OpenTelemetry available instrumentation libraries. More   will be added as the relevant SDKs are released.</li> <li>OpenTelemetry Collector: all services are instrumented   and sending the generated traces and metrics to the OpenTelemetry Collector   via gRPC. The received traces are then exported to the logs and to Jaeger;   received metrics and exemplars1 are exported to logs and Prometheus.</li> <li>Jaeger: all generated traces are being   sent to Jaeger.</li> <li>Synthetic Load Generation: the application demo comes with a background   job that creates realistic usage patterns on the website using   Locust load generator.</li> <li>Prometheus: all generated metrics and exemplars   are scraped by Prometheus.</li> <li>Grafana: all metric dashboards are stored in   Grafana.</li> <li>Envoy: Envoy is used as a reverse proxy for   user-facing web interfaces such as the frontend, load generator, and feature   flag service.</li> </ul> <ol> <li> <p>Only exemplars attached to histograms are currently exported to Prometheus. For details, see Collector issue #18201.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/demo/forking/","title":"\u5206\u53c9\u6f14\u793a\u5b58\u50a8\u5e93","text":"<p>The [demo repository][] is designed to be forked and used as a tool to show off what you are doing with OpenTelemetry.</p> <p>Setting up a fork or a demo usually only requires overriding some environment variables and possibly replacing some container images.</p> <p>Live demos can be added to the demo README.</p>"},{"location":"docs/demo/forking/#suggestions-for-fork-maintainers","title":"Suggestions for Fork Maintainers","text":"<ul> <li>If you'd like to enhance the telemetry data emitted or collected by the demo,   then we strongly encourage you to backport your changes to this repository.   For vendor or implementation specific changes, a strategy of modifying   telemetry in the pipeline via config is preferable to underlying code changes.</li> <li>Extend rather than replace. Adding net-new services that interface with the   existing API is a great way to add vendor-specific or tool-specific features   that can't be accomplished through telemetry modification.</li> <li>To support extensibility, please use repository or facade patterns around   resources like queues, databases, caches, etc. This will allow for different   implementations of these services to be shimmed in for different platforms.</li> <li>Please do not attempt to backport vendor or tool-specific enhancements to this   repository.</li> </ul> <p>If you have any questions or would like to suggest ways that we can make your life easier as a fork maintainer, please open an issue.</p> <p>[demo repository]: {{% param repo %}}</p>"},{"location":"docs/demo/kubernetes-deployment/","title":"Kubernetes \u5f00\u53d1","text":"<p>\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00 \u4e2aOpenTelemetry Demo Helm chart\u6765 \u5e2e\u52a9\u5c06 Demo \u90e8\u7f72\u5230\u73b0\u6709\u7684 Kubernetes \u96c6\u7fa4\u4e0a\u3002</p> <p>Helm\u5fc5\u987b\u5b89\u88c5\u624d\u80fd\u4f7f\u7528\u6d77\u56fe\u3002\u8bf7\u53c2\u8003 Helm \u7684\u6587\u6863\u5f00\u59cb\u3002</p>"},{"location":"docs/demo/kubernetes-deployment/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<ul> <li>Kubernetes 1.23+</li> <li>\u4e3a\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b 4 GB \u7684\u7a7a\u95f2 RAM</li> <li>Helm 3.9+ (\u4ec5\u9002\u7528\u4e8e\u8235\u673a\u7684\u5b89\u88c5\u65b9\u6cd5</li> </ul>"},{"location":"docs/demo/kubernetes-deployment/#helm","title":"\u4f7f\u7528 Helm \u5b89\u88c5(\u63a8\u8350)","text":"<p>\u6dfb\u52a0 OpenTelemetry Helm \u5b58\u50a8\u5e93:</p> <pre><code>helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\n</code></pre> <p>\u8981\u5b89\u88c5\u7248\u672c\u540d\u4e3a my-otel-demo \u7684\u56fe\u8868\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>helm install my-otel-demo open-telemetry/opentelemetry-demo\n</code></pre> <p>Note OpenTelemetry Demo Helm chart 0.11.0 \u6216\u66f4\u9ad8\u7248\u672c\u9700\u8981\u6267\u884c\u4e0b\u9762\u63d0\u5230\u7684\u6240\u6709 \u4f7f\u7528\u65b9\u6cd5\u3002</p>"},{"location":"docs/demo/kubernetes-deployment/#kubectl","title":"\u4f7f\u7528 kubectl \u5b89\u88c5","text":"<p>\u4e0b\u9762\u7684\u547d\u4ee4\u5c06\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f\u5b89\u88c5\u5230 Kubernetes \u96c6\u7fa4\u3002</p> <pre><code>kubectl create namespace otel-demo\nkubectl apply --namespace otel-demo -f https://raw.githubusercontent.com/open-telemetry/opentelemetry-demo/main/kubernetes/opentelemetry-demo.yaml\n</code></pre> <p>Note \u8fd9\u4e9b\u6e05\u5355\u662f\u4ece Helm \u56fe\u8868\u751f\u6210\u7684\uff0c\u63d0\u4f9b\u8fd9\u4e9b\u6e05\u5355\u662f\u4e3a\u4e86\u65b9\u4fbf\u3002\u5efa\u8bae\u4f7f\u7528 Helm \u56fe\u8fdb\u884c\u5b89\u88c5\u3002</p>"},{"location":"docs/demo/kubernetes-deployment/#_2","title":"\u4f7f\u7528\u6f14\u793a","text":"<p>\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f\u5c06\u9700\u8981\u5728 Kubernetes \u96c6\u7fa4\u5916\u90e8\u516c\u5f00\u7684\u670d\u52a1\u624d\u80fd\u4f7f\u7528\u5b83\u4eec\u3002\u60a8\u53ef\u4ee5\u4f7f \u7528<code>kubectl port-forward</code>\u547d\u4ee4\u6216\u901a\u8fc7\u914d\u7f6e\u670d\u52a1\u7c7b\u578b(\u4f8b\u5982:LoadBalancer)\u914d\u7f6e\u53ef\u9009\u90e8\u7f72\u7684 \u5165\u53e3\u8d44\u6e90\uff0c\u5c06\u670d\u52a1\u516c\u5f00\u7ed9\u672c\u5730\u7cfb\u7edf\u3002</p>"},{"location":"docs/demo/kubernetes-deployment/#kubectl_1","title":"\u4f7f\u7528 kubectl \u7aef\u53e3\u8f6c\u53d1\u516c\u5f00\u670d\u52a1","text":"<p>\u8981\u516c\u5f00 frontendproxy \u670d\u52a1\uff0c\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4(\u5c06 <code>my-otel-demo</code> \u66ff\u6362\u4e3a\u76f8\u5e94\u7684 Helm \u56fe\u8868 \u53d1\u5e03\u540d\u79f0):</p> <pre><code>kubectl port-forward svc/my-otel-demo-frontendproxy 8080:8080\n</code></pre> <p>\u4e3a\u4e86\u4ece\u6d4f\u89c8\u5668\u4e2d\u6b63\u786e\u6536\u96c6\u8de8\u5ea6\uff0c\u60a8\u8fd8\u9700\u8981\u516c\u5f00 OpenTelemetry Collector \u7684 OTLP/HTTP \u7aef \u53e3(\u5c06<code>my-otel-demo</code>\u66ff\u6362\u4e3a\u76f8\u5e94\u7684 Helm \u56fe\u8868\u53d1\u5e03\u540d\u79f0):</p> <pre><code>kubectl port-forward svc/my-otel-demo-otelcol 4318:4318\n</code></pre> <p>Note: <code>kubectl port-forward</code>\u5c06\u4ee3\u7406\u8be5\u7aef\u53e3\uff0c\u76f4\u5230\u8fdb\u7a0b\u7ec8\u6b62\u3002\u60a8\u53ef\u80fd\u9700\u8981\u4e3a\u6bcf\u6b21\u4f7f \u7528<code>kubectl port-forward</code>,\uff0c\u5e76\u4f7f\u7528Ctrl-C\u5728\u5b8c\u6210\u65f6\u7ec8\u6b62\u8fdb\u7a0b\u3002</p> <p>\u8bbe\u7f6e\u4e86 frontendproxy \u548c Collector \u7aef\u53e3\u8f6c\u53d1\u540e\uff0c\u60a8\u53ef\u4ee5\u8bbf\u95ee:</p> <ul> <li>Webstore: http://localhost:8080/</li> <li>Grafana: http://localhost:8080/grafana/</li> <li>Feature Flags UI: http://localhost:8080/feature/</li> <li>Load Generator UI: http://localhost:8080/loadgen/</li> <li>Jaeger UI: http://localhost:8080/jaeger/ui/</li> </ul>"},{"location":"docs/demo/kubernetes-deployment/#_3","title":"\u4f7f\u7528\u670d\u52a1\u7c7b\u578b\u914d\u7f6e\u516c\u5f00\u670d\u52a1","text":"<p>Note Kubernetes \u96c6\u7fa4\u53ef\u80fd\u6ca1\u6709\u9002\u5f53\u7684\u57fa\u7840\u8bbe\u65bd\u7ec4\u4ef6\u6765\u542f\u7528 LoadBalancer \u670d\u52a1\u7c7b\u578b \u6216\u5165\u53e3\u8d44\u6e90\u3002\u5728\u4f7f\u7528\u8fd9\u4e9b\u914d\u7f6e\u9009\u9879\u4e4b\u524d\uff0c\u8bf7\u9a8c\u8bc1\u60a8\u7684\u96c6\u7fa4\u5177\u6709\u9002\u5f53\u7684\u652f\u6301\u3002</p> <p>\u6bcf\u4e2a\u6f14\u793a\u670d\u52a1(\u4f8b\u5982:frontendproxy)\u90fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u914d\u7f6e Kubernetes \u670d\u52a1\u7c7b\u578b\u7684\u65b9\u6cd5\u3002\u9ed8\u8ba4 \u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u5c06\u662f <code>ClusterIP</code>\uff0c\u4f46\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>serviceType</code> \u5c5e\u6027\u4e3a\u6bcf\u4e2a\u670d\u52a1\u66f4\u6539\u6bcf\u4e2a \u3002</p> <p>\u8981\u914d\u7f6e frontendproxy \u670d\u52a1\u4f7f\u7528 LoadBalancer \u670d\u52a1\u7c7b\u578b\uff0c\u4f60\u9700\u8981\u5728\u4f60\u7684\u503c\u6587\u4ef6\u4e2d\u6307\u5b9a\u4ee5 \u4e0b\u5185\u5bb9:</p> <pre><code>components:\nfrontendProxy:\nservice:\ntype: LoadBalancer\n</code></pre> <p>Note \u5efa\u8bae\u5728\u5b89\u88c5 Helm \u56fe\u8868\u65f6\u4f7f\u7528 values \u6587\u4ef6\uff0c\u4ee5\u4fbf\u6307\u5b9a\u5176\u4ed6\u914d\u7f6e\u9009\u9879\u3002</p> <p>Helm \u56fe\u4e0d\u63d0\u4f9b\u521b\u5efa\u5165\u53e3\u8d44\u6e90\u7684\u5de5\u5177\u3002\u5982\u679c\u9700\u8981\uff0c\u8fd9\u4e9b\u5c06\u9700\u8981\u5728\u5b89\u88c5 Helm \u56fe\u8868\u540e\u624b\u52a8\u521b\u5efa \u3002\u4e00\u4e9b Kubernetes \u63d0\u4f9b\u7a0b\u5e8f\u9700\u8981\u7279\u5b9a\u7684\u670d\u52a1\u7c7b\u578b\u624d\u80fd\u88ab\u5165\u53e3\u8d44\u6e90\u4f7f\u7528(\u4f8b\u5982:EKS ALB \u5165\u53e3 \uff0c\u9700\u8981 NodePort \u670d\u52a1\u7c7b\u578b)\u3002</p> <p>\u4e3a\u4e86\u6b63\u786e\u5730\u6536\u96c6\u6765\u81ea\u6d4f\u89c8\u5668\u7684\u8de8\u5ea6\uff0c\u60a8\u8fd8\u9700\u8981\u516c\u5f00 OpenTelemetry Collector \u7684 OTLP/HTTP \u7aef\u53e3\uff0c\u4ee5\u4fbf\u7528\u6237 web \u6d4f\u89c8\u5668\u53ef\u4ee5\u8bbf\u95ee\u3002\u8fd8\u5fc5\u987b\u4f7f\u7528 <code>PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code>\u73af\u5883\u53d8\u91cf\u5c06\u516c\u5f00 OpenTelemetry Collector \u7684\u4f4d\u7f6e\u4f20\u9012\u7ed9\u524d\u7aef\u670d\u52a1\u3002\u4f60\u53ef\u4ee5\u5728\u4f60\u7684 values \u6587\u4ef6\u4e2d\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801:</p> <pre><code>components:\nfrontend:\nenv:\n- name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\nvalue: http://otel-demo-collector.mydomain.com:4318/v1/traces\n</code></pre> <p>To install the Helm chart with a custom <code>my-values-file.yaml</code> values file use:</p> <pre><code>helm install my-otel-demo open-telemetry/opentelemetry-demo --values my-values-file.yaml\n</code></pre> <p>With the frontendproxy and Collector exposed, you can access the demo UI at the base path for the frontendproxy. Other demo components can be accessed at the following sub-paths:</p> <ul> <li>Webstore: <code>/</code> (base)</li> <li>Grafana: <code>/grafana</code></li> <li>Feature Flags UI: <code>/feature</code></li> <li>Load Generator UI: <code>/loadgen/</code> (must include trailing slash)</li> <li>Jaeger UI: <code>/jaeger/ui</code></li> </ul>"},{"location":"docs/demo/kubernetes-deployment/#_4","title":"\u81ea\u5e26\u540e\u7aef","text":"<p>Likely you want to use the Webstore as a demo application for an observability backend you already have (e.g. an existing instance of Jaeger, Zipkin, or one of the vendor of your choice.</p> <p>The OpenTelemetry Collector's configuration is exposed in the Helm chart. Any additions you do will be merged into the default configuration. You can use this to add your own exporters, and add them to the desired pipeline(s)</p> <pre><code>opentelemetry-collector:\nconfig:\nexporters:\notlphttp/example:\nendpoint: &lt;your-endpoint-url&gt;\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlphttp/example]\n</code></pre> <p>Note \u5f53\u5c06 YAML \u503c\u4e0e Helm \u5408\u5e76\u65f6\uff0c\u5bf9\u8c61\u88ab\u5408\u5e76\uff0c\u6570\u7ec4\u88ab\u66ff\u6362\u3002</p> <p>Vendor backends might require you to add additional parameters for authentication, please check their documentation. Some backends require different exporters, you may find them and their documentation available at opentelemetry-collector-contrib/exporter.</p> <p>\u8981\u7528\u81ea\u5b9a\u4e49\u7684 <code>my-values-file.yaml</code> \u503c\u6587\u4ef6\u5b89\u88c5 Helm \u56fe\u8868\uff0c\u4f7f\u7528:</p> <pre><code>helm install my-otel-demo open-telemetry/opentelemetry-demo --values my-values-file.yaml\n</code></pre>"},{"location":"docs/demo/manual-span-attributes/","title":"\u624b\u52a8\u8de8\u5ea6\u5c5e\u6027","text":"<p>This document contains the list of manual Span Attributes used throughout the demo:</p>"},{"location":"docs/demo/manual-span-attributes/#adservice","title":"AdService","text":"Name Type Description <code>app.ads.category</code> string Category for returned ad <code>app.ads.contextKeys</code> string Context keys used to find related ads <code>app.ads.contextKeys.count</code> number Count of unique context keys used <code>app.ads.count</code> number Count of ads returned to user <code>app.ads.ad_request_type</code> string Either <code>targeted</code> or <code>not_targeted</code> <code>app.ads.ad_response_type</code> string Either <code>targeted</code> or <code>random</code>"},{"location":"docs/demo/manual-span-attributes/#cartservice","title":"CartService","text":"Name Type Description <code>app.cart.items.count</code> number Number of unique items in cart <code>app.product.id</code> string Product Id for cart item <code>app.product.quantity</code> string Quantity for cart item <code>app.user.id</code> string User Id"},{"location":"docs/demo/manual-span-attributes/#checkoutservice","title":"CheckoutService","text":"Name Type Description <code>app.cart.items.count</code> number Total number of items in cart <code>app.order.amount</code> number Order amount <code>app.order.id</code> string Order Id <code>app.order.items.count</code> number Number of unique items in order <code>app.payment.transaction.id</code> string Payment transaction Id <code>app.shipping.amount</code> number Shipping amount <code>app.shipping.tracking.id</code> string Shipping tracking Id <code>app.user.currency</code> string User currency <code>app.user.id</code> string User Id"},{"location":"docs/demo/manual-span-attributes/#currencyservice","title":"CurrencyService","text":"Name Type Description <code>app.currency.conversion.from</code> string Currency code to convert from <code>app.currency.conversion.to</code> string Currency code to convert to"},{"location":"docs/demo/manual-span-attributes/#emailservice","title":"EmailService","text":"Name Type Description <code>app.email.recipient</code> string Email used for order confirmation <code>app.order.id</code> string Order Id"},{"location":"docs/demo/manual-span-attributes/#featureflagservice","title":"FeatureFlagService","text":"Name Type Description <code>app.featureflag.name</code> string Name of the feature flag <code>app.featureflag.description</code> string Admin description <code>app.featureflag.enabled</code> boolean The feature flag status"},{"location":"docs/demo/manual-span-attributes/#frontend","title":"Frontend","text":"Name Type Description <code>app.cart.size</code> number Total number of items in cart <code>app.cart.items.count</code> number Count of unique items in cart <code>app.cart.shipping.cost</code> number Cart shipping cost <code>app.cart.total.price</code> number Cart total price <code>app.currency</code> string User currency <code>app.currency.new</code> string New currency to set <code>app.order.total</code> number Order total cost <code>app.product.id</code> string Product Id <code>app.product.quantity</code> number Product quantity <code>app.products.count</code> number Total products displayed <code>app.request.id</code> string Request Id <code>app.session.id</code> string Session Id <code>app.user.id</code> string User Id"},{"location":"docs/demo/manual-span-attributes/#loadgenerator","title":"LoadGenerator","text":"Name Type Description None yet"},{"location":"docs/demo/manual-span-attributes/#paymentservice","title":"PaymentService","text":"Name Type Description <code>app.payment.amount</code> number Total payment amount <code>app.payment.card_type</code> string Type of card used for payment <code>app.payment.card_valid</code> boolean Was the card used valid <code>app.payment.charged</code> boolean Was the charge successful (false with loadgenerator)"},{"location":"docs/demo/manual-span-attributes/#productcatalogservice","title":"ProductCatalogService","text":"Name Type Description <code>app.product.id</code> string Product Id <code>app.product.name</code> string Product name <code>app.products.count</code> number Number of products in catalog <code>app.products_search.count</code> number Number of products returned in search"},{"location":"docs/demo/manual-span-attributes/#quoteservice","title":"QuoteService","text":"Name Type Description <code>app.quote.items.count</code> number Total items to ship <code>app.quote.cost.total</code> number Total shipping quote"},{"location":"docs/demo/manual-span-attributes/#recommendationservice","title":"RecommendationService","text":"Name Type Description <code>app.filtered_products.count</code> number Number of filtered products returned <code>app.products.count</code> number Number of products in catalog <code>app.products_recommended.count</code> number Number of recommended products returned <code>app.cache_hit</code> boolean If cache was accessed or not"},{"location":"docs/demo/manual-span-attributes/#shippingservice","title":"ShippingService","text":"Name Type Description <code>app.shipping.cost.total</code> number Total shipping cost <code>app.shipping.items.count</code> number Total items to ship <code>app.shipping.tracking.id</code> string Shipping tracking Id <code>app.shipping.zip_code</code> string Zip code used to ship item(s)"},{"location":"docs/demo/metric-features/","title":"\u6309\u670d\u52a1\u5ea6\u91cf\u7279\u5f81\u8986\u76d6\u7387","text":"Service Language Auto-instrumentation Manual Instrumentation Multiple Instruments Views Custom Attributes Resource Detection Trace Exemplars Accounting Go \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Ad Java \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 \u2705 \u2705 \u2705 Cart .NET \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Checkout Go \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Currency C++ \ud83d\udd15 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Email Ruby \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Feature Flag Erlang / Elixir \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Fraud Detection Kotlin \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \u2705 \ud83d\udea7 Frontend TypeScript \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Payment JavaScript \ud83d\udea7 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \u2705 \ud83d\udea7 Product Catalog Go \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Quote PHP \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Recommendation Python \u2705 \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Shipping Rust \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 <p>Emoji Legend:</p> <ul> <li>Completed: \u2705</li> <li>Not Applicable: \ud83d\udd15</li> <li>Not Present (Yet): \ud83d\udea7</li> </ul>"},{"location":"docs/demo/service-table/","title":"\u670d\u52a1\u7684\u89d2\u8272","text":"<p>View Service Graph to visualize request flows.</p> Service Language Description accountingservice Go Processes incoming orders and count the sum of all orders (mock/). adservice Java Provides text ads based on given context words. cartservice DotNet Stores the items in the user's shopping cart in Redis and retrieves it. checkoutservice Go Retrieves user cart, prepares order and orchestrates the payment, shipping and the email notification. currencyservice C++ Converts one money amount to another currency. Uses real values fetched from European Central Bank. It's the highest QPS service. emailservice Ruby Sends users an order confirmation email (mock/). frauddetectionservice Kotlin Analyzes incoming orders and detects fraud attempts (mock/). featureflagservice Erlang/Elixir CRUD feature flag service to demonstrate various scenarios like fault injection &amp; how to emit telemetry from a feature flag reliant service. frontend JavaScript Exposes an HTTP server to serve the website. Does not require signup/login and generates session IDs for all users automatically. loadgenerator Python/Locust Continuously sends requests imitating realistic user shopping flows to the frontend. paymentservice JavaScript Charges the given credit card info (mock/) with the given amount and returns a transaction ID. productcatalogservice Go Provides the list of products from a JSON file and ability to search products and get individual products. quoteservice PHP Calculates the shipping costs, based on the number of items to be shipped. recommendationservice Python Recommends other products based on what's given in the cart. shippingservice Rust Gives shipping cost estimates based on the shopping cart. Ships items to the given address (mock/)."},{"location":"docs/demo/tests/","title":"\u6d4b\u8bd5","text":"<p>Currently, the repository includes E2E tests for both the frontend and backend services. For the Frontend we are using Cypress execute the different flows in the webstore. While the backend services use AVA as the main testing framework.</p> <p>To run the test you can simply run <code>make run-tests</code> at the root directory.</p> <p>In case you need to run a specific suite of tests you can execute <code>docker compose run frontendTests</code>1 for the frontend tests or <code>docker compose run integrationTests</code>1 for the backend tests.</p> <ol> <li> <p>{{% _param notes.docker-compose-v2 %}}\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"docs/demo/trace-features/","title":"\u6309\u670d\u52a1\u8ddf\u8e2a\u529f\u80fd\u8986\u76d6","text":"\u670d\u52a1 \u8bed\u8a00 \u5de5\u5177\u5e93 \u624b\u521bSpan Span\u6570\u636e\u5145\u5b9e RPC\u4e0a\u4e0b\u6587\u4f20\u64ad Span\u94fe\u63a5 \u884c\u674e \u8d44\u6e90\u53d1\u73b0 Accounting Service Go \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \u2705 Ad Java \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Cart .NET \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \u2705 Checkout Go \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \u2705 Currency C++ \ud83d\udd15 \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Email Ruby \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Feature Flag Erlang / Elixir \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Fraud Detection Kotlin \u2705 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 \ud83d\udea7 Frontend JavaScript \u2705 \u2705 \u2705 \ud83d\udd15 \u2705 \u2705 \u2705 Payment JavaScript \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \u2705 \u2705 Product Catalog Go \u2705 \ud83d\udd15 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Quote Service PHP \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Recommendation Python \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 Shipping Rust \ud83d\udd15 \u2705 \u2705 \u2705 \ud83d\udd15 \ud83d\udd15 \ud83d\udea7 <p>Emoji\u4f20\u5947:</p> <ul> <li>\u5b8c\u6210: \u2705</li> <li>\u4e0d\u9002\u7528: \ud83d\udd15</li> <li>\u4e0d\u5728\u573a(\u5c1a\u672a): \ud83d\udea7</li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/","title":"\u91c7\u96c6\u5668\u6570\u636e\u6d41\u4eea\u8868\u677f","text":"<p>Monitoring data flow through the OpenTelemetry Collector is crucial for several reasons. Gaining a macro-level perspective on incoming data, such as sample counts and cardinality, is essential for comprehending the collector's internal dynamics. However, when delving into the details, the interconnections can become complex. The Collector Data Flow Dashboard aims to demonstrate the capabilities of the OpenTelemetry demo application, offering a solid foundation for users to build upon. Collector Data Flow Dashboard provides valuable guidance on which metrics to monitor. Users can tailor their own dashboard variations by adding necessary metrics specific to their use cases, such as memory_delimiter processor or other data flow indicators. This demo dashboard serves as a starting point, enabling users to explore diverse usage scenarios and adapt the tool to their unique monitoring needs.</p>"},{"location":"docs/demo/collector-data-flow-dashboard/#data-flow-overview","title":"Data Flow Overview","text":"<p>The diagram below provides an overview of the system components, showcasing the configuration derived from the OpenTelemetry Collector (otelcol) configuration file utilized by the OpenTelemetry demo application. Additionally, it highlights the observability data (traces and metrics) flow within the system.</p> <p></p>"},{"location":"docs/demo/collector-data-flow-dashboard/#ingressegress-metrics","title":"Ingress/Egress Metrics","text":"<p>The metrics depicted in the diagram below are employed to monitor both egress and ingress data flows. These metrics are generated by the otelcol process, exported on port 8888, and subsequently scraped by Prometheus. The namespace associated with these metrics is \"otelcol,\" and the job name is labeled as <code>otel.</code></p> <p></p> <p>Labels serve as a valuable tool for identifying specific metric sets (such as exporter, receiver, or job), enabling differentiation among metric sets within the overall namespace. It is important to note that you will only encounter refused metrics if the memory limits, as defined in the memory delimiter processor, are exceeded.</p>"},{"location":"docs/demo/collector-data-flow-dashboard/#ingress-traces-pipeline","title":"Ingress Traces Pipeline","text":"<ul> <li><code>otelcol_receiver_accepted_spans</code></li> <li><code>otelcol_receiver_refused_spans</code></li> <li><code>by (receiver,transport)</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#ingress-metrics-pipeline","title":"Ingress Metrics Pipeline","text":"<ul> <li><code>otelcol_receiver_accepted_metric_points</code></li> <li><code>otelcol_receiver_refused_metric_points</code></li> <li><code>by (receiver,transport)</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#processor","title":"Processor","text":"<p>Currently, the only processor present in the demo application is a batch processor, which is used by both traces and metrics pipelines.</p> <ul> <li><code>otelcol_processor_batch_batch_send_size_sum</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#egress-traces-pipeline","title":"Egress Traces Pipeline","text":"<ul> <li><code>otelcol_exporter_sent_spans</code></li> <li><code>otelcol_exporter_send_failed_spans</code></li> <li><code>by (exporter)</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#egress-metrics-pipeline","title":"Egress Metrics Pipeline","text":"<ul> <li><code>otelcol_exporter_sent_metric_points</code></li> <li><code>otelcol_exporter_send_failed_metric_points</code></li> <li><code>by (exporter)</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#prometheus-scraping","title":"Prometheus Scraping","text":"<ul> <li><code>scrape_samples_scraped</code></li> <li><code>by (job)</code></li> </ul>"},{"location":"docs/demo/collector-data-flow-dashboard/#dashboard","title":"Dashboard","text":"<p>You can access the dashboard by navigating to the Grafana UI, selecting the OpenTelemetry Collector Data Flow dashboard under browse icon on the left-hand side of the screen.</p> <p></p> <p>The dashboard has four main sections:</p> <ol> <li>Process Metrics</li> <li>Traces Pipeline</li> <li>Metrics Pipeline</li> <li>Prometheus Scraping</li> </ol> <p>Sections 2,3 and 4 represent overall data flow using the metrics mentioned above. Additionally, export ratio is calculated for each pipeline to understand the data flow.</p>"},{"location":"docs/demo/collector-data-flow-dashboard/#export-ratio","title":"Export Ratio","text":"<p>Export ratio is basically the ratio between receiver and exporter metrics. You can notice over the dashboard screenshot above that the export ratio on metrics is way too high than the received metrics. This is because the demo application is configured to generate span metrics which is a processor that generates metrics from spans inside collector as illustrated in overview diagram.</p>"},{"location":"docs/demo/collector-data-flow-dashboard/#process-metrics","title":"Process Metrics","text":"<p>Very limited but informative process metrics are added to dashboard. For example, you might observe more than one instance of otelcol running on the system during restarts or similar. This can be useful for understanding spikes on dataflow.</p> <p></p>"},{"location":"docs/demo/requirements/","title":"\u6f14\u793a\u9700\u6c42","text":"<p>\u4e0b\u9762\u7684\u6587\u6863\u6355\u83b7\u4e86\u6211\u4eec\u5171\u4eab\u7684\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f\u7684\u5e94\u7528\u7a0b\u5e8f\u3001OpenTelemetry (OTel)\u548c\u7cfb\u7edf\u9700\u6c42\u3002 \u8fd9\u4e9b\u662f\u5728\u6b63\u5728\u8fdb\u884c\u7684SIG\u4f1a\u8bae\u4e0a\u51b3\u5b9a\u7684\u3002</p> <ol> <li>\u5e94\u7528\u7a0b\u5e8f\u9700\u6c42</li> <li>OpenTelemetry\u9700\u6c42</li> <li>\u7cfb\u7edf\u9700\u6c42</li> </ol>"},{"location":"docs/demo/requirements/#_1","title":"\u76ee\u6807\u89d2\u8272","text":"<p>\u6211\u4eec\u6b63\u5728\u7528\u51e0\u4e2a\u4e0d\u540c\u7684\u76ee\u6807\u4eba\u7269\u89d2\u8272\u6784\u5efa\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f:</p> <ol> <li>\u516c\u53f8\u7684 \u7231\u597d\u8005 \u53ef\u4ee5\u4f7f\u7528\u6f14\u793a\u5e94\u7528\u7a0b\u5e8f\u4f5c\u4e3a\u4e2a\u4eba\u5728\u4ed6\u4eec\u7684\u7ec4\u7ec7\u5185\u5021\u5bfcOTel\u3002</li> <li>\u5177\u6709\u7279\u5b9a\u8bed\u8a00\u6280\u80fd\u7684 \u5f00\u53d1\u4eba\u5458 \u5e0c\u671b\u770b\u5230\u66f4\u5927\u7684\u89c6\u56fe\u3002</li> <li>APM\u4f9b\u5e94\u5546 \u53ef\u4ee5\u5bf9OTel\u8fdb\u884c\u603b\u4f53\u8bc4\u4f30\u6216\u9700\u8981\u4e3a\u5ba2\u6237\u63d0\u4f9bOTel\u529f\u80fd\u6f14\u793a\u3002</li> <li>\u8003\u8651\u91c7\u7528OTel\u5e76\u5bf9\u4e86\u89e3\u751f\u4ea7-\u751f\u6d3b\u4f53\u9a8c\u611f\u5174\u8da3\u7684 \u4f01\u4e1a\u3002</li> </ol>"},{"location":"docs/demo/requirements/application/","title":"\u5e94\u7528\u9700\u6c42","text":"<p>The following requirements were decided upon to define what OpenTelemetry (OTel) signals the application will produce &amp; when support for future SDKs should be added:</p> <ol> <li> <p>Every supported language that has a GA Traces or Metrics SDK must have at    least 1 service example.</p> </li> <li> <p>Mobile support (Swift) is not an initial priority and not included in the      above requirement.</p> </li> <li> <p>Application processes must be language independent.</p> </li> <li> <p>gRPC is preferred where available and HTTP is to be used where it is not.</p> </li> <li> <p>Services should be architected to be modular components that can be switched    out.</p> </li> <li> <p>Individual services can and should be encouraged to have multiple language      options available.</p> </li> <li> <p>The architecture must allow for the possible integration of platform generic    components like a database, queue, or blob storage.</p> </li> <li> <p>There is no requirement for a particular component type - at least 1      generic component should be present in general.</p> </li> <li> <p>A load generator must be provided to simulate user load against the demo.</p> </li> </ol>"},{"location":"docs/demo/requirements/architecture/","title":"\u67b6\u6784\u9700\u6c42","text":""},{"location":"docs/demo/requirements/architecture/#summary","title":"Summary","text":"<p>The OpenTelemetry Community Demo application is intended to be a 'showcase' for OpenTelemetry API, SDK, and tools in a production-lite cloud native application. The overall goal of this application is not only to provide a canonical 'demo' of OpenTelemetry components, but also to act as a framework for further customization by end-users, vendors, and other stakeholders.</p>"},{"location":"docs/demo/requirements/architecture/#requirements","title":"Requirements","text":"<ul> <li>Application Requirements</li> <li>OpenTelemetry Requirements</li> <li>System Requirements</li> </ul>"},{"location":"docs/demo/requirements/architecture/#application-goals","title":"Application Goals","text":"<ul> <li>Provide developers with a robust sample application they can use in learning   OpenTelemetry instrumentation.</li> <li>Provide observability vendors with a single, well-supported, demo platform   that they can further customize (or simply use OOB).</li> <li>Provide the OpenTelemetry community with a living artifact that demonstrates   the features and capabilities of OTel APIs, SDKs, and tools.</li> <li>Provide OpenTelemetry maintainers and WGs a platform to demonstrate new   features/concepts 'in the wild'.</li> </ul> <p>The following is a general description of the logical components of the demo application.</p>"},{"location":"docs/demo/requirements/architecture/#main-application","title":"Main Application","text":"<p>The bulk of the demo app is a self-contained microservice-based application that does some useful 'real-world' work, such as an eCommerce site. This application is composed of multiple services that communicate with each other over gRPC and HTTP and runs on Kubernetes (or Docker, locally).</p> <p>Each service shall be instrumented with OpenTelemetry for traces, metrics, and logs (as applicable/available).</p> <p>Each service should be interchangeable with a service that performs the same business logic, implementing the same gRPC endpoints, but written in a different language/implementation. For the initial implementation of the demo, we should focus on adding as many missing languages as possible by swapping out existing services with implementations in un-represented languages. For future versions we will look to add more distinct language options per service.</p> <p>Each service should communicate with a feature flag service in order to enable/disable 'faults' that can be used to illustrate how telemetry helps solve problems in distributed applications.</p> <p>A PHP service should be added to the main application as an 'admin service'. A Database should be added to enable CRUD functionality on the Product Catalog.</p> <p>The 'shippingservice' should be reimplemented in Rust.</p> <p>The 'currencyservice' should be reimplemented in C++.</p> <p>The 'emailservice' should be reimplemented in Ruby.</p> <p>For future iterations, the 'frontend' service can be extended with a mobile application written in Swift.</p>"},{"location":"docs/demo/requirements/architecture/#feature-flag-component","title":"Feature Flag Component","text":"<p>This component should consist of one (or more) services that provides a simple feature flag configuration utility for the main application. It is made up of a browser-based client/admin interface and a backend service or services. The role of the client is to allow an operator to visualize the available feature flags and toggle their state. The server should provide a catalog of feature flags that main application services can register with and interrogate for their current status and targeting rules.</p> <p>The feature flag component should be implemented as an Erlang+Elixir/Phoenix service. The catalog of feature flags should be stored in a Database.</p>"},{"location":"docs/demo/requirements/architecture/#orchestration-and-deployment","title":"Orchestration and Deployment","text":"<p>All services should run on Kubernetes. The OpenTelemetry Collector should be deployed via the OpenTelemetry Operator, and run in a sidecar + gateway mode. Telemetry from each pod should be routed from agents to a gateway, and the gateway should export telemetry by default to an open source trace + metrics visualizer.</p> <p>For local/non-Kubernetes deployment, the Collector should be deployed via compose file and monitor not only traces/metrics from applications, but also the docker containers via <code>dockerstatsreceiver</code>.</p> <p>A design goal of this project is to include a CI/CD pipeline for self-deployment into cloud environments. This could be skipped for local development.</p>"},{"location":"docs/demo/requirements/opentelemetry/","title":"OpenTelemetry \u9700\u6c42","text":"<p>The following requirements were decided upon to define what OpenTelemetry (OTel) signals the application will produce &amp; when support for future SDKs should be added:</p> <ol> <li>The demo must produce OTel logs, traces, &amp; metrics out of the box for    languages that have a GA SDK.</li> <li>Languages that have a Beta SDK available may be included but are not required    like GA SDKs.</li> <li>Native OTel metrics should be produced where possible.</li> <li>Both manual instrumentation and instrumentation libraries    (auto-instrumentation) should be demonstrated in each language.</li> <li>All data should be exported to the Collector first.</li> <li>The Collector must be configurable to allow for a variety of consumption    experiences but default tools must be selected for each signal.</li> <li>The demo application architecture using the Collector should be designed to    be a best practices reference architecture.</li> </ol>"},{"location":"docs/demo/requirements/system/","title":"\u7cfb\u7edf\u9700\u6c42","text":"<p>To ensure the demo runs correctly please ensure your environment meets the following system requirements:</p> <ol> <li>Your system must meet Docker Desktop    system requirements or you should use your preferred Cloud Service.</li> <li>The demo must be able to work on the following Operating Systems (OS): Linux,    macOS and Windows with documentation provided for each OS.</li> </ol>"},{"location":"docs/demo/scenarios/recommendation-cache/","title":"\u4f7f\u7528\u5ea6\u91cf\u548c\u8ddf\u8e2a\u6765\u8bca\u65ad\u5185\u5b58\u6cc4\u6f0f","text":"<p>Application telemetry, such as the kind that OpenTelemetry can provide, is very useful for diagnosing issues in a distributed system. In this scenario, we will walk through a scenario demonstrating how to move from high-level metrics and traces to determine the cause of a memory leak.</p>"},{"location":"docs/demo/scenarios/recommendation-cache/#setup","title":"Setup","text":"<p>To run this scenario, you will need to deploy the demo application and enable the <code>recommendationCache</code> feature flag. Let the application run for about 10 minutes or so after enabling the feature flag to allow for data to populate.</p>"},{"location":"docs/demo/scenarios/recommendation-cache/#diagnosis","title":"Diagnosis","text":"<p>The first step in diagnosing a problem is to determine that a problem exists. Often the first stop will be a metrics dashboard provided by a tool such as Grafana.</p> <p>A demo dashboard folder should exist after launching the demo with two dashboards; One is to monitor your OpenTelemetry Collector, and the other contains several queries and charts to analyze latency and request rate from each service.</p> <p></p> <p>This dashboard will contain a number of charts, but a few should appear interesting:</p> <ul> <li>Recommendation Service (CPU% and Memory)</li> <li>Service Latency (from SpanMetrics)</li> <li>Error Rate</li> </ul> <p>Recommendation Service charts are generated from OpenTelemetry Metrics exported to Prometheus, while the Service Latency and Error Rate charts are generated through the OpenTelemetry Collector Span Metrics processor.</p> <p>From our dashboard, we can see that there seems to be anomalous behavior in the recommendation service -- spiky CPU utilization, as well as long tail latency in our p95, 99, and 99.9 histograms. We can also see that there are intermittent spikes in the memory utilization of this service.</p> <p>We know that we're emitting trace data from our application as well, so let's think about another way that we'd be able to determine that a problem exist.</p> <p></p> <p>Jaeger allows us to search for traces and display the end-to-end latency of an entire request with visibility into each individual part of the overall request. Perhaps we noticed an increase in tail latency on our frontend requests. Jaeger allows us to then search and filter our traces to include only those that include requests to recommendation service.</p> <p>By sorting by latency, we're able to quickly find specific traces that took a long time. Clicking on a trace in the right panel, we're able to view the waterfall view.</p> <p></p> <p>We can see that the recommendation service is taking a long time to complete its work, and viewing the details allows us to get a better idea of what's going on.</p>"},{"location":"docs/demo/scenarios/recommendation-cache/#confirming-the-diagnosis","title":"Confirming the Diagnosis","text":"<p>We can see in our waterfall view that the <code>app.cache_hit</code> attribute is set to <code>false</code>, and that the <code>app.products.count</code> value is extremely high.</p> <p>Returning to the search UI, filter to <code>recommendationservice</code> in the Service dropdown, and search for <code>app.cache_hit=true</code> in the Tags box. Notice that requests tend to be faster when the cache is hit. Now search for <code>app.cache_hit=false</code> and compare the latency. You should notice some changes in the visualization at the top of the trace list.</p> <p>Now, since this is a contrived scenario, we know where to find the underlying bug in our code. However, in a real-world scenario, we may need to perform further searching to find out what's going on in our code, or the interactions between services that cause it.</p>"},{"location":"docs/demo/screenshots/","title":"\u6f14\u793a\u622a\u56fe","text":""},{"location":"docs/demo/screenshots/#webstore","title":"Webstore","text":"Home Page Checkout Screen"},{"location":"docs/demo/screenshots/#jaeger","title":"Jaeger","text":"Jaeger UI Trace View System Architecture"},{"location":"docs/demo/screenshots/#prometheus","title":"Prometheus","text":""},{"location":"docs/demo/screenshots/#grafana","title":"Grafana","text":"Prometheus Data Source Jaeger Data Source"},{"location":"docs/demo/screenshots/#feature-flag-ui","title":"Feature Flag UI","text":""},{"location":"docs/demo/screenshots/#load-generator-ui","title":"Load Generator UI","text":""},{"location":"docs/demo/services/accounting/","title":"Accounting Service","text":"<p>This service calculates the total amount of sold products. This is only mocked and received orders are printed out.</p> <p>Accounting Service</p>"},{"location":"docs/demo/services/accounting/#traces","title":"Traces","text":""},{"location":"docs/demo/services/accounting/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initTracerProvider</code> function.</p> <pre><code>func initTracerProvider() (*sdktrace.TracerProvider, error) {\nctx := context.Background()\nexporter, err := otlptracegrpc.New(ctx)\nif err != nil {\nreturn nil, err\n}\ntp := sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(exporter),\nsdktrace.WithResource(initResource()),\n)\notel.SetTracerProvider(tp)\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{}))\nreturn tp, nil\n}\n</code></pre> <p>You should call <code>TracerProvider.Shutdown()</code> when your service is shutdown to ensure all spans are exported. This service makes that call as part of a deferred function in main</p> <pre><code>    tp, err := initTracerProvider()\nif err != nil {\nlog.Fatal(err)\n}\ndefer func() {\nif err := tp.Shutdown(context.Background()); err != nil {\nlog.Printf(\"Error shutting down tracer provider: %v\", err)\n}\n}()\n</code></pre>"},{"location":"docs/demo/services/accounting/#adding-kafka-sarama-auto-instrumentation","title":"Adding Kafka ( Sarama ) auto-instrumentation","text":"<p>This service will receive the processed results of the Checkout Service via a Kafka topic. To instrument the Kafka client the ConsumerHandler implemented by the developer has to be wrapped.</p> <pre><code>    handler := groupHandler{} // implements sarama.ConsumerGroupHandler\nwrappedHandler := otelsarama.WrapConsumerGroupHandler(&amp;handler)\n</code></pre>"},{"location":"docs/demo/services/ad/","title":"Ad Service","text":"<p>This service determines appropriate ads to serve to users based on context keys. The ads will be for products available in the store.</p> <p>Ad service source</p>"},{"location":"docs/demo/services/ad/#auto-instrumentation","title":"Auto-instrumentation","text":"<p>This service relies on the OpenTelemetry Java Agent to automatically instrument libraries such as gRPC, and to configure the OpenTelemetry SDK. The agent is passed into the process using the <code>-javaagent</code> command line argument. Command line arguments are added through the <code>JAVA_TOOL_OPTIONS</code> in the <code>Dockerfile</code>, and leveraged during the automatically generated Gradle startup script.</p> <pre><code>ENV JAVA_TOOL_OPTIONS=-javaagent:/app/opentelemetry-javaagent.jar\n</code></pre>"},{"location":"docs/demo/services/ad/#traces","title":"Traces","text":""},{"location":"docs/demo/services/ad/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>    Span span = Span.current();\n</code></pre> <p>Adding attributes to a span is accomplished using <code>setAttribute</code> on the span object. In the <code>getAds</code> function multiples attribute are added to the span.</p> <pre><code>    span.setAttribute(\"app.ads.contextKeys\", req.getContextKeysList().toString());\nspan.setAttribute(\"app.ads.contextKeys.count\", req.getContextKeysCount());\n</code></pre>"},{"location":"docs/demo/services/ad/#add-span-events","title":"Add span events","text":"<p>Adding an event to a span is accomplished using <code>addEvent</code> on the span object. In the <code>getAds</code> function an event with an attribute is added when an exception is caught.</p> <pre><code>    span.addEvent(\"Error\", Attributes.of(AttributeKey.stringKey(\"exception.message\"), e.getMessage()));\n</code></pre>"},{"location":"docs/demo/services/ad/#setting-span-status","title":"Setting span status","text":"<p>If the result of the operation is an error, the span status should be set accordingly using <code>setStatus</code> on the span object. In the <code>getAds</code> function the span status is set when an exception is caught.</p> <pre><code>    span.setStatus(StatusCode.ERROR);\n</code></pre>"},{"location":"docs/demo/services/ad/#create-new-spans","title":"Create new spans","text":"<p>New spans can be created and started using <code>Tracer.spanBuilder(\"spanName\").startSpan()</code>. Newly created spans should be set into context using <code>Span.makeCurrent()</code>. The <code>getRandomAds</code> function will create a new span, set it into context, perform an operation, and finally end the span.</p> <pre><code>    // create and start a new span manually\nTracer tracer = GlobalOpenTelemetry.getTracer(\"adservice\");\nSpan span = tracer.spanBuilder(\"getRandomAds\").startSpan();\n// put the span into context, so if any child span is started the parent will be set properly\ntry (Scope ignored = span.makeCurrent()) {\nCollection&lt;Ad&gt; allAds = adsMap.values();\nfor (int i = 0; i &lt; MAX_ADS_TO_SERVE; i++) {\nads.add(Iterables.get(allAds, random.nextInt(allAds.size())));\n}\nspan.setAttribute(\"app.ads.count\", ads.size());\n} finally {\nspan.end();\n}\n</code></pre>"},{"location":"docs/demo/services/ad/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/ad/#initializing-metrics","title":"Initializing Metrics","text":"<p>Similar to creating spans, the first step in creating metrics is initializing a <code>Meter</code> instance, e.g. <code>GlobalOpenTelemetry.getMeter(\"adservice\")</code>. From there, use the various builder methods available on the <code>Meter</code> instance to create the desired metric instrument, e.g.:</p> <pre><code>meter\n.counterBuilder(\"app.ads.ad_requests\")\n.setDescription(\"Counts ad requests by request and response type\")\n.build();\n</code></pre>"},{"location":"docs/demo/services/ad/#current-metrics-produced","title":"Current Metrics Produced","text":"<p>Note that all the metric names below appear in Prometheus/Grafana with <code>.</code> characters transformed to <code>_</code>.</p>"},{"location":"docs/demo/services/ad/#custom-metrics","title":"Custom metrics","text":"<p>The following custom metrics are currently available:</p> <ul> <li><code>app.ads.ad_requests</code>: A counter of ad requests with dimensions describing   whether the request was targeted with context keys or not, and whether the   response was targeted or random ads.</li> </ul>"},{"location":"docs/demo/services/ad/#auto-instrumented-metrics","title":"Auto-instrumented metrics","text":"<p>The following auto-instrumented metrics are available for the application:</p> <ul> <li>Runtime metrics for the JVM.</li> <li>Latency metrics for RPCs</li> </ul>"},{"location":"docs/demo/services/ad/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/cart/","title":"Cart Service","text":"<p>This service maintains items placed in the shopping cart by users. It interacts with a Redis caching service for fast access to shopping cart data.</p> <p>Cart service source</p> <p>Note OpenTelemetry for .NET uses the <code>System.Diagnostic</code> library as its API in lieu of the standard OpenTelemetry API.</p>"},{"location":"docs/demo/services/cart/#traces","title":"Traces","text":""},{"location":"docs/demo/services/cart/#initializing-tracing","title":"Initializing Tracing","text":"<p>OpenTelemetry is configured in the .NET DI container. The <code>AddOpenTelemetry()</code> builder method is used to configure desired instrumentation libraries, add exporters, and set other options. Configuration of the exporter and resource attributes is performed through environment variables.</p> <pre><code>Action&lt;ResourceBuilder&gt; appResourceBuilder =\nresource =&gt; resource\n.AddTelemetrySdk()\n.AddEnvironmentVariableDetector()\n.AddDetector(new ContainerResourceDetector());\nbuilder.Services.AddOpenTelemetry()\n.ConfigureResource(appResourceBuilder)\n.WithTracing(builder =&gt; builder\n.AddRedisInstrumentation(\ncartStore.GetConnection(),\noptions =&gt; options.SetVerboseDatabaseStatements = true)\n.AddAspNetCoreInstrumentation()\n.AddGrpcClientInstrumentation()\n.AddHttpClientInstrumentation()\n.AddOtlpExporter());\n</code></pre>"},{"location":"docs/demo/services/cart/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span (activity) from context.</p> <pre><code>    var activity = Activity.Current;\n</code></pre> <p>Adding attributes (tags in .NET) to a span (activity) is accomplished using <code>SetTag</code> on the activity object. In the <code>AddItem</code> function from <code>services/CartService.cs</code> several attributes are added to the auto-instrumented span.</p> <pre><code>    activity?.SetTag(\"app.user.id\", request.UserId);\nactivity?.SetTag(\"app.product.quantity\", request.Item.Quantity);\nactivity?.SetTag(\"app.product.id\", request.Item.ProductId);\n</code></pre>"},{"location":"docs/demo/services/cart/#add-span-events","title":"Add span events","text":"<p>Adding span (activity) events is accomplished using <code>AddEvent</code> on the activity object. In the <code>GetCart</code> function from <code>services/CartService.cs</code> a span event is added.</p> <pre><code>    activity?.AddEvent(new(\"Fetch cart\"));\n</code></pre>"},{"location":"docs/demo/services/cart/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/cart/#initializing-metrics","title":"Initializing Metrics","text":"<p>Similar to configuring OpenTelemetry Traces, the .NET DI container requires a call to <code>AddOpenTelemetry()</code>. This builder configures desired instrumentation libraries, exporters, etc.</p> <pre><code>Action&lt;ResourceBuilder&gt; appResourceBuilder =\nresource =&gt; resource\n.AddTelemetrySdk()\n.AddEnvironmentVariableDetector()\n.AddDetector(new ContainerResourceDetector());\nbuilder.Services.AddOpenTelemetry()\n.ConfigureResource(appResourceBuilder)\n.WithMetrics(builder =&gt; builder\n.AddRuntimeInstrumentation()\n.AddAspNetCoreInstrumentation()\n.AddOtlpExporter());\n</code></pre>"},{"location":"docs/demo/services/cart/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/checkout/","title":"Checkout Service","text":"<p>This service is responsible to process a checkout order from the user. The checkout service will call many other services in order to process an order.</p> <p>Checkout service source</p>"},{"location":"docs/demo/services/checkout/#traces","title":"Traces","text":""},{"location":"docs/demo/services/checkout/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initTracerProvider</code> function.</p> <pre><code>func initTracerProvider() *sdktrace.TracerProvider {\nctx := context.Background()\nexporter, err := otlptracegrpc.New(ctx)\nif err != nil {\nlog.Fatal(err)\n}\ntp := sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(exporter),\nsdktrace.WithResource(initResource()),\n)\notel.SetTracerProvider(tp)\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{}))\nreturn tp\n}\n</code></pre> <p>You should call <code>TracerProvider.Shutdown()</code> when your service is shutdown to ensure all spans are exported. This service makes that call as part of a deferred function in main</p> <pre><code>    tp := initTracerProvider()\ndefer func() {\nif err := tp.Shutdown(context.Background()); err != nil {\nlog.Printf(\"Error shutting down tracer provider: %v\", err)\n}\n}()\n</code></pre>"},{"location":"docs/demo/services/checkout/#adding-grpc-auto-instrumentation","title":"Adding gRPC auto-instrumentation","text":"<p>This service receives gRPC requests, which are instrumented in the main function as part of the gRPC server creation.</p> <pre><code>    var srv = grpc.NewServer(\ngrpc.UnaryInterceptor(otelgrpc.UnaryServerInterceptor()),\ngrpc.StreamInterceptor(otelgrpc.StreamServerInterceptor()),\n)\n</code></pre> <p>This service will issue several outgoing gRPC calls, which are all instrumented by wrapping the gRPC client with instrumentation</p> <pre><code>func createClient(ctx context.Context, svcAddr string) (*grpc.ClientConn, error) {\nreturn grpc.DialContext(ctx, svcAddr,\ngrpc.WithTransportCredentials(insecure.NewCredentials()),\ngrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\ngrpc.WithStreamInterceptor(otelgrpc.StreamClientInterceptor()),\n)\n}\n</code></pre>"},{"location":"docs/demo/services/checkout/#adding-kafka-sarama-auto-instrumentation","title":"Adding Kafka ( Sarama ) auto-instrumentation","text":"<p>This service will write the processed results onto a Kafka topic which will then be in turn be processed by other microservices. To instrument the Kafka client the Producer has to be wrapped after it has been created.</p> <pre><code>    saramaConfig := sarama.NewConfig()\nproducer, err := sarama.NewAsyncProducer(brokers, saramaConfig)\nif err != nil {\nreturn nil, err\n}\nproducer = otelsarama.WrapAsyncProducer(saramaConfig, producer)\n</code></pre>"},{"location":"docs/demo/services/checkout/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>    span := trace.SpanFromContext(ctx)\n</code></pre> <p>Adding attributes to a span is accomplished using <code>SetAttributes</code> on the span object. In the <code>PlaceOrder</code> function several attributes are added to the span.</p> <pre><code>    span.SetAttributes(\nattribute.String(\"app.order.id\", orderID.String()), shippingTrackingAttribute,\nattribute.Float64(\"app.shipping.amount\", shippingCostFloat),\nattribute.Float64(\"app.order.amount\", totalPriceFloat),\nattribute.Int(\"app.order.items.count\", len(prep.orderItems)),\n)\n</code></pre>"},{"location":"docs/demo/services/checkout/#add-span-events","title":"Add span events","text":"<p>Adding span events is accomplished using <code>AddEvent</code> on the span object. In the <code>PlaceOrder</code> function several span events are added. Some events have additional attributes, others do not.</p> <p>Adding a span event without attributes:</p> <pre><code>    span.AddEvent(\"prepared\")\n</code></pre> <p>Adding a span event with additional attributes:</p> <pre><code>    span.AddEvent(\"charged\",\ntrace.WithAttributes(attribute.String(\"app.payment.transaction.id\", txID)))\n</code></pre>"},{"location":"docs/demo/services/checkout/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/checkout/#initializing-metrics","title":"Initializing Metrics","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initMeterProvider</code> function.</p> <pre><code>func initMeterProvider() *sdkmetric.MeterProvider {\nctx := context.Background()\nexporter, err := otlpmetricgrpc.New(ctx)\nif err != nil {\nlog.Fatalf(\"new otlp metric grpc exporter failed: %v\", err)\n}\nmp := sdkmetric.NewMeterProvider(sdkmetric.WithReader(sdkmetric.NewPeriodicReader(exporter)))\nglobal.SetMeterProvider(mp)\nreturn mp\n}\n</code></pre> <p>You should call <code>MeterProvider.Shutdown()</code> when your service is shutdown to ensure all records are exported. This service makes that call as part of a deferred function in main</p> <pre><code>    mp := initMeterProvider()\ndefer func() {\nif err := mp.Shutdown(context.Background()); err != nil {\nlog.Printf(\"Error shutting down meter provider: %v\", err)\n}\n}()\n</code></pre>"},{"location":"docs/demo/services/checkout/#adding-golang-runtime-auto-instrumentation","title":"Adding golang runtime auto-instrumentation","text":"<p>Golang runtime are instrumented in the main function</p> <pre><code>    err := runtime.Start(runtime.WithMinimumReadMemStatsInterval(time.Second))\nif err != nil {\nlog.Fatal(err)\n}\n</code></pre>"},{"location":"docs/demo/services/checkout/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/currency/","title":"Currency Service","text":"<p>This service provides functionality to convert amounts between different currencies.</p> <p>Currency service source</p>"},{"location":"docs/demo/services/currency/#traces","title":"Traces","text":""},{"location":"docs/demo/services/currency/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initTracer</code> function defined in <code>tracer_common.h</code></p> <pre><code>void initTracer()\n{\nauto exporter = opentelemetry::exporter::otlp::OtlpGrpcExporterFactory::Create();\nauto processor =\nopentelemetry::sdk::trace::SimpleSpanProcessorFactory::Create(std::move(exporter));\nstd::vector&lt;std::unique_ptr&lt;opentelemetry::sdk::trace::SpanProcessor&gt;&gt; processors;\nprocessors.push_back(std::move(processor));\nstd::shared_ptr&lt;opentelemetry::sdk::trace::TracerContext&gt; context =\nopentelemetry::sdk::trace::TracerContextFactory::Create(std::move(processors));\nstd::shared_ptr&lt;opentelemetry::trace::TracerProvider&gt; provider =\nopentelemetry::sdk::trace::TracerProviderFactory::Create(context);\n// Set the global trace provider\nopentelemetry::trace::Provider::SetTracerProvider(provider);\n// set global propagator\nopentelemetry::context::propagation::GlobalTextMapPropagator::SetGlobalPropagator(\nopentelemetry::nostd::shared_ptr&lt;opentelemetry::context::propagation::TextMapPropagator&gt;(\nnew opentelemetry::trace::propagation::HttpTraceContext()));\n}\n</code></pre>"},{"location":"docs/demo/services/currency/#create-new-spans","title":"Create new spans","text":"<p>New spans can be created and started using <code>Tracer-&gt;StartSpan(\"spanName\", attributes, options)</code>. After a span is created you need to start and put it into active context using <code>Tracer-&gt;WithActiveSpan(span)</code>. You can find an example of this in the <code>Convert</code> function.</p> <pre><code>    std::string span_name = \"CurrencyService/Convert\";\nauto span =\nget_tracer(\"currencyservice\")-&gt;StartSpan(span_name,\n{{SemanticConventions::kRpcSystem, \"grpc\"},\n{SemanticConventions::kRpcService, \"CurrencyService\"},\n{SemanticConventions::kRpcMethod, \"Convert\"},\n{SemanticConventions::kRpcGrpcStatusCode, 0}},\noptions);\nauto scope = get_tracer(\"currencyservice\")-&gt;WithActiveSpan(span);\n</code></pre>"},{"location":"docs/demo/services/currency/#adding-attributes-to-spans","title":"Adding attributes to spans","text":"<p>You can add an attribute to a span using <code>Span-&gt;SetAttribute(key, value)</code>.</p> <pre><code>    span-&gt;SetAttribute(\"app.currency.conversion.from\", from_code);\nspan-&gt;SetAttribute(\"app.currency.conversion.to\", to_code);\n</code></pre>"},{"location":"docs/demo/services/currency/#add-span-events","title":"Add span events","text":"<p>Adding span events is accomplished using <code>Span-&gt;AddEvent(name)</code>.</p> <pre><code>    span-&gt;AddEvent(\"Conversion successful, response sent back\");\n</code></pre>"},{"location":"docs/demo/services/currency/#set-span-status","title":"Set span status","text":"<p>Make sure to set your span status to Ok, or Error accordingly. You can do this using <code>Span-&gt;SetStatus(status)</code></p> <pre><code>    span-&gt;SetStatus(StatusCode::kOk);\n</code></pre>"},{"location":"docs/demo/services/currency/#tracing-context-propagation","title":"Tracing context propagation","text":"<p>In C++ propagation is not automatically handled. You need to extract it from the caller and inject the propagation context into subsequent spans. The <code>GrpcServerCarrier</code> class defines a method to extract context from inbound gRPC requests which is leveraged in the service call implementations.</p> <p>The <code>GrpcServerCarrier</code> class is defined in <code>tracer_common.h</code> as follows:</p> <pre><code>class GrpcServerCarrier : public opentelemetry::context::propagation::TextMapCarrier\n{\npublic:\nGrpcServerCarrier(ServerContext *context) : context_(context) {}\nGrpcServerCarrier() = default;\nvirtual opentelemetry::nostd::string_view Get(\nopentelemetry::nostd::string_view key) const noexcept override\n{\nauto it = context_-&gt;client_metadata().find(key.data());\nif (it != context_-&gt;client_metadata().end())\n{\nreturn it-&gt;second.data();\n}\nreturn \"\";\n}\nvirtual void Set(opentelemetry::nostd::string_view key,\nopentelemetry::nostd::string_view value) noexcept override\n{\n// Not required for server\n}\nServerContext *context_;\n};\n</code></pre> <p>This class is leveraged in the <code>Convert</code> method to extract context and create a <code>StartSpanOptions</code> object to contain the right context which is used when creating new spans.</p> <pre><code>    StartSpanOptions options;\noptions.kind = SpanKind::kServer;\nGrpcServerCarrier carrier(context);\nauto prop        = context::propagation::GlobalTextMapPropagator::GetGlobalPropagator();\nauto current_ctx = context::RuntimeContext::GetCurrent();\nauto new_context = prop-&gt;Extract(carrier, current_ctx);\noptions.parent   = GetSpan(new_context)-&gt;GetContext();\n</code></pre>"},{"location":"docs/demo/services/currency/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/currency/#initializing-metrics","title":"Initializing Metrics","text":"<p>The OpenTelemetry <code>MeterProvider</code> is initialized from <code>main()</code> using the <code>initMeter()</code> function defined in <code>meter_common.h</code>.</p> <pre><code>void initMeter()\n{\n// Build MetricExporter\notlp_exporter::OtlpGrpcMetricExporterOptions otlpOptions;\n// Configuration via environment variable not supported yet\notlpOptions.endpoint = \"otelcol:4317\";\notlpOptions.aggregation_temporality = metric_sdk::AggregationTemporality::kDelta;\nauto exporter = otlp_exporter::OtlpGrpcMetricExporterFactory::Create(otlpOptions);\n// Build MeterProvider and Reader\nmetric_sdk::PeriodicExportingMetricReaderOptions options;\noptions.export_interval_millis = std::chrono::milliseconds(1000);\noptions.export_timeout_millis = std::chrono::milliseconds(500);\nstd::unique_ptr&lt;metric_sdk::MetricReader&gt; reader{\nnew metric_sdk::PeriodicExportingMetricReader(std::move(exporter), options) };\nauto provider = std::shared_ptr&lt;metrics_api::MeterProvider&gt;(new metric_sdk::MeterProvider());\nauto p = std::static_pointer_cast&lt;metric_sdk::MeterProvider&gt;(provider);\np-&gt;AddMetricReader(std::move(reader));\nmetrics_api::Provider::SetMeterProvider(provider);\n}\n</code></pre>"},{"location":"docs/demo/services/currency/#starting-intcounter","title":"Starting IntCounter","text":"<p>A global <code>currency_counter</code> variable is created at <code>main()</code> calling the function <code>initIntCounter()</code> defined in <code>meter_common.h</code>.</p> <pre><code>nostd::unique_ptr&lt;metrics_api::Counter&lt;uint64_t&gt;&gt; initIntCounter()\n{\nstd::string counter_name = name + \"_counter\";\nauto provider = metrics_api::Provider::GetMeterProvider();\nnostd::shared_ptr&lt;metrics_api::Meter&gt; meter = provider-&gt;GetMeter(name, version);\nauto int_counter = meter-&gt;CreateUInt64Counter(counter_name);\nreturn int_counter;\n}\n</code></pre>"},{"location":"docs/demo/services/currency/#counting-currency-conversion-requests","title":"Counting currency conversion requests","text":"<p>The method <code>CurrencyCounter()</code> is implemented as follows:</p> <pre><code>void CurrencyCounter(const std::string&amp; currency_code)\n{\nstd::map&lt;std::string, std::string&gt; labels = { {\"currency_code\", currency_code} };\nauto labelkv = common::KeyValueIterableView&lt;decltype(labels)&gt;{ labels };\ncurrency_counter-&gt;Add(1, labelkv);\n}\n</code></pre> <p>Every time the function <code>Convert()</code> is called, the currency code received as <code>to_code</code> is used to count the conversions.</p> <pre><code>CurrencyCounter(to_code);\n</code></pre>"},{"location":"docs/demo/services/currency/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/email/","title":"Email Service","text":"<p>This service will send a confirmation email to the user when an order is placed.</p> <p>Email service source</p>"},{"location":"docs/demo/services/email/#initializing-tracing","title":"Initializing Tracing","text":"<p>You will need to require the core OpenTelemetry SDK and exporter Ruby gems, as well as any gem that will be needed for auto-instrumentation libraries (ie: Sinatra)</p> <pre><code>require \"opentelemetry/sdk\"\nrequire \"opentelemetry/exporter/otlp\"\nrequire \"opentelemetry/instrumentation/sinatra\"\n</code></pre> <p>The Ruby SDK uses OpenTelemetry standard environment variables to configure OTLP export, resource attributes, and service name automatically. When initializing the OpenTelemetry SDK, you will also specify which auto-instrumentation libraries to leverage (ie: Sinatra)</p> <pre><code>OpenTelemetry::SDK.configure do |c|\nc.use \"OpenTelemetry::Instrumentation::Sinatra\"\nend\n</code></pre>"},{"location":"docs/demo/services/email/#traces","title":"Traces","text":""},{"location":"docs/demo/services/email/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>  current_span = OpenTelemetry::Trace.current_span\n</code></pre> <p>Adding multiple attributes to a span is accomplished using <code>add_attributes</code> on the span object.</p> <pre><code>  current_span.add_attributes({\n\"app.order.id\" =&gt; data.order.order_id,\n})\n</code></pre> <p>Adding only a single attribute can be accomplished using <code>set_attribute</code> on the span object.</p> <pre><code>    span.set_attribute(\"app.email.recipient\", data.email)\n</code></pre>"},{"location":"docs/demo/services/email/#create-new-spans","title":"Create new spans","text":"<p>New spans can be created and placed into active context using <code>in_span</code> from an OpenTelemetry Tracer object. When used in conjunction with a <code>do..end</code> block, the span will automatically be ended when the block ends execution.</p> <pre><code>  tracer = OpenTelemetry.tracer_provider.tracer('emailservice')\ntracer.in_span(\"send_email\") do |span|\n# logic in context of span here\nend\n</code></pre>"},{"location":"docs/demo/services/email/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/email/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/feature-flag/","title":"Feature Flag Service","text":"<p>This service is written in Erlang/Elixir and it is responsible for creating, reading, updating and deleting feature flags in a PostgreSQL DB. It is called by Product Catalog and Shipping services.</p> <p>Feature Flag Service Source</p>"},{"location":"docs/demo/services/feature-flag/#traces","title":"Traces","text":""},{"location":"docs/demo/services/feature-flag/#initializing-tracing","title":"Initializing Tracing","text":"<p>In order to set up OpenTelemetry instrumentation for Phoenix, and Ecto, , we need to call the setup methods of their instrumentation packages before starting the Supervisor.</p> <p>This is done in the <code>application.ex</code> as follows:</p> <pre><code>@impl true\ndef start(_type, _args) do\nOpentelemetryEcto.setup([:featureflagservice, :repo])\nOpentelemetryPhoenix.setup()\nchildren = [\n# Start the Ecto repository\nFeatureflagservice.Repo,\n# Start the PubSub system\n{Phoenix.PubSub, name: Featureflagservice.PubSub},\n# Start the Endpoint (http/https)\nFeatureflagserviceWeb.Endpoint\n# Start a worker by calling: Featureflagservice.Worker.start_link(arg)\n# {Featureflagservice.Worker, arg}\n]\n# See https://hexdocs.pm/elixir/Supervisor.html\n# for other strategies and supported options\nopts = [strategy: :one_for_one, name: Featureflagservice.Supervisor]\nSupervisor.start_link(children, opts)\nend\n</code></pre> <p>To add tracing to grpcbox, we need to add the appropriate interceptor.</p> <p>This is configured in the <code>runtime.exs</code> file, as follows:</p> <pre><code>config :grpcbox,\nservers: [\n%{\n:grpc_opts =&gt; %{\n:service_protos =&gt; [:ffs_demo_pb],\n:unary_interceptor =&gt; {:otel_grpcbox_interceptor, :unary},\n:services =&gt; %{:\"hipstershop.FeatureFlagService\" =&gt; :ffs_service}\n},\n:listen_opts =&gt; %{:port =&gt; grpc_port}\n}\n]\n</code></pre>"},{"location":"docs/demo/services/feature-flag/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Adding attributes to a span is accomplished by using <code>?set_attribute</code> on the span object. In the <code>get_flag</code> function two attributes are added to the span.</p> <pre><code>-include_lib(\"grpcbox/include/grpcbox.hrl\").\n-include_lib(\"opentelemetry_api/include/otel_tracer.hrl\").\n-spec get_flag(ctx:t(), ffs_demo_pb:get_flag_request()) -&gt;\n{ok, ffs_demo_pb:get_flag_response(), ctx:t()} | grpcbox_stream:grpc_error_response().\nget_flag(Ctx, #{name := Name}) -&gt;\ncase 'Elixir.Featureflagservice.FeatureFlags':get_feature_flag_by_name(Name) of\nnil -&gt;\n{grpc_error, {?GRPC_STATUS_NOT_FOUND, &lt;&lt;\"the requested feature flag does not exist\"&gt;&gt;}};\n#{'__struct__' := 'Elixir.Featureflagservice.FeatureFlags.FeatureFlag',\ndescription := Description,\nenabled := Enabled,\ninserted_at := CreatedAt,\nupdated_at := UpdatedAt\n} -&gt;\n?set_attribute('app.featureflag.name', Name),\n?set_attribute('app.featureflag.enabled', Enabled),\n{ok, Epoch} = 'Elixir.NaiveDateTime':from_erl({{1970, 1, 1}, {0, 0, 0}}),\nCreatedAtSeconds = 'Elixir.NaiveDateTime':diff(CreatedAt, Epoch),\nUpdatedAtSeconds = 'Elixir.NaiveDateTime':diff(UpdatedAt, Epoch),\nFlag = #{name =&gt; Name,\ndescription =&gt; Description,\nenabled =&gt; Enabled,\ncreated_at =&gt; #{seconds =&gt; CreatedAtSeconds, nanos =&gt; 0},\nupdated_at =&gt; #{seconds =&gt; UpdatedAtSeconds, nanos =&gt; 0}},\n{ok, #{flag =&gt; Flag}, Ctx}\nend.\n</code></pre>"},{"location":"docs/demo/services/feature-flag/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/feature-flag/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/fraud-detection/","title":"Fraud Detection Service","text":"<p>This service analyses incoming orders and detects malicious customers. This is only mocked and received orders are printed out.</p>"},{"location":"docs/demo/services/fraud-detection/#auto-instrumentation","title":"Auto-instrumentation","text":"<p>This service relies on the OpenTelemetry Java Agent to automatically instrument libraries such as Kafka, and to configure the OpenTelemetry SDK. The agent is passed into the process using the <code>-javaagent</code> command line argument. Command line arguments are added through the <code>JAVA_TOOL_OPTIONS</code> in the <code>Dockerfile</code>, and leveraged during the automatically generated Gradle startup script.</p> <pre><code>ENV JAVA_TOOL_OPTIONS=-javaagent:/app/opentelemetry-javaagent.jar\n</code></pre>"},{"location":"docs/demo/services/frontend-proxy/","title":"Frontend Proxy (Envoy)","text":"<p>The frontend proxy is used as a reverse proxy for user-facing web interfaces such as the frontend, Jaeger, Grafana, load generator, and feature flag service.</p>"},{"location":"docs/demo/services/frontend-proxy/#enabling-opentelemetry","title":"Enabling OpenTelemetry","text":"<p>NOTE: Only non-synthetic requests will trigger the envoy tracing.</p> <p>In order to enable Envoy to produce spans whenever receiving a request, the following configuration is required:</p> <pre><code>static_resources:\nlisteners:\n- address:\nsocket_address:\naddress: 0.0.0.0\nport_value: ${ENVOY_PORT}\nfilter_chains:\n- filters:\n- name: envoy.filters.network.http_connection_manager\ntyped_config:\n'@type': type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\ncodec_type: AUTO\nstat_prefix: ingress_http\ntracing:\nprovider:\nname: envoy.tracers.opentelemetry\ntyped_config:\n'@type': type.googleapis.com/envoy.config.trace.v3.OpenTelemetryConfig\ngrpc_service:\nenvoy_grpc:\ncluster_name: opentelemetry_collector\ntimeout: 0.250s\nservice_name: frontend-proxy\nclusters:\n- name: opentelemetry_collector\ntype: STRICT_DNS\nlb_policy: ROUND_ROBIN\ntyped_extension_protocol_options:\nenvoy.extensions.upstreams.http.v3.HttpProtocolOptions:\n'@type': type.googleapis.com/envoy.extensions.upstreams.http.v3.HttpProtocolOptions\nexplicit_http_config:\nhttp2_protocol_options: {}\nload_assignment:\ncluster_name: opentelemetry_collector\nendpoints:\n- lb_endpoints:\n- endpoint:\naddress:\nsocket_address:\naddress: ${OTEL_COLLECTOR_HOST}\nport_value: ${OTEL_COLLECTOR_PORT}\n</code></pre> <p>Where <code>OTEL_COLLECTOR_HOST</code> and <code>OTEL_COLLECTOR_PORT</code> are passed via environment variables.</p>"},{"location":"docs/demo/services/frontend/","title":"Frontend","text":"<p>The frontend is responsible to provide a UI for users, as well as an API leveraged by the UI or other clients. The application is based on Next.JS to provide a React web-based UI and API routes.</p> <p>Frontend source</p>"},{"location":"docs/demo/services/frontend/#server-instrumentation","title":"Server Instrumentation","text":"<p>It is recommended to use a Node required module when starting your NodeJS application to initialize the SDK and auto-instrumentation. When initializing the OpenTelemetry Node.js SDK, you optionally specify which auto-instrumentation libraries to leverage, or make use of the <code>getNodeAutoInstrumentations()</code> function which includes most popular frameworks. The <code>utils/telemetry/Instrumentation.js</code> file contains all code required to initialize the SDK and auto-instrumentation based on standard OpenTelemetry environment variables for OTLP export, resource attributes, and service name.</p> <pre><code>const opentelemetry = require('@opentelemetry/sdk-node');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst {\nOTLPTraceExporter,\n} = require('@opentelemetry/exporter-trace-otlp-grpc');\nconst {\nOTLPMetricExporter,\n} = require('@opentelemetry/exporter-metrics-otlp-grpc');\nconst { PeriodicExportingMetricReader } = require('@opentelemetry/sdk-metrics');\nconst {\nalibabaCloudEcsDetector,\n} = require('@opentelemetry/resource-detector-alibaba-cloud');\nconst {\nawsEc2Detector,\nawsEksDetector,\n} = require('@opentelemetry/resource-detector-aws');\nconst {\ncontainerDetector,\n} = require('@opentelemetry/resource-detector-container');\nconst { gcpDetector } = require('@opentelemetry/resource-detector-gcp');\nconst {\nenvDetector,\nhostDetector,\nosDetector,\nprocessDetector,\n} = require('@opentelemetry/resources');\nconst sdk = new opentelemetry.NodeSDK({\ntraceExporter: new OTLPTraceExporter(),\ninstrumentations: [\ngetNodeAutoInstrumentations({\n// only instrument fs if it is part of another trace\n'@opentelemetry/instrumentation-fs': {\nrequireParentSpan: true,\n},\n}),\n],\nmetricReader: new PeriodicExportingMetricReader({\nexporter: new OTLPMetricExporter(),\n}),\nresourceDetectors: [\ncontainerDetector,\nenvDetector,\nhostDetector,\nosDetector,\nprocessDetector,\nalibabaCloudEcsDetector,\nawsEksDetector,\nawsEc2Detector,\ngcpDetector,\n],\n});\nsdk.start();\n</code></pre> <p>Node required modules are loaded using the <code>--require</code> command line argument. This can be done in the <code>scripts.start</code> section of <code>package.json</code> and starting the application using <code>npm start</code>.</p> <pre><code>  \"scripts\": {\n\"start\": \"node --require ./Instrumentation.js server.js\",\n},\n</code></pre>"},{"location":"docs/demo/services/frontend/#traces","title":"Traces","text":""},{"location":"docs/demo/services/frontend/#span-exceptions-and-status","title":"Span Exceptions and status","text":"<p>You can use the span object's <code>recordException</code> function to create a span event with the full stack trace of a handled error. When recording an exception also be sure to set the span's status accordingly. You can see this in the catch block of the <code>NextApiHandler</code> function in the <code>utils/telemetry/InstrumentationMiddleware.ts</code> file.</p> <pre><code>span.recordException(error as Exception);\nspan.setStatus({ code: SpanStatusCode.ERROR });\n</code></pre>"},{"location":"docs/demo/services/frontend/#create-new-spans","title":"Create new spans","text":"<p>New spans can be created and started using <code>Tracer.startSpan(\"spanName\", options)</code>. Several options can be used to specify how the span can be created.</p> <ul> <li><code>root: true</code> will create a new trace, setting this span as the root.</li> <li><code>links</code> are used to specify links to other spans (even within another trace)   that should be referenced.</li> <li><code>attributes</code> are key/value pairs added to a span, typically used for   application context.</li> </ul> <pre><code>span = tracer.startSpan(`HTTP ${method}`, {\nroot: true,\nkind: SpanKind.SERVER,\nlinks: [{ context: syntheticSpan.spanContext() }],\nattributes: {\n'app.synthetic_request': true,\n[SemanticAttributes.HTTP_TARGET]: target,\n[SemanticAttributes.HTTP_STATUS_CODE]: response.statusCode,\n[SemanticAttributes.HTTP_METHOD]: method,\n[SemanticAttributes.HTTP_USER_AGENT]: headers['user-agent'] || '',\n[SemanticAttributes.HTTP_URL]: `${headers.host}${url}`,\n[SemanticAttributes.HTTP_FLAVOR]: httpVersion,\n},\n});\n</code></pre>"},{"location":"docs/demo/services/frontend/#browser-instrumentation","title":"Browser Instrumentation","text":"<p>The web-based UI that the frontend provides is also instrumented for web browsers. OpenTelemetry instrumentation is included as part of the Next.js App component in <code>pages/_app.tsx</code>. Here instrumentation is imported and initialized.</p> <pre><code>import FrontendTracer from '../utils/telemetry/FrontendTracer';\nif (typeof window !== 'undefined') FrontendTracer();\n</code></pre> <p>The <code>utils/telemetry/FrontendTracer.ts</code> file contains code to initialize a TracerProvider, establish an OTLP export, register trace context propagators, and register web specific auto-instrumentation libraries. Since the browser will send data to an OpenTelemetry collector that will likely be on a separate domain, CORS headers are also setup accordingly.</p> <p>As part of the changes to carry over the <code>synthetic_request</code> attribute flag for the backend services, the <code>applyCustomAttributesOnSpan</code> configuration function has been added to the <code>instrumentation-fetch</code> library custom span attributes logic that way every browser-side span will include it.</p> <pre><code>import {\nCompositePropagator,\nW3CBaggagePropagator,\nW3CTraceContextPropagator,\n} from '@opentelemetry/core';\nimport { WebTracerProvider } from '@opentelemetry/sdk-trace-web';\nimport { SimpleSpanProcessor } from '@opentelemetry/sdk-trace-base';\nimport { registerInstrumentations } from '@opentelemetry/instrumentation';\nimport { getWebAutoInstrumentations } from '@opentelemetry/auto-instrumentations-web';\nimport { Resource } from '@opentelemetry/resources';\nimport { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';\nconst FrontendTracer = async () =&gt; {\nconst { ZoneContextManager } = await import('@opentelemetry/context-zone');\nconst provider = new WebTracerProvider({\nresource: new Resource({\n[SemanticResourceAttributes.SERVICE_NAME]:\nprocess.env.NEXT_PUBLIC_OTEL_SERVICE_NAME,\n}),\n});\nprovider.addSpanProcessor(new SimpleSpanProcessor(new OTLPTraceExporter()));\nconst contextManager = new ZoneContextManager();\nprovider.register({\ncontextManager,\npropagator: new CompositePropagator({\npropagators: [\nnew W3CBaggagePropagator(),\nnew W3CTraceContextPropagator(),\n],\n}),\n});\nregisterInstrumentations({\ntracerProvider: provider,\ninstrumentations: [\ngetWebAutoInstrumentations({\n'@opentelemetry/instrumentation-fetch': {\npropagateTraceHeaderCorsUrls: /.*/,\nclearTimingResources: true,\napplyCustomAttributesOnSpan(span) {\nspan.setAttribute('app.synthetic_request', 'false');\n},\n},\n}),\n],\n});\n};\nexport default FrontendTracer;\n</code></pre>"},{"location":"docs/demo/services/frontend/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/frontend/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/frontend/#baggage","title":"Baggage","text":"<p>OpenTelemetry Baggage is leveraged in the frontend to check if the request is synthetic (from the load generator). Synthetic requests will force the creation of a new trace. The root span from the new trace will contain many of the same attributes as an HTTP request instrumented span.</p> <p>To determine if a Baggage item is set, you can leverage the <code>propagation</code> API to parse the Baggage header, and leverage the <code>baggage</code> API to get or set entries.</p> <pre><code>    const baggage = propagation.getBaggage(context.active());\nif (baggage?.getEntry(\"synthetic_request\")?.value == \"true\") {...}\n</code></pre>"},{"location":"docs/demo/services/kafka/","title":"Kafka","text":"<p>This is used as a message queue service to connect the checkout service with the accounting and fraud detection services.</p> <p>Kafka service source</p>"},{"location":"docs/demo/services/kafka/#auto-instrumentation","title":"Auto-instrumentation","text":"<p>This service relies on the OpenTelemetry Java Agent and the built in JMX Metric Insight Module to capture kafka broker metrics and send them off to the collector via OTLP.</p> <p>The agent is passed into the process using the <code>-javaagent</code> command line argument. Command line arguments are added through the <code>KAFKA_OPTS</code> in the <code>Dockerfile</code>.</p> <pre><code>ENV KAFKA_OPTS=\"-javaagent:/tmp/opentelemetry-javaagent.jar -Dotel.jmx.target.system=kafka-broker\"\n</code></pre>"},{"location":"docs/demo/services/load-generator/","title":"Load Generator","text":"<p>The load generator is based on the Python load testing framework Locust. By default it will simulate users requesting several different routes from the frontend.</p> <p>Load generator source</p>"},{"location":"docs/demo/services/load-generator/#traces","title":"Traces","text":""},{"location":"docs/demo/services/load-generator/#initializing-tracing","title":"Initializing Tracing","text":"<p>Since this service is a locustfile, the OpenTelemetry SDK is initialized after the import statements. This code will create a tracer provider, and establish a Span Processor to use. Export endpoints, resource attributes, and service name are automatically set using OpenTelemetry environment variables.</p> <pre><code>tracer_provider = TracerProvider()\ntrace.set_tracer_provider(tracer_provider)\ntracer_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter()))\n</code></pre>"},{"location":"docs/demo/services/load-generator/#adding-instrumentation-libraries","title":"Adding instrumentation libraries","text":"<p>To add instrumentation libraries you need to import the Instrumentors for each library in your Python code. Locust uses the <code>Requests</code> and<code>URLLib3</code> libraries, so we will import their Instrumentors.</p> <pre><code>from opentelemetry.instrumentation.requests import RequestsInstrumentor\nfrom opentelemetry.instrumentation.urllib3 import URLLib3Instrumentor\n</code></pre> <p>In your code before the library is leveraged, the Instrumentor needs to be initialized by calling <code>instrument()</code>.</p> <pre><code>RequestsInstrumentor().instrument()\nURLLib3Instrumentor().instrument()\n</code></pre> <p>Once initialized, every Locust requests for this load generator will have their own trace with a span for each of the <code>Requests</code> and <code>URLLib3</code> libraries.</p>"},{"location":"docs/demo/services/load-generator/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/load-generator/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/load-generator/#baggage","title":"Baggage","text":"<p>OpenTelemetry Baggage is used by the load generator to indicate that the traces are synthetically generated. This is done in the <code>on_start</code> function by creating a context object containing the baggage item, and associating that context for all tasks by the load generator.</p> <pre><code>    ctx = baggage.set_baggage(\"synthetic_request\", \"true\")\ncontext.attach(ctx)\n</code></pre>"},{"location":"docs/demo/services/payment/","title":"Payment Service","text":"<p>This service is responsible to process credit card payments for orders. It will return an error if the credit card is invalid or the payment can not be processed.</p> <p>Payment service source</p>"},{"location":"docs/demo/services/payment/#initializing-opentelemetry","title":"Initializing OpenTelemetry","text":"<p>It is recommended to <code>require</code> Node.js app using an initializer file that initializes the SDK and auto-instrumentation. When initializing the OpenTelemetry Node.js SDK in that module, you optionally specify which auto-instrumentation libraries to leverage, or make use of the <code>getNodeAutoInstrumentations()</code> function which includes most popular frameworks. The below example of an initializer file (<code>opentelemetry.js</code>) contains all code required to initialize the SDK and auto-instrumentation based on standard OpenTelemetry environment variables for OTLP export, resource attributes, and service name. It then <code>require</code>s your app at <code>./index.js</code> to start it up once the SDK is initialized.</p> <pre><code>const opentelemetry = require('@opentelemetry/sdk-node');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst {\nOTLPTraceExporter,\n} = require('@opentelemetry/exporter-trace-otlp-grpc');\nconst {\nOTLPMetricExporter,\n} = require('@opentelemetry/exporter-metrics-otlp-grpc');\nconst { PeriodicExportingMetricReader } = require('@opentelemetry/sdk-metrics');\nconst {\nalibabaCloudEcsDetector,\n} = require('@opentelemetry/resource-detector-alibaba-cloud');\nconst {\nawsEc2Detector,\nawsEksDetector,\n} = require('@opentelemetry/resource-detector-aws');\nconst {\ncontainerDetector,\n} = require('@opentelemetry/resource-detector-container');\nconst { gcpDetector } = require('@opentelemetry/resource-detector-gcp');\nconst {\nenvDetector,\nhostDetector,\nosDetector,\nprocessDetector,\n} = require('@opentelemetry/resources');\nconst sdk = new opentelemetry.NodeSDK({\ntraceExporter: new OTLPTraceExporter(),\ninstrumentations: [\ngetNodeAutoInstrumentations({\n// only instrument fs if it is part of another trace\n'@opentelemetry/instrumentation-fs': {\nrequireParentSpan: true,\n},\n}),\n],\nmetricReader: new PeriodicExportingMetricReader({\nexporter: new OTLPMetricExporter(),\n}),\nresourceDetectors: [\ncontainerDetector,\nenvDetector,\nhostDetector,\nosDetector,\nprocessDetector,\nalibabaCloudEcsDetector,\nawsEksDetector,\nawsEc2Detector,\ngcpDetector,\n],\n});\nsdk.start();\n</code></pre> <p>You can then use <code>opentelemetry.js</code> to start your app. This can be done in the <code>ENTRYPOINT</code> command for the service's <code>Dockerfile</code>.</p> <pre><code>ENTRYPOINT [ \"node\", \"./opentelemetry.js\" ]\n</code></pre>"},{"location":"docs/demo/services/payment/#traces","title":"Traces","text":""},{"location":"docs/demo/services/payment/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>const span = opentelemetry.trace.getActiveSpan();\n</code></pre> <p>Adding attributes to a span is accomplished using <code>setAttributes</code> on the span object. In the <code>chargeServiceHandler</code> function an attributes is added to the span as an anonymous object (map) for the attribute key/values pair.</p> <pre><code>span.setAttributes({\n'app.payment.amount': parseFloat(`${amount.units}.${amount.nanos}`),\n});\n</code></pre>"},{"location":"docs/demo/services/payment/#span-exceptions-and-status","title":"Span Exceptions and status","text":"<p>You can use the span object's <code>recordException</code> function to create a span event with the full stack trace of a handled error. When recording an exception also be sure to set the span's status accordingly. You can see this in the <code>chargeServiceHandler</code> function</p> <pre><code>span.recordException(err);\nspan.setStatus({ code: opentelemetry.SpanStatusCode.ERROR });\n</code></pre>"},{"location":"docs/demo/services/payment/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/payment/#creating-meters-and-instruments","title":"Creating Meters and Instruments","text":"<p>Meters can be created using the <code>@opentelemetry/api-metrics</code> package. You can create meters as seen below, and then use the created meter to create instruments.</p> <pre><code>const { metrics } = require('@opentelemetry/api-metrics');\nconst meter = metrics.getMeter('paymentservice');\nconst transactionsCounter = meter.createCounter('app.payment.transactions');\n</code></pre> <p>Meters and Instruments are supposed to stick around. This means you should get a Meter or an Instrument once , and then re-use it as needed, if possible.</p>"},{"location":"docs/demo/services/payment/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/payment/#baggage","title":"Baggage","text":"<p>OpenTelemetry Baggage is leveraged in this service to check if the request is synthetic (from the load generator). Synthetic requests will not be charged, which is indicated with a span attribute. The <code>charge.js</code> file which does the actual payment processing, has logic to check the baggage.</p> <pre><code>// check baggage for synthetic_request=true, and add charged attribute accordingly\nconst baggage = propagation.getBaggage(context.active());\nif (\nbaggage &amp;&amp;\nbaggage.getEntry('synthetic_request') &amp;&amp;\nbaggage.getEntry('synthetic_request').value == 'true'\n) {\nspan.setAttribute('app.payment.charged', false);\n} else {\nspan.setAttribute('app.payment.charged', true);\n}\n</code></pre>"},{"location":"docs/demo/services/product-catalog/","title":"Product Catalog Service","text":"<p>This service is responsible to return information about products. The service can be used to get all products, search for specific products, or return details about any single product.</p> <p>Product Catalog service source</p>"},{"location":"docs/demo/services/product-catalog/#traces","title":"Traces","text":""},{"location":"docs/demo/services/product-catalog/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initTracerProvider</code> function.</p> <pre><code>func initTracerProvider() *sdktrace.TracerProvider {\nctx := context.Background()\nexporter, err := otlptracegrpc.New(ctx)\nif err != nil {\nlog.Fatalf(\"OTLP Trace gRPC Creation: %v\", err)\n}\ntp := sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(exporter),\nsdktrace.WithResource(initResource()),\n)\notel.SetTracerProvider(tp)\notel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{}))\nreturn tp\n}\n</code></pre> <p>You should call <code>TracerProvider.Shutdown()</code> when your service is shutdown to ensure all spans are exported. This service makes that call as part of a deferred function in main</p> <pre><code>    tp := InitTracerProvider()\ndefer func() {\nif err := tp.Shutdown(context.Background()); err != nil {\nlog.Fatalf(\"Tracer Provider Shutdown: %v\", err)\n}\n}()\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#adding-grpc-auto-instrumentation","title":"Adding gRPC auto-instrumentation","text":"<p>This service receives gRPC requests, which are instrumented in the main function as part of the gRPC server creation.</p> <pre><code>    srv := grpc.NewServer(\ngrpc.UnaryInterceptor(otelgrpc.UnaryServerInterceptor()),\ngrpc.StreamInterceptor(otelgrpc.StreamServerInterceptor()),\n)\n</code></pre> <p>This service will issue outgoing gRPC calls, which are all instrumented by wrapping the gRPC client with instrumentation.</p> <pre><code>func createClient(ctx context.Context, svcAddr string) (*grpc.ClientConn, error) {\nreturn grpc.DialContext(ctx, svcAddr,\ngrpc.WithTransportCredentials(insecure.NewCredentials()),\ngrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\ngrpc.WithStreamInterceptor(otelgrpc.StreamClientInterceptor()),\n)\n}\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>    span := trace.SpanFromContext(ctx)\n</code></pre> <p>Adding attributes to a span is accomplished using <code>SetAttributes</code> on the span object. In the <code>GetProduct</code> function an attribute for the product id is added to the span.</p> <pre><code>    span.SetAttributes(\nattribute.String(\"app.product.id\", req.Id),\n)\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#setting-span-status","title":"Setting span status","text":"<p>This service can catch and handle an error condition based on a feature flag. In an error condition, the span status is set accordingly using <code>SetStatus</code> on the span object. You can see this in the <code>GetProduct</code> function.</p> <pre><code>    msg := fmt.Sprintf(\"Error: ProductCatalogService Fail Feature Flag Enabled\")\nspan.SetStatus(otelcodes.Error, msg)\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#add-span-events","title":"Add span events","text":"<p>Adding span events is accomplished using <code>AddEvent</code> on the span object. In the <code>GetProduct</code> function a span event is added when an error condition is handled, or when a product is successfully found.</p> <pre><code>    span.AddEvent(msg)\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/product-catalog/#initializing-metrics","title":"Initializing Metrics","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code> using the <code>initMeterProvider</code> function.</p> <pre><code>func initMeterProvider() *sdkmetric.MeterProvider {\nctx := context.Background()\nexporter, err := otlpmetricgrpc.New(ctx)\nif err != nil {\nlog.Fatalf(\"new otlp metric grpc exporter failed: %v\", err)\n}\nmp := sdkmetric.NewMeterProvider(sdkmetric.WithReader(sdkmetric.NewPeriodicReader(exporter)))\nglobal.SetMeterProvider(mp)\nreturn mp\n}\n</code></pre> <p>You should call <code>initMeterProvider.Shutdown()</code> when your service is shutdown to ensure all records are exported. This service makes that call as part of a deferred function in main.</p> <pre><code>    mp := initMeterProvider()\ndefer func() {\nif err := mp.Shutdown(context.Background()); err != nil {\nlog.Fatalf(\"Error shutting down meter provider: %v\", err)\n}\n}()\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#adding-golang-runtime-auto-instrumentation","title":"Adding golang runtime auto-instrumentation","text":"<p>Golang runtime is instrumented in the main function</p> <pre><code>    err := runtime.Start(runtime.WithMinimumReadMemStatsInterval(time.Second))\nif err != nil {\nlog.Fatal(err)\n}\n</code></pre>"},{"location":"docs/demo/services/product-catalog/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/quote/","title":"Quote Service","text":"<p>This service is responsible for calculating shipping costs, based on the number of items to be shipped. The quote service is called from Shipping Service via HTTP.</p> <p>The Quote Service is implemented using the Slim framework and php-di for managing the Dependency Injection.</p> <p>The PHP instrumentation may vary when using a different framework.</p> <p>Quote service source</p>"},{"location":"docs/demo/services/quote/#traces","title":"Traces","text":""},{"location":"docs/demo/services/quote/#initializing-tracing","title":"Initializing Tracing","text":"<p>In this demo, the OpenTelemetry SDK has been automatically created as part of SDK autoloading, which happens as part of composer autoloading.</p> <p>This is enabled by setting the environment variable <code>OTEL_PHP_AUTOLOAD_ENABLED=true</code>.</p> <pre><code>    require __DIR__ . '/../vendor/autoload.php';\n</code></pre> <p>There are multiple ways to create or obtain a <code>Tracer</code>, in this example we obtain one from the global tracer provider which was initialized above, as part of SDK autoloading:</p> <pre><code>    $tracer = Globals::tracerProvider()-&gt;getTracer('manual-instrumentation');\n</code></pre>"},{"location":"docs/demo/services/quote/#manually-creating-spans","title":"Manually creating spans","text":"<p>Creating a span manually can be done via a <code>Tracer</code>. The span will be default be a child of the active span in the current execution context:</p> <pre><code>    $span = Globals::tracerProvider()\n        -&gt;getTracer('manual-instrumentation')\n        -&gt;spanBuilder('calculate-quote')\n        -&gt;setSpanKind(SpanKind::KIND_INTERNAL)\n        -&gt;startSpan();\n    /* calculate quote */\n    $span-&gt;end();\n</code></pre>"},{"location":"docs/demo/services/quote/#add-span-attributes","title":"Add span attributes","text":"<p>You can obtain the current span using <code>OpenTelemetry\\API\\Trace\\Span</code>.</p> <pre><code>    $span = Span::getCurrent();\n</code></pre> <p>Adding attributes to a span is accomplished using <code>setAttribute</code> on the span object. In the <code>calculateQuote</code> function 2 attributes are added to the <code>childSpan</code>.</p> <pre><code>    $childSpan-&gt;setAttribute('app.quote.items.count', $numberOfItems);\n    $childSpan-&gt;setAttribute('app.quote.cost.total', $quote);\n</code></pre>"},{"location":"docs/demo/services/quote/#add-span-events","title":"Add span events","text":"<p>Adding span events is accomplished using <code>addEvent</code> on the span object. In the <code>getquote</code> route span events are added. Some events have additional attributes, others do not.</p> <p>Adding a span event without attributes:</p> <pre><code>    $span-&gt;addEvent('Received get quote request, processing it');\n</code></pre> <p>Adding a span event with additional attributes:</p> <pre><code>    $span-&gt;addEvent('Quote processed, response sent back', [\n        'app.quote.cost.total' =&gt; $payload\n    ]);\n</code></pre>"},{"location":"docs/demo/services/quote/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/quote/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/recommendation/","title":"Recommendation Service","text":"<p>This service is responsible to get a list of recommended products for the user based on existing product ids the user is browsing.</p> <p>Recommendation service source</p>"},{"location":"docs/demo/services/recommendation/#auto-instrumentation","title":"Auto-instrumentation","text":"<p>This Python based service, makes use of the OpenTelemetry auto-instrumentor for Python, accomplished by leveraging the <code>opentelemetry-instrument</code> Python wrapper to run the scripts. This can be done in the <code>ENTRYPOINT</code> command for the service's <code>Dockerfile</code>.</p> <pre><code>ENTRYPOINT [ \"opentelemetry-instrument\", \"python\", \"recommendation_server.py\" ]\n</code></pre>"},{"location":"docs/demo/services/recommendation/#traces","title":"Traces","text":""},{"location":"docs/demo/services/recommendation/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized in the <code>__main__</code> code block. This code will create a tracer provider, and establish a Span Processor to use. Export endpoints, resource attributes, and service name are automatically set by the OpenTelemetry auto instrumentor based on environment variables.</p> <pre><code>    tracer = trace.get_tracer_provider().get_tracer(\"recommendationservice\")\n</code></pre>"},{"location":"docs/demo/services/recommendation/#add-attributes-to-auto-instrumented-spans","title":"Add attributes to auto-instrumented spans","text":"<p>Within the execution of auto-instrumented code you can get current span from context.</p> <pre><code>    span = trace.get_current_span()\n</code></pre> <p>Adding attributes to a span is accomplished using <code>set_attribute</code> on the span object. In the <code>ListRecommendations</code> function an attribute is added to the span.</p> <pre><code>    span.set_attribute(\"app.products_recommended.count\", len(prod_list))\n</code></pre>"},{"location":"docs/demo/services/recommendation/#create-new-spans","title":"Create new spans","text":"<p>New spans can be created and placed into active context using <code>start_as_current_span</code> from an OpenTelemetry Tracer object. When used in conjunction with a <code>with</code> block, the span will automatically be ended when the block ends execution. This is done in the <code>get_product_list</code> function.</p> <pre><code>    with tracer.start_as_current_span(\"get_product_list\") as span:\n</code></pre>"},{"location":"docs/demo/services/recommendation/#metrics","title":"Metrics","text":""},{"location":"docs/demo/services/recommendation/#initializing-metrics","title":"Initializing Metrics","text":"<p>The OpenTelemetry SDK is initialized in the <code>__main__</code> code block. This code will create a meter provider. Export endpoints, resource attributes, and service name are automatically set by the OpenTelemetry auto instrumentor based on environment variables.</p> <pre><code>    meter = metrics.get_meter_provider().get_meter(\"recommendationservice\")\n</code></pre>"},{"location":"docs/demo/services/recommendation/#custom-metrics","title":"Custom metrics","text":"<p>The following custom metrics are currently available:</p> <ul> <li><code>app_recommendations_counter</code>: Cumulative count of # recommended products per   service call</li> </ul>"},{"location":"docs/demo/services/recommendation/#auto-instrumented-metrics","title":"Auto-instrumented metrics","text":"<p>The following metrics are available through auto-instrumentation, courtesy of the <code>opentelemetry-instrumentation-system-metrics</code>, which is installed as part of <code>opentelemetry-bootstrap</code> on building the recommendationservice Docker image:</p> <ul> <li><code>runtime.cpython.cpu_time</code></li> <li><code>runtime.cpython.memory</code></li> <li><code>runtime.cpython.gc_count</code></li> </ul>"},{"location":"docs/demo/services/recommendation/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/demo/services/shipping/","title":"Shipping Service","text":"<p>This service is responsible for providing shipping information including pricing and tracking information, when requested from Checkout Service.</p> <p>Shipping service is built primarily with Tonic, Reqwest, and OpenTelemetry Libraries/Components. Other sub-dependencies are included in <code>Cargo.toml</code>.</p> <p>Depending on your framework and runtime, you may consider consulting rust docs to supplement. You'll find examples of async and sync spans in quote requests and tracking ID's respectively.</p> <p>The <code>build.rs</code> supports development outside docker, given a rust installation. Otherwise, consider building with <code>docker compose</code> to edit / assess changes as needed.</p> <p>Shipping service source</p>"},{"location":"docs/demo/services/shipping/#traces","title":"Traces","text":""},{"location":"docs/demo/services/shipping/#initializing-tracing","title":"Initializing Tracing","text":"<p>The OpenTelemetry SDK is initialized from <code>main</code>.</p> <pre><code>fn init_tracer() -&gt; Result&lt;sdktrace::Tracer, TraceError&gt; {\nglobal::set_text_map_propagator(TraceContextPropagator::new());\nlet os_resource = OsResourceDetector.detect(Duration::from_secs(0));\nlet process_resource = ProcessResourceDetector.detect(Duration::from_secs(0));\nlet sdk_resource = SdkProvidedResourceDetector.detect(Duration::from_secs(0));\nlet env_resource = EnvResourceDetector::new().detect(Duration::from_secs(0));\nopentelemetry_otlp::new_pipeline()\n.tracing()\n.with_exporter(\nopentelemetry_otlp::new_exporter()\n.tonic()\n.with_endpoint(format!(\n\"{}{}\",\nenv::var(\"OTEL_EXPORTER_OTLP_TRACES_ENDPOINT\")\n.unwrap_or_else(|_| \"http://otelcol:4317\".to_string()),\n\"/v1/traces\"\n)), // TODO: assume this ^ is true from config when opentelemetry crate &gt; v0.17.0\n// https://github.com/open-telemetry/opentelemetry-rust/pull/806 includes the environment variable.\n)\n.with_trace_config(\nsdktrace::config()\n.with_resource(os_resource.merge(&amp;process_resource).merge(&amp;sdk_resource).merge(&amp;env_resource)),\n)\n.install_batch(opentelemetry::runtime::Tokio)\n}\n</code></pre> <p>Spans and other metrics are created in this example throughout <code>tokio</code> async runtimes found within <code>tonic</code> server functions. Be mindful of async runtime, context guards, and inability to move and clone <code>spans</code> when replicating from these samples.</p>"},{"location":"docs/demo/services/shipping/#adding-grpc-instrumentation","title":"Adding gRPC instrumentation","text":"<p>This service receives gRPC requests, which are instrumented in the middleware.</p> <p>The root span is started and passed down as reference in the same thread to another closure where we call <code>quoteservice</code>.</p> <pre><code>    let tracer = global::tracer(\"shippingservice\");\nlet mut span = tracer.span_builder(\"hipstershop.ShippingService/GetQuote\").with_kind(SpanKind::Server).start_with_context(&amp;tracer, &amp;parent_cx);\nspan.set_attribute(semcov::trace::RPC_SYSTEM.string(RPC_SYSTEM_GRPC));\nspan.add_event(\"Processing get quote request\".to_string(), vec![]);\nlet cx = Context::current_with_span(span);\nlet q = match create_quote_from_count(itemct)\n.with_context(cx.clone())\n.await\n//-&gt; create_quote_from_count()...\nlet f = match request_quote(count).await {\nOk(float) =&gt; float,\nErr(err) =&gt; {\nlet msg = format!(\"{}\", err);\nreturn Err(tonic::Status::unknown(msg));\n}\n};\nOk(get_active_span(|span| {\nlet q = create_quote_from_float(f);\nspan.add_event(\n\"Received Quote\".to_string(),\nvec![KeyValue::new(\"app.shipping.cost.total\", format!(\"{}\", q))],\n);\nspan.set_attribute(KeyValue::new(\"app.shipping.items.count\", count as i64));\nspan.set_attribute(KeyValue::new(\"app.shipping.cost.total\", format!(\"{}\", q)));\nq\n}))\n//&lt;- create_quote_from_count()...\ncx.span().set_attribute(semcov::trace::RPC_GRPC_STATUS_CODE.i64(RPC_GRPC_STATUS_CODE_OK));\n</code></pre> <p>Note that we create a context around the root span and send a clone to the async function create_quote_from_count(). After create_quote_from_count() completes, we can add additional attributes to the root span as appropriate.</p> <p>You may also notice the <code>attributes</code> set on the span in this example, and <code>events</code> propagated similarly. With any valid <code>span</code> pointer (attached to context) the OpenTelemetry API will work.</p>"},{"location":"docs/demo/services/shipping/#adding-http-instrumentation","title":"Adding HTTP instrumentation","text":"<p>A child client span is also produced for the outgoing HTTP call to <code>quoteservice</code> via the <code>reqwest</code> client. This span pairs up with the corresponding <code>quoteservice</code> server span. The tracing instrumentation is implemented in the client middleware making use of the available <code>reqwest-middleware</code>, <code>reqwest-tracing</code> and <code>tracing-opentelementry</code> libraries:</p> <pre><code>    let reqwest_client = reqwest::Client::new();\nlet client = ClientBuilder::new(reqwest_client)\n.with(TracingMiddleware::&lt;SpanBackendWithUrl&gt;::new())\n.build();\n</code></pre>"},{"location":"docs/demo/services/shipping/#add-span-attributes","title":"Add span attributes","text":"<p>Provided you are on the same thread, or in a context passed from a span-owning thread, or a <code>ContextGuard</code> is in scope, you can get an active span with <code>get_active_span</code>. You can find examples of all of these in the demo, with context available in <code>shipping_service</code> for sync/async runtime. You should consult <code>quote.rs</code> and/or the example above to see context-passed-to-async runtime.</p> <p>See below for a snippet from <code>shiporder</code> that holds context and a span in scope. This is appropriate in our case of a sync runtime.</p> <pre><code>    let parent_cx =\nglobal::get_text_map_propagator(|prop| prop.extract(&amp;MetadataMap(request.metadata())));\n// in this case, generating a tracking ID is trivial\n// we'll create a span and associated events all in this function.\nlet tracer = global::tracer(\"shippingservice\");\nlet mut span = tracer\n.span_builder(\"hipstershop.ShippingService/ShipOrder\").with_kind(SpanKind::Server).start_with_context(&amp;tracer, &amp;parent_cx);\n</code></pre> <p>You must add attributes to a span in context with <code>set_attribute</code>, followed by a <code>KeyValue</code> object, containing a key, and value.</p> <pre><code>    let tid = create_tracking_id();\nspan.set_attribute(KeyValue::new(\"app.shipping.tracking.id\", tid.clone()));\ninfo!(\"Tracking ID Created: {}\", tid);\n</code></pre>"},{"location":"docs/demo/services/shipping/#add-span-events","title":"Add span events","text":"<p>Adding span events is accomplished using <code>add_event</code> on the span object. Both server routes, for <code>ShipOrderRequest</code> (sync) and <code>GetQuoteRequest</code> (async), have events on spans. Attributes are not included here, but are simple to include.</p> <p>Adding a span event:</p> <pre><code>    let tid = create_tracking_id();\nspan.set_attribute(KeyValue::new(\"app.shipping.tracking.id\", tid.clone()));\ninfo!(\"Tracking ID Created: {}\", tid);\n</code></pre>"},{"location":"docs/demo/services/shipping/#metrics","title":"Metrics","text":"<p>TBD</p>"},{"location":"docs/demo/services/shipping/#logs","title":"Logs","text":"<p>TBD</p>"},{"location":"docs/faas/","title":"Functions as a Service","text":"<p>Functions as a Service (FaaS) is an important serverless compute platform for cloud native applications. However, platform quirks usually mean these applications have slightly different monitoring guidance and requirements than applications running on Kubernetes or Virtual Machines.</p> <p>The initial vendor scope of the FaaS documentation is around Microsoft Azure, Google Cloud Platform (GCP), and Amazon Web Services (AWS). AWS functions are also known as Lambda.</p>"},{"location":"docs/faas/#community-assets","title":"Community Assets","text":"<p>The OpenTelemetry community currently provides pre-built Lambda layers able to auto-instrument your application as well as a the option of standalone Collector Lambda layer that can be used when instrumenting applications manually or automatically.</p> <p>The release status can be tracked in the OpenTelemetry-Lambda repository.</p>"},{"location":"docs/faas/lambda-auto-instrument/","title":"Lambda Auto-Instrumentation","text":"<p>The OpenTelemetry community provides standalone instrumentation Lambda layers for the following languages:</p> <ul> <li>Java</li> <li>JavaScript</li> <li>Python</li> </ul> <p>These can be added to your Lambda using the AWS portal to automatically instrument your application. These layers do not include the Collector which is a required addition unless you configure an external Collector instance to send your data.</p>"},{"location":"docs/faas/lambda-auto-instrument/#add-the-arn-of-the-otel-collector-lambda-layer","title":"Add the ARN of the OTel Collector Lambda layer","text":"<p>See the Collector Lambda layer guidance to add the layer to your application and configure the Collector. We recommend you add this first.</p>"},{"location":"docs/faas/lambda-auto-instrument/#language-requirements","title":"Language Requirements","text":"<p>{{&lt; tabpane text=true &gt;}} {{% tab Java %}}</p> <p>The Lambda layer supports the Java 11 (Corretto) Lambda runtime. It does not support the Java 8 Lambda runtimes. For more information about supported Java versions, see the OpenTelemetry Java documentation.</p> <p>Note: The Java Auto-instrumentation Agent is in the Lambda layer - Automatic instrumentation has a notable impact on startup time on AWS Lambda and you will generally need to use this along with provisioned concurrency and warmup requests to serve production requests without causing timeouts on initial requests while it initializes.</p> <p>By default, the OTel Java Agent in the Layer will try to auto-instrument all the code in your application. This can have a negative impact on the Lambda cold startup time.</p> <p>We recommend that you only enable auto-instrumentation for the libraries/frameworks that are used by your application.</p> <p>To enable only specific instrumentations you can use the following environment variables:</p> <pre><code>* OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED - When set to false, disables auto-instrumentation in the Layer, requiring each instrumentation to be enabled individually.\n* OTEL_INSTRUMENTATION_[NAME]_ENABLED - Set to true to enable auto-instrumentation for a specific library or framework. [NAME] should be replaced by the instrumentation that you want to enable. The full list of available instrumentations can be found in this link.\n</code></pre> <p>For example, to only enable auto-instrumentation for Lambda and the AWS SDK, you would have to set the following environment variables:</p> <pre><code>```bash\nCopy\nOTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false\nOTEL_INSTRUMENTATION_AWS_LAMBDA_ENABLED=true\nOTEL_INSTRUMENTATION_AWS_SDK_ENABLED=true\n```\n</code></pre> <p>{{% /tab %}} {{% tab JavaScript %}}</p> <p>The Lambda layer supports Node.js v14+ Lambda runtimes. For more information about supported JavaScript and Node.js versions, see the OpenTelemetry JavaScript documentation.</p> <p>{{% /tab %}} {{% tab Python %}}</p> <p>The Lambda layer supports Python 3.8 and Python 3.9 Lambda runtimes. For more information about supported Python versions, see the OpenTelemetry Python documentation and the package on PyPi.</p> <p>{{% /tab %}} {{&lt; /tabpane &gt;}}</p>"},{"location":"docs/faas/lambda-auto-instrument/#add-the-arn-of-instrumentation-lambda-layer","title":"Add the ARN of Instrumentation Lambda Layer","text":"<p>To enable the OTel auto-instrumentation in your Lambda function, you need to add and configure the instrumentation and Collector layers, and then enable tracing.</p> <ol> <li>Open the Lambda function you intend to instrument in the AWS console.</li> <li>In the Layers in Designer section, choose Add a layer.</li> <li>Under specify an ARN, paste the layer ARN, and then choose Add.</li> </ol>"},{"location":"docs/faas/lambda-auto-instrument/#configure-your-sdk-exporters","title":"Configure your SDK exporters","text":"<p>The default exporters used by the Lambda layers will work without any changes if there is an embedded Collector with gRPC / HTTP receivers. The environment variables do not need to be updated. However, there are varying levels of protocol support and default values by language which are documented below.</p> <p>{{&lt; tabpane text=true &gt;}} {{% tab Java %}}</p> <p><code>OTEL_EXPORTER_OTLP_PROTOCOL=grpc</code> Supports: <code>grpc</code>, <code>http/protobuf</code> and <code>http/json</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317</code></p> <p>{{% /tab %}} {{% tab Python %}}</p> <p><code>OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf</code> Supports: <code>http/protobuf</code> and <code>http/json</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318</code></p> <p>{{% /tab %}} {{% tab JavaScript %}}</p> <p><code>OTEL_EXPORTER_OTLP_PROTOCOL</code> env var is not supported The hard coded exporter uses the protocol <code>http/protobuf</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318</code></p> <p>{{% /tab %}} {{&lt; /tabpane &gt;}}</p>"},{"location":"docs/faas/lambda-auto-instrument/#publish-your-lambda","title":"Publish your Lambda","text":"<p>Publish a new version of your Lambda to deploy the new changes and instrumentation.</p>"},{"location":"docs/faas/lambda-collector/","title":"Lambda Collector Configuration","text":"<p>The OpenTelemetry community offers the Collector in a separate Lambda layer from the instrumentation layers to give users maximum flexibility. This is different than the current AWS Distribution of OpenTelemetry (ADOT) implementation which bundles instrumentation and the Collector together.</p>"},{"location":"docs/faas/lambda-collector/#add-the-arn-of-the-otel-collector-lambda-layer","title":"Add the ARN of the OTel Collector Lambda layer","text":"<p>Once you've instrumented your application you should add the Collector Lambda layer to collect and submit your data to your chosen backend.</p> <p>Note: Lambda layers are a regionalized resource, meaning that they can only be used in the Region in which they are published. Make sure to use the layer in the same region as your Lambda functions.</p> <p>Find the supported regions and amd64(x86_64)/arm64 layer ARN in the table below for the ARNs to consume.</p>"},{"location":"docs/faas/lambda-collector/#configure-the-otel-collector","title":"Configure the OTel Collector","text":"<p>The configuration of the OTel Collector Lambda layer follows the OpenTelemetry standard.</p> <p>By default, the OTel Collector Lambda layer uses the config.yaml.</p>"},{"location":"docs/faas/lambda-collector/#set-the-environment-variable-for-your-preferred-backend","title":"Set the Environment Variable for your Preferred Backend","text":"<p>In the Lambda environment variable settings create a new variable that holds your authorization token.</p>"},{"location":"docs/faas/lambda-collector/#update-the-default-exporters","title":"Update the Default Exporters","text":"<p>In your <code>config.yaml</code> file add your preferred exporter(s) if they are not already present. Configure your exporter(s) using the environment variables you set for your access tokens in the previous step.</p> <p>Without an environment variable being set for your exporters the default configuration only supports emitting data using the logging exporter. Here is the default configuration:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nendpoint: 'localhost:4317'\nhttp:\nendpoint: 'localhost:4318'\nexporters:\nlogging:\nloglevel: debug\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [logging]\nmetrics:\nreceivers: [otlp]\nexporters: [logging]\ntelemetry:\nmetrics:\naddress: localhost:8888\n</code></pre>"},{"location":"docs/faas/lambda-collector/#publish-your-lambda","title":"Publish your Lambda","text":"<p>Publish a new version of your Lambda to enable the changes you made.</p>"},{"location":"docs/faas/lambda-collector/#advanced-otel-collector-configuration","title":"Advanced OTel Collector Configuration","text":"<p>Please find the list of available components supported for custom configuration here. To enable debugging, you can use the configuration file to set log level to debug. See the example below.</p>"},{"location":"docs/faas/lambda-collector/#choose-your-preferred-confmap-provider","title":"Choose your Preferred Confmap Provider","text":"<p>The OTel Lambda Layers supports the following types of confmap providers: <code>file</code>, <code>env</code>, <code>yaml</code>, <code>http</code>, <code>https</code>, and <code>s3</code>. To customize the OTel collector configuration using different Confmap providers, Please refer to Amazon Distribution of OpenTelemetry Confmap providers document for more information.</p>"},{"location":"docs/faas/lambda-collector/#create-a-custom-configuration-file","title":"Create a Custom Configuration File","text":"<p>Here is a sample configuration file of <code>collector.yaml</code> in the root directory:</p> <pre><code>#collector.yaml in the root directory\n#Set an environment variable 'OPENTELEMETRY_COLLECTOR_CONFIG_FILE' to '/var/task/collector.yaml'\nreceivers:\notlp:\nprotocols:\ngrpc:\nendpoint: 'localhost:4317'\nhttp:\nendpoint: 'localhost:4318'\nexporters:\nlogging:\nawsxray:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [awsxray]\nmetrics:\nreceivers: [otlp]\nexporters: [logging]\ntelemetry:\nmetrics:\naddress: localhost:8888\n</code></pre>"},{"location":"docs/faas/lambda-collector/#map-your-custom-configuration-file-using-environment-variables","title":"Map your Custom Configuration File using Environment Variables","text":"<p>Once your collector configuration is set through a confmap provider, create an environment variable on your Lambda function <code>OPENTELEMETRY_COLLECTOR_CONFIG_FILE</code> and set the path of configuration w.r.t to the confmap provider as its value. for e.g, if you are using a file configmap provider, set its value to <code>/var/task/&lt;path&gt;/&lt;to&gt;/&lt;filename&gt;</code>. This will tell the extension where to find the collector configuration.</p>"},{"location":"docs/faas/lambda-collector/#custom-collector-configuration-using-the-cli","title":"Custom Collector Configuration Using the CLI","text":"<p>You can set this via the Lambda console, or via the AWS CLI.</p> <pre><code>aws lambda update-function-configuration --function-name Function --environment Variables={OPENTELEMETRY_COLLECTOR_CONFIG_FILE=/var/task/collector.yaml}\n</code></pre>"},{"location":"docs/faas/lambda-collector/#set-configuration-environment-variables-from-cloudformation","title":"Set Configuration Environment Variables from CloudFormation","text":"<p>You can configure environment variables via CloudFormation template as well:</p> <pre><code>Function:\nType: AWS::Serverless::Function\nProperties:\n...\nEnvironment:\nVariables:\nOPENTELEMETRY_COLLECTOR_CONFIG_FILE: /var/task/collector.yaml\n</code></pre>"},{"location":"docs/faas/lambda-collector/#load-configuration-from-an-s3-object","title":"Load Configuration from an S3 Object","text":"<p>Loading configuration from S3 will require that the IAM role attached to your function includes read access to the relevant bucket.</p> <pre><code>  Function:\nType: AWS::Serverless::Function\nProperties:\n...\nEnvironment:\nVariables:\nOPENTELEMETRY_COLLECTOR_CONFIG_FILE: s3://&lt;bucket_name&gt;.s3.&lt;region&gt;.amazonaws.com/collector_config.yaml\n</code></pre>"},{"location":"docs/faas/lambda-manual-instrument/","title":"Lambda Manual Instrumentation","text":"<p>For languages not covered in the Lambda auto-instrumentation document, the community does not have a standalone instrumentation layer.</p> <p>Users will need to follow the generic instrumentation guidance for their chosen language and add the Collector Lambda layer to submit their data.</p>"},{"location":"docs/faas/lambda-manual-instrument/#add-the-arn-of-the-otel-collector-lambda-layer","title":"Add the ARN of the OTel Collector Lambda layer","text":"<p>See the Collector Lambda layer guidance to add the layer to your application and configure the Collector. We recommend you add this first.</p>"},{"location":"docs/faas/lambda-manual-instrument/#instrument-the-lambda-with-otel","title":"Instrument the Lambda with OTel","text":"<p>Review the language instrumentation guidance on how to manually instrument your application.</p>"},{"location":"docs/faas/lambda-manual-instrument/#publish-your-lambda","title":"Publish your Lambda","text":"<p>Publish a new version of your Lambda to deploy the new changes and instrumentation.</p>"},{"location":"docs/getting-started/","title":"\u5165\u95e8","text":"<p>\u9009\u62e9\u89d2\u82721\u5f00\u59cb:</p> <ul> <li>Dev</li> <li>Ops</li> </ul> <p>\u4f60\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u5b98\u65b9\u7684OpenTelemetry\u6f14\u793a\u6765 \u770b\u770b OpenTelemetry\u7684\u53ef\u89c2\u5bdf\u6027\u662f\u4ec0\u4e48\u6837\u7684!</p> <ul> <li>\u8bd5\u8bd5\u8fd9\u4e2a\u6f14\u793a</li> </ul> <ol> <li> <p>\u5982\u679c\u8fd9\u4e9b\u89d2\u8272\u90fd\u4e0d\u9002\u5408\u4f60\uff0c\u3010\u8bf7\u544a\u8bc9\u6211\u4eec!]let us know!.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/getting-started/dev/","title":"\u5f00\u53d1\u8005\u5165\u95e8\u6307\u5357","text":"<p>\u8fd9\u662f\u60a8\u7684\u5165\u95e8\u9875\u9762\uff0c\u5982\u679c:</p> <ul> <li>\u4f60\u5f00\u53d1\u8f6f\u4ef6\u3002</li> <li>\u60a8\u7684\u76ee\u6807\u662f\u901a\u8fc7\u7f16\u5199\u4ee3\u7801\u83b7\u5f97\u53ef\u89c2\u5bdf\u6027\u3002</li> <li>\u60a8\u5e0c\u671b\u8ba9\u60a8\u7684\u4f9d\u8d56\u9879\u81ea\u52a8\u4e3a\u60a8\u53d1\u9001\u9065\u6d4b\u4fe1\u606f\u3002</li> </ul> <p>\u5f00\u653e\u9065\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u4f60! \u4e3a\u4e86\u5b9e\u73b0\u81ea\u52a8\u68c0\u6d4b\u4f9d\u8d56\u5173\u7cfb\u548c\u624b\u52a8\u4f7f\u7528\u6211\u4eec\u7684API\u68c0\u6d4b\u81ea\u5df1\u7684\u4ee3\u7801\u7684\u76ee\u6807\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u9996\u5148\u5b66\u4e60\u4ee5\u4e0b\u6982\u5ff5:</p> <ul> <li>\u4ec0\u4e48\u662f\u5f00\u653e\u5f0f\u9065\u6d4b?</li> <li>\u5982\u4f55\u5728\u4e0d\u89e6\u53ca\u5b83\u4eec\u7684\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u4f9d\u8d56\u5173\u7cfb?</li> <li>\u5982\u4f55\u624b\u52a8\u68c0\u6d4b\u6211\u7684\u5e94\u7528\u7a0b\u5e8f?</li> </ul> <p>\u5982\u679c\u4f60\u5f00\u53d1\u7684\u5e93\u3001\u6846\u67b6\u6216\u4e2d\u95f4\u4ef6\u5728\u5176\u4ed6\u8f6f\u4ef6\u4e2d\u88ab\u7528\u4f5c\u4f9d\u8d56\u9879\uff0c\u6211\u4eec\u5efa\u8bae\u4f60\u5b66\u4e60\u5982\u4f55\u672c\u5730\u63d0\u4f9b\u9065\u6d4b:</p> <ul> <li>\u5982\u4f55\u5c06\u672c\u673a\u68c0\u6d4b\u6dfb\u52a0\u5230\u5e93\u4e2d?</li> </ul> <p>\u63a5\u4e0b\u6765\uff0c\u4f60\u53ef\u4ee5\u6df1\u5165\u4e86\u89e3\u4f60\u6b63\u5728\u4f7f\u7528\u7684\u8bed\u8a00\u7684\u6587\u6863:</p> <ul> <li>C++</li> <li>.NET</li> <li>Erlang / Elixir</li> <li>Go</li> <li>Java</li> <li>JavaScript / TypeScript</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> <li>Rust</li> <li>Swift</li> <li>\u5176\u5b83</li> </ul>"},{"location":"docs/getting-started/ops/","title":"Ops\u5165\u95e8","text":"<p>\u8fd9\u662f\u4f60\u7684\u5165\u95e8\u9875\u9762\uff0c\u5982\u679c:</p> <ul> <li>\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fd0\u884c\u4e00\u7ec4\u5e94\u7528\u7a0b\u5e8f\u3002</li> <li>\u60a8\u7684\u76ee\u6807\u662f\u5728\u4e0d\u89e6\u53ca\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u9065\u6d4b\u529f\u80fd\u3002</li> <li>\u60a8\u5e0c\u671b\u4ece\u591a\u4e2a\u670d\u52a1\u6536\u96c6\u8ddf\u8e2a\u3001\u6307\u6807\u548c\u65e5\u5fd7\uff0c\u5e76\u5c06\u5b83\u4eec\u53d1\u9001\u5230\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\u3002</li> </ul> <p>\u5f00\u653e\u9065\u6d4b\u53ef\u4ee5\u5e2e\u52a9\u4f60!\u4e3a\u4e86\u5b9e\u73b0\u5728\u4e0d\u89e6\u53ca\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u4ece\u5e94\u7528\u7a0b\u5e8f\u4e2d\u83b7\u53d6\u9065\u6d4b\u6280\u672f \u7684\u76ee\u6807\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u5b66\u4e60\u4ee5\u4e0b\u5185\u5bb9:</p> <ul> <li>\u4ec0\u4e48\u662f\u5f00\u653e\u9065\u6d4b?</li> <li>\u5982\u4f55\u5728\u4e0d\u89e6\u53ca\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u5e94\u7528\u7a0b\u5e8f?</li> <li>\u6211\u5982\u4f55\u5efa\u7acb\u4e00\u4e2a\u6536\u96c6\u5668?</li> <li>\u5982\u4f55\u4f7f\u7528 OpenTelemetry Operator \u5b9e\u73b0 Kubernetes \u7684\u81ea\u52a8\u5316?</li> </ul> <p>\u5982\u679c\u4f60\u6b63\u5728\u5bfb\u627e\u4e00\u7ec4\u5e94\u7528\u7a0b\u5e8f\u6765\u5c1d\u8bd5\uff0c\u4f60\u4f1a\u53d1\u73b0\u6211\u4eec\u7684\u5b98 \u65b9OpenTelemetry \u6f14\u793a\u5f88\u6709\u7528!</p>"},{"location":"docs/instrumentation/","title":"\u63d2\u88c5","text":"<p>OpenTelemetry \u4ee3\u7801\u690d\u5165\u652f\u6301\u4e0b\u9762\u5217\u51fa\u7684\u8bed\u8a00\u3002\u6839\u636e\u8bed\u8a00\u7684\u4e0d\u540c\uff0c\u6240\u6db5\u76d6\u7684\u4e3b\u9898\u5c06\u5305\u62ec \u4ee5\u4e0b\u90e8\u5206\u6216\u5168\u90e8:</p> <ul> <li>\u81ea\u52a8\u63d2\u88c5</li> <li>\u624b\u52a8\u5de5\u5177</li> <li>\u5bfc\u51fa\u6570\u636e</li> </ul> <p>\u5982\u679c\u4f60\u6b63\u5728\u4f7f\u7528 Kubernetes\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528OpenTelemetry Operator for Kubernetes\u6765\u4e3a Java, Node.js \u548c Python \u6ce8\u5165\u81ea\u52a8\u68c0\u6d4b\u5e93\u3002</p>"},{"location":"docs/instrumentation/#_1","title":"\u72b6\u6001\u548c\u53d1\u5e03","text":"<p>OpenTelemetry \u4e3b\u8981\u529f\u80fd\u7ec4\u4ef6\u7684\u73b0\u72b6\u5982\u4e0b:</p>"},{"location":"docs/instrumentation/cpp/","title":"C++","text":"<p>{{% lang_instrumentation_index_head cpp /%}}</p>"},{"location":"docs/instrumentation/cpp/#repositories","title":"Repositories","text":"<ul> <li>Main: opentelemetry-cpp</li> <li>Contrib:   opentelemetry-cpp-contrib</li> </ul>"},{"location":"docs/instrumentation/cpp/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your traces and metrics, you will need to export them to a backend.</p>"},{"location":"docs/instrumentation/cpp/exporters/#trace-exporters","title":"Trace exporters","text":""},{"location":"docs/instrumentation/cpp/exporters/#ostream-exporter","title":"OStream exporter","text":"<p>The OStream exporter is useful for development and debugging tasks, and is the simplest to set up.</p> <pre><code>#include \"opentelemetry/exporters/ostream/span_exporter_factory.h\"\nnamespace trace_exporter = opentelemetry::exporter::trace;\nauto exporter = trace_exporter::OStreamSpanExporterFactory::Create();\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#otlp-endpoint","title":"OTLP endpoint","text":"<p>To send trace data to an OTLP endpoint (like the collector or Jaeger) you'll want to configure an OTLP exporter that sends to your endpoint.</p>"},{"location":"docs/instrumentation/cpp/exporters/#otlp-http-exporter","title":"OTLP HTTP Exporter","text":"<pre><code>#include \"opentelemetry/exporters/otlp/otlp_http_exporter_factory.h\"\n#include \"opentelemetry/exporters/otlp/otlp_http_exporter_options.h\"\nnamespace otlp = opentelemetry::exporter::otlp;\notlp::OtlpHttpExporterOptions opts;\nopts.url = \"http://localhost:4318/v1/traces\";\nauto exporter = otlp::OtlpHttpExporterFactory::Create(opts);\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#otlp-grpc-exporter","title":"OTLP gRPC Exporter","text":"<pre><code>#include \"opentelemetry/exporters/otlp/otlp_grpc_exporter_factory.h\"\n#include \"opentelemetry/exporters/otlp/otlp_grpc_exporter_options.h\"\nnamespace otlp = opentelemetry::exporter::otlp;\notlp::OtlpGrpcExporterOptions opts;\nopts.endpoint = \"localhost:4317\";\nopts.use_ssl_credentials = true;\nopts.ssl_credentials_cacert_as_string = \"ssl-certificate\";\nauto exporter = otlp::OtlpGrpcExporterFactory::Create(opts);\n</code></pre> <p>You can find an example of how to use the OTLP exporter here.</p>"},{"location":"docs/instrumentation/cpp/exporters/#jaeger","title":"Jaeger","text":"<p>To try out the OTLP exporter, you can run Jaeger as an OTLP endpoint and for trace visualization in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 4317:4317 \\\n-p 4318:4318 \\\n-p 14250:14250 \\\n-p 14268:14268 \\\n-p 14269:14269 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#zipkin","title":"Zipkin","text":"<p>To send trace data to a Zipkin endpoint you'll want to configure a Zipkin exporter that sends to your endpoint.</p> <pre><code>#include \"opentelemetry/exporters/zipkin/zipkin_exporter_factory.h\"\n#include \"opentelemetry/exporters/zipkin/zipkin_exporter_options.h\"\nnamespace zipkin = opentelemetry::exporter::zipkin;\nzipkin::ZipkinExporterOptions opts;\nopts.endpoint = \"http://localhost:9411/api/v2/spans\" ; // or export OTEL_EXPORTER_ZIPKIN_ENDPOINT=\"...\"\nopts.service_name = \"default_service\" ;\nauto exporter = zipkin::ZipkinExporterFactory::Create(opts);\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#trace-processors","title":"Trace processors","text":""},{"location":"docs/instrumentation/cpp/exporters/#simple-span-processor","title":"Simple span processor","text":"<p>A simple processor will send spans one by one to an exporter.</p> <pre><code>#include \"opentelemetry/sdk/trace/simple_processor_factory.h\"\nnamespace trace_sdk = opentelemetry::sdk::trace;\nauto processor = trace_sdk::SimpleSpanProcessorFactory::Create(std::move(exporter));\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#batch-span-processor","title":"Batch span processor","text":"<p>A batch span processor will group several spans together, before sending them to an exporter.</p> <pre><code>#include \"opentelemetry/sdk/trace/batch_span_processor_factory.h\"\n#include \"opentelemetry/sdk/trace/batch_span_processor_options.h\"\nnamespace trace_sdk = opentelemetry::sdk::trace;\ntrace_sdk::BatchSpanProcessorOptions opts;\nopts.max_queue_size = 2048;\nopts.max_export_batch_size = 512;\nauto processor = trace_sdk::BatchSpanProcessorFactory::Create(std::move(exporter), opts);\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#tracer-provider","title":"Tracer provider","text":"<pre><code>#include \"opentelemetry/sdk/trace/tracer_provider_factory.h\"\nnamespace trace_api = opentelemetry::trace;\nnamespace trace_sdk = opentelemetry::sdk::trace;\nauto provider = trace_sdk::TracerProviderFactory::Create(std::move(processor));\ntrace_api::Provider::SetTracerProvider(provider);\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#metrics-exporters","title":"Metrics exporters","text":""},{"location":"docs/instrumentation/cpp/exporters/#otlp-http-exporter_1","title":"OTLP HTTP Exporter","text":"<pre><code>opentelemetry::exporter::otlp::OtlpHttpExporterOptions otlpOptions;\notlpOptions.url = \"http://localhost:4318/v1/metrics\"; // or \"http://localhost:4318/\notlpOptions.aggregation_temporality = opentelemetry::sdk::metrics::AggregationTemporality::kCumulative; // or kDelta\nauto exporter = opentelemetry::exporter::otlp::OtlpHttpMetricExporterFactory::Create(otlpOptions);\n// Initialize and set the periodic metrics reader\nopentelemetry::sdk::metrics::PeriodicExportingMetricReaderOptions options;\noptions.export_interval_millis = std::chrono::milliseconds(1000);\noptions.export_timeout_millis  = std::chrono::milliseconds(500);\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MetricReader&gt; reader{\nnew opentelemetry::sdk::metrics::PeriodicExportingMetricReader(std::move(exporter), options)};\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#otlp-grpc-exporter_1","title":"OTLP gRPC Exporter","text":"<pre><code>opentelemetry::exporter::otlp::OtlpGrpcMetricExporterOptions otlpOptions;\notlpOptions.endpoint = \"localhost:4317/v1/metrics\";  // or \"localhost:4317\notlpOptions.aggregation_temporality = opentelemetry::sdk::metrics::AggregationTemporality::kDelta; // or kCumulative\nauto exporter = opentelemetry::exporter::otlp::OtlpGrpcMetricExporterFactory::Create(otlpOptions);\n// Initialize and set the periodic metrics reader\nopentelemetry::sdk::metrics::PeriodicExportingMetricReaderOptions options;\noptions.export_interval_millis = std::chrono::milliseconds(1000);\noptions.export_timeout_millis  = std::chrono::milliseconds(500);\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MetricReader&gt; reader{\nnew opentelemetry::sdk::metrics::PeriodicExportingMetricReader(std::move(exporter), options)};\n</code></pre>"},{"location":"docs/instrumentation/cpp/exporters/#prometheus","title":"Prometheus","text":"<p>To send metrics to a Prometheus endpoint you'll want to configure a prometheus exporter</p> <pre><code>opentelemetry::sdk::metrics::PeriodicExportingMetricReaderOptions options;\noptions.export_interval_millis = std::chrono::milliseconds(1000); //optional, to override default values\noptions.export_timeout_millis  = std::chrono::milliseconds(500); // optional, to override default values\nopentelemetry::exporter::metrics::PrometheusExporterOptions prometheusOptions;\nprometheusOptions.url = \"localhost:8080\";\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MetricExporter&gt; exporter{new opentelemetry::exporter::metrics::PrometheusExporter(prometheusOptions)};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MetricReader&gt; reader{\nnew opentelemetry::sdk::metrics::PeriodicExportingMetricReader(std::move(exporter), options)};\n</code></pre> <p>To learn more on how to use the Prometheus exporter, try out the prometheus example</p>"},{"location":"docs/instrumentation/cpp/exporters/#next-steps","title":"Next steps","text":"<p>Enriching your codebase with manual instrumentation gives you customized observability data.</p>"},{"location":"docs/instrumentation/cpp/getting-started/","title":"Getting Started","text":"<p>Welcome to the OpenTelemetry C++ getting started guide! This guide will walk you through the basic steps in installing, instrumenting with, configuring, and exporting data from OpenTelemetry.</p> <p>You can use CMake or Bazel for building OpenTelemetry C++. The following getting started guide will make use of CMake and only provide you the most essential steps to have a working example application (a HTTP server &amp; HTTP client). For more details read these instructions.</p>"},{"location":"docs/instrumentation/cpp/getting-started/#prerequisites","title":"Prerequisites","text":"<p>You can build OpenTelemetry C++ on Windows, macOS or Linux. First you need to install some dependencies:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab \"Linux (apt)\" &gt;}} sudo apt-get install git cmake g++ libcurl4-openssl-dev {{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"Linux (yum)\" &gt;}} sudo yum install git cmake g++ libcurl-devel {{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"Linux (alpine)\" &gt;}} sudo apk add git cmake g++ make curl-dev {{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"MacOS (homebrew)\" &gt;}} xcode-select \u2014install /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" brew install git cmake {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/cpp/getting-started/#building","title":"Building","text":"<p>Get the <code>opentelemetry-cpp</code> source:</p> <pre><code>git clone --recursive https://github.com/open-telemetry/opentelemetry-cpp\n</code></pre> <p>Navigate to the repository cloned above, and create the CMake build configuration:</p> <pre><code>cd opentelemetry-cpp\nmkdir build &amp;&amp; cd build\ncmake -DBUILD_TESTING=OFF -DWITH_EXAMPLES_HTTP=ON ..\n</code></pre> <p>Once build configuration is created, build the CMake targets <code>http_client</code> and <code>http_server</code>:</p> <pre><code>cmake --build . --target http_client http_server\n</code></pre> <p>If all goes well, you should find binaries <code>http_server</code> and <code>http_client</code> in <code>./examples/http</code>:</p> <pre><code>$ ls ./examples/http\nCMakeFiles  Makefile  cmake_install.cmake  http_client  http_server\n</code></pre>"},{"location":"docs/instrumentation/cpp/getting-started/#run-application","title":"Run Application","text":"<p>Open two terminals, in the first terminal, start the HTTP server:</p> <pre><code>$ ./examples/http/http_server\nServer is running..Press ctrl-c to exit..\n</code></pre> <p>In the other terminal, run the HTTP client:</p> <pre><code>./examples/http/http_client\n</code></pre> <p>You should see client output similar to this:</p> <pre><code>{\nname          : /helloworld\ntrace_id      : 05eec7a55d3544434265dad89d7fe96f\nspan_id       : 45fb62c58c907f05\ntracestate    :\nparent_span_id: 0000000000000000\nstart         : 1665577080650384378\nduration      : 1640298\ndescription   :\nspan kind     : Client\nstatus        : Unset\nattributes    :\nhttp.header.Date: Wed, 12 Oct 2022 12:18:00 GMT\nhttp.header.Content-Length: 0\nhttp.status_code: 200\nhttp.method: GET\n.header.Host: localhost\nhttp.header.Content-Type: text/plain\nhttp.header.Connection: keep-alive\n.scheme: http\nhttp.url: http://localhost:8800/helloworld\nevents        :\nlinks         :\nresources     :\nservice.name: unknown_service\ntelemetry.sdk.version: 1.6.1\ntelemetry.sdk.name: opentelemetry\ntelemetry.sdk.language: cpp\ninstr-lib     : http-client\n}\n</code></pre> <p>Also the server should dump you a trace to the console:</p> <pre><code>{\nname          : /helloworld\ntrace_id      : 05eec7a55d3544434265dad89d7fe96f\nspan_id       : 8df967d8547813fe\ntracestate    :\nparent_span_id: 45fb62c58c907f05\nstart         : 1665577080651459495\nduration      : 46331\ndescription   :\nspan kind     : Server\nstatus        : Unset\nattributes    :\nhttp.header.Traceparent: 00-05eec7a55d3544434265dad89d7fe96f-45fb62c58c907f05-01\nhttp.header.Accept: */*\nhttp.request_content_length: 0\nhttp.header.Host: localhost:8800\nhttp.scheme: http\nhttp.client_ip: 127.0.0.1:49466\nhttp.method: GET\nnet.host.port: 8800\nnet.host.name: localhost\nevents        :\n{\nname          : Processing request\ntimestamp     : 1665577080651472827\nattributes    :\n}\nlinks         :\nresources     :\nservice.name: unknown_service\ntelemetry.sdk.version: 1.6.1\ntelemetry.sdk.name: opentelemetry\ntelemetry.sdk.language: cpp\ninstr-lib     : http-server\n}\n</code></pre>"},{"location":"docs/instrumentation/cpp/getting-started/#whats-next","title":"What's next","text":"<p>Enrich your instrumentation generated automatically with manual of your own codebase. This gets you customized observability data.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/cpp/manual/","title":"Manual Instrumentation","text":"<p>Manual instrumentation is the process of adding observability code to your application.</p>"},{"location":"docs/instrumentation/cpp/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/cpp/manual/#initializing-tracing","title":"Initializing tracing","text":"<pre><code>auto provider = opentelemetry::trace::Provider::GetTracerProvider();\nauto tracer = provider-&gt;GetTracer(\"foo_library\", \"1.0.0\");\n</code></pre> <p>The <code>TracerProvider</code> acquired in the first step is a singleton object that is usually provided by the OpenTelemetry C++ SDK. It is used to provide specific implementations for API interfaces. In case no SDK is used, the API provides a default no-op implementation of a <code>TracerProvider</code>.</p> <p>The <code>Tracer</code> acquired in the second step is needed to create and start Spans.</p>"},{"location":"docs/instrumentation/cpp/manual/#start-a-span","title":"Start a span","text":"<pre><code>auto span = tracer-&gt;StartSpan(\"HandleRequest\");\n</code></pre> <p>This creates a span, sets its name to <code>\"HandleRequest\"</code>, and sets its start time to the current time. Refer to the API documentation for other operations that are available to enrich spans with additional data.</p>"},{"location":"docs/instrumentation/cpp/manual/#mark-a-span-as-active","title":"Mark a span as active","text":"<pre><code>auto scope = tracer-&gt;WithActiveSpan(span);\n</code></pre> <p>This marks a span as active and returns a <code>Scope</code> object. The scope object controls how long a span is active. The span remains active for the lifetime of the scope object.</p> <p>The concept of an active span is important, as any span that is created without explicitly specifying a parent is parented to the currently active span. A span without a parent is called root span.</p>"},{"location":"docs/instrumentation/cpp/manual/#create-nested-spans","title":"Create nested Spans","text":"<pre><code>auto outer_span = tracer-&gt;StartSpan(\"Outer operation\");\nauto outer_scope = tracer-&gt;WithActiveSpan(outer_span);\n{\nauto inner_span = tracer-&gt;StartSpan(\"Inner operation\");\nauto inner_scope = tracer-&gt;WithActiveSpan(inner_span);\n// ... perform inner operation\ninner_span-&gt;End();\n}\n// ... perform outer operation\nouter_span-&gt;End();\n</code></pre> <p>Spans can be nested, and have a parent-child relationship with other spans. When a given span is active, the newly created span inherits the active span\u2019s trace ID, and other context attributes.</p>"},{"location":"docs/instrumentation/cpp/manual/#context-propagation","title":"Context Propagation","text":"<pre><code>// set global propagator\nopentelemetry::context::propagation::GlobalTextMapPropagator::SetGlobalPropagator(\nnostd::shared_ptr&lt;opentelemetry::context::propagation::TextMapPropagator&gt;(\nnew opentelemetry::trace::propagation::HttpTraceContext()));\n// get global propagator\nHttpTextMapCarrier&lt;opentelemetry::ext::http::client::Headers&gt; carrier;\nauto propagator =\nopentelemetry::context::propagation::GlobalTextMapPropagator::GetGlobalPropagator();\n//inject context to headers\nauto current_ctx = opentelemetry::context::RuntimeContext::GetCurrent();\npropagator-&gt;Inject(carrier, current_ctx);\n//Extract headers to context\nauto current_ctx = opentelemetry::context::RuntimeContext::GetCurrent();\nauto new_context = propagator-&gt;Extract(carrier, current_ctx);\nauto remote_span = opentelemetry::trace::propagation::GetSpan(new_context);\n</code></pre> <p><code>Context</code> contains the meta-data of the currently active Span including Span Id, Trace Id, and flags. Context Propagation is an important mechanism in distributed tracing to transfer this Context across service boundary often through HTTP headers. OpenTelemetry provides a text-based approach to propagate context to remote services using the W3C Trace Context HTTP headers.</p>"},{"location":"docs/instrumentation/cpp/manual/#further-reading","title":"Further Reading","text":"<ul> <li>Traces API</li> <li>Traces SDK</li> <li>Simple Metrics Example</li> </ul>"},{"location":"docs/instrumentation/cpp/manual/#metrics","title":"Metrics","text":""},{"location":"docs/instrumentation/cpp/manual/#initialize-exporter-and-reader","title":"Initialize Exporter and Reader","text":"<p>Initialize an exporter and a reader. In this case, we initialize an OStream Exporter which will print to stdout by default. The reader periodically collects metrics from the Aggregation Store and exports them.</p> <pre><code>std::unique_ptr&lt;opentelemetry::sdk::metrics::MetricExporter&gt; exporter{new opentelemetry::exporters::OStreamMetricExporter};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MetricReader&gt; reader{\nnew opentelemetry::sdk::metrics::PeriodicExportingMetricReader(std::move(exporter), options)};\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#initialize-meter-provider","title":"Initialize Meter Provider","text":"<p>Initialize a MeterProvider and add the reader. We will use this to obtain Meter objects in the future.</p> <pre><code>auto provider = std::shared_ptr&lt;opentelemetry::metrics::MeterProvider&gt;(new opentelemetry::sdk::metrics::MeterProvider());\nauto p = std::static_pointer_cast&lt;opentelemetry::sdk::metrics::MeterProvider&gt;(provider);\np-&gt;AddMetricReader(std::move(reader));\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#create-a-counter","title":"Create a Counter","text":"<p>Create a Counter instrument from the Meter, and record the measurement. Every Meter pointer returned by the MeterProvider points to the same Meter. This means that the Meter will be able to combine metrics captured from different functions without having to constantly pass the Meter around the library.</p> <pre><code>auto meter = provider-&gt;GetMeter(name, \"1.2.0\");\nauto double_counter = meter-&gt;CreateDoubleCounter(counter_name);\n// Create a label set which annotates metric values\nstd::map&lt;std::string, std::string&gt; labels = {{\"key\", \"value\"}};\nauto labelkv = common::KeyValueIterableView&lt;decltype(labels)&gt;{labels};\ndouble_counter-&gt;Add(val, labelkv);\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#create-a-histogram","title":"Create a Histogram","text":"<p>Create a Histogram instrument from the Meter, and record the measurement.</p> <pre><code>auto meter = provider-&gt;GetMeter(name, \"1.2.0\");\nauto histogram_counter = meter-&gt;CreateDoubleHistogram(\"histogram_name\");\nhistogram_counter-&gt;Record(val, labelkv);\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#create-observable-counter","title":"Create Observable Counter","text":"<p>Create a Observable Counter Instrument from the Meter, and add a callback. The callback would be used to record the measurement during metrics collection. Ensure to keep the Instrument object active for the lifetime of collection.</p> <pre><code>auto meter = provider-&gt;GetMeter(name, \"1.2.0\");\nauto counter = meter-&gt;CreateDoubleObservableCounter(counter_name);\ncounter-&gt;AddCallback(MeasurementFetcher::Fetcher, nullptr);\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#create-views","title":"Create Views","text":""},{"location":"docs/instrumentation/cpp/manual/#map-the-counter-instrument-to-sum-aggregation","title":"Map the Counter Instrument to Sum Aggregation","text":"<p>Create a view to map the Counter Instrument to Sum Aggregation. Add this view to provider. View creation is optional unless we want to add custom aggregation config, and attribute processor. Metrics SDK will implicitly create a missing view with default mapping between Instrument and Aggregation.</p> <pre><code>std::unique_ptr&lt;opentelemetry::sdk::metrics::InstrumentSelector&gt; instrument_selector{\nnew opentelemetry::sdk::metrics::InstrumentSelector(opentelemetry::sdk::metrics::InstrumentType::kCounter, \"counter_name\")};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MeterSelector&gt; meter_selector{\nnew opentelemetry::sdk::metrics::MeterSelector(name, version, schema)};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::View&gt; sum_view{\nnew opentelemetry::sdk::metrics::View{name, \"description\", opentelemetry::sdk::metrics::AggregationType::kSum}};\np-&gt;AddView(std::move(instrument_selector), std::move(meter_selector), std::move(sum_view));\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#map-the-histogram-instrument-to-histogram-aggregation","title":"Map the Histogram Instrument to Histogram Aggregation","text":"<pre><code>std::unique_ptr&lt;opentelemetry::sdk::metrics::InstrumentSelector&gt; histogram_instrument_selector{\nnew opentelemetry::sdk::metrics::InstrumentSelector(opentelemetry::sdk::metrics::InstrumentType::kHistogram, \"histogram_name\")};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MeterSelector&gt; histogram_meter_selector{\nnew opentelemetry::sdk::metrics::MeterSelector(name, version, schema)};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::View&gt; histogram_view{\nnew opentelemetry::sdk::metrics::View{name, \"description\", opentelemetry::sdk::metrics::AggregationType::kHistogram}};\np-&gt;AddView(std::move(histogram_instrument_selector), std::move(histogram_meter_selector),\nstd::move(histogram_view));\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#map-the-observable-counter-instrument-to-sum-aggregation","title":"Map the Observable Counter Instrument to Sum Aggregation","text":"<pre><code>std::unique_ptr&lt;opentelemetry::sdk::metrics::InstrumentSelector&gt; observable_instrument_selector{\nnew opentelemetry::sdk::metrics::InstrumentSelector(opentelemetry::sdk::metrics::InstrumentType::kObservableCounter,\n\"observable_counter_name\")};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::MeterSelector&gt; observable_meter_selector{\nnew opentelemetry::sdk::metrics::MeterSelector(name, version, schema)};\nstd::unique_ptr&lt;opentelemetry::sdk::metrics::View&gt; observable_sum_view{\nnew opentelemetry::sdk::metrics::View{name, \"description\", opentelemetry::sdk::metrics::AggregationType::kSum}};\np-&gt;AddView(std::move(observable_instrument_selector), std::move(observable_meter_selector),\nstd::move(observable_sum_view));\n</code></pre>"},{"location":"docs/instrumentation/cpp/manual/#further-reading_1","title":"Further Reading","text":"<ul> <li>Metrics API</li> <li>Metrics SDK</li> <li>Simple Metrics Example</li> </ul>"},{"location":"docs/instrumentation/erlang/","title":"Erlang/Elixir","text":"<p>{{% lang_instrumentation_index_head erlang %}}</p> <p>Packages of the API, SDK and OTLP exporter are published to <code>hex.pm</code> as <code>opentelemetry_api</code>, <code>opentelemetry</code> and <code>opentelemetry_exporter</code>.</p>"},{"location":"docs/instrumentation/erlang/#version-support","title":"Version support","text":"<p>OpenTelemetry Erlang supports Erlang 23+ and Elixir 1.13+.</p>"},{"location":"docs/instrumentation/erlang/#repositories","title":"Repositories","text":"<ul> <li>opentelemetry-erlang:   Main repo containing the API, SDK and OTLP Exporter.</li> <li>opentelemetry-erlang-contrib:   Helpful libraries and instrumentation libraries for Erlang/Elixir projects   like Phoenix and   Ecto</li> </ul> <p>{{% /lang_instrumentation_index_head %}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/","title":"Getting Started","text":"<p>Welcome to the OpenTelemetry for Erlang/Elixir getting started guide! This guide will walk you through the basic steps in installing, configuring, and exporting data from OpenTelemetry.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#phoenix","title":"Phoenix","text":"<p>This part of the guide will show you how to get started with OpenTelemetry in the Phoenix Web Framework.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have erlang, elixir, postgres (or the database of your choice), and phoenix installed locally. The phoenix installation guide will help you get set up with everything you need.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#example-application","title":"Example Application","text":"<p>The following example uses a basic Phoenix web application. For reference, a complete example of the code you will build can be found here: opentelemetry-erlang-contrib/examples/dice_game. You can git clone that project or just follow along in your browser.</p> <p>Additional examples can be found here.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#dependencies","title":"Dependencies","text":"<p>We'll need a few other dependencies that Phoenix doesn't come with.</p> <ul> <li><code>opentelemetry_api</code>: contains the interfaces you'll use to instrument your   code. Things like <code>Tracer.with_span</code> and <code>Tracer.set_attribute</code> are defined   here.</li> <li><code>opentelemetry</code>: contains the SDK that implements the interfaces defined in   the API. Without it, all the functions in the API are no-ops.</li> <li><code>opentelemetry_exporter</code>: allows you to send your telemetry data to an   OpenTelemetry Collector and/or to self-hosted or commercial services.</li> <li><code>opentelemetry_phoenix</code>: creates OpenTelemetry spans from the Elixir   <code>:telemetry</code> events created by Phoenix.</li> <li><code>opentelemetry_cowboy</code>: creates OpenTelemetry spans from the Elixir   <code>:telemetry</code> events created by the Cowboy web server (which is used by   Phoenix).</li> </ul> <pre><code># mix.exs\ndef deps do\n[\n{:opentelemetry, \"~&gt; 1.3\"},\n{:opentelemetry_api, \"~&gt; 1.2\"},\n{:opentelemetry_exporter, \"~&gt; 1.4\"},\n{:opentelemetry_phoenix, \"~&gt; 1.1\"},\n{:opentelemetry_cowboy, \"~&gt; 0.2\"}\n]\nend\n</code></pre> <p>The last two also need to be setup when your application starts:</p> <pre><code># application.ex\n@impl true\ndef start(_type, _args) do\n:opentelemetry_cowboy.setup()\nOpentelemetryPhoenix.setup(adapter: :cowboy2)\nend\n</code></pre> <p>If you're using ecto, you'll also want to add <code>OpentelemetryEcto.setup([:dice_game, :repo])</code>.</p> <p>We also need to configure the <code>opentelemetry</code> application as temporary by adding a <code>releases</code> section to your project configuration. This will ensure that if it terminates, even abnormally, the <code>dice_game</code> application will be terminated.</p> <pre><code># mix.exs\ndef project do\n[\napp: :dice_game,\nversion: \"0.1.0\",\nelixir: \"~&gt; 1.14\",\nelixirc_paths: elixirc_paths(Mix.env()),\nstart_permanent: Mix.env() == :prod,\nreleases: [\ndice_game: [\napplications: [opentelemetry: :temporary]\n]\n],\naliases: aliases(),\ndeps: deps()\n]\nend\n</code></pre> <p>Now we can use the new <code>mix setup</code> command to install the dependencies, build the assets, and create and migrate the database.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#try-it-out","title":"Try It Out","text":"<p>We can ensure everything is working by setting the stdout exporter as opentelemetry's traces_exporter and then starting the app with <code>mix phx.server</code>.</p> <pre><code># config/dev.exs\nconfig :opentelemetry, traces_exporter: {:otel_exporter_stdout, []}\n</code></pre> <p>If everything went well, you should be able to visit <code>localhost:4000</code> in your browser and see quite a few lines that look like this in your terminal.</p> <p>(Don't worry if the format looks a little unfamiliar. Spans are recorded in the Erlang <code>record</code> data structure. You can find more information about records here, and this file describes the <code>span</code> record structure, and explains what the different fields are.)</p> <pre><code>*SPANS FOR DEBUG*\n{span,64480120921600870463539706779905870846,11592009751350035697,[],\n      undefined,&lt;&lt;\"/\"&gt;&gt;,server,-576460731933544855,-576460731890088522,\n      {attributes,128,infinity,0,\n                  #{'http.status_code' =&gt; 200,\n'http.client_ip' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'http.flavor' =&gt; '1.1','http.method' =&gt; &lt;&lt;\"GET\"&gt;&gt;,\n                    'http.scheme' =&gt; &lt;&lt;\"http\"&gt;&gt;,'http.target' =&gt; &lt;&lt;\"/\"&gt;&gt;,\n                    'http.user_agent' =&gt;\n                        &lt;&lt;\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"&gt;&gt;,\n                    'net.transport' =&gt; 'IP.TCP',\n                    'net.host.name' =&gt; &lt;&lt;\"localhost\"&gt;&gt;,\n                    'net.host.port' =&gt; 4000,'net.peer.port' =&gt; 62839,\n                    'net.sock.host.addr' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'net.sock.peer.addr' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'http.route' =&gt; &lt;&lt;\"/\"&gt;&gt;,'phoenix.action' =&gt; home,\n                    'phoenix.plug' =&gt;\n                        'Elixir.DiceGameWeb.PageController'}},\n      {events,128,128,infinity,0,[]},\n      {links,128,128,infinity,0,[]},\n      undefined,1,false,\n      {instrumentation_scope,&lt;&lt;\"opentelemetry_phoenix\"&gt;&gt;,&lt;&lt;\"1.1.0\"&gt;&gt;,\n                             undefined}}\n</code></pre> <p>These are the raw Erlang records that will get serialized and sent when you configure the exporter for your preferred service.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#rolling-the-dice","title":"Rolling The Dice","text":"<p>Now we'll check out the API endpoint that will let us roll the dice and return a random number between 1 and 6.</p> <p>Before we call our API, let's add our first bit of manual instrumentation. In our <code>DiceController</code> we call a private <code>dice_roll</code> method that generates our random number. This seems like a pretty important operation, so in order to capture it in our trace we'll need to wrap it in a span.</p> <pre><code>  defp dice_roll do\nTracer.with_span(\"dice_roll\") do\nto_string(Enum.random(1..6))\nend\nend\n</code></pre> <p>It would also be nice to know what number it generated, so we can extract it as a local variable and add it as an attribute on the span.</p> <pre><code>  defp dice_roll do\nTracer.with_span(\"dice_roll\") do\nroll = Enum.random(1..6)\nTracer.set_attribute(:roll, roll)\nto_string(roll)\nend\nend\n</code></pre> <p>Now if you point your browser/curl/etc. to <code>localhost:4000/api/rolldice</code> you should get a random number in response, and 3 spans in your console.</p> View the full spans <pre><code>*SPANS FOR DEBUG*\n{span,224439009126930788594246993907621543552,5581431573601075988,[],\n      undefined,&lt;&lt;\"/api/rolldice\"&gt;&gt;,server,-576460729549928500,\n      -576460729491912750,\n      {attributes,128,infinity,0,\n                  #{'http.request_content_length' =&gt; 0,\n'http.response_content_length' =&gt; 1,\n                    'http.status_code' =&gt; 200,\n                    'http.client_ip' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'http.flavor' =&gt; '1.1','http.host' =&gt; &lt;&lt;\"localhost\"&gt;&gt;,\n                    'http.host.port' =&gt; 4000,'http.method' =&gt; &lt;&lt;\"GET\"&gt;&gt;,\n                    'http.scheme' =&gt; &lt;&lt;\"http\"&gt;&gt;,\n                    'http.target' =&gt; &lt;&lt;\"/api/rolldice\"&gt;&gt;,\n                    'http.user_agent' =&gt;\n                        &lt;&lt;\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"&gt;&gt;,\n                    'net.host.ip' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'net.transport' =&gt; 'IP.TCP',\n                    'http.route' =&gt; &lt;&lt;\"/api/rolldice\"&gt;&gt;,\n                    'phoenix.action' =&gt; roll,\n                    'phoenix.plug' =&gt; 'Elixir.DiceGameWeb.DiceController'}},\n      {events,128,128,infinity,0,[]},\n      {links,128,128,infinity,0,[]},\n      undefined,1,false,\n      {instrumentation_scope,&lt;&lt;\"opentelemetry_cowboy\"&gt;&gt;,&lt;&lt;\"0.2.1\"&gt;&gt;,\n                             undefined}}\n{span,237952789931001653450543952469252891760,13016664705250513820,[],\n      undefined,&lt;&lt;\"HTTP GET\"&gt;&gt;,server,-576460729422104083,-576460729421433042,\n      {attributes,128,infinity,0,\n                  #{'http.request_content_length' =&gt; 0,\n'http.response_content_length' =&gt; 1258,\n                    'http.status_code' =&gt; 200,\n                    'http.client_ip' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'http.flavor' =&gt; '1.1','http.host' =&gt; &lt;&lt;\"localhost\"&gt;&gt;,\n                    'http.host.port' =&gt; 4000,'http.method' =&gt; &lt;&lt;\"GET\"&gt;&gt;,\n                    'http.scheme' =&gt; &lt;&lt;\"http\"&gt;&gt;,\n                    'http.target' =&gt; &lt;&lt;\"/favicon.ico\"&gt;&gt;,\n                    'http.user_agent' =&gt;\n                        &lt;&lt;\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"&gt;&gt;,\n                    'net.host.ip' =&gt; &lt;&lt;\"127.0.0.1\"&gt;&gt;,\n                    'net.transport' =&gt; 'IP.TCP'}},\n      {events,128,128,infinity,0,[]},\n      {links,128,128,infinity,0,[]},\n      undefined,1,false,\n      {instrumentation_scope,&lt;&lt;\"opentelemetry_cowboy\"&gt;&gt;,&lt;&lt;\"0.2.1\"&gt;&gt;,\n                             undefined}}\n{span,224439009126930788594246993907621543552,17387612312604368700,[],\n      5581431573601075988,&lt;&lt;\"dice_roll\"&gt;&gt;,internal,-576460729494399167,\n      -576460729494359917,\n      {attributes,128,infinity,0,#{roll =&gt; 2}},\n      {events,128,128,infinity,0,[]},\n      {links,128,128,infinity,0,[]},\n      undefined,1,false,\n      {instrumentation_scope,&lt;&lt;\"dice_game\"&gt;&gt;,&lt;&lt;\"0.1.0\"&gt;&gt;,undefined}}\n</code></pre>"},{"location":"docs/instrumentation/erlang/getting-started/#apirolldice","title":"<code>&lt;&lt;\"/api/rolldice\"&gt;&gt;</code>","text":"<p>This is the first span in the request, aka the root span. That <code>undefined</code> next to the span name tells you that it doesn't have a parent span. The two very large negative numbers are the start and end time of the span, in the <code>native</code> time unit. If you're curious, you can calculate the duration in milliseconds like so <code>System.convert_time_unit(-576460729491912750 - -576460729549928500, :native, :millisecond)</code>. The <code>phoenix.plug</code> and <code>phoenix.action</code> will tell you the controller and function that handled the request. You'll notice however, that the instrumentation_scope is <code>opentelemetry_cowboy</code>. When we told opentelemetry_phoenix's setup function that we want to use the <code>:cowboy2</code> adapter, that let it know not to create and additional span, but to instead append attributes to the existing cowboy span. This ensures we have more accurate latency data in our traces.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#http-get","title":"<code>&lt;&lt;\"HTTP GET\"&gt;&gt;</code>","text":"<p>This is the request for the favicon, which you can see in the <code>'http.target' =&gt; &lt;&lt;\"/favicon.ico\"&gt;&gt;</code> attribute. I believe it has a generic name because it does not have an <code>http.route</code>.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#dice_roll","title":"<code>&lt;&lt;\"dice_roll\"&gt;&gt;</code>","text":"<p>This is the custom span we added to our private method. You'll notice it only has the one attribute that we set, <code>roll =&gt; 2</code>. You should also note that it is part of the same trace as our <code>&lt;&lt;\"/api/rolldice\"&gt;&gt;</code> span, <code>224439009126930788594246993907621543552</code> and has a parent span id of <code>5581431573601075988</code> which is the span id of the <code>&lt;&lt;\"/api/rolldice\"&gt;&gt;</code> span. That means that this span is a child of that one, and will be shown below it when rendered in your tracing tool of choice.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#next-steps","title":"Next Steps","text":"<p>Enrich your automatically generated instrumentation with manual instrumentation of your own codebase. This allows you to customize the observability data your application emits.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#creating-a-new-mixrebar-project","title":"Creating a New Mix/Rebar Project","text":"<p>To get started with this guide, create a new project with <code>rebar3</code> or <code>mix</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} rebar3 new release otel_getting_started {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} mix new --sup otel_getting_started {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Then, in the project you just created, add both <code>opentelemetry_api</code> and <code>opentelemetry</code> as dependencies. We add both because this is a project we will run as a Release and export spans from.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} {deps, [{opentelemetry_api, \"~&gt; 1.2\"},         {opentelemetry, \"~&gt; 1.3\"}]}. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} def deps do   [     {:opentelemetry_api, \"~&gt; 1.2\"},     {:opentelemetry, \"~&gt; 1.3\"}   ] end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>In the case of Erlang, the API Application will also need to be added to <code>src/otel_getting_started.app.src</code> and a <code>relx</code> section to <code>rebar.config</code>. In an Elixir project, a <code>releases</code> section needs to be added to <code>mix.exs</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% src/otel_getting_started.app.src ... {applications, [kernel,                 stdlib,                 opentelemetry_api]}, ...</p> <p>%% rebar.config {relx, [{release, {otel_getting_started, \"0.1.0\"},          [{opentelemetry, temporary},           otel_getting_started]},</p> <pre><code>   ...]}.\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#mixexs","title":"mix.exs","text":"<p>releases: [   otel_getting_started: [     version: \"0.0.1\",     applications: [opentelemetry: :temporary, otel_getting_started: :permanent]   ] ] {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>The SDK <code>opentelemetry</code> should be added as early as possible in the Release boot process to ensure it is available before any telemetry is produced. Here it is also set to <code>temporary</code> under the assumption that we prefer to have a running Release not producing telemetry over crashing the entire Release.</p> <p>In addition to the API and SDK, an exporter for getting data out is needed. The SDK comes with an exporter for debugging purposes that prints to stdout and there are separate packages for exporting over the OpenTelemetry Protocol (OTLP) and the Zipkin protocol.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#initialization-and-configuration","title":"Initialization and Configuration","text":"<p>Configuration is done through the Application environment or OS Environment Variables. The SDK (<code>opentelemetry</code> Application) uses the configuration to initialize a Tracer Provider, its Span Processors and the Exporter.</p>"},{"location":"docs/instrumentation/erlang/getting-started/#using-the-console-exporter","title":"Using the Console Exporter","text":"<p>Exporters are packages that allow telemetry data to be emitted somewhere - either to the console (which is what we're doing here), or to a remote system or collector for further analysis and/or enrichment. OpenTelemetry supports a variety of exporters through its ecosystem, including popular open source tools like Jaeger and Zipkin.</p> <p>To configure OpenTelemetry to use a particular exporter, in this case <code>otel_exporter_stdout</code>, the Application environment for <code>opentelemetry</code> must set the <code>exporter</code> for the span processor <code>otel_batch_processor</code>, a type of span processor that batches up multiple spans over a period of time:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% config/sys.config.src [  {opentelemetry,   [{span_processor, batch},    {traces_exporter, {otel_exporter_stdout, []}}]} ]. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#configruntimeexs","title":"config/runtime.exs","text":"<p>config :opentelemetry,   span_processor: :batch,   traces_exporter: {:otel_exporter_stdout, []} {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#working-with-spans","title":"Working with Spans","text":"<p>Now that the dependencies and configuration are set up, we can create a module with a function <code>hello/0</code> that starts some spans:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% apps/otel_getting_started/src/otel_getting_started.erl -module(otel_getting_started).</p> <p>-export([hello/0]).</p> <p>-include_lib(\"opentelemetry_api/include/otel_tracer.hrl\").</p> <p>hello() -&gt;     %% start an active span and run a local function     ?with_span(operation, #{}, fun nice_operation/1).</p> <p>nice_operation(_SpanCtx) -&gt;     ?add_event(&lt;&lt;\"Nice operation!\"&gt;&gt;, [{&lt;&lt;\"bogons\"&gt;&gt;, 100}]),     ?set_attributes([{another_key, &lt;&lt;\"yes\"&gt;&gt;}]),</p> <pre><code>%% start an active span and run an anonymous function\n?with_span(&lt;&lt;\"Sub operation...\"&gt;&gt;, #{},\n           fun(_ChildSpanCtx) -&gt;\n                   ?set_attributes([{lemons_key, &lt;&lt;\"five\"&gt;&gt;}]),\n                   ?add_event(&lt;&lt;\"Sub span event!\"&gt;&gt;, [])\n           end).\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#libotel_getting_startedex","title":"lib/otel_getting_started.ex","text":"<p>defmodule OtelGettingStarted do   require OpenTelemetry.Tracer, as: Tracer</p> <p>def hello do     Tracer.with_span :operation do       Tracer.add_event(\"Nice operation!\", [{\"bogons\", 100}])       Tracer.set_attributes([{:another_key, \"yes\"}])</p> <pre><code>  Tracer.with_span \"Sub operation...\" do\n    Tracer.set_attributes([{:lemons_key, \"five\"}])\n    Tracer.add_event(\"Sub span event!\", [])\n  end\nend\n</code></pre> <p>end end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>In this example, we're using macros that use the process dictionary for context propagation and for getting the tracer.</p> <p>Inside our function, we're creating a new span named <code>operation</code> with the <code>with_span</code> macro. The macro sets the new span as <code>active</code> in the current context -- stored in the process dictionary, since we aren't passing a context as a variable.</p> <p>Spans can have attributes and events, which are metadata and log statements that help you interpret traces after-the-fact. The first span has an event <code>Nice operation!</code>, with attributes on the event, as well as an attribute set on the span itself.</p> <p>Finally, in this code snippet, we can see an example of creating a child span of the currently-active span. When the <code>with_span</code> macro starts a new span, it uses the active span of the current context as the parent. So when you run this program, you'll see that the <code>Sub operation...</code> span has been created as a child of the <code>operation</code> span.</p> <p>To test out this project and see the spans created, you can run with <code>rebar3 shell</code> or <code>iex -S mix</code>, each will pick up the corresponding configuration for the release, resulting in the tracer and exporter to started.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} $ rebar3 shell ===&gt; Compiling otel_getting_started Erlang/OTP 23 [erts-11.1] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:1] [hipe]</p> <p>Eshell V11.1  (abort with ^G) 1&gt; 1&gt; otel_getting_started:hello(). true SPANS FOR DEBUG {span,177312096541376795265675405126880478701,5706454085098543673,undefined,       13736713257910636645,&lt;&lt;\"Sub operation...\"&gt;&gt;,internal,       -576460750077844044,-576460750077773674,       [{lemons_key,&lt;&lt;\"five\"&gt;&gt;}],       [{event,-576460750077786044,&lt;&lt;\"Sub span event!\"&gt;&gt;,[]}],       [],undefined,1,false,undefined} {span,177312096541376795265675405126880478701,13736713257910636645,undefined,       undefined,operation,internal,-576460750086570890,       -576460750077752627,       [{another_key,&lt;&lt;\"yes\"&gt;&gt;}],       [{event,-576460750077877345,&lt;&lt;\"Nice operation!\"&gt;&gt;,[{&lt;&lt;\"bogons\"&gt;&gt;,100}]}],       [],undefined,1,false,undefined} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} $ iex -S mix Erlang/OTP 23 [erts-11.1] [source] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:1] [hipe]</p> <p>Compiling 1 file (.ex) Interactive Elixir (1.11.0) - press Ctrl+C to exit (type h() ENTER for help) iex(1)&gt; OtelGettingStarted.hello() true iex(2)&gt; SPANS FOR DEBUG {span,180094370450826032544967824850795294459,5969980227405956772,undefined,       14276444653144535440,&lt;&lt;\"Sub operation...\"&gt;&gt;,'INTERNAL',       -576460741349434100,-576460741349408901,       [{lemons_key,&lt;&lt;\"five\"&gt;&gt;}],       [{event,-576460741349414157,&lt;&lt;\"Sub span event!\"&gt;&gt;,[]}],       [],undefined,1,false,undefined} {span,180094370450826032544967824850795294459,14276444653144535440,undefined,       undefined,:operation,'INTERNAL',-576460741353342627,       -576460741349400034,       [{another_key,&lt;&lt;\"yes\"&gt;&gt;}],       [{event,-576460741349446725,&lt;&lt;\"Nice operation!\"&gt;&gt;,[{&lt;&lt;\"bogons\"&gt;&gt;,100}]}],       [],undefined,1,false,undefined} {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#exporting-to-the-opentelemetry-collector","title":"Exporting to the OpenTelemetry Collector","text":"<p>The Collector provides a vendor agnostic way to receive, process and export telemetry data. The package opentelemetry_exporter provides support for both exporting over both HTTP (the default) and gRPC to the collector, which can then export Spans to a self-hosted service like Zipkin or Jaeger, as well as commercial services. For a full list of available exporters, see the registry.</p> <p>For testing purposes the <code>opentelemetry-erlang</code> repo has a Collector configuration, config/otel-collector-config.yaml that can be used as a starting point. This configuration is used in docker-compose.yml to start the Collector with receivers for both HTTP and gRPC that then export to Zipkin also run by docker-compose.</p> <p>To export to the running Collector the <code>opentelemetry_exporter</code> package must be added to the project's dependencies:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} {deps, [{opentelemetry_api, \"~&gt; 1.3\"},         {opentelemetry, \"~&gt; 1.3\"},         {opentelemetry_exporter, \"~&gt; 1.4\"}]}. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} def deps do   [     {:opentelemetry_api, \"~&gt; 1.3\"},     {:opentelemetry, \"~&gt; 1.3\"},     {:opentelemetry_exporter, \"~&gt; 1.4\"}   ] end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>It should then be added to the configuration of the Release before the SDK Application to ensure the exporter's dependencies are started before the SDK attempts to initialize and use the exporter.</p> <p>Example of Release configuration in <code>rebar.config</code> and for mix's Release task:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% rebar.config {relx, [{release, {my_instrumented_release, \"0.1.0\"},          [opentelemetry_exporter,           {opentelemetry, temporary},           my_instrumented_app]},</p> <pre><code>   ...]}.\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#mixexs_1","title":"mix.exs","text":"<p>def project do   [     releases: [       my_instrumented_release: [         applications: [opentelemetry_exporter: :permanent, opentelemetry: :temporary]       ],</p> <pre><code>  ...\n]\n</code></pre> <p>] end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Finally, the runtime configuration of the <code>opentelemetry</code> and <code>opentelemetry_exporter</code> Applications are set to export to the Collector. The configurations below show the defaults that are used if none are set, which are the HTTP protocol with endpoint of <code>localhost</code> on port <code>4318</code>. If using <code>grpc</code> for the <code>otlp_protocol</code> the endpoint should be changed to <code>http://localhost:4317</code>.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% config/sys.config.src [  {opentelemetry,   [{span_processor, batch},    {traces_exporter, otlp}]},</p> <p>{opentelemetry_exporter,   [{otlp_protocol, http_protobuf},    {otlp_endpoint, \"http://localhost:4318\"}]}]} ]. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/getting-started/#configruntimeexs_1","title":"config/runtime.exs","text":"<p>config :opentelemetry,   span_processor: :batch,   traces_exporter: :otlp</p> <p>config :opentelemetry_exporter,   otlp_protocol: :http_protobuf,   otlp_endpoint: \"http://localhost:4318\" {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/","title":"Manual","text":"<p>Instrumentation is the act of adding observability code to your application. This can be done with direct calls to the OpenTelemetry API within your code or including a dependency which calls the API and hooks into your project, like a middleware for an HTTP server.</p>"},{"location":"docs/instrumentation/erlang/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/erlang/manual/#initialize-tracing","title":"Initialize Tracing","text":"<p>To start tracing a <code>TracerProvider</code> is required for creating a <code>Tracer</code>. When the OpenTelemetry SDK Application (<code>opentelemetry</code>) boots, it starts and configures a global <code>TracerProvider</code>. A <code>Tracer</code> for each loaded OTP Application is created once the <code>TracerProvider</code> has started.</p> <p>If a TracerProvider is not successfully created (for example, the <code>opentelemetry</code> application is not booted or fails to boot), the OpenTelemetry APIs for tracing will use a no-op implementation and will not generate data.</p>"},{"location":"docs/instrumentation/erlang/manual/#acquiring-a-tracer","title":"Acquiring a Tracer","text":"<p>Each OTP Application has a <code>Tracer</code> created for it when the <code>opentelemetry</code> Application boots. The name and version of each <code>Tracer</code> is the same as the name and version of the OTP Application the module using the <code>Tracer</code> is in. If the call to use a <code>Tracer</code> is not in a module, for example when using the interactive shell, a <code>Tracer</code> with a blank name and version is used.</p> <p>The created <code>Tracer</code>'s record can be looked up by the name of a module in the OTP Application:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} opentelemetry:get_application_tracer(?MODULE) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} :opentelemetry.get_application_tracer(MODULE) {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>This is how the Erlang and Elixir macros for starting and updating <code>Spans</code> get a <code>Tracer</code> automatically without need for you to pass the variable in each call.</p>"},{"location":"docs/instrumentation/erlang/manual/#create-spans","title":"Create Spans","text":"<p>Now that you have Tracers initialized, you can create Spans.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} ?with_span(main, #{}, fun() -&gt;                         %% do work here.                         %% when this function returns the Span ends                       end).</p> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} require OpenTelemetry.Tracer</p> <p>...</p> <p>OpenTelemetry.Tracer.with_span :main do   # do work here   # when the block ends the Span ends end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>The above code sample shows how to create an active Span, which is the most common kind of Span to create.</p>"},{"location":"docs/instrumentation/erlang/manual/#create-nested-spans","title":"Create Nested Spans","text":"<p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} parent_function() -&gt;     ?with_span(parent, #{}, fun child_function/0).</p> <p>child_function() -&gt;     %% this is the same process, so the span parent set as the active     %% span in the with_span call above will be the active span in this function     ?with_span(child, #{},                fun() -&gt;                    %% do work here. when this function returns, child will complete.                end).</p> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} require OpenTelemetry.Tracer</p> <p>def parent_function() do     OpenTelemetry.Tracer.with_span :parent do         child_function()     end end</p> <p>def child_function() do     # this is the same process, so the span :parent set as the active     # span in the with_span call above will be the active span in this function     OpenTelemetry.Tracer.with_span :child do         ## do work here. when this function returns, :child will complete.     end end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#spans-in-separate-processes","title":"Spans in Separate Processes","text":"<p>The examples in the previous section were Spans with a child-parent relationship within the same process where the parent is available in the process dictionary when creating a child Span. Using the process dictionary this way isn't possible when crossing processes, either by spawning a new process or sending a message to an existing process. Instead, the context must be manually passed as a variable.</p> <p>To pass Spans across processes we need to start a Span that isn't connected to particular process. This can be done with the macro <code>start_span</code>. Unlike <code>with_span</code>, the <code>start_span</code> macro does not set the new span as the currently active span in the context of the process dictionary.</p> <p>Connecting a span as a parent to a child in a new process can be done by attaching the context and setting the new span as currently active in the process. The whole context should be attached in order to not lose other telemetry data like baggage.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} SpanCtx = ?start_span(child),</p> <p>Ctx = otel_ctx:get_current(),</p> <p>proc_lib:spawn_link(fun() -&gt;                         otel_ctx:attach(Ctx),                         ?set_current_span(SpanCtx),</p> <pre><code>                    %% do work here\n\n                    ?end_span(SpanCtx)\n                end),\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} span_ctx = OpenTelemetry.Tracer.start_span(:child) ctx = OpenTelemetry.Ctx.get_current()</p> <p>task = Task.async(fn -&gt;                       OpenTelemetry.Ctx.attach(ctx)                       OpenTelemetry.Tracer.set_current_span(span_ctx)                       # do work here</p> <pre><code>                  # end span here\n                  OpenTelemetry.Tracer.end_span(span_ctx)\n              end)\n</code></pre> <p>_ = Task.await(task) {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#linking-the-new-span","title":"Linking the New Span","text":"<p>A Span can be created with zero or more Span Links that causally link it to another Span. A Link needs a Span context to be created.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} Parent = ?current_span_ctx, proc_lib:spawn_link(fun() -&gt;                         %% a new process has a new context so the span created                         %% by the following <code>with_span</code> will have no parent                         Link = opentelemetry:link(Parent),                         ?with_span('other-process', #{links =&gt; [Link]},                                    fun() -&gt; ok end)                     end), {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} parent = OpenTelemetry.current_span_ctx() task = Task.async(fn -&gt;                     # a new process has a new context so the span created                     # by the following <code>with_span</code> will have no parent                     link = OpenTelemetry.link(parent)                     Tracer.with_span :\"my-task\", %{links: [link]} do                       :hello                     end                  end) {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#adding-attributes-to-a-span","title":"Adding Attributes to a Span","text":"<p>Attributes let you attach key/value pairs to a Span so it carries more information about the current operation that it\u2019s tracking.</p> <p>The following example shows the two ways of setting attributes on a span by both setting an attribute in the start options and then again with <code>set_attributes</code> in the body of the span operation:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} ?with_span(my_span, #{attributes =&gt; [{'start-opts-attr', &lt;&lt;\"start-opts-value\"&gt;&gt;}]},            fun() -&gt;                ?set_attributes([{'my-attribute', &lt;&lt;\"my-value\"&gt;&gt;},                                 {another_attribute, &lt;&lt;\"value-of-attribute\"&gt;&gt;}])            end) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} Tracer.with_span :span_1, %{attributes: [{:\"start-opts-attr\", &lt;&lt;\"start-opts-value\"&gt;&gt;}]} do   Tracer.set_attributes([{:\"my-attributes\", \"my-value\"},                          {:another_attribute, \"value-of-attributes\"}]) end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#semantic-attributes","title":"Semantic Attributes","text":"<p>Semantic Attributes are attributes that are defined by the OpenTelemetry Specification in order to provide a shared set of attribute keys across multiple languages, frameworks, and runtimes for common concepts like HTTP methods, status codes, user agents, and more. These attribute keys are generated from the specification and provided in opentelemetry_semantic_conventions.</p> <p>For example, an instrumentation for an HTTP client or server would need to include semantic attributes like the scheme of the URL:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} -include_lib(\"opentelemetry_semantic_conventions/include/trace.hrl\").</p> <p>?with_span(my_span, #{attributes =&gt; [{?HTTP_SCHEME, &lt;&lt;\"https\"&gt;&gt;}]},            fun() -&gt;              ...            end) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} alias OpenTelemetry.SemanticConventions.Trace, as: Trace</p> <p>Tracer.with_span :span_1, %{attributes: [{Trace.http_scheme(), &lt;&lt;\"https\"&gt;&gt;}]} do</p> <p>end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#adding-events","title":"Adding Events","text":"<p>A Span Event is a human-readable message on an Span that represents a discrete event with no duration that can be tracked by a single time stamp. You can think of it like a primitive log.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} ?add_event(&lt;&lt;\"Gonna try it\"&gt;&gt;),</p> <p>%% Do the thing</p> <p>?add_event(&lt;&lt;\"Did it!\"&gt;&gt;), {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} Tracer.add_event(\"Gonna try it\")</p> <p>%% Do the thing</p> <p>Tracer.add_event(\"Did it!\") {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Events can also have attributes of their own:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} ?add_event(&lt;&lt;\"Process exited with reason\"&gt;&gt;, [{pid, Pid)}, {reason, Reason}])) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} Tracer.add_event(\"Process exited with reason\", pid: pid, reason: Reason) {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#set-span-status","title":"Set Span Status","text":"<p>A Status can be set on a Span, typically used to specify that a Span has not completed successfully - <code>StatusCode.ERROR</code>. In rare scenarios, you could override the Error status with <code>StatusCode.OK</code>, but don\u2019t set <code>StatusCode.OK</code> on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} -include_lib(\"opentelemetry_api/include/opentelemetry.hrl\").</p> <p>?set_status(?OTEL_STATUS_ERROR, &lt;&lt;\"this is not ok\"&gt;&gt;) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} Tracer.set_status(:error, \"this is not ok\") {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/manual/#creating-metrics","title":"Creating Metrics","text":"<p>The metrics API, found in <code>apps/opentelemetry_experimental_api</code> of the opentelemetry-erlang repository, is currently unstable, documentation TBA.</p>"},{"location":"docs/instrumentation/erlang/propagation/","title":"Propagation","text":""},{"location":"docs/instrumentation/erlang/propagation/#cross-service-propagators","title":"Cross Service Propagators","text":"<p>Distributed traces extend beyond a single service, meaning some context must be propagated across services to create the parent-child relationship between Spans. This requires cross service context propagation, a mechanism where identifiers for a Trace are sent to remote processes.</p> <p>Instrumentation Libraries for HTTP frameworks and servers like Phoenix, Cowboy, Elli and clients like Tesla will automatically inject or extract context using the globally registered propagators. By default the global propagators used are the W3C Trace Context and Baggage formats.</p> <p>These global propagators can be configured by the Application environment variable <code>text_map_propagators</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% sys.config ... {text_map_propagators, [baggage,                         trace_context]}, ... {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/propagation/#runtimeexs","title":"runtime.exs","text":"<p>... text_map_propagators: [:baggage, :trace_context], ... {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Or through a comma separated list with the environment variable <code>OTEL_PROPAGATORS</code>. Both forms of configuration accept the values <code>trace_context</code>, <code>baggage</code>, <code>b3</code> and <code>b3multi</code>.</p> <p>To manually inject or extract context the <code>otel_propagator_text_map</code> module can be used:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% uses the context from the process dictionary to add to an empty list of headers Headers = otel_propagator_text_map:inject([]),</p> <p>%% creates a context in the process dictionary from Headers otel_propagator_text_map:extract(Headers), {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/propagation/#uses-the-context-from-the-process-dictionary-to-add-to-an-empty-list-of-headers","title":"uses the context from the process dictionary to add to an empty list of headers","text":"<p>headers = :otel_propagator_text_map.inject([])</p>"},{"location":"docs/instrumentation/erlang/propagation/#creates-a-context-in-the-process-dictionary-from-headers","title":"creates a context in the process dictionary from headers","text":"<p>:otel_propagator_text_map.extract(headers) {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p><code>otel_propagator_text_map:inject/1</code> and <code>otel_propagator_text_map:extract/1</code> use globally registered propagators. To use a specific propagator <code>otel_propagator_text_map:inject/2</code> and <code>otel_propagator_text_map:extract/2</code> can be used with the first argument being the name of the propagator module to call.</p>"},{"location":"docs/instrumentation/erlang/resources/","title":"Resources","text":"<p>A resource represents an entity producing telemetry as attributes. For example, an OTP Release producing telemetry that is running in a container on Kubernetes has an OTP Release name, a Pod name, a namespace, and possibly a deployment name. All four of these attributes can be included in the resource.</p> <p>In your observability backend, you can use resource information to better investigate interesting behavior. For example, if your trace or metrics data indicate latency in your system, you can narrow it down to a specific container, pod, or Kubernetes deployment.</p>"},{"location":"docs/instrumentation/erlang/resources/#using-resource-detectors","title":"Using resource detectors","text":"<p>Resource detectors fetch resource attributes from various sources. The default detectors use the OS environment variable <code>OTEL_RESOURCE_ATTRIBUTES</code> and the <code>opentelemetry</code> OTP Application environment variable <code>resource</code>.</p> <p>The detectors to use is a list of module names and can be configured in the Application configuration:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% sys.config {resource_detectors, [otel_resource_env_var, otel_resource_app_env]} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/resources/#runtimeexs","title":"runtime.exs","text":"<p>resource_detectors: [:otel_resource_env_var, :otel_resource_app_env] {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Or through the environment variable <code>OTEL_RESOURCE_DETECTORS</code>:</p> <pre><code>OTEL_RESOURCE_DETECTORS=otel_resource_env_var,otel_resource_app_env\n</code></pre> <p>All resources detectors are protected with a timeout, in milliseconds, after which they return an empty value. This allows for resource detectors to do things like hit the network without potentially hanging the entire program indefinitely. The default is 5000 milliseconds and can be set with environment variable <code>OTEL_RESOURCE_DETECTOR_TIMEOUT</code> or Application variable <code>otel_resource_detector_timeout</code>.</p>"},{"location":"docs/instrumentation/erlang/resources/#adding-resources-with-os-and-application-environment-variables","title":"Adding resources with OS and Application environment variables","text":"<p>With the two default resource detectors enabled you can set resource attributes either with the OS environment variable <code>OTEL_RESOURCE_ATTRIBUTES</code>:</p> <pre><code>OTEL_RESOURCE_ATTRIBUTES=\"deployment.environment=development\"\n</code></pre> <p>Alternatively, use the <code>resource</code> Application environment under the <code>opentelemetry</code> Application configuration of <code>sys.config</code> or <code>runtime.exs</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% sys.config {resource, #{deployment =&gt; #{environment =&gt; &lt;&lt;\"development\"&gt;&gt;}} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/resources/#runtimeexs_1","title":"runtime.exs","text":"<p>resource: %{deployment: %{environment: \"development\" }} {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Resource attributes in the <code>resource</code> Application environment variable are flattened and combined with <code>.</code>, so <code>#{deployment =&gt; #{environment =&gt; &lt;&lt;\"development\"&gt;&gt; }</code> is the same as <code>#{'deployment.environment' =&gt; &lt;&lt;\"development\"&gt;&gt;}</code>.</p>"},{"location":"docs/instrumentation/erlang/resources/#custom-resource-detectors","title":"Custom resource detectors","text":"<p>Custom resource detectors can be created by implementing the <code>otel_resource_detector</code> behaviour which contains a single callback <code>get_resource/1</code> that returns an <code>otel_resource</code>.</p> <p>Note that there are semantic conventions defined for <code>resource</code> that should be followed if they apply when adding new resource attributes.</p>"},{"location":"docs/instrumentation/erlang/sampling/","title":"Sampling","text":"<p>Sampling is a process that restricts the amount of traces that are generated by a system. The Erlang SDK offers several head samplers.</p>"},{"location":"docs/instrumentation/erlang/sampling/#default-behavior","title":"Default behavior","text":"<p>By default, all spans are sampled, and thus, 100% of traces are sampled. If you do not need to manage data volume, you do not need to configure a sampler.</p>"},{"location":"docs/instrumentation/erlang/sampling/#parentbasedsampler","title":"ParentBasedSampler","text":"<p>When sampling, the <code>ParentBasedSampler</code> is most often used for head sampling. It uses the sampling decision of the Span's parent, or the fact that there is no parent, to know which secondary sampler to use.</p> <p>The sampler can be configured with the environment variables <code>OTEL_TRACES_SAMPLER</code> and <code>OTEL_TRACES_SAMPLER_ARG</code> or using the Application configuration allows you to configure each of the 5 potential states of a Span's parent:</p> <ul> <li><code>root</code> - No parent</li> <li><code>remote_parent_sampled</code> - Parent is from a remote Span that is sampled</li> <li><code>remote_parent_not_sampled</code> - Parent is from a remote Span that is not sampled</li> <li><code>local_parent_sampled</code> - Parent is from a local Span that is sampled</li> <li><code>local_parent_not_sampled</code> - Parent is from a local Span that is not sampled</li> </ul>"},{"location":"docs/instrumentation/erlang/sampling/#traceidratiobasedsampler","title":"TraceIdRatioBasedSampler","text":"<p>Within the <code>ParentBasedSampler</code> the most common is the <code>TraceIdRatioBasedSampler</code>. It deterministically samples a percentage of traces that you pass in as a parameter.</p>"},{"location":"docs/instrumentation/erlang/sampling/#environment-variables","title":"Environment Variables","text":"<p>You can configure the <code>TraceIdRatioBasedSampler</code> with environment variables:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"parentbased_traceidratio\"\nexport OTEL_TRACES_SAMPLER_ARG=\"0.1\"\n</code></pre> <p>This tells the SDK to sample spans such that only 10% of Traces get created.</p>"},{"location":"docs/instrumentation/erlang/sampling/#application-configuration","title":"Application configuration","text":"<p>Example in the Application configuration with a root sampler for sampling 10% of Traces and using the parent decision in the other cases:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% config/sys.config.src {sampler, {parent_based, #{root =&gt; {trace_id_ratio_based, 0.10},                            remote_parent_sampled =&gt; always_on,                            remote_parent_not_sampled =&gt; always_off,                            local_parent_sampled =&gt; always_on,                            local_parent_not_sampled =&gt; always_off}}} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/sampling/#configruntimeexs","title":"config/runtime.exs","text":"<p>sampler: {:parent_based, %{root: {:trace_id_ratio_based, 0.10},                            remote_parent_sampled: :always_on,                            remote_parent_not_sampled: :always_off,                            local_parent_sampled: :always_on,                            local_parent_not_sampled: :always_off}}</p> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/sampling/#alwayson-and-alwaysoff-sampler","title":"AlwaysOn and AlwaysOff Sampler","text":"<p>The other two built-in samplers are the <code>AlwaysOnSampler</code> and the <code>AlwaysOffSampler</code>.</p>"},{"location":"docs/instrumentation/erlang/sampling/#environment-variables_1","title":"Environment Variables","text":"<p>You can configure the <code>ParentBasedSampler</code> to use either the <code>AlwaysOnSampler</code> or <code>AlwaysOffSampler</code> with the environment variable <code>OTEL_TRACES_SAMPLER</code>:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"parentbased_always_on\"\n</code></pre> <p>And for the <code>AlwaysOffSampler</code>:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"parentbased_always_off\"\n</code></pre>"},{"location":"docs/instrumentation/erlang/sampling/#application-configuration_1","title":"Application configuration","text":"<p>Here's an example in the Application configuration with a root sampler that always samples and using the parent decision in the other cases:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% config/sys.config.src {sampler, {parent_based, #{root =&gt; always_on,                            remote_parent_sampled =&gt; always_on,                            remote_parent_not_sampled =&gt; always_off,                            local_parent_sampled =&gt; always_on,                            local_parent_not_sampled =&gt; always_off}}} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/sampling/#configruntimeexs_1","title":"config/runtime.exs","text":"<p>sampler: {:parent_based, %{root: :always_on,                            remote_parent_sampled: :always_on,                            remote_parent_not_sampled: :always_off,                            local_parent_sampled: :always_on,                            local_parent_not_sampled: :always_off}}</p> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/sampling/#custom-sampler","title":"Custom Sampler","text":"<p>Custom samplers can be created by implementing the <code>otel_sampler</code> behaviour. This example sampler:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} -module(attribute_sampler).</p> <p>-behavior(otel_sampler).</p> <p>-export([description/1,          setup/1,          should_sample/7]).</p> <p>-include(\"otel_sampler.hrl\").</p> <p>setup(Attributes) when is_map(Attributes) -&gt;     Attributes; setup(_) -&gt;     #{}.</p> <p>description(_) -&gt;     &lt;&lt;\"AttributeSampler\"&gt;&gt;.</p> <p>should_sample(_Ctx, _TraceId, _Links, _SpanName, _SpanKind, Attributes, ConfigAttributes) -&gt;     case maps:intersect(Attributes, ConfigAttributes) of         Map when map_size(Map) &gt; 0 -&gt;             {?DROP, [], []};         _ -&gt;             {?RECORD_AND_SAMPLE, [], []}     end. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} defmodule AttributesSampler do   def setup(attributes) when is_map(attributes) do     attributes   end</p> <p>def setup(_) do     %{}   end</p> <p>def description(_) do     \"ExampleSampler\"   end</p> <p>def should_sample(_ctx, _trace_id, _links, _span_name, _span_kind, attributes, config_attributes) do     case :maps.intersect(attributes, config_attributes) do       map when map_size(map) &gt; 0 -&gt;         {:drop, [], []}       _ -&gt;         {:record_and_sample, [], []}     end   end end  {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Will sample Spans that do not have any attributes that match the attributes passed as the sampler's configuration.</p> <p>Example configuration to not sample any Span with an attribute specifying the URL requested is <code>/healthcheck</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} {sampler, {attributes_sampler, #{'http.target' =&gt; &lt;&lt;\"/healthcheck\"&gt;&gt;}}} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} sampler: {AttributesSampler, %{\"http.target\": \"/healthcheck\"}} {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/testing/","title":"Testing","text":"<p>When relying on OpenTelemetry for your Observability needs, it can be important to test that certain spans are created and attributes correctly set. For example, can you be sure that you attaching the right metadata to data that ultimately powers an SLO? This document covers an approach to that kind of validation.</p>"},{"location":"docs/instrumentation/erlang/testing/#setup","title":"Setup","text":"<p>Only the <code>opentelemetry</code> and <code>opentelemetry_api</code> libraries are required for testing in Elixir/Erlang:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} {deps, [{opentelemetry_api, \"~&gt; 1.0\"},         {opentelemetry, \"~&gt; 1.0\"}]}. {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} def deps do   [     {:opentelemetry_api, \"~&gt; 1.0\"},     {:opentelemetry, \"~&gt; 1.0\"}   ] end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Set your <code>exporter</code> to <code>:none</code> and the span processor to <code>:otel_simple_processor</code>. This ensure that your tests don't actually export data to a destination, and that spans can be analyzed after they are processed.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% config/sys.config.src {opentelemetry,   [{traces_exporter, none},    {processors,      [{otel_simple_processor, #{}}]}]} {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/testing/#configtestexs","title":"config/test.exs","text":"<p>import Config</p> <p>config :opentelemetry,     traces_exporter: :none</p> <p>config :opentelemetry, :processors, [   {:otel_simple_processor, %{}} ] {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>A modified version of the <code>hello</code> function from the Getting Started guide will serve as our test case:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} %% apps/otel_getting_started/src/otel_getting_started.erl -module(otel_getting_started).</p> <p>-export([hello/0]).</p> <p>-include_lib(\"opentelemetry_api/include/otel_tracer.hrl\").</p> <p>hello() -&gt;     %% start an active span and run a local function     ?with_span(&lt;&lt;\"operation\"&gt;&gt;, #{}, fun nice_operation/1).</p> <p>nice_operation(_SpanCtx) -&gt;     ?set_attributes([{a_key, &lt;&lt;\"a value\"&gt;&gt;}]),     world {{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}}</p>"},{"location":"docs/instrumentation/erlang/testing/#libotel_getting_startedex","title":"lib/otel_getting_started.ex","text":"<p>defmodule OtelGettingStarted do   require OpenTelemetry.Tracer, as: Tracer</p> <p>def hello do     Tracer.with_span \"operation\" do       Tracer.set_attributes([{:a_key, \"a value\"}])       :world     end   end end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/erlang/testing/#testing","title":"Testing","text":"<p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab Erlang &gt;}} -module(otel_getting_started_SUITE).</p> <p>-compile(export_all).</p> <p>-include_lib(\"stdlib/include/assert.hrl\"). -include_lib(\"common_test/include/ct.hrl\").</p> <p>-include_lib(\"opentelemetry/include/otel_span.hrl\").</p> <p>-define(assertReceive(SpanName),         receive             {span, Span=#span{name=SpanName}} -&gt;                 Span         after             1000 -&gt;                 ct:fail(\"Did not receive the span after 1s\")         end).</p> <p>all() -&gt;     [greets_the_world].</p> <p>init_per_suite(Config) -&gt;     application:load(opentelemetry),     application:set_env(opentelemetry, processors, [{otel_simple_processor, #{}}]),     {ok, _} = application:ensure_all_started(opentelemetry),     Config.</p> <p>end_per_suite(_Config) -&gt;     _ = application:stop(opentelemetry),     _ = application:unload(opentelemetry),     ok.</p> <p>init_per_testcase(greets_the_world, Config) -&gt;     otel_simple_processor:set_exporter(otel_exporter_pid, self()),     Config.</p> <p>end_per_testcase(greets_the_world, _Config) -&gt;     otel_simple_processor:set_exporter(none),     ok.</p> <p>greets_the_world(_Config) -&gt;     otel_getting_started:hello(),</p> <pre><code>ExpectedAttributes = otel_attributes:new(#{a_key =&gt; &lt;&lt;\"a_value\"&gt;&gt;}, 128, infinity),\n#span{attributes=ReceivedAttributes} = ?assertReceive(&lt;&lt;\"operation\"&gt;&gt;),\n\n%% use an assertMatch instead of matching in the `receive'\n%% so we get a nice error message if it fails\n?assertMatch(ReceivedAttributes, ExpectedAttributes),\n\nok.\n</code></pre> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; tab Elixir &gt;}} defmodule OtelGettingStartedTest do   use ExUnit.Case</p> <p># Use Record module to extract fields of the Span record from the opentelemetry dependency.   require Record   @fields Record.extract(:span, from: \"deps/opentelemetry/include/otel_span.hrl\")   # Define macros for <code>Span</code>.   Record.defrecordp(:span, @fields)</p> <p>test \"greets the world\" do     # Set exporter to :otel_exporter_pid, which sends spans     # to the given process - in this case self() - in the format {:span, span}     :otel_simple_processor.set_exporter(:otel_exporter_pid, self())</p> <pre><code># Call the function to be tested.\nOtelGettingStarted.hello()\n\n# Use Erlang's `:otel_attributes` module to create attributes to match against.\n# See the `:otel_events` module for testing events.\nattributes = :otel_attributes.new([a_key: \"a value\"], 128, :infinity)\n\n# Assert that the span emitted by OtelGettingStarted.hello/0 was received and contains the desired attributes.\nassert_receive {:span,\n                span(\n                  name: \"operation\",\n                  attributes: ^attributes\n                )}\n</code></pre> <p>end end {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/go/","title":"Go","text":"<p>{{% lang_instrumentation_index_head \"go\" /%}}</p>"},{"location":"docs/instrumentation/go/#more","title":"More","text":"<ul> <li>Contrib repository</li> </ul>"},{"location":"docs/instrumentation/go/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your traces and metrics, you will need to export them to a backend.</p>"},{"location":"docs/instrumentation/go/exporters/#otlp-endpoint","title":"OTLP endpoint","text":"<p>To send trace data to an OTLP endpoint (like the collector or Jaeger &gt;= v1.35.0) you'll want to configure an OTLP exporter that sends to your endpoint.</p>"},{"location":"docs/instrumentation/go/exporters/#using-http","title":"Using HTTP","text":"<pre><code>import (\n\"go.opentelemetry.io/otel/exporters/otlp/otlptrace\"\n\"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\"\n)\nfunc installExportPipeline(ctx context.Context) (func(context.Context) error, error) {\nclient := otlptracehttp.NewClient()\nexporter, err := otlptrace.New(ctx, client)\nif err != nil {\nreturn nil, fmt.Errorf(\"creating OTLP trace exporter: %w\", err)\n}\n/* \u2026 */\n}\n</code></pre> <p>To learn more on how to use the OTLP HTTP exporter, try out the otel-collector</p>"},{"location":"docs/instrumentation/go/exporters/#jaeger","title":"Jaeger","text":"<p>To try out the OTLP exporter, since v1.35.0 you can run Jaeger as an OTLP endpoint and for trace visualization in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 16686:16686 \\\n-p 4318:4318 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/go/exporters/#prometheus","title":"Prometheus","text":"<p>Prometheus export is available in the <code>go.opentelemetry.io/otel/exporters/prometheus</code> package.</p> <p>Please find more documentation on GitHub</p>"},{"location":"docs/instrumentation/go/getting-started/","title":"Getting Started","text":"<p>Welcome to the OpenTelemetry for Go getting started guide! This guide will walk you through the basic steps in installing, instrumenting with, configuring, and exporting data from OpenTelemetry. Before you get started, be sure to have Go 1.16 or newer installed.</p> <p>Understanding how a system is functioning when it is failing or having issues is critical to resolving those issues. One strategy to understand this is with tracing. This guide shows how the OpenTelemetry Go project can be used to trace an example application. You will start with an application that computes Fibonacci numbers for users, and from there you will add instrumentation to produce tracing telemetry with OpenTelemetry Go.</p> <p>For reference, a complete example of the code you will build can be found here.</p> <p>To start building the application, make a new directory named <code>fib</code> to house our Fibonacci project. Next, add the following to a new file named <code>fib.go</code> in that directory.</p> <pre><code>package main\n// Fibonacci returns the n-th fibonacci number.\nfunc Fibonacci(n uint) (uint64, error) {\nif n &lt;= 1 {\nreturn uint64(n), nil\n}\nvar n2, n1 uint64 = 0, 1\nfor i := uint(2); i &lt; n; i++ {\nn2, n1 = n1, n1+n2\n}\nreturn n2 + n1, nil\n}\n</code></pre> <p>With your core logic added, you can now build your application around it. Add a new <code>app.go</code> file with the following application logic.</p> <pre><code>package main\nimport (\n\"context\"\n\"fmt\"\n\"io\"\n\"log\"\n)\n// App is a Fibonacci computation application.\ntype App struct {\nr io.Reader\nl *log.Logger\n}\n// NewApp returns a new App.\nfunc NewApp(r io.Reader, l *log.Logger) *App {\nreturn &amp;App{r: r, l: l}\n}\n// Run starts polling users for Fibonacci number requests and writes results.\nfunc (a *App) Run(ctx context.Context) error {\nfor {\nn, err := a.Poll(ctx)\nif err != nil {\nreturn err\n}\na.Write(ctx, n)\n}\n}\n// Poll asks a user for input and returns the request.\nfunc (a *App) Poll(ctx context.Context) (uint, error) {\na.l.Print(\"What Fibonacci number would you like to know: \")\nvar n uint\n_, err := fmt.Fscanf(a.r, \"%d\\n\", &amp;n)\nreturn n, err\n}\n// Write writes the n-th Fibonacci number back to the user.\nfunc (a *App) Write(ctx context.Context, n uint) {\nf, err := Fibonacci(n)\nif err != nil {\na.l.Printf(\"Fibonacci(%d): %v\\n\", n, err)\n} else {\na.l.Printf(\"Fibonacci(%d) = %d\\n\", n, f)\n}\n}\n</code></pre> <p>With your application fully composed, you need a <code>main()</code> function to actually run the application. In a new <code>main.go</code> file add the following run logic.</p> <pre><code>package main\nimport (\n\"context\"\n\"log\"\n\"os\"\n\"os/signal\"\n)\nfunc main() {\nl := log.New(os.Stdout, \"\", 0)\nsigCh := make(chan os.Signal, 1)\nsignal.Notify(sigCh, os.Interrupt)\nerrCh := make(chan error)\napp := NewApp(os.Stdin, l)\ngo func() {\nerrCh &lt;- app.Run(context.Background())\n}()\nselect {\ncase &lt;-sigCh:\nl.Println(\"\\ngoodbye\")\nreturn\ncase err := &lt;-errCh:\nif err != nil {\nl.Fatal(err)\n}\n}\n}\n</code></pre> <p>With the code complete it is almost time to run the application. Before you can do that you need to initialize this directory as a Go module. From your terminal, run the command <code>go mod init fib</code> in the <code>fib</code> directory. This will create a <code>go.mod</code> file, which is used by Go to manage imports. Now you should be able to run the application!</p> <pre><code>$ go run .\nWhat Fibonacci number would you like to know:\n42\nFibonacci(42) = 267914296\nWhat Fibonacci number would you like to know:\n^C\ngoodbye\n</code></pre> <p>The application can be exited with Ctrl+C. You should see a similar output as above, if not make sure to go back and fix any errors.</p>"},{"location":"docs/instrumentation/go/getting-started/#trace-instrumentation","title":"Trace Instrumentation","text":"<p>OpenTelemetry is split into two parts: an API to instrument code with, and SDKs that implement the API. To start integrating OpenTelemetry into any project, the API is used to define how telemetry is generated. To generate tracing telemetry in your application you will use the OpenTelemetry Trace API from the [<code>go.opentelemetry.io/otel/trace</code>] package.</p> <p>First, you need to install the necessary packages for the Trace API. Run the following command in your working directory.</p> <pre><code>go get go.opentelemetry.io/otel \\\ngo.opentelemetry.io/otel/trace\n</code></pre> <p>Now that the packages installed you can start updating your application with imports you will use in the <code>app.go</code> file.</p> <pre><code>import (\n\"context\"\n\"fmt\"\n\"io\"\n\"log\"\n\"strconv\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/attribute\"\n\"go.opentelemetry.io/otel/trace\"\n)\n</code></pre> <p>With the imports added, you can start instrumenting.</p> <p>The OpenTelemetry Tracing API provides a [<code>Tracer</code>] to create traces. These [<code>Tracer</code>]s are designed to be associated with one instrumentation library. That way telemetry they produce can be understood to come from that part of a code base. To uniquely identify your application to the [<code>Tracer</code>] you will create a constant with the package name in <code>app.go</code>.</p> <pre><code>// name is the Tracer name used to identify this instrumentation library.\nconst name = \"fib\"\n</code></pre> <p>Using the full-qualified package name, something that should be unique for Go packages, is the standard way to identify a [<code>Tracer</code>]. If your example package name differs, be sure to update the name you use here to match.</p> <p>Everything should be in place now to start tracing your application. But first, what is a trace? And, how exactly should you build them for your application?</p> <p>To back up a bit, a trace is a type of telemetry that represents work being done by a service. A trace is a record of the connection(s) between participants processing a transaction, often through client/server requests processing and other forms of communication.</p> <p>Each part of the work that a service performs is represented in the trace by a span. Those spans are not just an unordered collection. Like the call stack of our application, those spans are defined with relationships to one another. The \"root\" span is the only span without a parent, it represents how a service request is started. All other spans have a parent relationship to another span in the same trace.</p> <p>If this last part about span relationships doesn't make complete sense now, don't worry. The most important takeaway is that each part of your code, which does some work, should be represented as a span. You will have a better understanding of these span relationships after you instrument your code, so let's get started.</p> <p>Start by instrumenting the <code>Run</code> method.</p> <pre><code>// Run starts polling users for Fibonacci number requests and writes results.\nfunc (a *App) Run(ctx context.Context) error {\nfor {\n// Each execution of the run loop, we should get a new \"root\" span and context.\nnewCtx, span := otel.Tracer(name).Start(ctx, \"Run\")\nn, err := a.Poll(newCtx)\nif err != nil {\nspan.End()\nreturn err\n}\na.Write(newCtx, n)\nspan.End()\n}\n}\n</code></pre> <p>The above code creates a span for every iteration of the for loop. The span is created using a [<code>Tracer</code>] from the global <code>TracerProvider</code>. You will learn more about [<code>TracerProvider</code>]s and handle the other side of setting up a global [<code>TracerProvider</code>] when you install an SDK in a later section. For now, as an instrumentation author, all you need to worry about is that you are using an appropriately named [<code>Tracer</code>] from a [<code>TracerProvider</code>] when you write <code>otel.Tracer(name)</code>.</p> <p>Next, instrument the <code>Poll</code> method.</p> <pre><code>// Poll asks a user for input and returns the request.\nfunc (a *App) Poll(ctx context.Context) (uint, error) {\n_, span := otel.Tracer(name).Start(ctx, \"Poll\")\ndefer span.End()\na.l.Print(\"What Fibonacci number would you like to know: \")\nvar n uint\n_, err := fmt.Fscanf(a.r, \"%d\\n\", &amp;n)\n// Store n as a string to not overflow an int64.\nnStr := strconv.FormatUint(uint64(n), 10)\nspan.SetAttributes(attribute.String(\"request.n\", nStr))\nreturn n, err\n}\n</code></pre> <p>Similar to the <code>Run</code> method instrumentation, this adds a span to the method to track the computation performed. However, it also adds an attribute to annotate the span. This annotation is something you can add when you think a user of your application will want to see the state or details about the run environment when looking at telemetry.</p> <p>Finally, instrument the <code>Write</code> method.</p> <pre><code>// Write writes the n-th Fibonacci number back to the user.\nfunc (a *App) Write(ctx context.Context, n uint) {\nvar span trace.Span\nctx, span = otel.Tracer(name).Start(ctx, \"Write\")\ndefer span.End()\nf, err := func(ctx context.Context) (uint64, error) {\n_, span := otel.Tracer(name).Start(ctx, \"Fibonacci\")\ndefer span.End()\nreturn Fibonacci(n)\n}(ctx)\nif err != nil {\na.l.Printf(\"Fibonacci(%d): %v\\n\", n, err)\n} else {\na.l.Printf(\"Fibonacci(%d) = %d\\n\", n, f)\n}\n}\n</code></pre> <p>This method is instrumented with two spans. One to track the <code>Write</code> method itself, and another to track the call to the core logic with the <code>Fibonacci</code> function. Do you see how context is passed through the spans? Do you see how this also defines the relationship between spans?</p> <p>In OpenTelemetry Go the span relationships are defined explicitly with a <code>context.Context</code>. When a span is created a context is returned alongside the span. That context will contain a reference to the created span. If that context is used when creating another span the two spans will be related. The original span will become the new span's parent, and as a corollary, the new span is said to be a child of the original. This hierarchy gives traces structure, structure that helps show a computation path through a system. Based on what you instrumented above and this understanding of span relationships you should expect a trace for each execution of the run loop to look like this.</p> <pre><code>Run\n\u251c\u2500\u2500 Poll\n\u2514\u2500\u2500 Write\n    \u2514\u2500\u2500 Fibonacci\n</code></pre> <p>A <code>Run</code> span will be a parent to both a <code>Poll</code> and <code>Write</code> span, and the <code>Write</code> span will be a parent to a <code>Fibonacci</code> span.</p> <p>Now how do you actually see the produced spans? To do this you will need to configure and install an SDK.</p>"},{"location":"docs/instrumentation/go/getting-started/#sdk-installation","title":"SDK Installation","text":"<p>OpenTelemetry is designed to be modular in its implementation of the OpenTelemetry API. The OpenTelemetry Go project offers an SDK package, [<code>go.opentelemetry.io/otel/sdk</code>], that implements this API and adheres to the OpenTelemetry specification. To start using this SDK you will first need to create an exporter, but before anything can happen we need to install some packages. Run the following in the <code>fib</code> directory to install the trace STDOUT exporter and the SDK.</p> <pre><code>go get go.opentelemetry.io/otel/sdk \\\ngo.opentelemetry.io/otel/exporters/stdout/stdouttrace\n</code></pre> <p>Now add the needed imports to <code>main.go</code>.</p> <pre><code>import (\n\"context\"\n\"io\"\n\"log\"\n\"os\"\n\"os/signal\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/attribute\"\n\"go.opentelemetry.io/otel/exporters/stdout/stdouttrace\"\n\"go.opentelemetry.io/otel/sdk/resource\"\n\"go.opentelemetry.io/otel/sdk/trace\"\nsemconv \"go.opentelemetry.io/otel/semconv/v1.17.0\"\n)\n</code></pre>"},{"location":"docs/instrumentation/go/getting-started/#creating-a-console-exporter","title":"Creating a Console Exporter","text":"<p>The SDK connects telemetry from the OpenTelemetry API to exporters. Exporters are packages that allow telemetry data to be emitted somewhere - either to the console (which is what we're doing here), or to a remote system or collector for further analysis and/or enrichment. OpenTelemetry supports a variety of exporters through its ecosystem including popular open source tools like Jaeger, Zipkin, and Prometheus.</p> <p>To initialize the console exporter, add the following function to the <code>main.go</code> file:</p> <pre><code>// newExporter returns a console exporter.\nfunc newExporter(w io.Writer) (trace.SpanExporter, error) {\nreturn stdouttrace.New(\nstdouttrace.WithWriter(w),\n// Use human-readable output.\nstdouttrace.WithPrettyPrint(),\n// Do not print timestamps for the demo.\nstdouttrace.WithoutTimestamps(),\n)\n}\n</code></pre> <p>This creates a new console exporter with basic options. You will use this function later when you configure the SDK to send telemetry data to it, but first you need to make sure that data is identifiable.</p>"},{"location":"docs/instrumentation/go/getting-started/#creating-a-resource","title":"Creating a Resource","text":"<p>Telemetry data can be crucial to solving issues with a service. The catch is, you need a way to identify what service, or even what service instance, that data is coming from. OpenTelemetry uses a [<code>Resource</code>] to represent the entity producing telemetry. Add the following function to the <code>main.go</code> file to create an appropriate [<code>Resource</code>] for the application.</p> <pre><code>// newResource returns a resource describing this application.\nfunc newResource() *resource.Resource {\nr, _ := resource.Merge(\nresource.Default(),\nresource.NewWithAttributes(\nsemconv.SchemaURL,\nsemconv.ServiceName(\"fib\"),\nsemconv.ServiceVersion(\"v0.1.0\"),\nattribute.String(\"environment\", \"demo\"),\n),\n)\nreturn r\n}\n</code></pre> <p>Any information you would like to associate with all telemetry data the SDK handles can be added to the returned [<code>Resource</code>]. This is done by registering the [<code>Resource</code>] with the [<code>TracerProvider</code>]. Something you can now create!</p>"},{"location":"docs/instrumentation/go/getting-started/#installing-a-tracer-provider","title":"Installing a Tracer Provider","text":"<p>You have your application instrumented to produce telemetry data and you have an exporter to send that data to the console, but how are they connected? This is where the [<code>TracerProvider</code>] is used. It is a centralized point where instrumentation will get a [<code>Tracer</code>] from and funnels the telemetry data from these [<code>Tracer</code>]s to export pipelines.</p> <p>The pipelines that receive and ultimately transmit data to exporters are called [<code>SpanProcessor</code>]s. A [<code>TracerProvider</code>] can be configured to have multiple span processors, but for this example you will only need to configure only one. Update your <code>main</code> function in <code>main.go</code> with the following.</p> <pre><code>func main() {\nl := log.New(os.Stdout, \"\", 0)\n// Write telemetry data to a file.\nf, err := os.Create(\"traces.txt\")\nif err != nil {\nl.Fatal(err)\n}\ndefer f.Close()\nexp, err := newExporter(f)\nif err != nil {\nl.Fatal(err)\n}\ntp := trace.NewTracerProvider(\ntrace.WithBatcher(exp),\ntrace.WithResource(newResource()),\n)\ndefer func() {\nif err := tp.Shutdown(context.Background()); err != nil {\nl.Fatal(err)\n}\n}()\notel.SetTracerProvider(tp)\n/* \u2026 */\n}\n</code></pre> <p>There's a fair amount going on here. First you are creating a console exporter that will export to a file. You are then registering the exporter with a new [<code>TracerProvider</code>]. This is done with a [<code>BatchSpanProcessor</code>] when it is passed to the [<code>trace.WithBatcher</code>] option. Batching data is a good practice and will help not overload systems downstream. Finally, with the [<code>TracerProvider</code>] created, you are deferring a function to flush and stop it, and registering it as the global OpenTelemetry [<code>TracerProvider</code>].</p> <p>Do you remember in the previous instrumentation section when we used the global [<code>TracerProvider</code>] to get a [<code>Tracer</code>]? This last step, registering the [<code>TracerProvider</code>] globally, is what will connect that instrumentation's [<code>Tracer</code>] with this [<code>TracerProvider</code>]. This pattern, using a global [<code>TracerProvider</code>], is convenient, but not always appropriate. [<code>TracerProvider</code>]s can be explicitly passed to instrumentation or inferred from a context that contains a span. For this simple example using a global provider makes sense, but for more complex or distributed codebases these other ways of passing [<code>TracerProvider</code>]s may make more sense.</p>"},{"location":"docs/instrumentation/go/getting-started/#putting-it-all-together","title":"Putting It All Together","text":"<p>You should now have a working application that produces trace telemetry data! Give it a try.</p> <pre><code>$ go run .\nWhat Fibonacci number would you like to know:\n42\nFibonacci(42) = 267914296\nWhat Fibonacci number would you like to know:\n^C\ngoodbye\n</code></pre> <p>A new file named <code>traces.txt</code> should be created in your working directory. All the traces created from running your application should be in there!</p>"},{"location":"docs/instrumentation/go/getting-started/#bonus-errors","title":"(Bonus) Errors","text":"<p>At this point you have a working application and it is producing tracing telemetry data. Unfortunately, it was discovered that there is an error in the core functionality of the <code>fib</code> module.</p> <pre><code>$ go run .\nWhat Fibonacci number would you like to know:\n100\nFibonacci(100) = 3736710778780434371\n# \u2026\n</code></pre> <p>But the 100-th Fibonacci number is <code>354224848179261915075</code>, not <code>3736710778780434371</code>! This application is only meant as a demo, but it shouldn't return wrong values. Update the <code>Fibonacci</code> function to return an error instead of computing incorrect values.</p> <pre><code>// Fibonacci returns the n-th fibonacci number. An error is returned if the\n// fibonacci number cannot be represented as a uint64.\nfunc Fibonacci(n uint) (uint64, error) {\nif n &lt;= 1 {\nreturn uint64(n), nil\n}\nif n &gt; 93 {\nreturn 0, fmt.Errorf(\"unsupported fibonacci number %d: too large\", n)\n}\nvar n2, n1 uint64 = 0, 1\nfor i := uint(2); i &lt; n; i++ {\nn2, n1 = n1, n1+n2\n}\nreturn n2 + n1, nil\n}\n</code></pre> <p>Great, you have fixed the code, but it would be ideal to include errors returned to a user in the telemetry data. Luckily, spans can be configured to communicate this information. Update the <code>Write</code> method in <code>app.go</code> with the following code.</p> <pre><code>// Write writes the n-th Fibonacci number back to the user.\nfunc (a *App) Write(ctx context.Context, n uint) {\nvar span trace.Span\nctx, span = otel.Tracer(name).Start(ctx, \"Write\")\ndefer span.End()\nf, err := func(ctx context.Context) (uint64, error) {\n_, span := otel.Tracer(name).Start(ctx, \"Fibonacci\")\ndefer span.End()\nf, err := Fibonacci(n)\nif err != nil {\nspan.RecordError(err)\nspan.SetStatus(codes.Error, err.Error())\n}\nreturn f, err\n}(ctx)\n/* \u2026 */\n}\n</code></pre> <p>With this change any error returned from the <code>Fibonacci</code> function will mark that span as an error and record an event describing the error.</p> <p>This is a great start, but it is not the only error returned in from the application. If a user makes a request for a non unsigned integer value the application will fail. Update the <code>Poll</code> method with a similar fix to capture this error in the telemetry data.</p> <pre><code>// Poll asks a user for input and returns the request.\nfunc (a *App) Poll(ctx context.Context) (uint, error) {\n_, span := otel.Tracer(name).Start(ctx, \"Poll\")\ndefer span.End()\na.l.Print(\"What Fibonacci number would you like to know: \")\nvar n uint\n_, err := fmt.Fscanf(a.r, \"%d\\n\", &amp;n)\nif err != nil {\nspan.RecordError(err)\nspan.SetStatus(codes.Error, err.Error())\nreturn 0, err\n}\n// Store n as a string to not overflow an int64.\nnStr := strconv.FormatUint(uint64(n), 10)\nspan.SetAttributes(attribute.String(\"request.n\", nStr))\nreturn n, nil\n}\n</code></pre> <p>All that is left is updating imports for the <code>app.go</code> file to include the [<code>go.opentelemetry.io/otel/codes</code>] package.</p> <pre><code>import (\n\"context\"\n\"fmt\"\n\"io\"\n\"log\"\n\"strconv\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/attribute\"\n\"go.opentelemetry.io/otel/codes\"\n\"go.opentelemetry.io/otel/trace\"\n)\n</code></pre> <p>With these fixes in place and the instrumentation updated, re-trigger the bug.</p> <pre><code>$ go run .\nWhat Fibonacci number would you like to know:\n100\nFibonacci(100): unsupported fibonacci number 100: too large\nWhat Fibonacci number would you like to know:\n^C\ngoodbye\n</code></pre> <p>Excellent! The application no longer returns wrong values, and looking at the telemetry data in the <code>traces.txt</code> file you should see the error captured as an event.</p> <pre><code>\"Events\": [\n{\n\"Name\": \"exception\",\n\"Attributes\": [\n{\n\"Key\": \"exception.type\",\n\"Value\": {\n\"Type\": \"STRING\",\n\"Value\": \"*errors.errorString\"\n}\n},\n{\n\"Key\": \"exception.message\",\n\"Value\": {\n\"Type\": \"STRING\",\n\"Value\": \"unsupported fibonacci number 100: too large\"\n}\n}\n],\n...\n}\n]\n</code></pre>"},{"location":"docs/instrumentation/go/getting-started/#whats-next","title":"What's Next","text":"<p>This guide has walked you through adding tracing instrumentation to an application and using a console exporter to send telemetry data to a file. There are many other topics to cover in OpenTelemetry, but you should be ready to start adding OpenTelemetry Go to your projects at this point. Go instrument your code!</p> <p>For more information about instrumenting your code and things you can do with spans, refer to the manual instrumentation documentation.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/go/libraries/","title":"Using instrumentation libraries","text":"<p>Go does not support truly automatic instrumentation like other languages today. Instead, you'll need to depend on instrumentation libraries that generate telemetry data for a particular instrumented library. For example, the instrumentation library for <code>net/http</code> will automatically create spans that track inbound and outbound requests once you configure it in your code.</p>"},{"location":"docs/instrumentation/go/libraries/#setup","title":"Setup","text":"<p>Each instrumentation library is a package. In general, this means you need to <code>go get</code> the appropriate package:</p> <pre><code>go get go.opentelemetry.io/contrib/instrumentation/{import-path}/otel{package-name}\n</code></pre> <p>And then configure it in your code based on what the library requires to be activated.</p>"},{"location":"docs/instrumentation/go/libraries/#example-with-nethttp","title":"Example with <code>net/http</code>","text":"<p>As an example, here's how you can set up automatic instrumentation for inbound HTTP requests for <code>net/http</code>:</p> <p>First, get the <code>net/http</code> instrumentation library:</p> <pre><code>go get go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\n</code></pre> <p>Next, use the library to wrap an HTTP handler in your code:</p> <pre><code>package main\nimport (\n\"context\"\n\"fmt\"\n\"log\"\n\"net/http\"\n\"time\"\n\"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/attribute\"\n)\n// Package-level tracer.\n// This should be configured in your code setup instead of here.\nvar tracer = otel.Tracer(\"github.com/full/path/to/mypkg\")\n// sleepy mocks work that your application does.\nfunc sleepy(ctx context.Context) {\n_, span := tracer.Start(ctx, \"sleep\")\ndefer span.End()\nsleepTime := 1 * time.Second\ntime.Sleep(sleepTime)\nspan.SetAttributes(attribute.Int(\"sleep.duration\", int(sleepTime)))\n}\n// httpHandler is an HTTP handler function that is going to be instrumented.\nfunc httpHandler(w http.ResponseWriter, r *http.Request) {\nfmt.Fprintf(w, \"Hello, World! I am instrumented automatically!\")\nctx := r.Context()\nsleepy(ctx)\n}\nfunc main() {\n// Wrap your httpHandler function.\nhandler := http.HandlerFunc(httpHandler)\nwrappedHandler := otelhttp.NewHandler(handler, \"hello-instrumented\")\nhttp.Handle(\"/hello-instrumented\", wrappedHandler)\n// And start the HTTP serve.\nlog.Fatal(http.ListenAndServe(\":3030\", nil))\n}\n</code></pre> <p>Assuming that you have a <code>Tracer</code> and exporter configured, this code will:</p> <ul> <li>Start an HTTP server on port <code>3030</code></li> <li>Automatically generate a span for each inbound HTTP request to   <code>/hello-instrumented</code></li> <li>Create a child span of the automatically-generated one that tracks the work   done in <code>sleepy</code></li> </ul> <p>Connecting manual instrumentation you write in your app with instrumentation generated from a library is essential to get good observability into your apps and services.</p>"},{"location":"docs/instrumentation/go/libraries/#available-packages","title":"Available packages","text":"<p>A full list of instrumentation libraries available can be found in the OpenTelemetry registry.</p>"},{"location":"docs/instrumentation/go/libraries/#next-steps","title":"Next steps","text":"<p>Instrumentation libraries can do things like generate telemetry data for inbound and outbound HTTP requests, but they don't instrument your actual application.</p> <p>To get richer telemetry data, use manual instrumentation to enrich your telemetry data from instrumentation libraries with instrumentation from your running application.</p>"},{"location":"docs/instrumentation/go/manual/","title":"Manual Instrumentation","text":"<p>Instrumentation is the process of adding observability code to your application. There are two general types of instrumentation - automatic, and manual - and you should be familiar with both in order to effectively instrument your software.</p>"},{"location":"docs/instrumentation/go/manual/#getting-a-tracer","title":"Getting a Tracer","text":"<p>To create spans, you'll need to acquire or initialize a tracer first.</p>"},{"location":"docs/instrumentation/go/manual/#initializing-a-new-tracer","title":"Initializing a new tracer","text":"<p>Ensure you have the right packages installed:</p> <pre><code>go get go.opentelemetry.io/otel \\\ngo.opentelemetry.io/otel/trace \\\ngo.opentelemetry.io/otel/sdk \\\n</code></pre> <p>Then initialize an exporter, resources, tracer provider, and finally a tracer.</p> <pre><code>package app\nimport (\n\"context\"\n\"fmt\"\n\"log\"\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/exporters/otlp/otlptrace\"\n\"go.opentelemetry.io/otel/sdk/resource\"\nsdktrace \"go.opentelemetry.io/otel/sdk/trace\"\nsemconv \"go.opentelemetry.io/otel/semconv/v1.17.0\"\n\"go.opentelemetry.io/otel/trace\"\n)\nvar tracer trace.Tracer\nfunc newExporter(ctx context.Context)  /* (someExporter.Exporter, error) */ {\n// Your preferred exporter: console, jaeger, zipkin, OTLP, etc.\n}\nfunc newTraceProvider(exp sdktrace.SpanExporter) *sdktrace.TracerProvider {\n// Ensure default SDK resources and the required service name are set.\nr, err := resource.Merge(\nresource.Default(),\nresource.NewWithAttributes(\nsemconv.SchemaURL,\nsemconv.ServiceName(\"ExampleService\"),\n),\n)\nif err != nil {\npanic(err)\n}\nreturn sdktrace.NewTracerProvider(\nsdktrace.WithBatcher(exp),\nsdktrace.WithResource(r),\n)\n}\nfunc main() {\nctx := context.Background()\nexp, err := newExporter(ctx)\nif err != nil {\nlog.Fatalf(\"failed to initialize exporter: %v\", err)\n}\n// Create a new tracer provider with a batch span processor and the given exporter.\ntp := newTraceProvider(exp)\n// Handle shutdown properly so nothing leaks.\ndefer func() { _ = tp.Shutdown(ctx) }()\notel.SetTracerProvider(tp)\n// Finally, set the tracer that can be used for this package.\ntracer = tp.Tracer(\"ExampleService\")\n}\n</code></pre> <p>You can now access <code>tracer</code> to manually instrument your code.</p>"},{"location":"docs/instrumentation/go/manual/#creating-spans","title":"Creating Spans","text":"<p>Spans are created by tracers. If you don't have one initialized, you'll need to do that.</p> <p>To create a span with a tracer, you'll also need a handle on a <code>context.Context</code> instance. These will typically come from things like a request object and may already contain a parent span from an instrumentation library.</p> <pre><code>func httpHandler(w http.ResponseWriter, r *http.Request) {\nctx, span := tracer.Start(r.Context(), \"hello-span\")\ndefer span.End()\n// do some work to track with hello-span\n}\n</code></pre> <p>In Go, the <code>context</code> package is used to store the active span. When you start a span, you'll get a handle on not only the span that's created, but the modified context that contains it.</p> <p>Once a span has completed, it is immutable and can no longer be modified.</p>"},{"location":"docs/instrumentation/go/manual/#get-the-current-span","title":"Get the current span","text":"<p>To get the current span, you'll need to pull it out of a <code>context.Context</code> you have a handle on:</p> <pre><code>// This context needs contain the active span you plan to extract.\nctx := context.TODO()\nspan := trace.SpanFromContext(ctx)\n// Do something with the current span, optionally calling `span.End()` if you want it to end\n</code></pre> <p>This can be helpful if you'd like to add information to the current span at a point in time.</p>"},{"location":"docs/instrumentation/go/manual/#create-nested-spans","title":"Create nested spans","text":"<p>You can create a nested span to track work in a nested operation.</p> <p>If the current <code>context.Context</code> you have a handle on already contains a span inside of it, creating a new span makes it a nested span. For example:</p> <pre><code>func parentFunction(ctx context.Context) {\nctx, parentSpan := tracer.Start(ctx, \"parent\")\ndefer parentSpan.End()\n// call the child function and start a nested span in there\nchildFunction(ctx)\n// do more work - when this function ends, parentSpan will complete.\n}\nfunc childFunction(ctx context.Context) {\n// Create a span to track `childFunction()` - this is a nested span whose parent is `parentSpan`\nctx, childSpan := tracer.Start(ctx, \"child\")\ndefer childSpan.End()\n// do work here, when this function returns, childSpan will complete.\n}\n</code></pre> <p>Once a span has completed, it is immutable and can no longer be modified.</p>"},{"location":"docs/instrumentation/go/manual/#span-attributes","title":"Span Attributes","text":"<p>Attributes are keys and values that are applied as metadata to your spans and are useful for aggregating, filtering, and grouping traces. Attributes can be added at span creation, or at any other time during the lifecycle of a span before it has completed.</p> <pre><code>// setting attributes at creation...\nctx, span = tracer.Start(ctx, \"attributesAtCreation\", trace.WithAttributes(attribute.String(\"hello\", \"world\")))\n// ... and after creation\nspan.SetAttributes(attribute.Bool(\"isTrue\", true), attribute.String(\"stringAttr\", \"hi!\"))\n</code></pre> <p>Attribute keys can be precomputed, as well:</p> <pre><code>var myKey = attribute.Key(\"myCoolAttribute\")\nspan.SetAttributes(myKey.String(\"a value\"))\n</code></pre>"},{"location":"docs/instrumentation/go/manual/#semantic-attributes","title":"Semantic Attributes","text":"<p>Semantic Attributes are attributes that are defined by the OpenTelemetry Specification in order to provide a shared set of attribute keys across multiple languages, frameworks, and runtimes for common concepts like HTTP methods, status codes, user agents, and more. These attributes are available in the <code>go.opentelemetry.io/otel/semconv/v1.12.0</code> package.</p> <p>For details, see Trace semantic conventions.</p>"},{"location":"docs/instrumentation/go/manual/#events","title":"Events","text":"<p>An event is a human-readable message on a span that represents \"something happening\" during it's lifetime. For example, imagine a function that requires exclusive access to a resource that is under a mutex. An event could be created at two points - once, when we try to gain access to the resource, and another when we acquire the mutex.</p> <pre><code>span.AddEvent(\"Acquiring lock\")\nmutex.Lock()\nspan.AddEvent(\"Got lock, doing work...\")\n// do stuff\nspan.AddEvent(\"Unlocking\")\nmutex.Unlock()\n</code></pre> <p>A useful characteristic of events is that their timestamps are displayed as offsets from the beginning of the span, allowing you to easily see how much time elapsed between them.</p> <p>Events can also have attributes of their own -</p> <pre><code>span.AddEvent(\"Cancelled wait due to external signal\", trace.WithAttributes(attribute.Int(\"pid\", 4328), attribute.String(\"signal\", \"SIGHUP\")))\n</code></pre>"},{"location":"docs/instrumentation/go/manual/#set-span-status","title":"Set span status","text":"<p>A status can be set on a span, typically used to specify that there was an error in the operation a span is tracking - .<code>Error</code>.</p> <pre><code>import (\n// ...\n\"go.opentelemetry.io/otel/codes\"\n// ...\n)\n// ...\nresult, err := operationThatCouldFail()\nif err != nil {\nspan.SetStatus(codes.Error, \"operationThatCouldFail failed\")\n}\n</code></pre> <p>By default, the status for all spans is <code>Unset</code>. In rare cases, you may also wish to set the status to <code>Ok</code>. This should generally not be necessary, though.</p>"},{"location":"docs/instrumentation/go/manual/#record-errors","title":"Record errors","text":"<p>If you have an operation that failed and you wish to capture the error it produced, you can record that error.</p> <pre><code>import (\n// ...\n\"go.opentelemetry.io/otel/codes\"\n// ...\n)\n// ...\nresult, err := operationThatCouldFail()\nif err != nil {\nspan.SetStatus(codes.Error, \"operationThatCouldFail failed\")\nspan.RecordError(err)\n}\n</code></pre> <p>It is highly recommended that you also set a span's status to <code>Error</code> when using <code>RecordError</code>, unless you do not wish to consider the span tracking a failed operation as an error span. The <code>RecordError</code> function does not automatically set a span status when called.</p>"},{"location":"docs/instrumentation/go/manual/#creating-metrics","title":"Creating Metrics","text":"<p>The metrics API is currently unstable, documentation TBA.</p>"},{"location":"docs/instrumentation/go/manual/#propagators-and-context","title":"Propagators and Context","text":"<p>Traces can extend beyond a single process. This requires context propagation, a mechanism where identifiers for a trace are sent to remote processes.</p> <p>In order to propagate trace context over the wire, a propagator must be registered with the OpenTelemetry API.</p> <pre><code>import (\n\"go.opentelemetry.io/otel\"\n\"go.opentelemetry.io/otel/propagation\"\n)\n...\notel.SetTextMapPropagator(propagation.TraceContext{})\n</code></pre> <p>OpenTelemetry also supports the B3 header format, for compatibility with existing tracing systems (<code>go.opentelemetry.io/contrib/propagators/b3</code>) that do not support the W3C TraceContext standard.</p> <p>After configuring context propagation, you'll most likely want to use automatic instrumentation to handle the behind-the-scenes work of actually managing serializing the context.</p>"},{"location":"docs/instrumentation/go/resources/","title":"Resources","text":"<p>Resources are a special type of attribute that apply to all spans generated by a process. These should be used to represent underlying metadata about a process that's non-ephemeral \u2014 for example, the hostname of a process, or its instance ID.</p> <p>Resources should be assigned to a tracer provider at its initialization, and are created much like attributes:</p> <pre><code>resources := resource.NewWithAttributes(\nsemconv.SchemaURL,\nsemconv.ServiceNameKey.String(\"myService\"),\nsemconv.ServiceVersionKey.String(\"1.0.0\"),\nsemconv.ServiceInstanceIDKey.String(\"abcdef12345\"),\n)\nprovider := sdktrace.NewTracerProvider(\n...\nsdktrace.WithResource(resources),\n)\n</code></pre> <p>Note the use of the <code>semconv</code> package to provide conventional names for resource attributes. This helps ensure that consumers of telemetry produced with these semantic conventions can easily discover relevant attributes and understand their meaning.</p> <p>Resources can also be detected automatically through <code>resource.Detector</code> implementations. These <code>Detector</code>s may discover information about the currently running process, the operating system it is running on, the cloud provider hosting that operating system instance, or any number of other resource attributes.</p> <pre><code>resources := resource.New(context.Background(),\nresource.WithFromEnv(), // pull attributes from OTEL_RESOURCE_ATTRIBUTES and OTEL_SERVICE_NAME environment variables\nresource.WithProcess(), // This option configures a set of Detectors that discover process information\nresource.WithOS(), // This option configures a set of Detectors that discover OS information\nresource.WithContainer(), // This option configures a set of Detectors that discover container information\nresource.WithHost(), // This option configures a set of Detectors that discover host information\nresource.WithDetectors(thirdparty.Detector{}), // Bring your own external Detector implementation\nresource.WithAttributes(attribute.String(\"foo\", \"bar\")), // Or specify resource attributes directly\n)\n</code></pre>"},{"location":"docs/instrumentation/go/sampling/","title":"Sampling","text":"<p>Sampling is a process that restricts the amount of traces that are generated by a system. The exact sampler you should use depends on your specific needs, but in general you should make a decision at the start of a trace, and allow the sampling decision to propagate to other services.</p> <p>A sampler needs to be set on the tracer provider when its configured, as follows:</p> <pre><code>provider := sdktrace.NewTracerProvider(\nsdktrace.WithSampler(sdktrace.AlwaysSample()),\n)\n</code></pre> <p><code>AlwaysSample</code> and <code>NeverSample</code> are fairly self-explanatory. Always means that every trace will be sampled, the converse holds as true for Never. When you're getting started, or in a development environment, you'll almost always want to use <code>AlwaysSample</code>.</p> <p>Other samplers include:</p> <ul> <li><code>TraceIDRatioBased</code>, which will sample a fraction of traces, based on the   fraction given to the sampler. Thus, if you set this to .5, half of traces   will be sampled.</li> <li><code>ParentBased</code>, which behaves differently based on the incoming sampling   decision. In general, this will sample spans that have parents that were   sampled, and will not sample spans whose parents were not sampled.</li> </ul> <p>When you're in production, you should consider using the <code>TraceIDRatioBased</code> sampler with the <code>ParentBased</code> sampler.</p>"},{"location":"docs/instrumentation/java/","title":"Java","text":"<p>{{% lang_instrumentation_index_head java /%}}</p>"},{"location":"docs/instrumentation/java/#repositories","title":"Repositories","text":"<p>OpenTelemetry Java consists of the following repositories:</p> <ul> <li>opentelemetry-java:   Components for manual instrumentation including API and SDK as well as   extensions, the OpenTracing shim.</li> <li>opentelemetry-java-docs: Manual instrumentation examples.</li> <li>opentelemetry-java-instrumentation:   Built on top of opentelemetry-java and provides a Java agent JAR that can be   attached to any Java 8+ application and dynamically injects bytecode to   capture telemetry from a number of popular libraries and frameworks.</li> <li>opentelemetry-java-contrib:   Provides helpful libraries and standalone OpenTelemetry-based utilities that   don't fit the express scope of the OpenTelemetry Java or Java Instrumentation   projects. For example, JMX metric gathering.</li> </ul>"},{"location":"docs/instrumentation/java/#components","title":"Components","text":"<p>See components for a complete list of published components.</p>"},{"location":"docs/instrumentation/java/#releases","title":"Releases","text":"<p>Published releases are available on maven central. We strongly recommend using our BOM to keep the versions of the various components in sync.</p>"},{"location":"docs/instrumentation/java/#maven","title":"Maven","text":"<pre><code>&lt;project&gt;\n&lt;dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-bom&lt;/artifactId&gt;\n&lt;version&gt;{{% param javaVersion %}}&lt;/version&gt;\n&lt;type&gt;pom&lt;/type&gt;\n&lt;scope&gt;import&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-api&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre>"},{"location":"docs/instrumentation/java/#gradle","title":"Gradle","text":"<pre><code>dependencies {\nimplementation(platform(\"io.opentelemetry:opentelemetry-bom:{{% param javaVersion %}}\"))\nimplementation(\"io.opentelemetry:opentelemetry-api\")\n}\n</code></pre>"},{"location":"docs/instrumentation/java/getting-started/","title":"Getting Started","text":"<p>This page will show you how to get started with OpenTelemetry in Java.</p> <p>You will learn how you can instrument a simple java application automatically, in such a way that traces, metrics and logs are emitted to the console.</p>"},{"location":"docs/instrumentation/java/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have the following installed locally:</p> <ul> <li>Java JDK</li> <li>Gradle</li> </ul>"},{"location":"docs/instrumentation/java/getting-started/#example-application","title":"Example Application","text":"<p>The following example uses a basic Spring Boot application. If you are not using Spring Boot, that's ok \u2014 you can use OpenTelemetry Java with other web frameworks as well, such as Apache Wicket and Play. For a complete list of libraries for supported frameworks, see the registry.</p> <p>For more elaborate examples, see examples.</p>"},{"location":"docs/instrumentation/java/getting-started/#dependencies","title":"Dependencies","text":"<p>To begin, set up an environment in a new directory called <code>java-simple</code>. Within that directory, create a file called <code>build.gradle.kts</code> with the following content:</p> <pre><code>plugins {\nid(\"java\")\nid(\"org.springframework.boot\") version \"3.0.6\"\nid(\"io.spring.dependency-management\") version \"1.1.0\"\n}\nsourceSets {\nmain {\njava.setSrcDirs(setOf(\".\"))\n}\n}\nrepositories {\nmavenCentral()\n}\ndependencies {\nimplementation(\"org.springframework.boot:spring-boot-starter-web\")\n}\n</code></pre>"},{"location":"docs/instrumentation/java/getting-started/#create-and-launch-an-http-server","title":"Create and launch an HTTP Server","text":"<p>In that same folder, create a file called <code>DiceApplication.java</code> and add the following code to the file:</p> <pre><code>package otel;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.Banner;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n@SpringBootApplication\npublic class DiceApplication {\npublic static void main(String[] args) {\nSpringApplication app = new SpringApplication(DiceApplication.class);\napp.setBannerMode(Banner.Mode.OFF);\napp.run(args);\n}\n}\n</code></pre> <p>Create another file called <code>RollController.java</code> and add the following code to the file:</p> <pre><code>package otel;\nimport java.util.Optional;\nimport java.util.concurrent.ThreadLocalRandom;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RequestParam;\nimport org.springframework.web.bind.annotation.RestController;\n@RestController\npublic class RollController {\nprivate static final Logger logger = LoggerFactory.getLogger(RollController.class);\n@GetMapping(\"/rolldice\")\npublic String index(@RequestParam(\"player\") Optional&lt;String&gt; player) {\nint result = this.getRandomNumber(1, 6);\nif (player.isPresent()) {\nlogger.info(\"{} is rolling the dice: {}\", player.get(), result);\n} else {\nlogger.info(\"Anonymous player is rolling the dice: {}\", result);\n}\nreturn Integer.toString(result);\n}\npublic int getRandomNumber(int min, int max) {\nreturn ThreadLocalRandom.current().nextInt(min, max + 1);\n}\n}\n</code></pre> <p>Build and run the application with the following command, then open http://localhost:8080/rolldice in your web browser to ensure it is working.</p> <pre><code>gradle assemble\njava -jar ./build/libs/java-simple.jar\n</code></pre>"},{"location":"docs/instrumentation/java/getting-started/#instrumentation","title":"Instrumentation","text":"<p>Next, you'll use a Java agent to automatically instrument the application at launch time. While you can configure the Java agent in a number of ways, the steps below use environment variables.</p> <ol> <li>Download opentelemetry-javaagent.jar from Releases of the    <code>opentelemetry-java-instrumentation</code> repo. The JAR file contains the agent    and all automatic instrumentation packages:</li> </ol> <pre><code>curl -L -O https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar\n</code></pre> <p>{{% alert color=\"info\" %}} Take note of the path    to the JAR file.{{% /alert %}}</p> <ol> <li>Set and export variables that specify the Java agent JAR and a console    exporter, using a notation suitable for your shell/terminal environment    \u2014 we illustrate a notation for bash-like shells:</li> </ol> <pre><code>$ export JAVA_TOOL_OPTIONS=\"-javaagent:PATH/TO/opentelemetry-javaagent.jar\" \\\nOTEL_TRACES_EXPORTER=logging \\\nOTEL_METRICS_EXPORTER=logging \\\nOTEL_LOGS_EXPORTER=logging\n</code></pre> <p>{{% alert title=\"Important\" color=\"warning\" %}}Replace <code>PATH/TO</code> above, with    your path to the JAR.{{% /alert %}}</p> <ol> <li>Run your application once again:</li> </ol> <pre><code>$ java -jar ./build/libs/java-simple.jar\n...\n</code></pre> <p>Note the output from the <code>otel.javaagent</code>.</p> <ol> <li>From another terminal, send a request using <code>curl</code>:</li> </ol> <pre><code>$ curl localhost:8080/rolldice\n</code></pre> <ol> <li>Stop the server process.</li> </ol> <p>At step 4, you should have seen trace &amp; log output from the server and client that looks something like this (trace output is line-wrapped for convenience):</p> <pre><code>[otel.javaagent 2023-04-24 17:33:54:567 +0200] [http-nio-8080-exec-1] INFO\nio.opentelemetry.exporter.logging.LoggingSpanExporter - 'RollController.index' :\n 70c2f04ec863a956e9af975ba0d983ee 7fd145f5cda13625 INTERNAL [tracer:\n io.opentelemetry.spring-webmvc-6.0:1.25.0-alpha] AttributesMap{data=\n{thread.id=39, thread.name=http-nio-8080-exec-1}, capacity=128,\n totalAddedValues=2}\n[otel.javaagent 2023-04-24 17:33:54:568 +0200] [http-nio-8080-exec-1] INFO\nio.opentelemetry.exporter.logging.LoggingSpanExporter - 'GET /rolldice' :\n70c2f04ec863a956e9af975ba0d983ee 647ad186ad53eccf SERVER [tracer:\nio.opentelemetry.tomcat-10.0:1.25.0-alpha] AttributesMap{\ndata={user_agent.original=curl/7.87.0, net.host.name=localhost,\n  net.transport=ip_tcp, http.target=/rolldice, net.sock.peer.addr=127.0.0.1,\n  thread.name=http-nio-8080-exec-1, net.sock.peer.port=53422,\n  http.route=/rolldice, net.sock.host.addr=127.0.0.1, thread.id=39,\n  net.protocol.name=http, http.status_code=200, http.scheme=http,\n  net.protocol.version=1.1, http.response_content_length=1,\n  net.host.port=8080, http.method=GET}, capacity=128, totalAddedValues=17}\n</code></pre> <p>At step 5, when stopping the server, you should see an output of all the metrics collected (metrics output is line-wrapped and shortened for convenience):</p> <pre><code>[otel.javaagent 2023-04-24 17:34:25:347 +0200] [PeriodicMetricReader-1] INFO\nio.opentelemetry.exporter.logging.LoggingMetricExporter - Received a collection\n of 19 metrics for export.\n[otel.javaagent 2023-04-24 17:34:25:347 +0200] [PeriodicMetricReader-1] INFO\nio.opentelemetry.exporter.logging.LoggingMetricExporter - metric:\nImmutableMetricData{resource=Resource{schemaUrl=\nhttps://opentelemetry.io/schemas/1.19.0, attributes={host.arch=\"aarch64\",\nhost.name=\"OPENTELEMETRY\", os.description=\"Mac OS X 13.3.1\", os.type=\"darwin\",\nprocess.command_args=[/bin/java, -jar, java-simple.jar],\nprocess.executable.path=\"/bin/java\", process.pid=64497,\nprocess.runtime.description=\"Homebrew OpenJDK 64-Bit Server VM 20\",\nprocess.runtime.name=\"OpenJDK Runtime Environment\",\nprocess.runtime.version=\"20\", service.name=\"java-simple\",\ntelemetry.auto.version=\"1.25.0\", telemetry.sdk.language=\"java\",\ntelemetry.sdk.name=\"opentelemetry\", telemetry.sdk.version=\"1.25.0\"}},\ninstrumentationScopeInfo=InstrumentationScopeInfo{name=io.opentelemetry.runtime-metrics,\nversion=1.25.0, schemaUrl=null, attributes={}},\nname=process.runtime.jvm.buffer.limit, description=Total capacity of the buffers\nin this pool, unit=By, type=LONG_SUM, data=ImmutableSumData{points=\n[ImmutableLongPointData{startEpochNanos=1682350405319221000,\nepochNanos=1682350465326752000, attributes=\n{pool=\"mapped - 'non-volatile memory'\"}, value=0, exemplars=[]},\nImmutableLongPointData{startEpochNanos=1682350405319221000,\nepochNanos=1682350465326752000, attributes={pool=\"mapped\"},\nvalue=0, exemplars=[]},\nImmutableLongPointData{startEpochNanos=1682350405319221000,\nepochNanos=1682350465326752000, attributes={pool=\"direct\"},\nvalue=8192, exemplars=[]}], monotonic=false, aggregationTemporality=CUMULATIVE}}\n...\n</code></pre>"},{"location":"docs/instrumentation/java/getting-started/#what-next","title":"What next?","text":"<p>For more:</p> <ul> <li>Run this example with another exporter for telemetry data.</li> <li>Try automatic instrumentation on one of your own apps.</li> <li>For light-weight customized telemetry, try annotations.</li> <li>Learn about manual instrumentation and try out more   examples.</li> </ul>"},{"location":"docs/instrumentation/java/manual/","title":"Manual Instrumentation","text":"<p>Libraries that want to export telemetry data using OpenTelemetry MUST only depend on the <code>opentelemetry-api</code> package and should never configure or depend on the OpenTelemetry SDK. The SDK configuration must be provided by Applications which should also depend on the <code>opentelemetry-sdk</code> package, or any other implementation of the OpenTelemetry API. This way, libraries will obtain a real implementation only if the user application is configured for it. For more details, check out the Library Guidelines.</p>"},{"location":"docs/instrumentation/java/manual/#setup","title":"Setup","text":"<p>The first step is to get a handle to an instance of the <code>OpenTelemetry</code> interface.</p> <p>If you are an application developer, you need to configure an instance of the <code>OpenTelemetrySdk</code> as early as possible in your application. This can be done using the <code>OpenTelemetrySdk.builder()</code> method. The returned <code>OpenTelemetrySdkBuilder</code> instance gets the providers related to the signals, tracing and metrics, in order to build the <code>OpenTelemetry</code> instance.</p> <p>You can build the providers by using the <code>SdkTracerProvider.builder()</code> and <code>SdkMeterProvider.builder()</code> methods. It is also strongly recommended to define a <code>Resource</code> instance as a representation of the entity producing the telemetry; in particular the <code>service.name</code> attribute is the most important piece of telemetry source-identifying info.</p>"},{"location":"docs/instrumentation/java/manual/#maven","title":"Maven","text":"<pre><code>&lt;project&gt;\n&lt;dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-bom&lt;/artifactId&gt;\n&lt;version&gt;{{% param javaVersion %}}&lt;/version&gt;\n&lt;type&gt;pom&lt;/type&gt;\n&lt;scope&gt;import&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/dependencyManagement&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-api&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-sdk&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-exporter-otlp&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-semconv&lt;/artifactId&gt;\n&lt;version&gt;{{% param javaVersion %}}-alpha&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/project&gt;\n</code></pre> <p>See releases for a full list of artifact coordinates.</p>"},{"location":"docs/instrumentation/java/manual/#gradle","title":"Gradle","text":"<pre><code>dependencies {\nimplementation 'io.opentelemetry:opentelemetry-api:{{% param javaVersion %}}'\nimplementation 'io.opentelemetry:opentelemetry-sdk:{{% param javaVersion %}}'\nimplementation 'io.opentelemetry:opentelemetry-exporter-otlp:{{% param javaVersion %}}'\nimplementation 'io.opentelemetry:opentelemetry-semconv:{{% param javaVersion %}}-alpha'\n}\n</code></pre> <p>See releases for a full list of artifact coordinates.</p>"},{"location":"docs/instrumentation/java/manual/#imports","title":"Imports","text":"<pre><code>import io.opentelemetry.api.OpenTelemetry;\nimport io.opentelemetry.api.common.Attributes;\nimport io.opentelemetry.api.trace.propagation.W3CTraceContextPropagator;\nimport io.opentelemetry.context.propagation.ContextPropagators;\nimport io.opentelemetry.exporter.otlp.metrics.OtlpGrpcMetricExporter;\nimport io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter;\nimport io.opentelemetry.sdk.OpenTelemetrySdk;\nimport io.opentelemetry.sdk.metrics.SdkMeterProvider;\nimport io.opentelemetry.sdk.metrics.export.PeriodicMetricReader;\nimport io.opentelemetry.sdk.resources.Resource;\nimport io.opentelemetry.sdk.trace.SdkTracerProvider;\nimport io.opentelemetry.sdk.trace.export.BatchSpanProcessor;\nimport io.opentelemetry.semconv.resource.attributes.ResourceAttributes;\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#example","title":"Example","text":"<pre><code>Resource resource = Resource.getDefault()\n.merge(Resource.create(Attributes.of(ResourceAttributes.SERVICE_NAME, \"logical-service-name\")));\nSdkTracerProvider sdkTracerProvider = SdkTracerProvider.builder()\n.addSpanProcessor(BatchSpanProcessor.builder(OtlpGrpcSpanExporter.builder().build()).build())\n.setResource(resource)\n.build();\nSdkMeterProvider sdkMeterProvider = SdkMeterProvider.builder()\n.registerMetricReader(PeriodicMetricReader.builder(OtlpGrpcMetricExporter.builder().build()).build())\n.setResource(resource)\n.build();\n// TODO: add log configuration when stable\nOpenTelemetry openTelemetry = OpenTelemetrySdk.builder()\n.setTracerProvider(sdkTracerProvider)\n.setMeterProvider(sdkMeterProvider)\n.setPropagators(ContextPropagators.create(W3CTraceContextPropagator.getInstance()))\n.buildAndRegisterGlobal();\n</code></pre> <p>As an aside, if you are writing library instrumentation, it is strongly recommended that you provide your users the ability to inject an instance of <code>OpenTelemetry</code> into your instrumentation code. If this is not possible for some reason, you can fall back to using an instance from the <code>GlobalOpenTelemetry</code> class. Note that you can't force end-users to configure the global, so this is the most brittle option for library instrumentation.</p>"},{"location":"docs/instrumentation/java/manual/#acquiring-a-tracer","title":"Acquiring a Tracer","text":"<p>To do Tracing you'll need to acquire a <code>Tracer</code>.</p> <p>Note: Methods of the OpenTelemetry SDK should never be called.</p> <p>First, a <code>Tracer</code> must be acquired, which is responsible for creating spans and interacting with the Context. A tracer is acquired by using the OpenTelemetry API specifying the name and version of the library instrumenting the instrumented library or application to be monitored. More information is available in the specification chapter Obtaining a Tracer.</p> <pre><code>import io.opentelemetry.api;\n//...\nTracer tracer =\nopenTelemetry.getTracer(\"instrumentation-library-name\", \"1.0.0\");\n</code></pre> <p>Important: the \"name\" and optional version of the tracer are purely informational. All <code>Tracer</code>s that are created by a single <code>OpenTelemetry</code> instance will interoperate, regardless of name.</p>"},{"location":"docs/instrumentation/java/manual/#create-spans","title":"Create Spans","text":"<p>To create Spans, you only need to specify the name of the span. The start and end time of the span is automatically set by the OpenTelemetry SDK.</p> <pre><code>Span span = tracer.spanBuilder(\"my span\").startSpan();\n// Make the span the current span\ntry (Scope ss = span.makeCurrent()) {\n// In this scope, the span is the current/active span\n} finally {\nspan.end();\n}\n</code></pre> <p>It's required to call <code>end()</code> to end the span when you want it to end.</p>"},{"location":"docs/instrumentation/java/manual/#create-nested-spans","title":"Create nested Spans","text":"<p>Most of the time, we want to correlate spans for nested operations. OpenTelemetry supports tracing within processes and across remote processes. For more details how to share context between remote processes, see Context Propagation.</p> <p>For a method <code>a</code> calling a method <code>b</code>, the spans could be manually linked in the following way:</p> <pre><code>void parentOne() {\nSpan parentSpan = tracer.spanBuilder(\"parent\").startSpan();\ntry {\nchildOne(parentSpan);\n} finally {\nparentSpan.end();\n}\n}\nvoid childOne(Span parentSpan) {\nSpan childSpan = tracer.spanBuilder(\"child\")\n.setParent(Context.current().with(parentSpan))\n.startSpan();\ntry {\n// do stuff\n} finally {\nchildSpan.end();\n}\n}\n</code></pre> <p>The OpenTelemetry API offers also an automated way to propagate the parent span on the current thread:</p> <pre><code>void parentTwo() {\nSpan parentSpan = tracer.spanBuilder(\"parent\").startSpan();\ntry(Scope scope = parentSpan.makeCurrent()) {\nchildTwo();\n} finally {\nparentSpan.end();\n}\n}\nvoid childTwo() {\nSpan childSpan = tracer.spanBuilder(\"child\")\n// NOTE: setParent(...) is not required;\n// `Span.current()` is automatically added as the parent\n.startSpan();\ntry(Scope scope = childSpan.makeCurrent()) {\n// do stuff\n} finally {\nchildSpan.end();\n}\n}\n</code></pre> <p>To link spans from remote processes, it is sufficient to set the Remote Context as parent.</p> <pre><code>Span childRemoteParent = tracer.spanBuilder(\"Child\").setParent(remoteContext).startSpan();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#get-the-current-span","title":"Get the current span","text":"<p>Sometimes it's helpful to do something with the current/active span at a particular point in program execution.</p> <pre><code>Span span = Span.current()\n</code></pre> <p>And if you want the current span for a particular <code>Context</code> object:</p> <pre><code>Span span = Span.fromContext(context)\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#span-attributes","title":"Span Attributes","text":"<p>In OpenTelemetry spans can be created freely and it's up to the implementor to annotate them with attributes specific to the represented operation. Attributes provide additional context on a span about the specific operation it tracks, such as results or operation properties.</p> <pre><code>Span span = tracer.spanBuilder(\"/resource/path\").setSpanKind(SpanKind.CLIENT).startSpan();\nspan.setAttribute(\"http.method\", \"GET\");\nspan.setAttribute(\"http.url\", url.toString());\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#semantic-attributes","title":"Semantic Attributes","text":"<p>There are semantic conventions for spans representing operations in well-known protocols like HTTP or database calls. Semantic conventions for these spans are defined in the specification at Trace Semantic Conventions.</p> <p>First add the semantic conventions as a dependency to your application:</p>"},{"location":"docs/instrumentation/java/manual/#maven_1","title":"Maven","text":"<pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-semconv&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#gradle_1","title":"Gradle","text":"<pre><code>dependencies {\nimplementation(\"io.opentelemetry:opentelemetry-semconv\")\n}\n</code></pre> <p>Finally, you can update your file to include semantic attributes:</p> <pre><code>Span span = tracer.spanBuilder(\"/resource/path\").setSpanKind(SpanKind.CLIENT).startSpan();\nspan.setAttribute(SemanticAttributes.HTTP_METHOD, \"GET\");\nspan.setAttribute(SemanticAttributes.HTTP_URL, url.toString());\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#create-spans-with-events","title":"Create Spans with events","text":"<p>Spans can be annotated with named events (called Span Events) that can carry zero or more Span Attributes, each of which itself is a key:value map paired automatically with a timestamp.</p> <pre><code>span.addEvent(\"Init\");\n...\nspan.addEvent(\"End\");\n</code></pre> <pre><code>Attributes eventAttributes = Attributes.of(\nAttributeKey.stringKey(\"key\"), \"value\",\nAttributeKey.longKey(\"result\"), 0L);\nspan.addEvent(\"End Computation\", eventAttributes);\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#create-spans-with-links","title":"Create Spans with links","text":"<p>A Span may be linked to zero or more other Spans that are causally related via a Span Link. Links can be used to represent batched operations where a Span was initiated by multiple initiating Spans, each representing a single incoming item being processed in the batch.</p> <pre><code>Span child = tracer.spanBuilder(\"childWithLink\")\n.addLink(parentSpan1.getSpanContext())\n.addLink(parentSpan2.getSpanContext())\n.addLink(parentSpan3.getSpanContext())\n.addLink(remoteSpanContext)\n.startSpan();\n</code></pre> <p>For more details how to read context from remote processes, see Context Propagation.</p>"},{"location":"docs/instrumentation/java/manual/#set-span-status","title":"Set span status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>SpanStatus.Error</code>. In rare scenarios, you could override the <code>Error</code> status with <code>OK</code>, but don't set <code>OK</code> on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>Span span = tracer.spanBuilder(\"my span\").startSpan();\n// put the span into the current Context\ntry (Scope scope = span.makeCurrent()) {\n// do something\n} catch (Throwable t) {\nspan.setStatus(StatusCode.ERROR, \"Something bad happened!\");\nthrow t;\n} finally {\nspan.end(); // Cannot set a span after this call\n}\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#record-exceptions-in-spans","title":"Record exceptions in spans","text":"<p>It can be a good idea to record exceptions when they happen. It's recommended to do this in conjunction with setting span status.</p> <pre><code>Span span = tracer.spanBuilder(\"my span\").startSpan();\n// put the span into the current Context\ntry (Scope scope = span.makeCurrent()) {\n// do something\n} catch (Throwable throwable) {\nspan.setStatus(StatusCode.ERROR, \"Something bad happened!\");\nspan.recordException(throwable);\n} finally {\nspan.end(); // Cannot set a span after this call\n}\n</code></pre> <p>This will capture things like the current stack trace in the span.</p>"},{"location":"docs/instrumentation/java/manual/#context-propagation","title":"Context Propagation","text":"<p>OpenTelemetry provides a text-based approach to propagate context to remote services using the W3C Trace Context HTTP headers.</p> <p>The following presents an example of an outgoing HTTP request using <code>HttpURLConnection</code>.</p> <pre><code>// Tell OpenTelemetry to inject the context in the HTTP headers\nTextMapSetter&lt;HttpURLConnection&gt; setter =\nnew TextMapSetter&lt;HttpURLConnection&gt;() {\n@Override\npublic void set(HttpURLConnection carrier, String key, String value) {\n// Insert the context as Header\ncarrier.setRequestProperty(key, value);\n}\n};\nURL url = new URL(\"http://127.0.0.1:8080/resource\");\nSpan outGoing = tracer.spanBuilder(\"/resource\").setSpanKind(SpanKind.CLIENT).startSpan();\ntry (Scope scope = outGoing.makeCurrent()) {\n// Use the Semantic Conventions.\n// (Note that to set these, Span does not *need* to be the current instance in Context or Scope.)\noutGoing.setAttribute(SemanticAttributes.HTTP_METHOD, \"GET\");\noutGoing.setAttribute(SemanticAttributes.HTTP_URL, url.toString());\nHttpURLConnection transportLayer = (HttpURLConnection) url.openConnection();\n// Inject the request with the *current*  Context, which contains our current Span.\nopenTelemetry.getPropagators().getTextMapPropagator().inject(Context.current(), transportLayer, setter);\n// Make outgoing call\n} finally {\noutGoing.end();\n}\n...\n</code></pre> <p>Similarly, the text-based approach can be used to read the W3C Trace Context from incoming requests. The following presents an example of processing an incoming HTTP request using HttpExchange.</p> <pre><code>TextMapGetter&lt;HttpExchange&gt; getter =\nnew TextMapGetter&lt;&gt;() {\n@Override\npublic String get(HttpExchange carrier, String key) {\nif (carrier.getRequestHeaders().containsKey(key)) {\nreturn carrier.getRequestHeaders().get(key).get(0);\n}\nreturn null;\n}\n@Override\npublic Iterable&lt;String&gt; keys(HttpExchange carrier) {\nreturn carrier.getRequestHeaders().keySet();\n}\n};\n...\npublic void handle(HttpExchange httpExchange) {\n// Extract the SpanContext and other elements from the request.\nContext extractedContext = openTelemetry.getPropagators().getTextMapPropagator()\n.extract(Context.current(), httpExchange, getter);\ntry (Scope scope = extractedContext.makeCurrent()) {\n// Automatically use the extracted SpanContext as parent.\nSpan serverSpan = tracer.spanBuilder(\"GET /resource\")\n.setSpanKind(SpanKind.SERVER)\n.startSpan();\ntry {\n// Add the attributes defined in the Semantic Conventions\nserverSpan.setAttribute(SemanticAttributes.HTTP_METHOD, \"GET\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_SCHEME, \"http\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_HOST, \"localhost:8080\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_TARGET, \"/resource\");\n// Serve the request\n...\n} finally {\nserverSpan.end();\n}\n}\n}\n</code></pre> <p>The following code presents an example to read the W3C Trace Context from incoming request, add spans, and further propagate the context. The example utilizes HttpHeaders to fetch the traceparent header for context propagation.</p> <pre><code>TextMapGetter&lt;HttpHeaders&gt; getter =\nnew TextMapGetter&lt;HttpHeaders&gt;() {\n@Override\npublic String get(HttpHeaders headers, String s) {\nassert headers != null;\nreturn headers.getHeaderString(s);\n}\n@Override\npublic Iterable&lt;String&gt; keys(HttpHeaders headers) {\nList&lt;String&gt; keys = new ArrayList&lt;&gt;();\nMultivaluedMap&lt;String, String&gt; requestHeaders = headers.getRequestHeaders();\nrequestHeaders.forEach((k, v) -&gt;{\nkeys.add(k);\n});\n}\n};\nTextMapSetter&lt;HttpURLConnection&gt; setter =\nnew TextMapSetter&lt;HttpURLConnection&gt;() {\n@Override\npublic void set(HttpURLConnection carrier, String key, String value) {\n// Insert the context as Header\ncarrier.setRequestProperty(key, value);\n}\n};\n//...\npublic void handle(&lt;Library Specific Annotation&gt; HttpHeaders headers){\nContext extractedContext = opentelemetry.getPropagators().getTextMapPropagator()\n.extract(Context.current(), headers, getter);\ntry (Scope scope = extractedContext.makeCurrent()) {\n// Automatically use the extracted SpanContext as parent.\nSpan serverSpan = tracer.spanBuilder(\"GET /resource\")\n.setSpanKind(SpanKind.SERVER)\n.startSpan();\ntry(Scope ignored = serverSpan.makeCurrent()) {\n// Add the attributes defined in the Semantic Conventions\nserverSpan.setAttribute(SemanticAttributes.HTTP_METHOD, \"GET\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_SCHEME, \"http\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_HOST, \"localhost:8080\");\nserverSpan.setAttribute(SemanticAttributes.HTTP_TARGET, \"/resource\");\nHttpURLConnection transportLayer = (HttpURLConnection) url.openConnection();\n// Inject the request with the *current*  Context, which contains our current Span.\nopenTelemetry.getPropagators().getTextMapPropagator().inject(Context.current(), transportLayer, setter);\n// Make outgoing call\n}finally {\nserverSpan.end();\n}\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#metrics","title":"Metrics","text":"<p>Spans provide detailed information about your application, but produce data that is proportional to the load on the system. In contrast, metrics combine individual measurements into aggregations, and produce data which is constant as a function of system load. The aggregations lack details required to diagnose low level issues, but complement spans by helping to identify trends and providing application runtime telemetry.</p> <p>The metrics API defines a variety of instruments. Instruments record measurements, which are aggregated by the metrics SDK and eventually exported out of process. Instruments come in synchronous and asynchronous varieties. Synchronous instruments record measurements as they happen. Asynchronous instrument register a callback, which is invoked once per collection, and which records measurements at that point in time. The following instruments are available:</p> <ul> <li><code>LongCounter</code>/<code>DoubleCounter</code>: records only positive values, with synchronous   and asynchronous options. Useful for counting things, such as the number of   bytes sent over a network. Counter measurements are aggregated to   always-increasing monotonic sums by default.</li> <li><code>LongUpDownCounter</code>/<code>DoubleUpDownCounter</code>: records positive and negative   values, with synchronous and asynchronous options. Useful for counting things   that go up and down, like the size of a queue. Up down counter measurements   are aggregated to non-monotonic sums by default.</li> <li><code>LongGauge</code>/<code>DoubleGauge</code>: measures an instantaneous value with an   asynchronous callback. Useful for recording values that can't be merged across   attributes, like CPU utilization percentage. Gauge measurements are aggregated   as gauges by default.</li> <li><code>LongHistogram</code>/<code>DoubleHistogram</code>: records measurements that are most useful   to analyze as a histogram distribution. No asynchronous option is available.   Useful for recording things like the duration of time spent by an HTTP server   processing a request. Histogram measurements are aggregated to explicit bucket   histograms by default.</li> </ul> <p>Note: The asynchronous varieties of counter and up down counter assume that the registered callback is observing the cumulative sum. For example, if you register an asynchronous counter whose callback records bytes sent over a network, it must record the cumulative sum of all bytes sent over the network, rather than trying to compute and record the difference since last call.</p> <p>All metrics can be annotated with attributes: additional qualifiers that help describe what subdivision of the measurements the metric represents.</p> <p>The following is an example of counter usage:</p> <pre><code>OpenTelemetry openTelemetry = // obtain instance of OpenTelemetry\n// Gets or creates a named meter instance\nMeter meter = openTelemetry.meterBuilder(\"instrumentation-library-name\")\n.setInstrumentationVersion(\"1.0.0\")\n.build();\n// Build counter e.g. LongCounter\nLongCounter counter = meter\n.counterBuilder(\"processed_jobs\")\n.setDescription(\"Processed jobs\")\n.setUnit(\"1\")\n.build();\n// It is recommended that the API user keep a reference to Attributes they will record against\nAttributes attributes = Attributes.of(stringKey(\"Key\"), \"SomeWork\");\n// Record data\ncounter.add(123, attributes);\n</code></pre> <p>The following is an example of usage of an asynchronous instrument:</p> <pre><code>// Build an asynchronous instrument, e.g. Gauge\nmeter\n.gaugeBuilder(\"cpu_usage\")\n.setDescription(\"CPU Usage\")\n.setUnit(\"ms\")\n.buildWithCallback(measurement -&gt; {\nmeasurement.record(getCpuUsage(), Attributes.of(stringKey(\"Key\"), \"SomeWork\"));\n});\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#logs","title":"Logs","text":"<p>Logs are distinct from Metrics and Tracing in that there is no user-facing logs API. Instead, there is tooling to bridge logs from existing popular log frameworks (e.g. SLF4j, JUL, Logback, Log4j) into the OpenTelemetry ecosystem.</p> <p>The two typical workflows discussed below each cater to different application requirements.</p>"},{"location":"docs/instrumentation/java/manual/#direct-to-collector","title":"Direct to collector","text":"<p>In the direct to collector workflow, logs are emitted directly from an application to a collector using a network protocol (e.g. OTLP). This workflow is simple to set up as it doesn't require any additional log forwarding components, and allows an application to easily emit structured logs that conform to the log data model. However, the overhead required for applications to queue and export logs to a network location may not be suitable for all applications.</p> <p>To use this workflow:</p> <ul> <li>Install appropriate Log Appender.</li> <li>Configure the OpenTelemetry Log SDK to export log records to   desired target destination (the collector or   other).</li> </ul>"},{"location":"docs/instrumentation/java/manual/#log-appenders","title":"Log appenders","text":"<p>A log appender bridges logs from a log framework into the OpenTelemetry Log SDK using the Logs Bridge API. Log appenders are available for various popular java log frameworks:</p> <ul> <li>Log4j2 Appender</li> <li>Logback Appender</li> </ul> <p>The links above contain full usage and installation documentation, but installation is generally as follows:</p> <ul> <li>Add required dependency via gradle or maven.</li> <li>Extend the application's log configuration (i.e. <code>logback.xml</code>, <code>log4j.xml</code>,   etc) to include a reference to the OpenTelemetry log appender.</li> <li>Optionally configure the log framework to determine which logs (i.e. filter     by severity or logger name) are passed to the appender.</li> <li>Optionally configure the appender to indicate how logs are mapped to     OpenTelemetry Log Records (i.e. capture thread information, context data,     markers, etc).</li> </ul> <p>Log appenders automatically include the trace context in log records, enabling log correlation with traces.</p> <p>The Log Appender example demonstrates setup for a variety of scenarios.</p>"},{"location":"docs/instrumentation/java/manual/#via-file-or-stdout","title":"Via file or stdout","text":"<p>In the file or stdout workflow, logs are written to files or standout output. Another component (e.g. FluentBit) is responsible for reading / tailing the logs, parsing them to more structured format, and forwarding them a target, such as the collector. This workflow may be preferable in situations where application requirements do not permit additional overhead from direct to collector. However, it requires that all log fields required down stream are encoded into the logs, and that the component reading the logs parse the data into the log data model. The installation and configuration of log forwarding components is outside the scope of this document.</p> <p>Log correlation with traces is available by installing log context instrumentation.</p>"},{"location":"docs/instrumentation/java/manual/#log-context-instrumentation","title":"Log context instrumentation","text":"<p>OpenTelemetry provides components which enrich log context with trace context for various popular java log frameworks:</p> <ul> <li>Log4j context data instrumentation</li> <li>Logback MDC instrumentation</li> </ul> <p>This links above contain full usage and installation documentation, but installation is generally as follows:</p> <ul> <li>Add required dependency via gradle or maven.</li> <li>Extend the application's log configuration (i.e. <code>logback.xml</code> or <code>log4j.xml</code>,   etc) to reference the trace context fields in the log pattern.</li> </ul>"},{"location":"docs/instrumentation/java/manual/#sdk-configuration","title":"SDK Configuration","text":"<p>The configuration examples reported in this document only apply to the SDK provided by <code>opentelemetry-sdk</code>. Other implementation of the API might provide different configuration mechanisms.</p>"},{"location":"docs/instrumentation/java/manual/#tracing-sdk","title":"Tracing SDK","text":"<p>The application has to install a span processor with an exporter and may customize the behavior of the OpenTelemetry SDK.</p> <p>For example, a basic configuration instantiates the SDK tracer provider and sets to export the traces to a logging stream.</p> <pre><code>SdkTracerProvider tracerProvider = SdkTracerProvider.builder()\n.addSpanProcessor(BatchSpanProcessor.builder(LoggingSpanExporter.create()).build())\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#sampler","title":"Sampler","text":"<p>It is not always feasible to trace and export every user request in an application. In order to strike a balance between observability and expenses, traces can be sampled.</p> <p>The OpenTelemetry SDK offers four samplers out of the box:</p> <ul> <li>AlwaysOnSampler which samples every trace regardless of upstream sampling   decisions.</li> <li>AlwaysOffSampler which doesn't sample any trace, regardless of upstream   sampling decisions.</li> <li>ParentBased which uses the parent span to make sampling decisions, if   present.</li> <li>TraceIdRatioBased which samples a configurable percentage of traces, and   additionally samples any trace that was sampled upstream.</li> </ul> <p>Additional samplers can be provided by implementing the <code>io.opentelemetry.sdk.trace.Sampler</code> interface.</p> <pre><code>SdkTracerProvider tracerProvider = SdkTracerProvider.builder()\n.setSampler(Sampler.alwaysOn())\n//or\n.setSampler(Sampler.alwaysOff())\n//or\n.setSampler(Sampler.traceIdRatioBased(0.5))\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#span-processor","title":"Span Processor","text":"<p>Different Span processors are offered by OpenTelemetry. The <code>SimpleSpanProcessor</code> immediately forwards ended spans to the exporter, while the <code>BatchSpanProcessor</code> batches them and sends them in bulk. Multiple Span processors can be configured to be active at the same time using the <code>MultiSpanProcessor</code>.</p> <pre><code>SdkTracerProvider tracerProvider = SdkTracerProvider.builder()\n.addSpanProcessor(SimpleSpanProcessor.create(LoggingSpanExporter.create()))\n.addSpanProcessor(BatchSpanProcessor.builder(LoggingSpanExporter.create()).build())\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#exporter","title":"Exporter","text":"<p>Span processors are initialized with an exporter which is responsible for sending the telemetry data a particular backend. OpenTelemetry offers five exporters out of the box:</p> <ul> <li><code>InMemorySpanExporter</code>: keeps the data in memory, useful for testing and   debugging.</li> <li>Jaeger Exporter: prepares and sends the collected telemetry data to a Jaeger   backend via gRPC. Varieties include <code>JaegerGrpcSpanExporter</code> and   <code>JaegerThriftSpanExporter</code>.</li> <li><code>ZipkinSpanExporter</code>: prepares and sends the collected telemetry data to a   Zipkin backend via the Zipkin APIs.</li> <li>Logging Exporter: saves the telemetry data into log streams. Varieties include   <code>LoggingSpanExporter</code> and <code>OtlpJsonLoggingSpanExporter</code>.</li> <li>OpenTelemetry Protocol Exporter: sends the data in OTLP to the OpenTelemetry   Collector or other OTLP receivers. Varieties include <code>OtlpGrpcSpanExporter</code>   and <code>OtlpHttpSpanExporter</code>.</li> </ul> <p>Other exporters can be found in the OpenTelemetry Registry.</p> <pre><code>ManagedChannel jaegerChannel = ManagedChannelBuilder.forAddress(\"localhost\", 3336)\n.usePlaintext()\n.build();\nJaegerGrpcSpanExporter jaegerExporter = JaegerGrpcSpanExporter.builder()\n.setEndpoint(\"localhost:3336\")\n.setTimeout(30, TimeUnit.SECONDS)\n.build();\nSdkTracerProvider tracerProvider = SdkTracerProvider.builder()\n.addSpanProcessor(BatchSpanProcessor.builder(jaegerExporter).build())\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#metrics-sdk","title":"Metrics SDK","text":"<p>The application has to install a metric reader with an exporter, and may further customize the behavior of the OpenTelemetry SDK.</p> <p>For example, a basic configuration instantiates the SDK meter provider and sets to export the metrics to a logging stream.</p> <pre><code>SdkMeterProvider meterProvider = SdkMeterProvider.builder()\n.registerMetricReader(PeriodicMetricReader.builder(LoggingMetricExporter.create()).build())\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#metric-reader","title":"Metric Reader","text":"<p>Metric readers read aggregated metrics.</p> <pre><code>SdkMeterProvider meterProvider = SdkMeterProvider.builder()\n.registerMetricReader(...)\n.build();\n</code></pre> <p>OpenTelemetry provides a variety of metric readers out of the box:</p> <ul> <li><code>PeriodicMetricReader</code>: reads metrics on a configurable interval and pushes to   a <code>MetricExporter</code>.</li> <li><code>InMemoryMetricReader</code>: reads metrics into memory, useful for debugging and   testing.</li> <li><code>PrometheusHttpServer</code> (alpha): an HTTP server that reads metrics and   serializes to Prometheus text format.</li> </ul> <p>Custom metric reader implementations are not currently supported.</p>"},{"location":"docs/instrumentation/java/manual/#exporter_1","title":"Exporter","text":"<p>The <code>PeriodicMetricReader</code> is paired with a metric exporter, which is responsible for sending the telemetry data to a particular backend. OpenTelemetry provides the following exporters out of the box:</p> <ul> <li><code>InMemoryMetricExporter</code>: keeps the data in memory, useful for testing and   debugging.</li> <li>Logging Exporter: saves the telemetry data into log streams. Varieties include   <code>LoggingMetricExporter</code> and <code>OtlpJsonLoggingMetricExporter</code>.</li> <li>OpenTelemetry Protocol Exporter: sends the data in OTLP to the OpenTelemetry   Collector or other OTLP receivers. Varieties include <code>OtlpGrpcMetricExporter</code>   and <code>OtlpHttpMetricExporter</code>.</li> </ul> <p>Other exporters can be found in the OpenTelemetry Registry.</p>"},{"location":"docs/instrumentation/java/manual/#views","title":"Views","text":"<p>Views provide a mechanism for controlling how measurements are aggregated into metrics. They consist of an <code>InstrumentSelector</code> and a <code>View</code>. The instrument selector consists of a series of options for selecting which instruments the view applies to. Instruments can be selected by a combination of name, type, meter name, meter version, and meter schema url. The view describes how measurement should be aggregated. The view can change the name, description, the aggregation, and define the set of attribute keys that should be retained.</p> <pre><code>SdkMeterProvider meterProvider = SdkMeterProvider.builder()\n.registerView(\nInstrumentSelector.builder()\n.setName(\"my-counter\") // Select instrument(s) called \"my-counter\"\n.build(),\nView.builder()\n.setName(\"new-counter-name\") // Change the name to \"new-counter-name\"\n.build())\n.registerMetricReader(...)\n.build()\n</code></pre> <p>Every instrument has a default view, which retains the original name, description, and attributes, and has a default aggregation that is based on the type of instrument. When a registered view matches an instrument, the default view is replaced by the registered view. Additional registered views that match the instrument are additive, and result in multiple exported metrics for the instrument.</p>"},{"location":"docs/instrumentation/java/manual/#logs-sdk","title":"Logs SDK","text":"<p>The logs SDK dictates how logs are processed when using the direct to collector workflow. No log SDK is needed when using the log forwarding workflow.</p> <p>The typical log SDK configuration installs a log record processor and exporter. For example, the following installs the BatchLogRecordProcessor, which periodically exports to a network location via the OtlpGrpcLogRecordExporter:</p> <pre><code>SdkLoggerProvider loggerProvider = SdkLoggerProvider.builder()\n.addLogRecordProcessor(\nBatchLogRecordProcessor.builder(\nOtlpGrpcLogRecordExporter.builder()\n.setEndpoint(\"http://localhost:4317\")\n.build())\n.build())\n.build();\n</code></pre> <p>See releases for log specific artifact coordinates.</p>"},{"location":"docs/instrumentation/java/manual/#logrecord-processor","title":"LogRecord Processor","text":"<p>LogRecord processors process LogRecords emitted by log appenders.</p> <p>OpenTelemetry provides the following LogRecord processors out of the box:</p> <ul> <li><code>BatchLogRecordProcessor</code>: periodically sends batches of LogRecords to a   LogRecordExporter.</li> <li><code>SimpleLogRecordProcessor</code>: immediately sends each LogRecord to a   LogRecordExporter.</li> </ul> <p>Custom LogRecord processors are supported by implementing the <code>LogRecordProcessor</code> interface. Common use cases include enriching the LogRecords with contextual data like baggage, or filtering / obfuscating sensitive data.</p>"},{"location":"docs/instrumentation/java/manual/#logrecord-exporter","title":"LogRecord Exporter","text":"<p><code>BatchLogRecordProcessor</code> and <code>SimpleLogRecordProcessor</code> are paired with <code>LogRecordExporter</code>, which is responsible for sending telemetry data to a particular backend. OpenTelemetry provides the following exporters out of the box:</p> <ul> <li>OpenTelemetry Protocol Exporter: sends the data in OTLP to the OpenTelemetry   Collector or other OTLP receivers. Varieties include   <code>OtlpGrpcLogRecordExporter</code> and <code>OtlpHttpLogRecordExporter</code>.</li> <li><code>InMemoryLogRecordExporter</code>: keeps the data in memory, useful for testing and   debugging.</li> <li>Logging Exporter: saves the telemetry data into log streams. Varieties include   <code>SystemOutLogRecordExporter</code> and <code>OtlpJsonLoggingLogRecordExporter</code>. Note:   <code>OtlpJsonLoggingLogRecordExporter</code> logs to JUL, and may cause infinite loops   (i.e. JUL -&gt; SLF4J -&gt; Logback -&gt; OpenTelemetry Appender -&gt; OpenTelemetry Log   SDK -&gt; JUL) if not carefully configured.</li> </ul> <p>Custom exporters are supported by implementing the <code>LogRecordExporter</code> interface.</p>"},{"location":"docs/instrumentation/java/manual/#auto-configuration","title":"Auto Configuration","text":"<p>Instead of manually creating the <code>OpenTelemetry</code> instance by using the SDK builders directly from your code, it is also possible to use the SDK auto-configuration extension through the <code>opentelemetry-sdk-extension-autoconfigure</code> module.</p> <p>This module is made available by adding the following dependency to your application.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-sdk-extension-autoconfigure&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>It allows you to auto-configure the OpenTelemetry SDK based on a standard set of supported environment variables and system properties. Each environment variable has a corresponding system property named the same way but as lower case and using the <code>.</code> (dot) character instead of the <code>_</code> (underscore) as separator.</p> <p>The logical service name can be specified via the <code>OTEL_SERVICE_NAME</code> environment variable (or <code>otel.service.name</code> system property).</p> <p>The traces, metrics or logs exporters can be set via the <code>OTEL_TRACES_EXPORTER</code>, <code>OTEL_METRICS_EXPORTER</code> and <code>OTEL_LOGS_EXPORTER</code> environment variables. For example <code>OTEL_TRACES_EXPORTER=jaeger</code> configures your application to use the Jaeger exporter. The corresponding Jaeger exporter library has to be provided in the classpath of the application as well.</p> <p>It's also possible to set up the propagators via the <code>OTEL_PROPAGATORS</code> environment variable, like for example using the <code>tracecontext</code> value to use W3C Trace Context.</p> <p>For more details, see all the supported configuration options in the module's README.</p> <p>The SDK auto-configuration has to be initialized from your code in order to allow the module to go through the provided environment variables (or system properties) and set up the <code>OpenTelemetry</code> instance by using the builders internally.</p> <pre><code>OpenTelemetrySdk sdk = AutoConfiguredOpenTelemetrySdk.initialize()\n.getOpenTelemetrySdk();\n</code></pre> <p>When environment variables or system properties are not sufficient, you can use some extension points provided through the auto-configure SPI and several methods in the <code>AutoConfiguredOpenTelemetrySdk</code> class.</p> <p>Following an example with a code snippet for adding an additional custom span processor.</p> <pre><code>AutoConfiguredOpenTelemetrySdk.builder()\n.addTracerProviderCustomizer(\n(sdkTracerProviderBuilder, configProperties) -&gt;\nsdkTracerProviderBuilder.addSpanProcessor(\nnew SpanProcessor() { /* implementation omitted for brevity */ }))\n.build();\n</code></pre>"},{"location":"docs/instrumentation/java/manual/#sdk-logging-and-error-handling","title":"SDK Logging and Error Handling","text":"<p>OpenTelemetry uses java.util.logging to log information about OpenTelemetry, including errors and warnings about misconfigurations or failures exporting data.</p> <p>By default, log messages are handled by the root handler in your application. If you have not installed a custom root handler for your application, logs of level <code>INFO</code> or higher are sent to the console by default.</p> <p>You may want to change the behavior of the logger for OpenTelemetry. For example, you can reduce the logging level to output additional information when debugging, increase the level for a particular class to ignore errors coming from that class, or install a custom handler or filter to run custom code whenever OpenTelemetry logs a particular message.</p>"},{"location":"docs/instrumentation/java/manual/#examples","title":"Examples","text":"<pre><code>## Turn off all OpenTelemetry logging\nio.opentelemetry.level = OFF\n</code></pre> <pre><code>## Turn off logging for just the BatchSpanProcessor\nio.opentelemetry.sdk.trace.export.BatchSpanProcessor.level = OFF\n</code></pre> <pre><code>## Log \"FINE\" messages for help in debugging\nio.opentelemetry.level = FINE\n## Sets the default ConsoleHandler's logger's level\n## Note this impacts the logging outside of OpenTelemetry as well\njava.util.logging.ConsoleHandler.level = FINE\n</code></pre> <p>For more fine-grained control and special case handling, custom handlers and filters can be specified with code.</p> <pre><code>// Custom filter which does not log errors that come from the export\npublic class IgnoreExportErrorsFilter implements Filter {\npublic boolean isLoggable(LogRecord record) {\nreturn !record.getMessage().contains(\"Exception thrown by the export\");\n}\n}\n</code></pre> <pre><code>## Registering the custom filter on the BatchSpanProcessor\nio.opentelemetry.sdk.trace.export.BatchSpanProcessor = io.opentelemetry.extension.logging.IgnoreExportErrorsFilter\n</code></pre>"},{"location":"docs/instrumentation/java/automatic/","title":"Automatic Instrumentation","text":"<p>Automatic instrumentation with Java uses a Java agent JAR that can be attached to any Java 8+ application. It dynamically injects bytecode to capture telemetry from many popular libraries and frameworks. It can be used to capture telemetry data at the \"edges\" of an app or service, such as inbound requests, outbound HTTP calls, database calls, and so on. To learn how to manually instrument your service or app code, see Manual instrumentation.</p>"},{"location":"docs/instrumentation/java/automatic/#setup","title":"Setup","text":"<ol> <li>Download opentelemetry-javaagent.jar from Releases of the     <code>opentelemetry-java-instrumentation</code> repo and place the JAR in your     preferred directory. The JAR file contains the agent and instrumentation     libraries.</li> <li> <p>Add <code>-javaagent:path/to/opentelemetry-javaagent.jar</code> and other config to     your JVM startup arguments and launch your app:</p> <ul> <li>Directly on the startup command:</li> </ul> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar -Dotel.service.name=your-service-name -jar myapp.jar\n</code></pre> <ul> <li>Via the <code>JAVA_TOOL_OPTIONS</code> and other environment variables:</li> </ul> <pre><code>export JAVA_TOOL_OPTIONS=\"-javaagent:path/to/opentelemetry-javaagent.jar\"\nexport OTEL_SERVICE_NAME=\"your-service-name\"\njava -jar myapp.jar\n</code></pre> </li> </ol>"},{"location":"docs/instrumentation/java/automatic/#configuring-the-agent","title":"Configuring the agent","text":"<p>The agent is highly configurable.</p> <p>One option is to pass configuration properties via the <code>-D</code> flag. In this example, a service name and Zipkin exporter for traces are configured:</p> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar \\\n-Dotel.service.name=your-service-name \\\n-Dotel.traces.exporter=zipkin \\\n-jar myapp.jar\n</code></pre> <p>You can also use environment variables to configure the agent:</p> <pre><code>OTEL_SERVICE_NAME=your-service-name \\\nOTEL_TRACES_EXPORTER=zipkin \\\njava -javaagent:path/to/opentelemetry-javaagent.jar \\\n-jar myapp.jar\n</code></pre> <p>You can also supply a Java properties file and load configuration values from there:</p> <pre><code>java -javaagent:path/to/opentelemetry-javaagent.jar \\\n-Dotel.javaagent.configuration-file=path/to/properties/file.properties \\\n-jar myapp.jar\n</code></pre> <p>or</p> <pre><code>OTEL_JAVAAGENT_CONFIGURATION_FILE=path/to/properties/file.properties \\\njava -javaagent:path/to/opentelemetry-javaagent.jar \\\n-jar myapp.jar\n</code></pre> <p>To see the full range of configuration options, see Agent Configuration.</p>"},{"location":"docs/instrumentation/java/automatic/#supported-libraries-frameworks-application-services-and-jvms","title":"Supported libraries, frameworks, application services, and JVMs","text":"<p>The Java agent ships with instrumentation libraries for many popular components. For the full list, see Supported libraries, frameworks, application services, and JVMs.</p>"},{"location":"docs/instrumentation/java/automatic/#troubleshooting","title":"Troubleshooting","text":"<p>You can pass the <code>-Dotel.javaagent.debug=true</code> parameter to the agent to see debug logs. Note that these are quite verbose.</p>"},{"location":"docs/instrumentation/java/automatic/#next-steps","title":"Next steps","text":"<p>After you have automatic instrumentation configured for your app or service, you might want to annotate selected methods or add manual instrumentation to collect custom telemetry data.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/","title":"Agent Configuration","text":""},{"location":"docs/instrumentation/java/automatic/agent-config/#sdk-autoconfiguration","title":"SDK Autoconfiguration","text":"<p>The SDK's autoconfiguration module is used for basic configuration of the agent. Read the docs to find settings such as configuring export or sampling.</p> <p>Here are some quick links into those docs for the configuration options for specific portions of the SDK &amp; agent:</p> <ul> <li>Exporters</li> <li>OTLP exporter (both span and metric exporters)</li> <li>Jaeger exporter</li> <li>Zipkin exporter</li> <li>Prometheus exporter</li> <li>Logging exporter</li> <li>Trace context propagation</li> <li>OpenTelemetry Resource and service name</li> <li>Batch span processor</li> <li>Sampler</li> <li>Span limits</li> <li>Using SPI to further configure the SDK</li> </ul>"},{"location":"docs/instrumentation/java/automatic/agent-config/#configuring-the-agent","title":"Configuring the agent","text":"<p>The agent can consume configuration from one or more of the following sources (ordered from highest to lowest priority):</p> <ul> <li>system properties</li> <li>environment variables</li> <li>the configuration file</li> <li>properties provided by the   <code>AutoConfigurationCustomizer#addPropertiesSupplier()</code>   function; using the   <code>AutoConfigurationCustomizerProvider</code>   SPI</li> </ul>"},{"location":"docs/instrumentation/java/automatic/agent-config/#configuring-with-environment-variables","title":"Configuring with Environment Variables","text":"<p>In some environments, configuring via Environment Variables is more preferred. Any setting configurable with a System Property can also be configured with an Environment Variable. Many settings below include both options, but where they don't apply the following steps to determine the correct name mapping of the desired System Property:</p> <ul> <li>Convert the System Property to uppercase.</li> <li>Replace all <code>.</code> and <code>-</code> characters with <code>_</code>.</li> </ul> <p>For example <code>otel.instrumentation.common.default-enabled</code> would convert to <code>OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED</code>.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#configuration-file","title":"Configuration file","text":"<p>You can provide a path to agent configuration file by setting the following property:</p> <p>{{% config_option name=\"otel.javaagent.configuration-file\" %}} Path to valid Java properties file which contains the agent configuration. {{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#extensions","title":"Extensions","text":"<p>You can enable extensions by setting the following property:</p> <p>{{% config_option name=\"otel.javaagent.extensions\" %}}</p> <p>Path to an extension jar file or folder, containing jar files. If pointing to a folder, every jar file in that folder will be treated as separate, independent extension.</p> <p>{{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#javaagent-logging-output","title":"Javaagent logging output","text":"<p>The agent's logging output can be configured by setting the following property:</p> <p>{{% config_option name=\"otel.javaagent.logging\" %}}</p> <p>The javaagent logging mode. The following 3 modes are supported:</p> <ul> <li><code>simple</code>: The agent will print out its logs using the standard error stream.   Only <code>INFO</code> or higher logs will be printed. This is the default javaagent   logging mode.</li> <li><code>none</code>: The agent will not log anything - not even its own version.</li> <li><code>application</code>: The agent will attempt to redirect its own logs to the   instrumented application's slf4j logger. This works the best for simple   one-jar applications that do not use multiple classloaders; Spring Boot apps   are supported as well. The javaagent output logs can be further configured   using the instrumented application's logging configuration (e.g. <code>logback.xml</code>   or <code>log4j2.xml</code>). Make sure to test that this mode works for your   application before running it in a production environment.</li> </ul> <p>{{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#common-instrumentation-configuration","title":"Common instrumentation configuration","text":"<p>Common settings that apply to multiple instrumentations at once.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#peer-service-name","title":"Peer service name","text":"<p>The peer service name is the name of a remote service to which a connection is made. It corresponds to <code>service.name</code> in the resource for the local service.</p> <p>{{% config_option name=\"otel.instrumentation.common.peer-service-mapping\" %}}</p> <p>Used to specify a mapping from host names or IP addresses to peer services, as a comma-separated list of <code>&lt;host_or_ip&gt;=&lt;user_assigned_name&gt;</code> pairs. The peer service is added as an attribute to a span whose host or IP address match the mapping.</p> <p>For example, if set to the following:</p> <pre><code>1.2.3.4=cats-service,dogs-abcdef123.serverlessapis.com=dogs-api\n</code></pre> <p>Then, requests to <code>1.2.3.4</code> will have a <code>peer.service</code> attribute of <code>cats-service</code> and requests to <code>dogs-abcdef123.serverlessapis.com</code> will have an attribute of <code>dogs-api</code>.</p> <p>{{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#db-statement-sanitization","title":"DB statement sanitization","text":"<p>The agent sanitizes all database queries/statements before setting the <code>db.statement</code> semantic attribute. All values (strings, numbers) in the query string are replaced with a question mark (<code>?</code>).</p> <p>Note: JDBC bind parameters are not captured in <code>db.statement</code>. See the corresponding issue if you are looking to capture bind parameters.</p> <p>Examples:</p> <ul> <li>SQL query <code>SELECT a from b where password=\"secret\"</code> will appear as   <code>SELECT a from b where password=?</code> in the exported span;</li> <li>Redis command <code>HSET map password \"secret\"</code> will appear as   <code>HSET map password ?</code> in the exported span.</li> </ul> <p>This behavior is turned on by default for all database instrumentations. Use the following property to disable it:</p> <p>{{% config_option   name=\"otel.instrumentation.common.db-statement-sanitizer.enabled\"   default=true %}} Enables the DB statement sanitization. {{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#capturing-http-request-and-response-headers","title":"Capturing HTTP request and response headers","text":"<p>You can configure the agent to capture predefined HTTP headers as span attributes, according to the semantic convention. Use the following properties to define which HTTP headers you want to capture:</p> <p>{{% config_option name=\"otel.instrumentation.http.capture-headers.client.request\" %}} A comma-separated list of HTTP header names. HTTP client instrumentations will capture HTTP request header values for all configured header names. {{% /config_option %}}</p> <p>{{% config_option name=\"otel.instrumentation.http.capture-headers.client.response\" %}} A comma-separated list of HTTP header names. HTTP client instrumentations will capture HTTP response header values for all configured header names. {{% /config_option %}}</p> <p>{{% config_option name=\"otel.instrumentation.http.capture-headers.server.request\" %}} A comma-separated list of HTTP header names. HTTP server instrumentations will capture HTTP request header values for all configured header names. {{% /config_option %}}</p> <p>{{% config_option name=\"otel.instrumentation.http.capture-headers.server.response\" %}} A comma-separated list of HTTP header names. HTTP server instrumentations will capture HTTP response header values for all configured header names. {{% /config_option %}}</p> <p>These configuration options are supported by all HTTP client and server instrumentations.</p> <p>Note: The property/environment variable names listed in the table are still experimental, and thus are subject to change.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#capturing-servlet-request-parameters","title":"Capturing servlet request parameters","text":"<p>You can configure the agent to capture predefined HTTP request parameter as span attributes for requests that are handled by Servlet API. Use the following property to define which servlet request parameters you want to capture:</p> <p>{{% config_option name=\"otel.instrumentation.servlet.experimental.capture-request-parameters\" %}} A comma-separated list of request parameter names. {{% /config_option %}}</p> <p>Note: The property/environment variable names listed in the table are still experimental, and thus are subject to change.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#capturing-consumer-message-receive-telemetry-in-messaging-instrumentations","title":"Capturing consumer message receive telemetry in messaging instrumentations","text":"<p>You can configure the agent to capture the consumer message receive telemetry in messaging instrumentation. Use the following property to enable it:</p> <p>{{% config_option   name=\"otel.instrumentation.messaging.experimental.receive-telemetry.enabled\"   default=false %}} Enables the consumer message receive telemetry. {{% /config_option %}}</p> <p>Note that this will cause the consumer side to start a new trace, with only a span link connecting it to the producer trace.</p> <p>Note: The property/environment variable names listed in the table are still experimental, and thus are subject to change.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#suppressing-specific-auto-instrumentation","title":"Suppressing specific auto-instrumentation","text":""},{"location":"docs/instrumentation/java/automatic/agent-config/#disabling-the-agent-entirely","title":"Disabling the agent entirely","text":"<p>You can disable the agent using <code>-Dotel.javaagent.enabled=false</code> (or using the equivalent environment variable <code>OTEL_JAVAAGENT_ENABLED=false</code>).</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#enable-only-specific-instrumentation","title":"Enable only specific instrumentation","text":"<p>You can disable all default auto instrumentation and selectively re-enable individual instrumentation. This may be desirable to reduce startup overhead or to have more control of which instrumentation is applied.</p> <ul> <li>Disable all instrumentation in the agent using   <code>-Dotel.instrumentation.common.default-enabled=false</code> (or using the equivalent   environment variable <code>OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false</code>).</li> <li>Enable each desired instrumentation individually using   <code>-Dotel.instrumentation.[name].enabled=true</code> (or using the equivalent   environment variable <code>OTEL_INSTRUMENTATION_[NAME]_ENABLED</code>) where <code>[name]</code>   (<code>[NAME]</code>) is the corresponding instrumentation <code>name</code> below.</li> </ul> <p>Note: Some instrumentation relies on other instrumentation to function properly. When selectively enabling instrumentation, be sure to enable the transitive dependencies too. Determining this dependency relationship is left as an exercise to the user.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#enable-manual-instrumentation-only","title":"Enable manual instrumentation only","text":"<p>You can suppress all auto instrumentations but have support for manual instrumentation with <code>@WithSpan</code> and normal API interactions by using <code>-Dotel.instrumentation.common.default-enabled=false -Dotel.instrumentation.opentelemetry-api.enabled=true -Dotel.instrumentation.opentelemetry-instrumentation-annotations.enabled=true</code></p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#suppressing-specific-agent-instrumentation","title":"Suppressing specific agent instrumentation","text":"<p>You can suppress agent instrumentation of specific libraries by using <code>-Dotel.instrumentation.[name].enabled=false</code> (or using the equivalent environment variable <code>OTEL_INSTRUMENTATION_[NAME]_ENABLED</code>) where <code>name</code> (<code>NAME</code>) is the corresponding instrumentation <code>name</code>:</p> Library/Framework Instrumentation name Additional methods tracing <code>methods</code> Additional tracing annotations <code>external-annotations</code> Akka Actor <code>akka-actor</code> Akka HTTP <code>akka-http</code> Apache Axis2 <code>axis2</code> Apache Camel <code>camel</code> Apache Cassandra <code>cassandra</code> Apache CXF <code>cxf</code> Apache DBCP <code>apache-dbcp</code> Apache Dubbo <code>apache-dubbo</code> Apache Geode <code>geode</code> Apache HttpAsyncClient <code>apache-httpasyncclient</code> Apache HttpClient <code>apache-httpclient</code> Apache Kafka <code>kafka</code> Apache MyFaces <code>jsf-myfaces</code> Apache Pulsar <code>pulsar</code> Apache RocketMQ <code>rocketmq-client</code> Apache Struts 2 <code>struts</code> Apache Tapestry <code>tapestry</code> Apache Tomcat <code>tomcat</code> Apache Wicket <code>wicket</code> Armeria <code>armeria</code> AsyncHttpClient (AHC) <code>async-http-client</code> AWS Lambda <code>aws-lambda</code> AWS SDK <code>aws-sdk</code> Azure SDK <code>azure-core</code> Couchbase <code>couchbase</code> C3P0 <code>c3p0</code> Dropwizard Views <code>dropwizard-views</code> Dropwizard Metrics <code>dropwizard-metrics</code> Eclipse Grizzly <code>grizzly</code> Eclipse Jersey <code>jersey</code> Eclipse Jetty <code>jetty</code> Eclipse Jetty HTTP Client <code>jetty-httpclient</code> Eclipse Metro <code>metro</code> Eclipse Mojarra <code>jsf-mojarra</code> Eclipse Vert.x HttpClient <code>vertx-http-client</code> Eclipse Vert.x Kafka Client <code>vertx-kafka-client</code> Eclipse Vert.x RxJava <code>vertx-rx-java</code> Eclipse Vert.x Web <code>vertx-web</code> Elasticsearch client <code>elasticsearch-transport</code> Elasticsearch REST client <code>elasticsearch-rest</code> Google Guava <code>guava</code> Google HTTP client <code>google-http-client</code> Google Web Toolkit <code>gwt</code> Grails <code>grails</code> GraphQL Java <code>graphql-java</code> GRPC <code>grpc</code> Hibernate <code>hibernate</code> HikariCP <code>hikaricp</code> Java HTTP Client <code>java-http-client</code> Java <code>HttpURLConnection</code> <code>http-url-connection</code> Java JDBC <code>jdbc</code> Java JDBC <code>DataSource</code> <code>jdbc-datasource</code> Java RMI <code>rmi</code> Java Runtime <code>runtime-metrics</code> Java Servlet <code>servlet</code> java.util.concurrent <code>executors</code> java.util.logging <code>java-util-logging</code> JAX-RS (Client) <code>jaxrs-client</code> JAX-RS (Server) <code>jaxrs</code> JAX-WS <code>jaxws</code> JBoss Logging Appender <code>jboss-logmanager-appender</code> JBoss Logging MDC <code>jboss-logmanager-mdc</code> JMS <code>jms</code> Jodd HTTP <code>jodd-http</code> JSP <code>jsp</code> K8s Client <code>kubernetes-client</code> kotlinx.coroutines <code>kotlinx-coroutines</code> Log4j Appender <code>log4j-appender</code> Log4j MDC (1.x) <code>log4j-mdc</code> Log4j Context Data (2.x) <code>log4j-context-data</code> Logback Appender <code>logback-appender</code> Logback MDC <code>logback-mdc</code> Micrometer <code>micrometer</code> MongoDB <code>mongo</code> Netflix Hystrix <code>hystrix</code> Netty <code>netty</code> OkHttp <code>okhttp</code> OpenLiberty <code>liberty</code> OpenTelemetry Extension Annotations <code>opentelemetry-extension-annotations</code> OpenTelemetry Instrumentation Annotations <code>opentelemetry-instrumentation-annotations</code> OpenTelemetry API <code>opentelemetry-api</code> Oracle UCP <code>oracle-ucp</code> OSHI (Operating System and Hardware Information) <code>oshi</code> Play Framework <code>play</code> Play WS HTTP Client <code>play-ws</code> Quartz <code>quartz</code> R2DBC <code>r2dbc</code> RabbitMQ Client <code>rabbitmq</code> Ratpack <code>ratpack</code> ReactiveX RxJava <code>rxjava</code> Reactor <code>reactor</code> Reactor Netty <code>reactor-netty</code> Redis Jedis <code>jedis</code> Redis Lettuce <code>lettuce</code> Rediscala <code>rediscala</code> Redisson <code>redisson</code> Restlet <code>restlet</code> Scala ForkJoinPool <code>scala-fork-join</code> Spark Web Framework <code>spark</code> Spring Batch <code>spring-batch</code> Spring Core <code>spring-core</code> Spring Data <code>spring-data</code> Spring JMS <code>spring-jms</code> Spring Integration <code>spring-integration</code> Spring Kafka <code>spring-kafka</code> Spring RabbitMQ <code>spring-rabbit</code> Spring RMI <code>spring-rmi</code> Spring Scheduling <code>spring-scheduling</code> Spring Web <code>spring-web</code> Spring WebFlux <code>spring-webflux</code> Spring Web MVC <code>spring-webmvc</code> Spring Web Services <code>spring-ws</code> Spymemcached <code>spymemcached</code> Tomcat JDBC <code>tomcat-jdbc</code> Twilio SDK <code>twilio</code> Twitter Finatra <code>finatra</code> Undertow <code>undertow</code> Vaadin <code>vaadin</code> Vibur DBCP <code>vibur-dbcp</code> ZIO <code>zio</code> <p>Note: When using environment variables, dashes (<code>-</code>) should be converted to underscores (<code>_</code>). For example, to suppress traces from <code>akka-actor</code> library, set <code>OTEL_INSTRUMENTATION_AKKA_ACTOR_ENABLED</code> to <code>false</code>.</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#suppressing-controller-andor-view-spans","title":"Suppressing controller and/or view spans","text":"<p>Some instrumentations (e.g. Spring Web MVC instrumentation) produce SpanKind.Internal spans to capture the controller and/or view execution. These spans can be suppressed using the configuration settings below, without suppressing the entire instrumentation which would also disable the instrumentation's capturing of <code>http.route</code> and associated span name on the parent SpanKind.Server span.</p> <p>{{% config_option   name=\"otel.instrumentation.common.experimental.controller-telemetry.enabled\"   default=true %}} Enables the controller telemetry. {{% /config_option %}}</p> <p>{{% config_option   name=\"otel.instrumentation.common.experimental.view-telemetry.enabled\"   default=true %}} Enables the view telemetry. {{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/agent-config/#instrumentation-span-suppression-behavior","title":"Instrumentation span suppression behavior","text":"<p>Some libraries that this agent instruments in turn use lower-level libraries, that are also instrumented. This would normally result in nested spans containing duplicate telemetry data. For example:</p> <ul> <li>Spans produced by the Reactor Netty HTTP client instrumentation would have   duplicate HTTP client spans produced by the Netty instrumentation;</li> <li>Dynamo DB spans produced by the AWS SDK instrumentation would have children   HTTP client spans produced by its internal HTTP client library (which is also   instrumented);</li> <li>Spans produced by the Tomcat instrumentation would have duplicate HTTP server   spans produced by the generic Servlet API instrumentation.</li> </ul> <p>The javaagent prevents these situations by detecting and suppressing nested spans that duplicate telemetry data. The suppression behavior can be configured using the following configuration option:</p> <p>{{% config_option name=\"otel.instrumentation.experimental.span-suppression-strategy\" %}}</p> <p>The javaagent span suppression strategy. The following 3 strategies are supported:</p> <ul> <li><code>semconv</code>: The agent will suppress duplicate semantic conventions. This is the   default behavior of the javaagent.</li> <li><code>span-kind</code>: The agent will suppress spans with the same kind (except   <code>INTERNAL</code>).</li> <li><code>none</code>: The agent will not suppress anything at all. We do not recommend   using this option for anything other than debug purposes, as it generates lots   of duplicate telemetry data.</li> </ul> <p>{{% /config_option %}}</p> <p>For example, suppose we instrument a database client which internally uses the Reactor Netty HTTP client; which in turn uses Netty.</p> <p>Using the default <code>semconv</code> suppression strategy would result in 2 nested <code>CLIENT</code> spans:</p> <ul> <li><code>CLIENT</code> span with database client semantic attributes emitted by the database   client instrumentation;</li> <li><code>CLIENT</code> span with HTTP client semantic attributes emitted by the Reactor   Netty instrumentation.</li> </ul> <p>The Netty instrumentation would be suppressed, as it duplicates the Reactor Netty HTTP client instrumentation.</p> <p>Using the suppression strategy <code>span-kind</code> would result in just one span:</p> <ul> <li><code>CLIENT</code> span with database client semantic attributes emitted by the database   client instrumentation.</li> </ul> <p>Both Reactor Netty and Netty instrumentations would be suppressed, as they also emit <code>CLIENT</code> spans.</p> <p>Finally, using the suppression strategy <code>none</code> would result in 3 spans:</p> <ul> <li><code>CLIENT</code> span with database client semantic attributes emitted by the database   client instrumentation;</li> <li><code>CLIENT</code> span with HTTP client semantic attributes emitted by the Reactor   Netty instrumentation;</li> <li><code>CLIENT</code> span with HTTP client semantic attributes emitted by the Netty   instrumentation.</li> </ul>"},{"location":"docs/instrumentation/java/automatic/annotations/","title":"Annotations","text":"<p>For most users, the out-of-the-box instrumentation is completely sufficient and nothing more has to be done. Sometimes, however, users wish to create spans for their own custom code without doing too much code change.</p>"},{"location":"docs/instrumentation/java/automatic/annotations/#dependencies","title":"Dependencies","text":"<p>You'll need to add a dependency on the <code>opentelemetry-instrumentation-annotations</code> library to use the <code>@WithSpan</code> annotation.</p>"},{"location":"docs/instrumentation/java/automatic/annotations/#maven","title":"Maven","text":"<pre><code>&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;io.opentelemetry.instrumentation&lt;/groupId&gt;\n&lt;artifactId&gt;opentelemetry-instrumentation-annotations&lt;/artifactId&gt;\n&lt;version&gt;{{% param javaInstrumentationVersion %}}&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre>"},{"location":"docs/instrumentation/java/automatic/annotations/#gradle","title":"Gradle","text":"<pre><code>dependencies {\nimplementation('io.opentelemetry.instrumentation:opentelemetry-instrumentation-annotations:{{% param javaInstrumentationVersion %}}')\n}\n</code></pre>"},{"location":"docs/instrumentation/java/automatic/annotations/#creating-spans-around-methods-with-withspan","title":"Creating spans around methods with <code>@WithSpan</code>","text":"<p>To create a span corresponding to one of your method, annotate the method with <code>@WithSpan</code>.</p> <pre><code>import io.opentelemetry.instrumentation.annotations.WithSpan;\npublic class MyClass {\n@WithSpan\npublic void myMethod() {\n&lt;...&gt;\n}\n}\n</code></pre> <p>Each time the application invokes the annotated method, it creates a span that denotes its duration and provides any thrown exceptions. By default, the span name will be <code>&lt;className&gt;.&lt;methodName&gt;</code>, unless a name is provided as an argument to the annotation.</p> <p>If the return type of the method annotated by <code>@WithSpan</code> is one of the future- or promise-like types listed below, then the span will not be ended until the future completes.</p> <ul> <li>java.util.concurrent.CompletableFuture</li> <li>java.util.concurrent.CompletionStage</li> <li>com.google.common.util.concurrent.ListenableFuture</li> <li>org.reactivestreams.Publisher</li> <li>reactor.core.publisher.Mono</li> <li>reactor.core.publisher.Flux</li> <li>io.reactivex.Completable</li> <li>io.reactivex.Maybe</li> <li>io.reactivex.Single</li> <li>io.reactivex.Observable</li> <li>io.reactivex.Flowable</li> <li>io.reactivex.parallel.ParallelFlowable</li> </ul>"},{"location":"docs/instrumentation/java/automatic/annotations/#adding-attributes-to-the-span-with-spanattribute","title":"Adding attributes to the span with <code>@SpanAttribute</code>","text":"<p>When a span is created for an annotated method the values of the arguments to the method invocation can be automatically added as attributes to the created span by annotating the method parameters with the <code>@SpanAttribute</code> annotation.</p> <pre><code>import io.opentelemetry.instrumentation.annotations.SpanAttribute;\nimport io.opentelemetry.instrumentation.annotations.WithSpan;\npublic class MyClass {\n@WithSpan\npublic void myMethod(@SpanAttribute(\"parameter1\") String parameter1,\n@SpanAttribute(\"parameter2\") long parameter2) {\n&lt;...&gt;\n}\n}\n</code></pre> <p>Unless specified as an argument to the annotation, the attribute name will be derived from the formal parameter names if they are compiled into the <code>.class</code> files by passing the <code>-parameters</code> option to the <code>javac</code> compiler.</p>"},{"location":"docs/instrumentation/java/automatic/annotations/#suppressing-withspan-instrumentation","title":"Suppressing <code>@WithSpan</code> instrumentation","text":"<p>Suppressing <code>@WithSpan</code> is useful if you have code that is over-instrumented using <code>@WithSpan</code> and you want to suppress some of them without modifying the code.</p> <p>{{% config_option   name=\"otel.instrumentation.opentelemetry-instrumentation-annotations.exclude-methods\" %}} Suppress <code>@WithSpan</code> instrumentation for specific methods. Format is <code>my.package.MyClass1[method1,method2];my.package.MyClass2[method3]</code>. {{% /config_option %}}</p>"},{"location":"docs/instrumentation/java/automatic/annotations/#creating-spans-around-methods-with-otelinstrumentationmethodsinclude","title":"Creating spans around methods with <code>otel.instrumentation.methods.include</code>","text":"<p>In cases where you are unable to modify the code, you can still configure the javaagent to capture spans around specific methods.</p> <p>{{% config_option name=\"otel.instrumentation.methods.include\" %}} Add instrumentation for specific methods in lieu of <code>@WithSpan</code>. Format is <code>my.package.MyClass1[method1,method2];my.package.MyClass2[method3]</code>. {{% /config_option %}}</p> <p>If a method is overloaded (appears more than once on the same class with the same name but different parameters), all versions of the method will be instrumented.</p>"},{"location":"docs/instrumentation/java/automatic/annotations/#next-steps","title":"Next steps","text":"<p>Beyond the use of annotations, the OpenTelemetry API allows you to obtain a tracer that can be used for Manual Instrumentation and execute code within the scope of that span.</p>"},{"location":"docs/instrumentation/java/automatic/extensions/","title":"Extensions","text":""},{"location":"docs/instrumentation/java/automatic/extensions/#introduction","title":"Introduction","text":"<p>Extensions are designed to override or customize the instrumentation provided by the upstream agent without having to create a new OpenTelemetry distribution or alter the agent code in any way.</p> <p>Consider an instrumented database client that creates a span per database call and extracts data from the database connection to provide span attributes. The following are sample use cases for that scenario that can be solved by using extensions:</p> <ul> <li>\"I don't want this span at all\":</li> </ul> <p>Create an extension to disable selected instrumentation by providing new   default settings.</p> <ul> <li>\"I want to edit some attributes that don't depend on any db connection   instance\":</li> </ul> <p>Create an extension that provide a custom <code>SpanProcessor</code>.</p> <ul> <li>\"I want to edit some attributes and their values depend on a specific db   connection instance\":</li> </ul> <p>Create an extension with new instrumentation which injects its own advice into   the same method as the original one. You can use the <code>order</code> method to ensure   it runs after the original instrumentation and augment the current span with   new information.</p> <ul> <li>\"I want to remove some attributes\":</li> </ul> <p>Create an extension with a custom exporter or use the attribute filtering   functionality in the OpenTelemetry Collector.</p> <ul> <li>\"I don't like the OTel spans. I want to modify them and their lifecycle\":</li> </ul> <p>Create an extension that disables existing instrumentation and replace it with   new one that injects <code>Advice</code> into the same (or a better) method as the   original instrumentation. You can write your <code>Advice</code> for this and use the   existing <code>Tracer</code> directly or extend it. As you have your own <code>Advice</code>, you   can control which <code>Tracer</code> you use.</p>"},{"location":"docs/instrumentation/java/automatic/extensions/#extension-examples","title":"Extension examples","text":"<p>To get a demonstration how to create an extension for the OpenTelemetry Java instrumentation agent, build and run the extension project.</p>"},{"location":"docs/instrumentation/js/","title":"JavaScript","text":"<p>{{% lang_instrumentation_index_head js /%}}</p>"},{"location":"docs/instrumentation/js/#further-reading","title":"Further Reading","text":"<ul> <li>OpenTelemetry for JavaScript on GitHub</li> <li>\u5165\u95e8</li> <li>SDK\u548cAPI\u53c2\u8003</li> </ul>"},{"location":"docs/instrumentation/js/context/","title":"\u4e0a\u4e0b\u6587","text":"<p>\u521b\u5efa\u5e76\u542f\u52a8HTTP\u670d\u52a1\u5668\u4e3a\u4e86\u4f7fOpenTelemetry\u5de5\u4f5c\uff0c\u5b83\u5fc5\u987b\u5b58\u50a8\u548c\u4f20\u64ad\u91cd\u8981\u7684\u9065\u6d4b\u6570\u636e\u3002 \u4f8b\u5982\uff0c\u5f53\u63a5\u6536\u5230\u8bf7\u6c42\u5e76\u542f\u52a8span\u65f6\uff0c\u5b83\u5fc5\u987b\u5bf9\u521b\u5efa\u5b50span\u7684\u7ec4\u4ef6\u53ef\u7528\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cOpenTelemetry\u5c06\u8de8\u5ea6\u5b58\u50a8\u5728Context\u4e2d\u3002 \u672c\u6587\u6863\u63cf\u8ff0\u4e86JavaScript\u7684OpenTelemetry\u4e0a\u4e0b\u6587API\u4ee5\u53ca\u5982\u4f55\u4f7f\u7528\u5b83\u3002</p> <p>More information:</p> <ul> <li>Context specification</li> <li>Context API reference</li> </ul>"},{"location":"docs/instrumentation/js/context/#context-manager","title":"Context Manager","text":"<p>The context API depends on a context manager to work. The examples in this document will assume you have already configured a context manager. Typically the context manager is provided by your SDK, however it is possible to register one directly like this:</p> <pre><code>import * as api from '@opentelemetry/api';\nimport { AsyncHooksContextManager } from '@opentelemetry/context-async-hooks';\nconst contextManager = new AsyncHooksContextManager();\ncontextManager.enable();\napi.context.setGlobalContextManager(contextManager);\n</code></pre>"},{"location":"docs/instrumentation/js/context/#root-context","title":"Root Context","text":"<p>The <code>ROOT_CONTEXT</code> is the empty context. If no context is active, the <code>ROOT_CONTEXT</code> is active. Active context is explained below Active Context.</p>"},{"location":"docs/instrumentation/js/context/#context-keys","title":"Context Keys","text":"<p>Context entries are key-value pairs. Keys can be created by calling <code>api.createContextKey(description)</code>.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key1 = api.createContextKey('My first key');\nconst key2 = api.createContextKey('My second key');\n</code></pre>"},{"location":"docs/instrumentation/js/context/#basic-operations","title":"Basic Operations","text":""},{"location":"docs/instrumentation/js/context/#get-entry","title":"Get Entry","text":"<p>Entries are accessed using the <code>context.getValue(key)</code> method.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('some key');\n// ROOT_CONTEXT is the empty context\nconst ctx = api.ROOT_CONTEXT;\nconst value = ctx.getValue(key);\n</code></pre>"},{"location":"docs/instrumentation/js/context/#set-entry","title":"Set Entry","text":"<p>Entries are created by using the <code>context.setValue(key, value)</code> method. Setting a context entry creates a new context with all the entries of the previous context, but with the new entry. Setting a context entry does not modify the previous context.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('some key');\nconst ctx = api.ROOT_CONTEXT;\n// add a new entry\nconst ctx2 = ctx.setValue(key, 'context 2');\n// ctx2 contains the new entry\nconsole.log(ctx2.getValue(key)); // \"context 2\"\n// ctx is unchanged\nconsole.log(ctx.getValue(key)); // undefined\n</code></pre>"},{"location":"docs/instrumentation/js/context/#delete-entry","title":"Delete Entry","text":"<p>Entries are removed by calling <code>context.deleteValue(key)</code>. Deleting a context entry creates a new context with all the entries of the previous context, but without the entry identified by the key. Deleting a context entry does not modify the previous context.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('some key');\nconst ctx = api.ROOT_CONTEXT;\nconst ctx2 = ctx.setValue(key, 'context 2');\n// remove the entry\nconst ctx3 = ctx.deleteValue(key);\n// ctx3 does not contain the entry\nconsole.log(ctx3.getValue(key)); // undefined\n// ctx2 is unchanged\nconsole.log(ctx2.getValue(key)); // \"context 2\"\n// ctx is unchanged\nconsole.log(ctx.getValue(key)); // undefined\n</code></pre>"},{"location":"docs/instrumentation/js/context/#active-context","title":"Active Context","text":"<p>IMPORTANT: This assumes you have configured a Context Manager. Without one, <code>api.context.active()</code> will ALWAYS return the <code>ROOT_CONTEXT</code>.</p> <p>The active context is the context which is returned by <code>api.context.active()</code>. The context object contains entries which allow tracing components which are tracing a single thread of execution to communicate with each other and ensure the trace is successfully created. For example, when a span is created it may be added to the context. Later, when another span is created it may use the span from the context as its parent span. This is accomplished through the use of mechanisms like async_hooks or AsyncLocalStorage in node, or zone.js on the web in order to propagate the context through a single execution. If no context is active, the <code>ROOT_CONTEXT</code> is returned, which is just the empty context object.</p>"},{"location":"docs/instrumentation/js/context/#get-active-context","title":"Get Active Context","text":"<p>The active context is the context which is returned by <code>api.context.active()</code>.</p> <pre><code>import * as api from '@opentelemetry/api';\n// Returns the active context\n// If no context is active, the ROOT_CONTEXT is returned\nconst ctx = api.context.active();\n</code></pre>"},{"location":"docs/instrumentation/js/context/#set-active-context","title":"Set Active Context","text":"<p>A context can be made active by use of <code>api.context.with(ctx, callback)</code>. During execution of the <code>callback</code>, the context passed to <code>with</code> will be returned by <code>context.active</code>.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('Key to store a value');\nconst ctx = api.context.active();\napi.context.with(ctx.setValue(key, 'context 2'), async () =&gt; {\n// \"context 2\" is active\nconsole.log(api.context.active().getValue(key)); // \"context 2\"\n});\n</code></pre> <p>The return value of <code>api.context.with(context, callback)</code> is the return value of the callback. The callback is always called synchronously.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst name = await api.context.with(api.context.active(), async () =&gt; {\nconst row = await db.getSomeValue();\nreturn row['name'];\n});\nconsole.log(name); // name returned by the db\n</code></pre> <p>Active context executions may be nested.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('Key to store a value');\nconst ctx = api.context.active();\n// No context is active\nconsole.log(api.context.active().getValue(key)); // undefined\napi.context.with(ctx.setValue(key, 'context 2'), () =&gt; {\n// \"context 2\" is active\nconsole.log(api.context.active().getValue(key)); // \"context 2\"\napi.context.with(ctx.setValue(key, 'context 3'), () =&gt; {\n// \"context 3\" is active\nconsole.log(api.context.active().getValue(key)); // \"context 3\"\n});\n// \"context 2\" is active\nconsole.log(api.context.active().getValue(key)); // \"context 2\"\n});\n// No context is active\nconsole.log(api.context.active().getValue(key)); // undefined\n</code></pre>"},{"location":"docs/instrumentation/js/context/#example","title":"Example","text":"<p>This more complex example illustrates how the context is not modified, but new context objects are created.</p> <pre><code>import * as api from '@opentelemetry/api';\nconst key = api.createContextKey('Key to store a value');\nconst ctx = api.context.active(); // Returns ROOT_CONTEXT when no context is active\nconst ctx2 = ctx.setValue(key, 'context 2'); // does not modify ctx\nconsole.log(ctx.getValue(key)); //? undefined\nconsole.log(ctx2.getValue(key)); //? \"context 2\"\nconst ret = api.context.with(ctx2, () =&gt; {\nconst ctx3 = api.context.active().setValue(key, 'context 3');\nconsole.log(api.context.active().getValue(key)); //? \"context 2\"\nconsole.log(ctx.getValue(key)); //? undefined\nconsole.log(ctx2.getValue(key)); //? \"context 2\"\nconsole.log(ctx3.getValue(key)); //? \"context 3\"\napi.context.with(ctx3, () =&gt; {\nconsole.log(api.context.active().getValue(key)); //? \"context 3\"\n});\nconsole.log(api.context.active().getValue(key)); //? \"context 2\"\nreturn 'return value';\n});\n// The value returned by the callback is returned to the caller\nconsole.log(ret); //? \"return value\"\n</code></pre>"},{"location":"docs/instrumentation/js/examples/","title":"\u793a\u4f8b","text":"<p>\u4e0b\u9762\u662f\u4e00\u4e9bOpenTelemetry\u4eea\u5668\u793a\u4f8b\u7684\u8d44\u6e90\u3002</p>"},{"location":"docs/instrumentation/js/examples/#_1","title":"\u6838\u5fc3\u7684\u4f8b\u5b50","text":"<p>OpenTelemetry\u7684JavaScript\u7248\u672c\u7684\u5b58\u50a8\u5e93\u5305\u542b\u4e00\u4e9b[\u793a\u4f8b][]\u5982\u4f55\u8fd0\u884c\u771f\u5b9e\u7684\u5e94\u7528\u7a0b\u5e8f\u4e0eOpenTelemetry JavaScript\u3002</p>"},{"location":"docs/instrumentation/js/examples/#_2","title":"\u63d2\u4ef6\u548c\u5305\u793a\u4f8b","text":"<p>OpenTelemetry JavaScript\u7684\u8bb8\u591a\u5305\u548c\u63d2\u4ef6\u5728[\u8d21\u732e\u5e93][]\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f7f\u7528\u793a\u4f8b\u3002 \u4f60\u53ef\u4ee5\u5728[examples\u6587\u4ef6\u5939][]\u4e2d\u627e\u5230\u5b83\u4eec\u3002</p>"},{"location":"docs/instrumentation/js/examples/#_3","title":"\u793e\u533a\u8d44\u6e90","text":"<p>nodejs-opentelemetry-tempo\u9879\u76ee\u8bf4\u660e\u4e86OpenTelemetry\u7684\u4f7f\u7528(\u901a\u8fc7\u81ea\u52a8\u548c\u624b\u52a8\u68c0\u6d4b)\uff0c\u6d89\u53ca\u5e26\u6709DB\u4ea4\u4e92\u7684\u5fae\u670d\u52a1\u3002 \u5b83\u4f7f\u7528\u4ee5\u4e0b\u5185\u5bb9:</p> <ul> <li>Prometheus, for monitoring and alerting</li> <li>Loki, for distributed logging</li> <li>Tempo, for distributed tracing</li> <li>Grafana for visualization</li> </ul> <p>\u8981\u4e86\u89e3\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u8bbf\u95ee\u9879\u76ee\u5b58\u50a8\u5e93\u3002</p>"},{"location":"docs/instrumentation/js/exporters/","title":"\u5bfc\u51fa\u5668","text":"<p>\u4e3a\u4e86\u53ef\u89c6\u5316\u548c\u5206\u6790\u60a8\u7684\u75d5\u8ff9\uff0c\u60a8\u9700\u8981\u5c06\u5b83\u4eec\u5bfc\u51fa\u5230\u540e\u7aef\uff0c\u5982Jaeger\u6216Zipkin\u3002 OpenTelemetry JS\u4e3a\u4e00\u4e9b\u5e38\u89c1\u7684\u5f00\u6e90\u540e\u7aef\u63d0\u4f9b\u4e86\u5bfc\u51fa\u5668\u3002</p> <p>\u4e0b\u9762\u60a8\u5c06\u627e\u5230\u4e00\u4e9b\u5173\u4e8e\u5982\u4f55\u8bbe\u7f6e\u540e\u7aef\u548c\u5339\u914d\u7684\u5bfc\u51fa\u5668\u7684\u4ecb\u7ecd\u3002</p>"},{"location":"docs/instrumentation/js/exporters/#otlp-endpoint","title":"OTLP endpoint","text":"<p>\u8981\u5c06\u8ddf\u8e2a\u6570\u636e\u53d1\u9001\u5230OTLP\u7aef\u70b9(\u5982collector\u6216Jaeger)\uff0c\u60a8\u9700\u8981\u4f7f\u7528\u5bfc\u51fa\u5305\uff0c\u4f8b\u5982<code>@opentelemetry/exporter-trace-otlp-proto</code>:</p> <pre><code>npm install --save @opentelemetry/exporter-trace-otlp-proto \\\n@opentelemetry/exporter-metrics-otlp-proto\n</code></pre> <p>Next, configure the exporter to point at an OTLP endpoint. For example you can update <code>instrumentation.ts|js</code> from the Getting Started like the following:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab Typescript &gt;}} /tracing.ts/ import * as opentelemetry from \"opentelemetry/sdk-node\"; import {   getNodeAutoInstrumentations, } from \"opentelemetry/auto-instrumentations-node\"; import {   OTLPTraceExporter, } from \"opentelemetry/exporter-trace-otlp-proto\"; import {   OTLPMetricExporter } from \"opentelemetry/exporter-metrics-otlp-proto\"; import {   PeriodicExportingMetricReader } from \"opentelemetry/sdk-metrics\";</p> <p>const sdk = new opentelemetry.NodeSDK({   traceExporter: new OTLPTraceExporter({     // optional - default url is http://localhost:4318/v1/traces     url: \"/v1/traces\",     // optional - collection of custom headers to be sent with each request, empty by default     headers: {},   }),   metricReader: new PeriodicExportingMetricReader({     exporter: new OTLPMetricExporter({       url: '/v1/metrics', // url is optional and can be omitted - default is http://localhost:4318/v1/metrics       headers: {}, // an optional object containing custom headers to be sent with each request     }),   }),   instrumentations: [getNodeAutoInstrumentations()], }); sdk.start(); {{&lt; /tab &gt;}} <p>{{&lt; tab JavaScript &gt;}} /tracing.js/ const opentelemetry = require(\"opentelemetry/sdk-node\"); const {   getNodeAutoInstrumentations, } = require(\"opentelemetry/auto-instrumentations-node\"); const {   OTLPTraceExporter, } = require(\"opentelemetry/exporter-trace-otlp-proto\"); const {   OTLPMetricExporter } = require(\"opentelemetry/exporter-metrics-otlp-proto\"); const {   PeriodicExportingMetricReader } = require('opentelemetry/sdk-metrics');</p> <p>const sdk = new opentelemetry.NodeSDK({   traceExporter: new OTLPTraceExporter({     // optional - default url is http://localhost:4318/v1/traces     url: \"/v1/traces\",     // optional - collection of custom headers to be sent with each request, empty by default     headers: {},   }),   metricReader: new PeriodicExportingMetricReader({     exporter: new OTLPMetricExporter({       url: '/v1/metrics', // url is optional and can be omitted - default is http://localhost:4318/v1/metrics       headers: {}, // an optional object containing custom headers to be sent with each request       concurrencyLimit: 1, // an optional limit on pending requests     }),   }),   instrumentations: [getNodeAutoInstrumentations()], }); sdk.start(); {{&lt; /tab &gt;}} <p>{{&lt; /tabpane&gt;}}</p> <p>To try out the <code>OTLPTraceExporter</code> quickly, you can run Jaeger in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 4317:4317 \\\n-p 4318:4318 \\\n-p 14250:14250 \\\n-p 14268:14268 \\\n-p 14269:14269 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/js/exporters/#usage-with-the-webtracer","title":"Usage with the WebTracer","text":"<p>When you use the OTLP exporter in a browser-based application, you need to note that:</p> <ol> <li>Using gRPC &amp; http/proto for exporting is not supported</li> <li>Content Security Policies (CSPs) of your website might block your exports</li> <li>Cross-Origin Resource Sharing (CORS) headers might not allow your exports    to be sent</li> <li>You might need to expose your collector to the public internet</li> </ol> <p>Below you will find instructions to use the right exporter, to configure your CSPs and CORS headers and what precautions you have to take when exposing your collector.</p>"},{"location":"docs/instrumentation/js/exporters/#use-otlp-exporter-with-httpjson","title":"Use OTLP exporter with HTTP/JSON","text":"<p>OpenTelemetry Collector Exporter with gRPC and OpenTelemetry Collector Exporter with protobuf do only work with Node.js, therefore you are limited to use the OpenTelemetry Collector Exporter with HTTP.</p> <p>Make sure that the receiving end of your exporter (collector or observability backend) does support <code>http/json</code>, and that you are exporting your data to the right endpoint, i.e., make sure that your port is set to <code>4318</code>.</p>"},{"location":"docs/instrumentation/js/exporters/#configure-csps","title":"Configure CSPs","text":"<p>If your website is making use of Content Security Policies (CSPs), make sure that the domain of your OTLP endpoint is included. If your collector endpoint is <code>https://collector.example.com:4318/v1/traces</code>, add the following directive:</p> <pre><code>connect-src collector.example.com:4318/v1/traces\n</code></pre> <p>If your CSP is not including the OTLP endpoint, you will see an error message, stating that the request to your endpoint is violating the CSP directive.</p>"},{"location":"docs/instrumentation/js/exporters/#configure-cors-headers","title":"Configure CORS headers","text":"<p>If your website and collector are hosted at a different origin, your browser might block the requests going out to your collector. You need to configure special headers for Cross-Origin Resource Sharing (CORS).</p> <p>The OpenTelemetry collector provides a feature for http-based receivers to add the required headers to allow the receiver to accept traces from a web browser:</p> <pre><code>receivers:\notlp:\nprotocols:\nhttp:\ninclude_metadata: true\ncors:\nallowed_origins:\n- https://foo.bar.com\n- https://*.test.com\nallowed_headers:\n- Example-Header\nmax_age: 7200\n</code></pre>"},{"location":"docs/instrumentation/js/exporters/#securely-expose-your-collector","title":"Securely expose your collector","text":"<p>To receive telemetry from a web application you need to allow the browsers of your end-users to send data to your collector. If your web application is accessible from the public internet, you also have to make your collector accessible for everyone.</p> <p>It is recommended that you do not expose your collector directly, but that you put a reverse proxy (NGINX, Apache HTTP Server, ...) in front of it. The reverse proxy can take care of SSL-offloading, setting the right CORS headers, and many other features specific to web applications.</p> <p>Below you will find a configuration for the popular NGINX web server to get you started:</p> <pre><code>server {\nlisten 80 default_server;\nserver_name _;\nlocation / {\n# Take care of preflight requests\nif ($request_method = 'OPTIONS') {\nadd_header 'Access-Control-Max-Age' 1728000;\nadd_header 'Access-Control-Allow-Origin' 'name.of.your.website.example.com' always;\nadd_header 'Access-Control-Allow-Headers' 'Accept,Accept-Language,Content-Language,Content-Type' always;\nadd_header 'Access-Control-Allow-Credentials' 'true' always;\nadd_header 'Content-Type' 'text/plain charset=UTF-8';\nadd_header 'Content-Length' 0;\nreturn 204;\n}\nadd_header 'Access-Control-Allow-Origin' 'name.of.your.website.example.com' always;\nadd_header 'Access-Control-Allow-Credentials' 'true' always;\nadd_header 'Access-Control-Allow-Headers' 'Accept,Accept-Language,Content-Language,Content-Type' always;\nproxy_http_version 1.1;\nproxy_set_header Host $host;\nproxy_set_header X-Real-IP $remote_addr;\nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\nproxy_pass http://collector:4318;\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/js/exporters/#zipkin","title":"Zipkin","text":"<p>To set up Zipkin as quickly as possible, run it in a docker container:</p> <pre><code>docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n</code></pre> <p>Install the exporter package as a dependency for your application:</p> <pre><code>npm install --save @opentelemetry/exporter-zipkin\n</code></pre> <p>Update your OpenTelemetry configuration to use the exporter and to send data to your Zipkin backend:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab Typescript &gt;}} import { ZipkinExporter } from \"opentelemetry/exporter-zipkin\"; import { BatchSpanProcessor } from \"opentelemetry/sdk-trace-base\";</p> <p>provider.addSpanProcessor(new BatchSpanProcessor(new ZipkinExporter())); {{&lt; /tab&gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} const { ZipkinExporter } = require(\"opentelemetry/exporter-zipkin\"); const { BatchSpanProcessor } = require(\"opentelemetry/sdk-trace-base\");</p> <p>provider.addSpanProcessor(new BatchSpanProcessor(new ZipkinExporter())); {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p>"},{"location":"docs/instrumentation/js/libraries/","title":"\u4f7f\u7528\u5de5\u5177\u5e93","text":"<p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u63d2\u88c5\u5e93\u4e3a\u5e93\u6216\u6846\u67b6 \u751f\u6210\u9065\u6d4b\u6570\u636e\u3002</p> <p>\u4f8b\u5982 \uff0c\u7528\u4e8e Express \u7684 instrumentation \u5e93\u5c06 \u6839\u636e\u5165\u7ad9 HTTP \u8bf7\u6c42\u81ea\u52a8\u521b\u5efaspan\u3002</p>"},{"location":"docs/instrumentation/js/libraries/#setup","title":"Setup","text":"<p>\u6bcf\u4e2a\u5de5\u5177\u5e93\u90fd\u662f\u4e00\u4e2a NPM \u5305\uff0c\u5b89\u88c5\u901a\u5e38\u662f\u8fd9\u6837\u5b8c\u6210\u7684:</p> <pre><code>npm install &lt;name-of-package&gt;\n</code></pre> <p>\u5b83\u901a\u5e38\u5728\u5e94\u7528\u7a0b\u5e8f\u542f\u52a8\u65f6\u6ce8\u518c\uff0c\u4f8b\u5982\u5728\u521b \u5efaTracerProvider\u65f6.</p>"},{"location":"docs/instrumentation/js/libraries/#nodejs","title":"Node.js","text":""},{"location":"docs/instrumentation/js/libraries/#node","title":"Node \u81ea\u52a8\u63d2\u88c5\u5305","text":"<p>OpenTelemetry \u8fd8\u5b9a\u4e49 \u4e86auto-instrumentations-node\u5143 \u5305\uff0c\u5b83\u5c06\u6240\u6709\u57fa\u4e8e node .js \u7684\u4eea\u5668\u5e93\u6346\u7ed1\u5230\u4e00\u4e2a\u5305\u4e2d\u3002\u8fd9\u662f\u4e00\u79cd\u65b9\u4fbf\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4e3a\u6240\u6709 \u5e93\u6dfb\u52a0\u81ea\u52a8\u751f\u6210\u7684\u9065\u6d4b\u529f\u80fd\uff0c\u800c\u4e14\u5de5\u4f5c\u91cf\u5f88\u5c0f\u3002</p> <p>\u8981\u4f7f\u7528\u8fd9\u4e2a\u5305\uff0c\u9996\u5148\u5b89\u88c5\u5b83:</p> <pre><code>npm install @opentelemetry/auto-instrumentations-node\n</code></pre> <p>\u7136\u540e\u5728\u4f60\u7684\u8ddf\u8e2a\u521d\u59cb\u5316\u4ee3\u7801\u4e2d\uff0c\u4f7f\u7528<code>registerInstrumentations</code>:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>/* tracing.ts */\n// Import dependencies\nimport { getNodeAutoInstrumentations } from \"@opentelemetry/auto-instrumentations-node\";\nimport opentelemetry from \"@opentelemetry/api\";\nimport { Resource } from \"@opentelemetry/resources\";\nimport { SemanticResourceAttributes } from \"@opentelemetry/semantic-conventions\";\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\nimport { ConsoleSpanExporter, BatchSpanProcessor } from \"@opentelemetry/sdk-trace-base\";\n// This registers all instrumentation packages\nregisterInstrumentations({\ninstrumentations: [\ngetNodeAutoInstrumentations()\n],\n});\nconst resource =\nResource.default().merge(\nnew Resource({\n[SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",\n[SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",\n})\n);\nconst provider = new NodeTracerProvider({\nresource: resource,\n});\nconst exporter = new ConsoleSpanExporter();\nconst processor = new BatchSpanProcessor(exporter);\nprovider.addSpanProcessor(processor);\nprovider.register();\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>/* tracing.js */\n// Require dependencies\nconst { getNodeAutoInstrumentations } = require(\"@opentelemetry/auto-instrumentations-node\");\nconst opentelemetry = require(\"@opentelemetry/api\");\nconst { Resource } = require(\"@opentelemetry/resources\");\nconst { SemanticResourceAttributes } = require(\"@opentelemetry/semantic-conventions\");\nconst { NodeTracerProvider } = require(\"@opentelemetry/sdk-trace-node\");\nconst { registerInstrumentations } = require(\"@opentelemetry/instrumentation\");\nconst { ConsoleSpanExporter, BatchSpanProcessor } = require(\"@opentelemetry/sdk-trace-base\");\n// This registers all instrumentation packages\nregisterInstrumentations({\ninstrumentations: [\ngetNodeAutoInstrumentations()\n],\n});\nconst resource =\nResource.default().merge(\nnew Resource({\n[SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",\n[SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",\n})\n);\nconst provider = new NodeTracerProvider({\nresource: resource,\n});\nconst exporter = new ConsoleSpanExporter();\nconst processor = new BatchSpanProcessor(exporter);\nprovider.addSpanProcessor(processor);\nprovider.register();\n</code></pre></p> <p>{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/libraries/#_1","title":"\u4f7f\u7528\u5355\u72ec\u7684\u63d2\u88c5\u5305","text":"<p>\u5982\u679c\u60a8\u4e0d\u5e0c\u671b\u4f7f\u7528\u5143\u5305\uff0c\u4e5f\u8bb8\u662f\u4e3a\u4e86\u51cf\u5c11\u4f9d\u8d56\u5173\u7cfb\u56fe\u7684\u5927\u5c0f\uff0c\u60a8\u53ef\u4ee5\u5b89\u88c5\u548c\u6ce8\u518c\u5355\u72ec\u7684\u5de5\u5177 \u5305\u3002</p> <p>For example, here's how you can install and register only the instrumentation-express and instrumentation-http packages to instrument inbound and outbound HTTP traffic.</p> <pre><code>npm install --save @opentelemetry/instrumentation-http @opentelemetry/instrumentation-express\n</code></pre> <p>And then register each instrumentation library:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>/* tracing.ts */\n// Import dependencies\nimport { HttpInstrumentation } from \"@opentelemetry/instrumentation-http\";\nimport { ExpressInstrumentation } from \"@opentelemetry/instrumentation-express\";\nimport opentelemetry from \"@opentelemetry/api\";\nimport { Resource } from \"@opentelemetry/resources\";\nimport { SemanticResourceAttributes } from \"@opentelemetry/semantic-conventions\";\nimport { NodeTracerProvider } from \"@opentelemetry/sdk-trace-node\";\nimport { registerInstrumentations } from \"@opentelemetry/instrumentation\";\nimport { ConsoleSpanExporter, BatchSpanProcessor } from \"@opentelemetry/sdk-trace-base\";\n// This registers all instrumentation packages\nregisterInstrumentations({\ninstrumentations: [\n// Express instrumentation expects HTTP layer to be instrumented\nnew HttpInstrumentation(),\nnew ExpressInstrumentation(),\n],\n});\nconst resource =\nResource.default().merge(\nnew Resource({\n[SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",\n[SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",\n})\n);\nconst provider = new NodeTracerProvider({\nresource: resource,\n});\nconst exporter = new ConsoleSpanExporter();\nconst processor = new BatchSpanProcessor(exporter);\nprovider.addSpanProcessor(processor);\nprovider.register();\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>/* tracing.js */\n// Require dependencies\nconst { HttpInstrumentation } = require(\"@opentelemetry/instrumentation-http\");\nconst { ExpressInstrumentation } = require(\"@opentelemetry/instrumentation-express\");\nconst opentelemetry = require(\"@opentelemetry/api\");\nconst { Resource } = require(\"@opentelemetry/resources\");\nconst { SemanticResourceAttributes } = require(\"@opentelemetry/semantic-conventions\");\nconst { NodeTracerProvider } = require(\"@opentelemetry/sdk-trace-node\");\nconst { registerInstrumentations } = require(\"@opentelemetry/instrumentation\");\nconst { ConsoleSpanExporter, BatchSpanProcessor } = require(\"@opentelemetry/sdk-trace-base\");\n// This registers all instrumentation packages\nregisterInstrumentations({\ninstrumentations: [\n// Express instrumentation expects HTTP layer to be instrumented\nnew HttpInstrumentation(),\nnew ExpressInstrumentation(),\n],\n});\nconst resource =\nResource.default().merge(\nnew Resource({\n[SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",\n[SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",\n})\n);\nconst provider = new NodeTracerProvider({\nresource: resource,\n});\nconst exporter = new ConsoleSpanExporter();\nconst processor = new BatchSpanProcessor(exporter);\nprovider.addSpanProcessor(processor);\nprovider.register();\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/libraries/#_2","title":"\u914d\u7f6e\u5de5\u5177\u5e93","text":"<p>\u4e00\u4e9b\u5de5\u5177\u5e93\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u914d\u7f6e\u9009\u9879\u3002</p> <p>\u4f8b\u5982 \uff0cExpress instrumentation\u63d0 \u4f9b\u4e86\u5ffd\u7565\u6307\u5b9a\u4e2d\u95f4\u4ef6\u6216\u4e30\u5bcc\u4f7f\u7528\u8bf7\u6c42\u94a9\u5b50\u81ea\u52a8\u521b\u5efa\u7684\u8303\u56f4\u7684\u65b9\u6cd5\u3002</p> <p>\u60a8\u9700\u8981\u53c2\u8003\u6bcf\u4e2a\u4eea\u5668\u5e93\u7684\u6587\u6863\u6765\u8fdb\u884c\u9ad8\u7ea7\u914d\u7f6e\u3002</p>"},{"location":"docs/instrumentation/js/libraries/#_3","title":"\u53ef\u7528\u7684\u4eea\u5668\u5e93","text":"<p>OpenTelemetry \u751f\u6210\u7684\u4eea\u5668\u5e93\u7684\u5b8c\u6574\u5217\u8868\u53ef \u4eceopentelemetry-js-contrib\u5b58 \u50a8\u5e93\u83b7\u5f97\u3002</p> <p>\u60a8\u8fd8\u53ef\u4ee5 \u5728registry\u4e2d \u627e\u5230\u66f4\u591a\u53ef\u7528\u7684\u5de5\u5177\u3002</p>"},{"location":"docs/instrumentation/js/libraries/#_4","title":"\u4e0b\u4e00\u4e2a\u6b65\u9aa4","text":"<p>\u5728\u4f60\u8bbe\u7f6e\u597d\u4eea\u5668\u5e93\u4e4b\u540e\uff0c\u4f60\u53ef\u80fd\u60f3\u6dfb\u52a0\u624b\u52a8\u5de5\u5177\u6765\u6536\u96c6\u81ea\u5b9a\u4e49\u7684\u9065\u6d4b\u6570\u636e \u3002</p> <p>\u4f60\u8fd8\u9700\u8981\u914d\u7f6e\u4e00\u4e2a\u5408\u9002\u7684\u5bfc\u51fa\u5668\u6765\u5bfc\u51fa\u9065\u6d4b\u6570\u636e\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u9065\u6d4b\u540e\u7aef \u3002</p>"},{"location":"docs/instrumentation/js/manual/","title":"\u624b\u52a8","text":"<p>\u624b\u52a8\u63d2\u88c5\u662f\u5411\u5e94\u7528\u7a0b\u5e8f\u6dfb\u52a0\u53ef\u89c2\u5bdf\u6027\u4ee3\u7801\u7684\u8fc7\u7a0b\u3002</p>"},{"location":"docs/instrumentation/js/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/js/manual/#initialize-tracing","title":"Initialize Tracing","text":"<p>To start tracing, you'll need to have an initialized <code>TracerProvider</code> that will let you create a <code>Tracer</code>.</p> <p>If a <code>TracerProvider</code> is not created, the OpenTelemetry APIs for tracing will use a no-op implementation and fail to generate data.</p>"},{"location":"docs/instrumentation/js/manual/#nodejs","title":"Node.js","text":"<p>To initialize tracing with the Node.js SDK, first ensure you have the SDK package and OpenTelemetry API installed:</p> <pre><code>npm install \\\n@opentelemetry/api \\\n@opentelemetry/resources \\\n@opentelemetry/semantic-conventions \\\n@opentelemetry/sdk-trace-node \\\n@opentelemetry/instrumentation\n</code></pre> <p>Next, create a separate <code>tracing.js|ts</code> file that has all the SDK initialization code in it:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} /tracing.ts/ import { BatchSpanProcessor, ConsoleSpanExporter } from \"opentelemetry/sdk-trace-base\"; import { Resource } from \"opentelemetry/resources\"; import { SemanticResourceAttributes } from \"opentelemetry/semantic-conventions\"; import { NodeTracerProvider } from \"opentelemetry/sdk-trace-node\"; import { registerInstrumentations } from \"opentelemetry/instrumentation\";</p> <p>// Optionally register instrumentation libraries registerInstrumentations({   instrumentations: [], });</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const provider = new NodeTracerProvider({     resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor);</p> <p>provider.register(); {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} /tracing.js/ const { Resource } = require(\"opentelemetry/resources\"); const { SemanticResourceAttributes } = require(\"opentelemetry/semantic-conventions\"); const { NodeTracerProvider } = require(\"opentelemetry/sdk-trace-node\"); const { registerInstrumentations } = require(\"opentelemetry/instrumentation\"); const { ConsoleSpanExporter, BatchSpanProcessor } = require(\"opentelemetry/sdk-trace-base\");</p> <p>// Optionally register instrumentation libraries registerInstrumentations({   instrumentations: [], });</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const provider = new NodeTracerProvider({     resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor);</p> <p>provider.register(); {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane&gt;}}</p> <p>Next, ensure that <code>tracing.js|ts</code> is required in your node invocation. This is also required if you're registering instrumentation libraries. For example:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} ts-node --require ./tracing.ts  {{&lt; /tab &gt;}} <p>{{&lt; tab JavaScript &gt;}} node --require ./tracing.js  {{&lt; /tab &gt;}} <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/manual/#browser","title":"Browser","text":"<p>First, ensure you've got the right packages:</p> <pre><code>npm install \\\n@opentelemetry/api \\\n@opentelemetry/resources \\\n@opentelemetry/semantic-conventions \\\n@opentelemetry/sdk-trace-web \\\n@opentelemetry/instrumentation\n</code></pre> <p>Create a <code>tracing.js|ts</code> file that initialized the Web SDK, creates a <code>TracerProvider</code>, and exports a <code>Tracer</code>.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import { Resource } from \"opentelemetry/resources\"; import { SemanticResourceAttributes } from \"opentelemetry/semantic-conventions\"; import { WebTracerProvider } from \"opentelemetry/sdk-trace-web\"; import { registerInstrumentations } from \"opentelemetry/instrumentation\"; import { BatchSpanProcessor, ConsoleSpanExporter } from \"opentelemetry/sdk-trace-base\";</p> <p>// Optionally register automatic instrumentation libraries registerInstrumentations({   instrumentations: [], });</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const provider = new WebTracerProvider({     resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor);</p> <p>provider.register(); {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} const opentelemetry = require(\"opentelemetry/api\"); const { Resource } = require(\"opentelemetry/resources\"); const { SemanticResourceAttributes } = require(\"opentelemetry/semantic-conventions\"); const { WebTracerProvider } = require(\"opentelemetry/sdk-trace-web\"); const { registerInstrumentations } = require(\"opentelemetry/instrumentation\"); const { ConsoleSpanExporter, BatchSpanProcessor } = require(\"opentelemetry/sdk-trace-base\");</p> <p>// Optionally register automatic instrumentation libraries registerInstrumentations({   instrumentations: [], });</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const provider = new WebTracerProvider({     resource: resource, }); const exporter = new ConsoleSpanExporter(); const processor = new BatchSpanProcessor(exporter); provider.addSpanProcessor(processor);</p> <p>provider.register(); {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane&gt;}}</p> <p>You'll need to bundle this file with your web application to be able to use tracing throughout the rest of your web application.</p>"},{"location":"docs/instrumentation/js/manual/#picking-the-right-span-processor","title":"Picking the right span processor","text":"<p>By default, the Node SDK uses the <code>BatchSpanProcessor</code>, and this span processor is also chosen in the Web SDK example. The <code>BatchSpanProcessor</code> processes spans in batches before they are exported. This is usually the right processor to use for an application.</p> <p>In contrast, the <code>SimpleSpanProcessor</code> processes spans as they are created. This means that if you create 5 spans, each will be processed and exported before the next span is created in code. This can be helpful in scenarios where you do not want to risk losing a batch, or if you're experimenting with OpenTelemetry in development. However, it also comes with potentially significant overhead, especially if spans are being exported over a network - each time a call to create a span is made, it would be processed and sent over a network before your app's execution could continue.</p> <p>In most cases, stick with <code>BatchSpanProcessor</code> over <code>SimpleSpanProcessor</code>.</p>"},{"location":"docs/instrumentation/js/manual/#acquiring-a-tracer","title":"Acquiring a tracer","text":"<p>Anywhere in your application where you write manual tracing code should call <code>getTracer</code> to acquire a tracer. For example:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry from \"opentelemetry/api\"; //...</p> <p>const tracer = opentelemetry.trace.getTracer(   'my-service-tracer' );</p> <p>// You can now use a 'tracer' to do tracing! {{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}} const opentelemetry = require(\"opentelemetry/api\"); //...</p> <p>const tracer = opentelemetry.trace.getTracer(   'my-service-tracer' );</p> <p>// You can now use a 'tracer' to do tracing! {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>It's generally recommended to call <code>getTracer</code> in your app when you need it rather than exporting the <code>tracer</code> instance to the rest of your app. This helps avoid trickier application load issues when other required dependencies are involved.</p>"},{"location":"docs/instrumentation/js/manual/#create-spans","title":"Create spans","text":"<p>Now that you have a <code>Tracer</code> initialized, you can create <code>Span</code>s.</p> <pre><code>// Create a span. A span must be closed.\ntracer.startActiveSpan('main', (span) =&gt; {\nfor (let i = 0; i &lt; 10; i += 1) {\nconsole.log(i);\n}\n// Be sure to end the span!\nspan.end();\n});\n</code></pre> <p>The above code sample shows how to create an active span, which is the most common kind of span to create.</p>"},{"location":"docs/instrumentation/js/manual/#create-nested-spans","title":"Create nested spans","text":"<p>Nested spans let you track work that's nested in nature. For example, the <code>doWork</code> function below represents a nested operation. The following sample creates a nested span that tracks the <code>doWork</code> function:</p> <pre><code>const mainWork = () =&gt; {\ntracer.startActiveSpan('main', (parentSpan) =&gt; {\nfor (let i = 0; i &lt; 3; i += 1) {\ndoWork(i);\n}\n// Be sure to end the parent span!\nparentSpan.end();\n});\n};\nconst doWork = (i) =&gt; {\ntracer.startActiveSpan(`doWork:${i}`, (span) =&gt; {\n// simulate some random work.\nfor (let i = 0; i &lt;= Math.floor(Math.random() * 40000000); i += 1) {\n// empty\n}\n// Make sure to end this child span! If you don't,\n// it will continue to track work beyond 'doWork'!\nspan.end();\n});\n};\n</code></pre> <p>This code will create 3 child spans that have <code>parentSpan</code>'s span ID as their parent IDs.</p>"},{"location":"docs/instrumentation/js/manual/#create-independent-spans","title":"Create independent spans","text":"<p>The previous examples showed how to create an active span. In some cases, you'll want to create inactive spans that are siblings of one another rather than being nested.</p> <pre><code>const doWork = () =&gt; {\nconst span1 = tracer.startSpan('work-1');\n// do some work\nconst span2 = tracer.startSpan('work-2');\n// do some more work\nconst span3 = tracer.startSpan('work-3');\n// do even more work\nspan1.end();\nspan2.end();\nspan3.end();\n};\n</code></pre> <p>In this example, <code>span1</code>, <code>span2</code>, and <code>span3</code> are sibling spans and none of them are considered the currently active span. They share the same parent rather than being nested under one another.</p> <p>This arrangement can be helpful if you have units of work that are grouped together but are conceptually independent from one another.</p>"},{"location":"docs/instrumentation/js/manual/#get-the-current-span","title":"Get the current span","text":"<p>Sometimes it's helpful to do something with the current/active span at a particular point in program execution.</p> <pre><code>const activeSpan = opentelemetry.trace.getActiveSpan();\n// do something with the active span, optionally ending it if that is appropriate for your use case.\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#get-a-span-from-context","title":"Get a span from context","text":"<p>It can also be helpful to get the span from a given context that isn't necessarily the active span.</p> <pre><code>const ctx = getContextFromSomewhere();\nconst span = opentelemetry.trace.getSpan(ctx);\n// do something with the acquired span, optionally ending it if that is appropriate for your use case.\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#attributes","title":"Attributes","text":"<p>Attributes let you attach key/value pairs to a <code>Span</code> so it carries more information about the current operation that it's tracking.</p> <pre><code>tracer.startActiveSpan('app.new-span', (span) =&gt; {\n// do some work...\n// Add an attribute to the span\nspan.setAttribute('attribute1', 'value1');\nspan.end();\n});\n</code></pre> <p>You can also add attributes to a span as it's created:</p> <pre><code>tracer.startActiveSpan(\n'app.new-span',\n{ attributes: { attribute1: 'value1' } },\n(span) =&gt; {\n// do some work...\nspan.end();\n}\n);\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#semantic-attributes","title":"Semantic Attributes","text":"<p>There are semantic conventions for spans representing operations in well-known protocols like HTTP or database calls. Semantic conventions for these spans are defined in the specification at Trace Semantic Conventions. In the simple example of this guide the source code attributes can be used.</p> <p>First add the semantic conventions as a dependency to your application:</p> <pre><code>npm install --save @opentelemetry/semantic-conventions\n</code></pre> <p>Add the following to the top of your application file:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import { SemanticAttributes } from \"opentelemetry/semantic-conventions\"; {{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}} const { SemanticAttributes } = require('opentelemetry/semantic-conventions'); {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>Finally, you can update your file to include semantic attributes:</p> <pre><code>const doWork = () =&gt; {\ntracer.startActiveSpan('app.doWork', (span) =&gt; {\nspan.setAttribute(SemanticAttributes.CODE_FUNCTION, 'doWork');\nspan.setAttribute(SemanticAttributes.CODE_FILEPATH, __filename);\n// Do some work...\nspan.end();\n});\n};\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#span-events","title":"Span events","text":"<p>A Span Event is a human-readable message on an <code>Span</code> that represents a discrete event with no duration that can be tracked by a single time stamp. You can think of it like a primitive log.</p> <pre><code>span.addEvent('Doing something');\nconst result = doWork();\n</code></pre> <p>You can also create Span Events with additional Attributes:</p> <pre><code>span.addEvent('some log', {\n'log.severity': 'error',\n'log.message': 'Data not found',\n'request.id': requestId,\n});\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#span-links","title":"Span links","text":"<p><code>Span</code>s can be created with zero or more <code>Link</code>s to other Spans that are causally related. A common scenario is to correlate one or more traces with the current span.</p> <pre><code>const someFunction = (spanToLinkFrom) =&gt; {\nconst options = {\nlinks: [\n{\ncontext: spanToLinkFrom.spanContext()\n}\n]\n};\ntracer.startActiveSpan('app.someFunction', options: options, span =&gt; {\n// Do some work...\nspan.end();\n});\n}\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#span-status","title":"Span Status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>SpanStatusCode.ERROR</code>.</p> <p>The status can be set at any time before the span is finished:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry, { SpanStatusCode } from \"opentelemetry/api\";</p> <p>// ...</p> <p>tracer.startActiveSpan('app.doWork', span =&gt; {   for (let i = 0; i &lt;= Math.floor(Math.random() * 40000000); i += 1) {     if (i &gt; 10000) {       span.setStatus({         code: SpanStatusCode.ERROR,         message: 'Error'       });     }   }</p> <p>span.end(); }); {{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}} const opentelemetry = require(\"opentelemetry/api\");</p> <p>// ...</p> <p>tracer.startActiveSpan('app.doWork', span =&gt; {   for (let i = 0; i &lt;= Math.floor(Math.random() * 40000000); i += 1) {     if (i &gt; 10000) {       span.setStatus({         code: opentelemetry.SpanStatusCode.ERROR,         message: 'Error'       });     }   }</p> <p>span.end(); }); {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>By default, the status for all spans is <code>Unset</code> rather than <code>Ok</code>. It is typically the job of another component in your telemetry pipeline to interpret the <code>Unset</code> status of a span, so it's best not to override this unless you're explicitly tracking an error.</p>"},{"location":"docs/instrumentation/js/manual/#recording-exceptions","title":"Recording exceptions","text":"<p>It can be a good idea to record exceptions when they happen. It's recommended to do this in conjunction with setting span status.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry, { SpanStatusCode } from \"opentelemetry/api\";</p> <p>// ...</p> <p>try {   doWork(); } catch (ex) {   span.recordException(ex);   span.setStatus({ code: SpanStatusCode.ERROR }); } {{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}} const opentelemetry = require(\"opentelemetry/api\");</p> <p>// ...</p> <p>try {   doWork(); } catch (ex) {   span.recordException(ex);   span.setStatus({ code: opentelemetry.SpanStatusCode.ERROR }); } {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p>"},{"location":"docs/instrumentation/js/manual/#using-sdk-trace-base-and-manually-propagating-span-context","title":"Using <code>sdk-trace-base</code> and manually propagating span context","text":"<p>In some cases, you may not be able to use either the Node.js SDK nor the Web SDK. The biggest difference, aside from initialization code, is that you'll have to manually set spans as active in the current context to be able to create nested spans.</p>"},{"location":"docs/instrumentation/js/manual/#initializing-tracing-with-sdk-trace-base","title":"Initializing tracing with <code>sdk-trace-base</code>","text":"<p>Initializing tracing is similar to how you'd do it with Node.js or the Web SDK.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry from \"opentelemetry/api\"; import {   BasicTracerProvider,   BatchSpanProcessor,   ConsoleSpanExporter } from \"opentelemetry/sdk-trace-base\";</p> <p>const provider = new BasicTracerProvider();</p> <p>// Configure span processor to send spans to the exporter provider.addSpanProcessor(new BatchSpanProcessor(new ConsoleSpanExporter())); provider.register();</p> <p>// This is what we'll access in all instrumentation code const tracer = opentelemetry.trace.getTracer(   'example-basic-tracer-node' ); {{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}} const opentelemetry = require(\"opentelemetry/api\"); const {   BasicTracerProvider,   ConsoleSpanExporter,   BatchSpanProcessor, } = require(\"opentelemetry/sdk-trace-base\");</p> <p>const provider = new BasicTracerProvider();</p> <p>// Configure span processor to send spans to the exporter provider.addSpanProcessor(new BatchSpanProcessor(new ConsoleSpanExporter())); provider.register();</p> <p>// This is what we'll access in all instrumentation code const tracer = opentelemetry.trace.getTracer(     'example-basic-tracer-node' ); {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>Like the other examples in this document, this exports a tracer you can use throughout the app.</p>"},{"location":"docs/instrumentation/js/manual/#creating-nested-spans-with-sdk-trace-base","title":"Creating nested spans with <code>sdk-trace-base</code>","text":"<p>To create nested spans, you need to set whatever the currently-created span is as the active span in the current context. Don't bother using <code>startActiveSpan</code> because it won't do this for you.</p> <pre><code>const mainWork = () =&gt; {\nconst parentSpan = tracer.startSpan('main');\nfor (let i = 0; i &lt; 3; i += 1) {\ndoWork(parentSpan, i);\n}\n// Be sure to end the parent span!\nparentSpan.end();\n};\nconst doWork = (parent, i) =&gt; {\n// To create a child span, we need to mark the current (parent) span as the active span\n// in the context, then use the resulting context to create a child span.\nconst ctx = opentelemetry.trace.setSpan(\nopentelemetry.context.active(),\nparent\n);\nconst span = tracer.startSpan(`doWork:${i}`, undefined, ctx);\n// simulate some random work.\nfor (let i = 0; i &lt;= Math.floor(Math.random() * 40000000); i += 1) {\n// empty\n}\n// Make sure to end this child span! If you don't,\n// it will continue to track work beyond 'doWork'!\nspan.end();\n};\n</code></pre> <p>All other APIs behave the same when you use <code>sdk-trace-base</code> compared with the Node.js or Web SDKs.</p>"},{"location":"docs/instrumentation/js/manual/#metrics","title":"Metrics","text":"<p>To start metrics, you'll need to have an initialized <code>MeterProvider</code> that lets you create a <code>Meter</code>. <code>Meter</code>s let you create <code>Instrument</code>s that you can use to create different kinds of metrics. OpenTelemetry JavaScript currently supports the following <code>Instrument</code>s:</p> <ul> <li>Counter, a synchronous instrument which supports non-negative increments</li> <li>Asynchronous Counter, a asynchronous instrument which supports non-negative   increments</li> <li>Histogram, a synchronous instrument which supports arbitrary values that are   statistically meaningful, such as histograms, summaries or percentile</li> <li>Asynchronous Gauge, an asynchronous instrument which supports non-additive   values, such as room temperature</li> <li>UpDownCounter, a synchronous instrument which supports increments and   decrements, such as number of active requests</li> <li>Asynchronous UpDownCounter, an asynchronous instrument which supports   increments and decrements</li> </ul> <p>For more on synchronous and asynchronous instruments, and which kind is best suited for your use case, see Supplementary Guidelines.</p> <p>If a <code>MeterProvider</code> is not created either by an instrumentation library or manually, the OpenTelemetry Metrics API will use a no-op implementation and fail to generate data.</p>"},{"location":"docs/instrumentation/js/manual/#initialize-metrics","title":"Initialize Metrics","text":"<p>To initialize metrics, make sure you have the right packages installed:</p> <pre><code>npm install \\\n@opentelemetry/api \\\n@opentelemetry/resources \\\n@opentelemetry/semantic-conventions \\\n@opentelemetry/sdk-metrics \\\n@opentelemetry/instrumentation\n</code></pre> <p>Next, create a separate <code>instrumentation.js|ts</code> file that has all the SDK initialization code in it:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry from \"opentelemetry/api\"; import {   ConsoleMetricExporter,   MeterProvider,   PeriodicExportingMetricReader } from \"opentelemetry/sdk-metrics\"; import { Resource } from \"opentelemetry/resources\"; import { SemanticResourceAttributes } from \"opentelemetry/semantic-conventions\";</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const metricReader = new PeriodicExportingMetricReader({     exporter: new ConsoleMetricExporter(),</p> <pre><code>// Default is 60000ms (60 seconds). Set to 3 seconds for demonstrative purposes only.\nexportIntervalMillis: 3000,\n</code></pre> <p>});</p> <p>const myServiceMeterProvider = new MeterProvider({   resource: resource, });</p> <p>myServiceMeterProvider.addMetricReader(metricReader);</p> <p>// Set this MeterProvider to be global to the app being instrumented. opentelemetry.metrics.setGlobalMeterProvider(myServiceMeterProvider) {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} const opentelemetry = require('opentelemetry/api') const {     MeterProvider,     PeriodicExportingMetricReader,     ConsoleMetricExporter   } = require('opentelemetry/sdk-metrics'); const { Resource } = require('opentelemetry/resources'); const { SemanticResourceAttributes } = require('opentelemetry/semantic-conventions');</p> <p>const resource =   Resource.default().merge(     new Resource({       [SemanticResourceAttributes.SERVICE_NAME]: \"service-name-here\",       [SemanticResourceAttributes.SERVICE_VERSION]: \"0.1.0\",     })   );</p> <p>const metricReader = new PeriodicExportingMetricReader({     exporter: new ConsoleMetricExporter(),</p> <pre><code>// Default is 60000ms (60 seconds). Set to 3 seconds for demonstrative purposes only.\nexportIntervalMillis: 3000,\n</code></pre> <p>});</p> <p>const myServiceMeterProvider = new MeterProvider({   resource: resource, });</p> <p>myServiceMeterProvider.addMetricReader(metricReader);</p> <p>// Set this MeterProvider to be global to the app being instrumented. opentelemetry.metrics.setGlobalMeterProvider(myServiceMeterProvider) {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>You'll need to <code>--require</code> this file when you run your app, such as:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} ts-node --require ./instrumentation.ts  {{&lt; /tab &gt;}} <p>{{&lt; tab JavaScript &gt;}} node --require ./instrumentation.js  {{&lt; /tab &gt;}} <p>{{&lt; /tabpane &gt;}}</p> <p>Now that a <code>MeterProvider</code> is configured, you can acquire a <code>Meter</code>.</p>"},{"location":"docs/instrumentation/js/manual/#acquiring-a-meter","title":"Acquiring a Meter","text":"<p>Anywhere in your application where you have manually instrumented code you can call <code>getMeter</code> to acquire a meter. For example:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import opentelemetry from \"opentelemetry/api\";</p> <p>const myMeter = opentelemetry.metrics.getMeter(   'my-service-meter' );</p> <p>// You can now use a 'meter' to create instruments! {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} const opentelemetry = require('opentelemetry/api')</p> <p>const myMeter = opentelemetry.metrics.getMeter(   'my-service-meter' );</p> <p>// You can now use a 'meter' to create instruments! {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p> <p>It\u2019s generally recommended to call <code>getMeter</code> in your app when you need it rather than exporting the meter instance to the rest of your app. This helps avoid trickier application load issues when other required dependencies are involved.</p>"},{"location":"docs/instrumentation/js/manual/#synchronous-and-asynchronous-instruments","title":"Synchronous and asynchronous instruments","text":"<p>OpenTelemetry instruments are either synchronous or asynchronous (observable).</p> <p>Synchronous instruments take a measurement when they are called. The measurement is done as another call during program execution, just like any other function call. Periodically, the aggregation of these measurements is exported by a configured exporter. Because measurements are decoupled from exporting values, an export cycle may contain zero or multiple aggregated measurements.</p> <p>Asynchronous instruments, on the other hand, provide a measurement at the request of the SDK. When the SDK exports, a callback that was provided to the instrument on creation is invoked. This callback provides the SDK with a measurement that is immediately exported. All measurements on asynchronous instruments are performed once per export cycle.</p> <p>Asynchronous instruments are useful in several circumstances, such as:</p> <ul> <li>When updating a counter is not computationally cheap, and thus you don't want   the currently executing thread to have to wait for that measurement</li> <li>Observations need to happen at frequencies unrelated to program execution   (i.e., they cannot be accurately measured when tied to a request lifecycle)</li> <li>There is no value from knowing the precise timestamp of increments</li> </ul> <p>In cases like these, it's often better to observe a cumulative value directly, rather than aggregate a series of deltas in post-processing (the synchronous example). Take note of the use of <code>observe</code> rather than <code>add</code> in the appropriate code examples below.</p>"},{"location":"docs/instrumentation/js/manual/#using-counters","title":"Using Counters","text":"<p>Counters can by used to measure a non-negative, increasing value.</p> <pre><code>const counter = myMeter.createCounter('events.counter');\n//...\ncounter.add(1);\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#using-updown-counters","title":"Using UpDown Counters","text":"<p>UpDown counters can increment and decrement, allowing you to observe a cumulative value that goes up or down.</p> <pre><code>const counter = myMeter.createUpDownCounter('events.counter');\n//...\ncounter.add(1);\n//...\ncounter.add(-1);\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#using-histograms","title":"Using Histograms","text":"<p>Histograms are used to measure a distribution of values over time.</p> <p>For example, here's how you might report a distribution of response times for an API route with Express:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}} {{&lt; tab TypeScript &gt;}} import express from \"express\";</p> <p>const app = express();</p> <p>app.get('/', (_req, _res) =&gt; {   const histogram = myMeter.createHistogram(\"task.duration\");   const startTime = new Date().getTime()</p> <p>// do some work in an API call</p> <p>const endTime = new Date().getTime()   const executionTime = endTime - startTime</p> <p>// Record the duration of the task operation   histogram.record(executionTime) }); {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} const express = require('express');</p> <p>const app = express();</p> <p>app.get('/', (_req, _res) =&gt; {   const histogram = myMeter.createHistogram(\"task.duration\");   const startTime = new Date().getTime()</p> <p>// do some work in an API call</p> <p>const endTime = new Date().getTime()   const executionTime = endTime - startTime</p> <p>// Record the duration of the task operation   histogram.record(executionTime) }); {{&lt; /tab &gt;}} {{&lt; /tabpane&gt;}}</p>"},{"location":"docs/instrumentation/js/manual/#using-observable-async-counters","title":"Using Observable (Async) Counters","text":"<p>Observable counters can be used to measure an additive, non-negative, monotonically increasing value.</p> <pre><code>let events = [];\nconst addEvent = (name) =&gt; {\nevents = append(events, name);\n};\nconst counter = myMeter.createObservableCounter('events.counter');\ncounter.addCallback((result) =&gt; {\nresult.observe(len(events));\n});\n//... calls to addEvent\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#using-observable-async-updown-counters","title":"Using Observable (Async) UpDown Counters","text":"<p>Observable UpDown counters can increment and decrement, allowing you to measure an additive, non-negative, non-monotonically increasing cumulative value.</p> <pre><code>let events = [];\nconst addEvent = (name) =&gt; {\nevents = append(events, name);\n};\nconst removeEvent = () =&gt; {\nevents.pop();\n};\nconst counter = myMeter.createObservableUpDownCounter('events.counter');\ncounter.addCallback((result) =&gt; {\nresult.observe(len(events));\n});\n//... calls to addEvent and removeEvent\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#using-observable-async-gauges","title":"Using Observable (Async) Gauges","text":"<p>Observable Gauges should be used to measure non-additive values.</p> <pre><code>let temperature = 32;\nconst gauge = myMeter.createObservableGauge('temperature.gauge');\ngauge.addCallback((result) =&gt; {\nresult.observe(temperature);\n});\n//... temperature variable is modified by a sensor\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#describing-instruments","title":"Describing instruments","text":"<p>When you create instruments like counters, histograms, etc. you can give them a description.</p> <pre><code>const httpServerResponseDuration = myMeter.createHistogram(\n'http.server.duration',\n{\ndescription: 'A distribution of the HTTP server response times',\nunit: 'milliseconds',\nvalueType: ValueType.INT,\n}\n);\n</code></pre> <p>In JavaScript, each configuration type means the following:</p> <ul> <li><code>description</code> - a human-readable description for the instrument</li> <li><code>unit</code> - The description of the unit of measure that the value is intended to   represent. For example, <code>milliseconds</code> to measure duration, or <code>bytes</code> to   count number of bytes.</li> <li><code>valueType</code> - The kind of numeric value used in measurements.</li> </ul> <p>It's generally recommended to describe each instrument you create.</p>"},{"location":"docs/instrumentation/js/manual/#adding-attributes","title":"Adding attributes","text":"<p>You can add Attributes to metrics when they are generated.</p> <pre><code>const counter = myMeter.createCounter('my.counter');\ncounter.add(1, { 'some.optional.attribute': 'some value' });\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#configure-metric-views","title":"Configure Metric Views","text":"<p>A Metric View provides developers with the ability to customize metrics exposed by the Metrics SDK.</p>"},{"location":"docs/instrumentation/js/manual/#selectors","title":"Selectors","text":"<p>To instantiate a view, one must first select a target instrument. The following are valid selectors for metrics:</p> <ul> <li><code>instrumentType</code></li> <li><code>instrumentName</code></li> <li><code>meterName</code></li> <li><code>meterVersion</code></li> <li><code>meterSchemaUrl</code></li> </ul> <p>Selecting by <code>instrumentName</code> (of type string) has support for wildcards, so you can select all instruments using <code>*</code> or select all instruments whose name starts with <code>http</code> by using <code>http*</code>.</p>"},{"location":"docs/instrumentation/js/manual/#examples","title":"Examples","text":"<p>Filter attributes on all metric types:</p> <pre><code>const limitAttributesView = new View({\n// only export the attribute 'environment'\nattributeKeys: ['environment'],\n// apply the view to all instruments\ninstrumentName: '*',\n});\n</code></pre> <p>Drop all instruments with the meter name <code>pubsub</code>:</p> <pre><code>const dropView = new View({\naggregation: new DropAggregation(),\nmeterName: 'pubsub',\n});\n</code></pre> <p>Define explicit bucket sizes for the Histogram named <code>http.server.duration</code>:</p> <pre><code>const histogramView = new View({\naggregation: new ExplicitBucketHistogramAggregation([\n0, 1, 5, 10, 15, 20, 25, 30,\n]),\ninstrumentName: 'http.server.duration',\ninstrumentType: InstrumentType.HISTOGRAM,\n});\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#attach-to-meter-provider","title":"Attach to meter provider","text":"<p>Once views have been configured, attach them to the corresponding meter provider:</p> <pre><code>const meterProvider = new MeterProvider({\nviews: [limitAttributesView, dropView, histogramView],\n});\n</code></pre>"},{"location":"docs/instrumentation/js/manual/#next-steps","title":"Next steps","text":"<p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/js/propagation/","title":"\u4f20\u64ad","text":"<p>\u4f20\u64ad\u662f\u5728\u670d\u52a1\u548c\u8fdb\u7a0b\u4e4b\u95f4\u79fb\u52a8\u6570\u636e\u7684\u673a\u5236\u3002 \u867d\u7136\u4e0d\u9650\u4e8e\u8ddf\u8e2a\uff0c\u4f46\u5b83\u5141\u8bb8\u8ddf\u8e2a\u6784\u5efa\u5173\u4e8e\u7cfb\u7edf\u7684\u56e0\u679c\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u4ee5\u8de8\u4efb\u610f\u5206\u5e03\u5728\u8fdb\u7a0b\u548c\u7f51\u7edc\u8fb9\u754c\u4e0a\u7684\u670d\u52a1\u3002</p>"},{"location":"docs/instrumentation/js/propagation/#context-propagation-with-libraries","title":"Context propagation with libraries","text":"<p>For the vast majority of use cases, context propagation is done with instrumentation libraries.</p> <p>For example, if you have several Node.js services that communicate over HTTP, you can use the <code>express</code> and <code>http</code> instrumentation libraries to automatically propagate trace context across services for you.</p> <p>It is highly recommend that you use instrumentation libraries to propagate context. Although it is possible to propagate context manually, if your system uses libraries to communicate between services, use a matching instrumentation library to propagate context.</p> <p>Refer to Libraries to learn more about instrumentation libraries and how to use them.</p>"},{"location":"docs/instrumentation/js/propagation/#manual-w3c-trace-context-propagation","title":"Manual W3C Trace Context Propagation","text":"<p>In some cases, it is not possible to propagate context with an instrumentation library. There may not be an instrumentation library that matches a library you're using to have services communicate with one another. Or you many have requirements that instrumentation libraries cannot fulfill, even if they exist.</p> <p>When you must propagate context manually, you can use the context api.</p> <p>The following generic example demonstrates how you can propagate trace context manually.</p> <p>First, on the sending service, you'll need to inject the current <code>context</code>:</p> <pre><code>// Sending service\nimport { context, propagation, trace } from '@opentelemetry/api';\nconst output = {};\n// Serialize the traceparent and tracestate from context into\n// an output object.\n//\n// This example uses the active trace context, but you can\n// use whatever context is appropriate to your scenario.\npropagation.inject(context.active(), output);\nconst { traceparent, tracestate } = output;\n// You can then pass the traceparent and tracestate\n// data to whatever mechanism you use to propagate\n// across services.\n</code></pre> <p>On the receiving service, you'll need to extract <code>context</code> (for example, from parsed HTTP headers) and then set them as the current trace context.</p> <pre><code>// Receiving service\nimport { context, propagation, trace } from '@opentelemetry/api';\n// Assume \"input\" is an object with 'traceparent' &amp; 'tracestate' keys\nconst input = {};\n// Extracts the 'traceparent' and 'tracestate' data into a context object.\n//\n// You can then treat this context as the active context for your\n// traces.\nlet activeContext = propagation.extract(context.active(), input);\nlet tracer = trace.getTracer('app-name');\nlet span = tracer.startSpan(\nspanName,\n{\nattributes: {},\n},\nactiveContext\n);\n// Set the created span as active in the deserialized context.\ntrace.setSpan(activeContext, span);\n</code></pre> <p>From there, when you have a deserialized active context, you can create spans that will be a part of the same trace from the other service.</p> <p>You can also use the Context API to modify or set the deserialized context in other ways.</p>"},{"location":"docs/instrumentation/js/resources/","title":"\u8d44\u6e90","text":"<p>resource\u8868\u793a\u4f5c\u4e3a\u8d44\u6e90\u5c5e\u6027\u4ea7\u751f\u9065\u6d4b\u7684\u5b9e\u4f53\u3002 \u4f8b\u5982\uff0c\u5728Kubernetes\u4e0a\u7684\u5bb9\u5668\u4e2d\u8fd0\u884c\u7684\u4ea7\u751f\u9065\u6d4b\u7684\u8fdb\u7a0b\u6709\u4e00\u4e2aPod\u540d\u79f0\u3001\u4e00\u4e2a\u540d\u79f0\u7a7a\u95f4\uff0c\u53ef\u80fd\u8fd8\u6709\u4e00\u4e2a\u90e8\u7f72\u540d\u79f0\u3002 \u6240\u6709\u8fd9\u4e09\u4e2a\u5c5e\u6027\u90fd\u53ef\u4ee5\u5305\u542b\u5728\u8d44\u6e90\u4e2d\u3002</p> <p>\u5728\u53ef\u89c2\u5bdf\u6027\u540e\u7aef\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u8d44\u6e90\u4fe1\u606f\u6765\u66f4\u597d\u5730\u8c03\u67e5\u6709\u8da3\u7684\u884c\u4e3a\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u7684\u8ddf\u8e2a\u6216\u5ea6\u91cf\u6570\u636e\u8868\u660e\u7cfb\u7edf\u4e2d\u7684\u5ef6\u8fdf\uff0c\u5219\u53ef\u4ee5\u5c06\u5176\u7f29\u5c0f\u5230\u7279\u5b9a\u7684\u5bb9\u5668\u3001Pod\u6216Kubernetes\u90e8\u7f72\u3002</p> <p>\u4e0b\u9762\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e9b\u5173\u4e8e\u5982\u4f55\u4f7f\u7528Node.js SDK\u8bbe\u7f6e\u8d44\u6e90\u68c0\u6d4b\u7684\u4ecb\u7ecd\u3002</p>"},{"location":"docs/instrumentation/js/resources/#_1","title":"\u8bbe\u7f6e","text":"<p>\u6309\u7167\u5165\u95e8-Node.js\u4e2d\u7684\u8bf4\u660e\uff0c\u8fd9\u6837\u4f60\u5c31\u6709\u4e86 <code>package.json</code>\uff0c <code>app.js</code>\u548c<code>tracing.js</code>\u6587\u4ef6\u3002</p>"},{"location":"docs/instrumentation/js/resources/#_2","title":"\u8fdb\u7a0b\u548c\u73af\u5883\u8d44\u6e90\u68c0\u6d4b","text":"<p>Node.js SDK\u5f00\u7bb1\u5373\u7528\uff0c\u68c0\u6d4b[\u8fdb\u7a0b\u548c\u8fdb\u7a0b\u8fd0\u884c\u65f6\u8d44\u6e90][]\u5e76\u4ece\u73af\u5883\u53d8\u91cf <code>OTEL_RESOURCE_ATTRIBUTES</code> \u4e2d\u83b7\u53d6\u5c5e\u6027\u3002 \u4f60\u53ef\u4ee5\u901a\u8fc7\u5728 <code>trace.js</code> \u4e2d\u6253\u5f00\u8bca\u65ad\u65e5\u5fd7\u6765\u9a8c\u8bc1\u5b83\u68c0\u6d4b\u5230\u4ec0\u4e48:</p> <pre><code>// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\n</code></pre> <p>\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\uff0c\u5c06\u4e00\u4e9b\u503c\u8bbe\u7f6e\u4e3a<code>OTEL_RESOURCE_ATTRIBUTES</code>\uff0c\u4f8b\u5982\u6211\u4eec\u8bbe\u7f6e<code>host.name</code>\u6765\u6807\u8bc6host:</p> <pre><code>$ env OTEL_RESOURCE_ATTRIBUTES=\"host.name=localhost\" \\\nnode --require ./tracing.js app.js\n@opentelemetry/api: Registered a global for diag v1.2.0.\n...\nListening for requests on http://localhost:8080\nEnvDetector found resource. Resource { attributes: { 'host.name': 'localhost' } }\nProcessDetector found resource. Resource {\n  attributes: {\n    'process.pid': 12345,\n    'process.executable.name': 'node',\n    'process.command': '/app.js',\n    'process.command_line': '/bin/node /app.js',\n    'process.runtime.version': '16.17.0',\n    'process.runtime.name': 'nodejs',\n    'process.runtime.description': 'Node.js'\n  }\n}\n...\n</code></pre>"},{"location":"docs/instrumentation/js/resources/#_3","title":"\u4f7f\u7528\u73af\u5883\u53d8\u91cf\u6dfb\u52a0\u8d44\u6e90","text":"<p>\u5728\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\uff0cSDK\u68c0\u6d4b\u5230\u8fdb\u7a0b\uff0c\u5e76\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u81ea\u52a8\u6dfb\u52a0\u4e86 <code>host.name=localhost</code> \u5c5e\u6027\u96c6\u3002</p> <p>\u60a8\u5c06\u5728\u4e0b\u9762\u627e\u5230\u4e3a\u60a8\u81ea\u52a8\u68c0\u6d4b\u8d44\u6e90\u7684\u8bf4\u660e\u3002 \u4f46\u662f\uff0c\u60a8\u53ef\u80fd\u4f1a\u9047\u5230\u8fd9\u6837\u7684\u60c5\u51b5\uff0c\u5373\u60a8\u9700\u8981\u7684\u8d44\u6e90\u4e0d\u5b58\u5728\u68c0\u6d4b\u5668\u3002 \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u73af\u5883 <code>OTEL_RESOURCE_ATTRIBUTES</code> \u6765\u6ce8\u5165\u60a8\u9700\u8981\u7684\u4efb\u4f55\u5185\u5bb9\u3002 \u4f8b\u5982\uff0c\u4e0b\u9762\u7684\u811a\u672c\u6dfb\u52a0Service\uff0c Host\u548cOS\u8d44\u6e90\u5c5e\u6027:</p> <pre><code>$ env OTEL_RESOURCE_ATTRIBUTES=\"service.name=app.js,service.namespace=tutorial,service.version=1.0,service.instance.id=`uuidgen`,host.name=${HOSTNAME:},host.type=`uname -m`,os.name=`uname -s`,os.version=`uname -r`\" \\\nnode --require ./tracing.js app.js\n...\nEnvDetector found resource. Resource {\n  attributes: {\n    'service.name': 'app.js',\n    'service.namespace': 'tutorial',\n    'service.version': '1.0',\n    'service.instance.id': '46D99F44-27AB-4006-9F57-3B7C9032827B',\n    'host.name': 'myhost',\n    'host.type': 'arm64',\n    'os.name': 'linux',\n    'os.version': '6.0'\n  }\n}\n...\n</code></pre>"},{"location":"docs/instrumentation/js/resources/#_4","title":"\u5728\u4ee3\u7801\u4e2d\u6dfb\u52a0\u8d44\u6e90","text":"<p>\u81ea\u5b9a\u4e49\u8d44\u6e90\u4e5f\u53ef\u4ee5\u5728\u4ee3\u7801\u4e2d\u914d\u7f6e\u3002 <code>NodeSDK</code>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u914d\u7f6e\u9009\u9879\uff0c\u60a8\u53ef\u4ee5\u5728\u5176\u4e2d\u8bbe\u7f6e\u5b83\u4eec\u3002 \u4f8b\u5982\uff0c\u4f60\u53ef\u4ee5\u50cf\u4e0b\u9762\u8fd9\u6837\u66f4\u65b0<code>tracing.js</code>\uff0c\u8bbe\u7f6e<code>service.*</code>\u5c5e\u6027:</p> <pre><code>...\nconst { Resource } = require('@opentelemetry/resources');\nconst { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');\n...\nconst sdk = new opentelemetry.NodeSDK({\n...\nresource: new Resource({\n[ SemanticResourceAttributes.SERVICE_NAME ]: \"yourServiceName\",\n[ SemanticResourceAttributes.SERVICE_NAMESPACE ]: \"yourNameSpace\",\n[ SemanticResourceAttributes.SERVICE_VERSION ]: \"1.0\",\n[ SemanticResourceAttributes.SERVICE_INSTANCE_ID ]: \"my-instance-id-1\",\n})\n...\n});\n...\n</code></pre> <p>Note: \u5982\u679c\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u548c\u4ee3\u7801\u8bbe\u7f6e\u8d44\u6e90\u5c5e\u6027\uff0c\u5219\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u7684\u503c\u4f18\u5148\u3002</p>"},{"location":"docs/instrumentation/js/resources/#_5","title":"\u5bb9\u5668\u8d44\u6e90\u68c0\u6d4b","text":"<p>\u4f7f\u7528\u76f8\u540c\u7684\u8bbe\u7f6e(<code>package.json</code>, <code>app.js</code> and <code>tracing.js</code>\u6253\u5f00\u8c03\u8bd5)\u548c<code>Dockerfile</code>\u5728\u540c\u4e00\u76ee\u5f55\u4e0b\u5177\u6709\u4ee5\u4e0b\u5185\u5bb9:</p> <pre><code>FROM node:latest\nWORKDIR /usr/src/app\nCOPY package.json ./\nRUN npm install\nCOPY . .\nEXPOSE 8080\nCMD [ \"node\", \"--require\", \"./tracing.js\", \"app.js\" ]\n</code></pre> <p>\u8981\u786e\u4fdd\u60a8\u53ef\u4ee5\u4f7f\u7528Ctrl + C(<code>SIGINT</code>)\u5c06\u4ee5\u4e0b\u5185\u5bb9\u6dfb\u52a0\u5230<code>app.js</code>\u7684\u5e95\u90e8:</p> <pre><code>process.on('SIGINT', function () {\nprocess.exit();\n});\n</code></pre> <p>\u8981\u83b7\u5f97\u81ea\u52a8\u68c0\u6d4b\u5230\u7684\u5bb9\u5668id\uff0c\u5b89\u88c5\u4ee5\u4e0b\u9644\u52a0\u4f9d\u8d56\u9879:</p> <pre><code>npm install @opentelemetry/resource-detector-docker\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u50cf\u4e0b\u9762\u8fd9\u6837\u66f4\u65b0\u4f60\u7684 <code>tracing.js</code>:</p> <pre><code>const opentelemetry = require('@opentelemetry/sdk-node');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api');\nconst {\ndockerCGroupV1Detector,\n} = require('@opentelemetry/resource-detector-docker');\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\nconst sdk = new opentelemetry.NodeSDK({\ntraceExporter: new opentelemetry.tracing.ConsoleSpanExporter(),\ninstrumentations: [getNodeAutoInstrumentations()],\nresourceDetectors: [dockerCGroupV1Detector],\n});\nsdk.start();\n</code></pre> <p>\u6784\u5efadocker\u955c\u50cf:</p> <pre><code>docker build . -t nodejs-otel-getting-started\n</code></pre> <p>\u8fd0\u884c\u4f60\u7684docker\u5bb9\u5668:</p> <pre><code>$ docker run --rm -p 8080:8080 nodejs-otel-getting-started\n@opentelemetry/api: Registered a global for diag v1.2.0.\n...\nListening for requests on http://localhost:8080\nDockerCGroupV1Detector found resource. Resource {\nattributes: {\n'container.id': 'fffbeaf682f32ef86916f306ff9a7f88cc58048ab78f7de464da3c320ldb5c54'\n}\n}\n</code></pre> <p>The detector has extracted the <code>container.id</code> for you. However you might recognize that in this example, the process attributes and the attributes set via an environment variable are missing! To resolve this, when you set the <code>resourceDetectors</code> list you also need to specify the <code>envDetector</code> and <code>processDetector</code> detectors: \u68c0\u6d4b\u5668\u5df2\u7ecf\u4e3a\u60a8\u63d0\u53d6\u4e86<code>container.id</code>\u3002 \u7136\u800c\uff0c\u60a8\u53ef\u80fd\u4f1a\u53d1\u73b0\uff0c\u5728\u672c\u4f8b\u4e2d\uff0c\u6d41\u7a0b\u5c5e\u6027\u548c\u901a\u8fc7\u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u7684\u5c5e\u6027\u7f3a\u5931\u4e86! \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5f53\u4f60\u8bbe\u7f6e<code>resourceDetectors</code>\u5217\u8868\u65f6\uff0c\u4f60\u8fd8\u9700\u8981\u6307\u5b9a<code>envDetector</code>\u548c<code>processDetector</code>\u63a2\u6d4b\u5668: <pre><code>const opentelemetry = require('@opentelemetry/sdk-node');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api');\nconst {\ndockerCGroupV1Detector,\n} = require('@opentelemetry/resource-detector-docker');\nconst { envDetector, processDetector } = require('@opentelemetry/resources');\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);\nconst sdk = new opentelemetry.NodeSDK({\ntraceExporter: new opentelemetry.tracing.ConsoleSpanExporter(),\ninstrumentations: [getNodeAutoInstrumentations()],\n// Make sure to add all detectors you need here!\nresourceDetectors: [envDetector, processDetector, dockerCGroupV1Detector],\n});\nsdk.start();\n</code></pre></p> <p>\u91cd\u65b0\u6784\u5efa\u955c\u50cf\u5e76\u518d\u6b21\u8fd0\u884c\u5bb9\u5668:</p> <pre><code>docker run --rm -p 8080:8080 nodejs-otel-getting-started\n@opentelemetry/api: Registered a global for diag v1.2.0.\n...\nListening for requests on http://localhost:8080\nEnvDetector found resource. Resource { attributes: {} }\nProcessDetector found resource. Resource {\nattributes: {\n'process.pid': 1,\n    'process.executable.name': 'node',\n    'process.command': '/usr/src/app/app.js',\n    'process.command_line': '/usr/local/bin/node /usr/src/app/app.js',\n    'process.runtime.version': '18.9.0',\n    'process.runtime.name': 'nodejs',\n    'process.runtime.description': 'Node.js'\n}\n}\nDockerCGroupV1Detector found resource. Resource {\nattributes: {\n'container.id': '654d0670317b9a2d3fc70cbe021c80ea15339c4711fb8e8b3aa674143148d84e'\n}\n}\n...\n</code></pre>"},{"location":"docs/instrumentation/js/resources/#_6","title":"\u4e0b\u4e00\u6b65","text":"<p>\u60a8\u53ef\u4ee5\u5728\u914d\u7f6e\u4e2d\u6dfb\u52a0\u66f4\u591a\u7684\u8d44\u6e90\u68c0\u6d4b\u5668\uff0c\u4f8b\u5982\u83b7\u53d6\u6709\u5173\u60a8\u7684Cloud\u73af\u5883\u6216Deployment\u7684\u8be6\u7ec6\u4fe1\u606f\u3002 \u4f60\u53ef\u4ee5\u5728\u8fd9\u91cc\u627e\u5230\u4e00\u4efd\u6e05\u5355.</p>"},{"location":"docs/instrumentation/js/sampling/","title":"\u91c7\u6837","text":"<p>\u91c7\u6837\u662f\u4e00\u4e2a\u9650\u5236\u7cfb\u7edf\u4ea7\u751f\u7684\u8ff9\u7ebf\u6570\u91cf\u7684\u8fc7\u7a0b\u3002 JavaScript SDK\u63d0\u4f9b\u4e86\u51e0\u4e2ahead-sampling\u3002</p>"},{"location":"docs/instrumentation/js/sampling/#_1","title":"\u9ed8\u8ba4\u7684\u884c\u4e3a","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6240\u6709\u8de8\u5ea6\u8fdb\u884c\u91c7\u6837\uff0c\u56e0\u6b64\u5bf9100%\u7684\u8ddf\u8e2a\u8fdb\u884c\u91c7\u6837\u3002 \u5982\u679c\u60a8\u4e0d\u9700\u8981\u7ba1\u7406\u6570\u636e\u91cf\uff0c\u5219\u4e0d\u5fc5\u8bbe\u7f6e\u91c7\u6837\u5668\u3002</p>"},{"location":"docs/instrumentation/js/sampling/#traceidratiobasedsampler","title":"TraceIDRatioBasedSampler","text":"<p>\u91c7\u6837\u65f6\uff0c\u6700\u5e38\u7528\u7684\u5934\u90e8\u91c7\u6837\u5668\u662fTraceIdRatioBasedSampler\u3002 \u5b83\u786e\u5b9a\u5730\u91c7\u6837\u60a8\u4f5c\u4e3a\u53c2\u6570\u4f20\u5165\u7684\u8ddf\u8e2a\u7684\u767e\u5206\u6bd4\u3002</p>"},{"location":"docs/instrumentation/js/sampling/#_2","title":"\u73af\u5883\u53d8\u91cf","text":"<p>\u4f60\u53ef\u4ee5\u7528\u73af\u5883\u53d8\u91cf\u914d\u7f6eTraceIdRatioBasedSampler:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"traceidratio\"\nexport OTEL_TRACES_SAMPLER_ARG=\"0.1\"\n</code></pre> <p>This tells the SDK to sample spans such that only 10% of traces get created.</p>"},{"location":"docs/instrumentation/js/sampling/#nodejs","title":"Node.js","text":"<p>\u60a8\u8fd8\u53ef\u4ee5\u5728\u4ee3\u7801\u4e2d\u914d\u7f6eTraceIdRatioBasedSampler\u3002 \u4e0b\u9762\u662fNode.js\u7684\u4e00\u4e2a\u4f8b\u5b50:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>import { TraceIdRatioBasedSampler } from '@opentelemetry/sdk-trace-node';\nconst samplePercentage = 0.1;\nconst sdk = new NodeSDK({\n// Other SDK configuration parameters go here\nsampler: new TraceIdRatioBasedSampler(samplePercentage),\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>const { TraceIdRatioBasedSampler } = require('@opentelemetry/sdk-trace-node');\nconst samplePercentage = 0.1;\nconst sdk = new NodeSDK({\n// Other SDK configuration parameters go here\nsampler: new TraceIdRatioBasedSampler(samplePercentage),\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/sampling/#browser","title":"Browser","text":"<p>You can also configure the TraceIdRatioBasedSampler in code. Here's an example for browser apps:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>import { WebTracerProvider, TraceIdRatioBasedSampler } from '@opentelemetry/sdk-trace-web';\nconst samplePercentage = 0.1;\nconst provider = new WebTracerProvider({\nsampler: new TraceIdRatioBasedSampler(samplePercentage),\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>const { WebTracerProvider, TraceIdRatioBasedSampler } = require('@opentelemetry/sdk-trace-web');\nconst samplePercentage = 0.1;\nconst provider = new WebTracerProvider({\nsampler: new TraceIdRatioBasedSampler(samplePercentage),\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/serverless/","title":"\u65e0\u670d\u52a1","text":"<p>\u672c\u6307\u5357\u5c55\u793a\u4e86\u5982\u4f55\u5f00\u59cb\u4f7f\u7528OpenTelemetry\u5de5\u5177\u5e93\u8ddf\u8e2a\u65e0\u670d\u52a1\u5668\u51fd\u6570\u3002</p>"},{"location":"docs/instrumentation/js/serverless/#aws-lambda","title":"AWS Lambda","text":"<p>The following show how to use Lambda wrappers with OpenTelemetry to instrument AWS Lambda functions and send traces to a configured backend.</p> <p>If you are interested in a plug and play user experience, see OpenTelemetry Lambda Layers.</p>"},{"location":"docs/instrumentation/js/serverless/#dependencies","title":"Dependencies","text":"<p>First, create an empty package.json:</p> <pre><code>npm init -y\n</code></pre> <p>Then install the required dependencies:</p> <pre><code>npm install \\\n@opentelemetry/api \\\n@opentelemetry/auto-instrumentations-node \\\n@opentelemetry/exporter-trace-otlp-http \\\n@opentelemetry/instrumentation \\\n@opentelemetry/sdk-trace-base \\\n@opentelemetry/sdk-trace-node\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#aws-lambda-wrapper-code","title":"AWS Lambda wrapper code","text":"<p>This file contains all the OpenTelemetry logic, which enables tracing. Save the following code as <code>lambda-wrapper.js</code>.</p> <pre><code>/* lambda-wrapper.js */\nconst api = require('@opentelemetry/api');\nconst { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');\nconst {\nOTLPTraceExporter,\n} = require('@opentelemetry/exporter-trace-otlp-http');\nconst { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');\nconst { registerInstrumentations } = require('@opentelemetry/instrumentation');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\napi.diag.setLogger(new api.DiagConsoleLogger(), api.DiagLogLevel.ALL);\nconst provider = new NodeTracerProvider();\nconst collectorOptions = {\nurl: '&lt;backend_url&gt;',\n};\nconst spanProcessor = new BatchSpanProcessor(\nnew OTLPTraceExporter(collectorOptions)\n);\nprovider.addSpanProcessor(spanProcessor);\nprovider.register();\nregisterInstrumentations({\ninstrumentations: [\ngetNodeAutoInstrumentations({\n'@opentelemetry/instrumentation-aws-lambda': {\ndisableAwsContextPropagation: true,\n},\n}),\n],\n});\n</code></pre> <p>Replace <code>&lt;backend_url&gt;</code> with the URL of your favorite backend to export all traces to it. If you don't have one setup already, you can check out Jaeger or Zipkin.</p> <p>Note that <code>disableAwsContextPropagation</code> is set to true. The reason for this is that the Lambda instrumentation tries to use the X-Ray context headers by default, unless active tracing is enabled for this function, this results in a non-sampled context, which creates a <code>NonRecordingSpan</code>.</p> <p>More details can be found in the instrumentation documentation.</p>"},{"location":"docs/instrumentation/js/serverless/#aws-lambda-function-handler","title":"AWS Lambda function handler","text":"<p>Now that you have a Lambda wrapper, create a simple handler that serves as a Lambda function. Save the following code as <code>handler.js</code>.</p> <pre><code>/* handler.js */\n'use strict';\nconst https = require('https');\nfunction getRequest() {\nconst url = 'https://opentelemetry.io/';\nreturn new Promise((resolve, reject) =&gt; {\nconst req = https.get(url, (res) =&gt; {\nresolve(res.statusCode);\n});\nreq.on('error', (err) =&gt; {\nreject(new Error(err));\n});\n});\n}\nexports.handler = async (event) =&gt; {\ntry {\nconst result = await getRequest();\nreturn {\nstatusCode: result,\n};\n} catch (error) {\nreturn {\nstatusCode: 400,\nbody: error.message,\n};\n}\n};\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#deployment","title":"Deployment","text":"<p>There are multiple ways of deploying your Lambda function:</p> <ul> <li>AWS Console</li> <li>AWS CLI</li> <li>Serverless Framework</li> <li>Terraform</li> </ul> <p>Here we will be using Serverless Framework, more details can be found in the Setting Up Serverless Framework guide.</p> <p>Create a file called <code>serverless.yml</code>:</p> <pre><code>service: lambda-otel-native\nframeworkVersion: '3'\nprovider:\nname: aws\nruntime: nodejs14.x\nregion: '&lt;your-region&gt;'\nenvironment:\nNODE_OPTIONS: --require lambda-wrapper\nfunctions:\nlambda-otel-test:\nhandler: handler.hello\n</code></pre> <p>For OpenTelemetry to work properly, <code>lambda-wrapper.js</code> must be included before any other file: the <code>NODE_OPTIONS</code> setting ensures this.</p> <p>Note if you are not using Serverless Framework to deploy your Lambda function, you must manually add this environment variable using the AWS Console UI.</p> <p>Finally, run the following command to deploy the project to AWS:</p> <pre><code>serverless deploy\n</code></pre> <p>You can now invoke the newly deployed Lambda function by using the AWS Console UI. You should expect to see spans related to the invocation of the Lambda function.</p>"},{"location":"docs/instrumentation/js/serverless/#visiting-the-backend","title":"Visiting the backend","text":"<p>You should now be able to view traces produced by OpenTelemetry from your Lambda function in the backend!</p>"},{"location":"docs/instrumentation/js/serverless/#gcp-function","title":"GCP function","text":"<p>The following shows how to instrument HTTP triggered function using the Google Cloud Platform (GCP) UI.</p>"},{"location":"docs/instrumentation/js/serverless/#creating-function","title":"Creating function","text":"<p>Login to GCP and create or select a project where your function should be placed. In the side menu go to Serverless and select Cloud Functions. Next, click on Create Function, and select 2nd generation for your environment, provide a function name and select your region.</p>"},{"location":"docs/instrumentation/js/serverless/#setup-environment-variable-for-otelwrapper","title":"Setup environment variable for otelwrapper","text":"<p>If closed, open the Runtime, build, connections and security settings menu and scroll down and add the environment variable <code>NODE_OPTIONS</code> with the following value:</p> <pre><code>--require ./otelwrapper.js\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#select-runtime","title":"Select runtime","text":"<p>On the next screen (Code), select Node.js version 16 for your runtime.</p>"},{"location":"docs/instrumentation/js/serverless/#establish-otel-wrapper","title":"Establish OTel wrapper","text":"<p>Create a new file called <code>otelwrapper.js</code>, that will be used to instrument your service. Please make sure that you provide a <code>SERVICE_NAME</code> and that you set the <code>&lt;address for your backend&gt;</code>.</p> <pre><code>/* otelwrapper.js */\nconst { Resource } = require('@opentelemetry/resources');\nconst {\nSemanticResourceAttributes,\n} = require('@opentelemetry/semantic-conventions');\nconst api = require('@opentelemetry/api');\nconst { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');\nconst {\nOTLPTraceExporter,\n} = require('@opentelemetry/exporter-trace-otlp-http');\nconst { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');\nconst { registerInstrumentations } = require('@opentelemetry/instrumentation');\nconst {\ngetNodeAutoInstrumentations,\n} = require('@opentelemetry/auto-instrumentations-node');\nconst providerConfig = {\nresource: new Resource({\n[SemanticResourceAttributes.SERVICE_NAME]: '&lt;your function name&gt;',\n}),\n};\napi.diag.setLogger(new api.DiagConsoleLogger(), api.DiagLogLevel.ALL);\nconst provider = new NodeTracerProvider(providerConfig);\nconst collectorOptions = {\nurl: '&lt;address for your backend&gt;',\n};\nconst spanProcessor = new BatchSpanProcessor(\nnew OTLPTraceExporter(collectorOptions)\n);\nprovider.addSpanProcessor(spanProcessor);\nprovider.register();\nregisterInstrumentations({\ninstrumentations: [getNodeAutoInstrumentations()],\n});\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#establish-packagejson","title":"Establish package.json","text":"<p>Add the following content to your package.json:</p> <pre><code>{\n\"dependencies\": {\n\"@google-cloud/functions-framework\": \"^3.0.0\",\n\"@opentelemetry/api\": \"^1.3.0\",\n\"@opentelemetry/auto-instrumentations-node\": \"^0.35.0\",\n\"@opentelemetry/exporter-trace-otlp-http\": \"^0.34.0\",\n\"@opentelemetry/instrumentation\": \"^0.34.0\",\n\"@opentelemetry/sdk-node\": \"^0.34.0\",\n\"@opentelemetry/sdk-trace-base\": \"^1.8.0\",\n\"@opentelemetry/sdk-trace-node\": \"^1.8.0\",\n\"@opentelemetry/resources\": \"^1.8.0\",\n\"@opentelemetry/semantic-conventions\": \"^1.8.0\"\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#add-http-call-to-function","title":"Add HTTP call to function","text":"<p>The following code makes a call to the OpenTelemetry web site to demonstrate an outbound call.</p> <pre><code>/* index.js */\nconst functions = require('@google-cloud/functions-framework');\nconst https = require('https');\nfunctions.http('helloHttp', (req, res) =&gt; {\nlet url = 'https://opentelemetry.io/';\nhttps\n.get(url, (response) =&gt; {\nres.send(`Response ${response.body}!`);\n})\n.on('error', (e) =&gt; {\nres.send(`Error ${e}!`);\n});\n});\n</code></pre>"},{"location":"docs/instrumentation/js/serverless/#backend","title":"Backend","text":"<p>If you run OTel collector in GCP VM you are likely to need to create VPC access connector to be able to send traces.</p>"},{"location":"docs/instrumentation/js/serverless/#deploy","title":"Deploy","text":"<p>Select Deploy in UI and await deployment to be ready.</p>"},{"location":"docs/instrumentation/js/serverless/#testing","title":"Testing","text":"<p>You can test the function using cloud shell from test tab.</p>"},{"location":"docs/instrumentation/js/automatic/","title":"\u81ea\u52a8 Instrumentation","text":"<p>\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u4efb\u4f55 Node.js \u5e94\u7528\u7a0b\u5e8f\uff0c\u5e76\u4ece\u8bb8\u591a\u6d41\u884c\u7684\u5e93\u548c\u6846\u67b6\u4e2d\u6355\u83b7 \u9065\u6d4b\u6570\u636e\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4ee3\u7801\u66f4\u6539\u3002</p>"},{"location":"docs/instrumentation/js/automatic/#_1","title":"\u8bbe\u7f6e","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5b89\u88c5\u76f8\u5e94\u7684\u5305\u3002</p> <pre><code>npm install --save @opentelemetry/api\nnpm install --save @opentelemetry/auto-instrumentations-node\n</code></pre> <p><code>@opentelemetry/api</code> and <code>@opentelemetry/auto-instrumentations-node</code>\u5305\u5b89\u88c5 api\u3001SDK \u548c\u63d2\u88c5\u5de5\u5177\u3002</p>"},{"location":"docs/instrumentation/js/automatic/#_2","title":"\u914d\u7f6e\u6a21\u5757","text":"<p>\u8be5\u6a21\u5757\u5177\u6709\u9ad8\u5ea6\u53ef\u914d\u7f6e\u6027\u3002</p> <p>\u4e00\u79cd\u9009\u62e9\u662f\u901a\u8fc7\u4f7f\u7528<code>env</code> \u4ece CLI \u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\u6765\u914d\u7f6e\u6a21\u5757:</p> <pre><code>env OTEL_TRACES_EXPORTER=otlp OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=your-endpoint \\\nnode --require @opentelemetry/auto-instrumentations-node/register app.js\n</code></pre> <p>\u6216\u8005\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 <code>export</code> \u6765\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf:</p> <pre><code>export OTEL_TRACES_EXPORTER=\"otlp\"\nexport OTEL_METRICS_EXPORTER=\"otlp\"\nexport OTEL_EXPORTER_OTLP_ENDPOINT=\"your-endpoint\"\nexport OTEL_NODE_RESOURCE_DETECTORS=\"env,host,os\"\nexport OTEL_SERVICE_NAME=\"your-service-name\"\nexport NODE_OPTIONS=\"--require @opentelemetry/auto-instrumentations-node/register\"\nnode app.js\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u6240\u6709 SDK \u8d44\u6e90\u68c0\u6d4b\u5668\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u73af\u5883\u53d8 \u91cf<code>OTEL_NODE_RESOURCE_DETECTORS</code> \u6765\u53ea\u542f\u7528\u67d0\u4e9b\u68c0\u6d4b\u5668\uff0c\u6216\u8005\u5b8c\u5168\u7981\u7528\u5b83\u4eec\u3002</p> <p>\u8981\u67e5\u770b\u914d\u7f6e\u9009\u9879\u7684\u5168\u90e8\u8303\u56f4\uff0c\u8bf7\u53c2\u89c1\u6a21\u5757\u914d\u7f6e.</p>"},{"location":"docs/instrumentation/js/automatic/#_3","title":"\u652f\u6301\u7684\u5e93\u548c\u6846\u67b6","text":"<p>\u8bb8\u591a\u6d41\u884c\u7684 Node.js \u5e93\u90fd\u662f\u81ea\u52a8\u68c0\u6d4b\u7684\u3002\u6709\u5173\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2 \u89c1\u652f\u6301\u7684\u4eea\u5668.</p>"},{"location":"docs/instrumentation/js/automatic/#_4","title":"\u6545\u969c\u6392\u9664","text":"<p>\u60a8\u53ef\u4ee5\u901a\u8fc7\u5c06<code>OTEL_LOG_LEVEL</code>\u73af\u5883\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u4ee5\u4e0b\u5176\u4e2d\u4e00\u4e2a\u6765\u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b:</p> <ul> <li><code>none</code></li> <li><code>error</code></li> <li><code>warn</code></li> <li><code>info</code></li> <li><code>debug</code></li> <li><code>verbose</code></li> <li><code>all</code></li> <li>\u9ed8\u8ba4\u7ea7\u522b\u662f <code>info</code>\u3002</li> </ul> <p>Notes</p> <ul> <li>\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5efa\u8bae\u5c06 <code>OTEL_LOG_LEVEL</code> \u8bbe\u7f6e\u4e3a <code>info</code>\u3002</li> <li>\u65e0\u8bba\u73af\u5883\u6216\u8c03\u8bd5\u7ea7\u522b\u5982\u4f55\uff0c\u65e5\u5fd7\u603b\u662f\u88ab\u53d1\u9001\u5230\u63a7\u5236\u53f0\u3002</li> <li>\u8c03\u8bd5\u65e5\u5fd7\u975e\u5e38\u5197\u957f\uff0c\u53ef\u80fd\u4f1a\u5bf9\u5e94\u7528\u7a0b\u5e8f\u7684\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u53ea\u5728\u9700\u8981\u65f6\u542f\u7528\u8c03\u8bd5\u65e5\u5fd7\u3002</li> </ul>"},{"location":"docs/instrumentation/js/automatic/module-config/","title":"\u81ea\u52a8\u63d2\u88c5\u914d\u7f6e","text":"<p>\u8fd9\u4e2a\u6a21\u5757\u53ef\u4ee5\u901a\u8fc7\u8bbe \u7f6e\u73af\u5883\u53d8\u91cf\u8fdb \u884c\u9ad8\u5ea6\u914d\u7f6e\u3002\u81ea\u52a8\u68c0\u6d4b\u884c\u4e3a\u7684\u8bb8\u591a\u65b9\u9762\u53ef\u4ee5\u6839\u636e\u60a8\u7684\u9700\u8981\u8fdb\u884c\u914d\u7f6e\uff0c\u4f8b\u5982\u8d44\u6e90\u68c0\u6d4b\u5668\u3001\u5bfc\u51fa \u5668\u3001\u8ddf\u8e2a\u4e0a\u4e0b\u6587\u4f20\u64ad\u5934\u7b49\u7b49\u3002</p>"},{"location":"docs/instrumentation/js/automatic/module-config/#_1","title":"\u81ea\u52a8\u63d2\u88c5\u914d\u7f6e","text":"<p>SDK \u548c\u5bfc\u51fa\u5668\u914d\u7f6e\u53ef\u4ee5\u4f7f\u7528\u73af\u5883\u53d8\u91cf\u8fdb\u884c\u8bbe\u7f6e\u3002\u66f4\u591a\u4fe1\u606f\u53ef\u4ee5 \u5728\u8fd9\u91cc\u627e\u5230\u3002</p>"},{"location":"docs/instrumentation/js/automatic/module-config/#sdk","title":"SDK \u8d44\u6e90\u68c0\u6d4b\u5668\u914d\u7f6e","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u8be5\u6a21\u5757\u5c06\u542f\u7528\u6240\u6709 SDK \u8d44\u6e90\u68c0\u6d4b\u5668\u3002\u4f60\u53ef\u4ee5\u4f7f \u7528<code>OTEL_NODE_RESOURCE_DETECTORS</code>\u73af\u5883\u53d8\u91cf\u6765\u53ea\u542f\u7528\u67d0\u4e9b\u68c0\u6d4b\u5668\uff0c\u6216\u8005\u5b8c\u5168\u7981\u7528\u5b83\u4eec:</p> <ul> <li><code>env</code></li> <li><code>host</code></li> <li><code>os</code></li> <li><code>process</code></li> <li><code>container</code></li> <li><code>alibaba</code></li> <li><code>aws</code></li> <li><code>gcp</code></li> <li><code>all</code> - \u542f\u7528\u6240\u6709\u8d44\u6e90\u68c0\u6d4b\u5668</li> <li><code>none</code> - \u7981\u7528\u8d44\u6e90\u68c0\u6d4b</li> </ul> <p>\u4f8b\u5982\uff0c\u8981\u53ea\u542f\u7528<code>env</code> \u548c <code>host</code>\u63a2\u6d4b\u5668\uff0c\u4f60\u53ef\u4ee5\u8bbe\u7f6e:</p> <pre><code>OTEL_NODE_RESOURCE_DETECTORS=env,host\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/","title":"\u5165\u95e8","text":"<p>\u8fd9\u4e24\u4e2a\u5173\u4e8eNode.js\u548c\u6d4f\u89c8\u5668\u7684\u6307\u5357\u4f7f\u7528\u4e86\u7b80\u5355\u7684javascript\u793a\u4f8b\u6765\u5e2e\u52a9\u4f60\u5f00\u59cb\u4f7f\u7528OpenTelemetry\u3002 \u4e24\u8005\u90fd\u5c06\u5411\u60a8\u5c55\u793a\u4ee5\u4e0b\u6b65\u9aa4:</p> <ul> <li>\u5b89\u88c5\u6240\u9700\u7684OpenTelemetry\u5e93</li> <li>\u521d\u59cb\u5316\u5168\u5c40\u8ddf\u8e2a\u5668</li> <li>\u521d\u59cb\u5316\u5e76\u6ce8\u518c\u4e00\u4e2aspan export</li> </ul>"},{"location":"docs/instrumentation/js/getting-started/browser/","title":"Browser","text":"<p>While this guide uses the example application presented below, the steps to instrument your own application should be similar.</p>"},{"location":"docs/instrumentation/js/getting-started/browser/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<p>Ensure that you have the following installed locally:</p> <ul> <li>Node.js</li> <li>TypeScript, if you will be using   TypeScript.</li> </ul>"},{"location":"docs/instrumentation/js/getting-started/browser/#_2","title":"\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f","text":"<p>This is a very simple guide, if you'd like to see more complex examples go to examples/opentelemetry-web.</p> <p>Copy the following file into an empty directory and call it <code>index.html</code>.</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\" /&gt;\n&lt;title&gt;Document Load Instrumentation Example&lt;/title&gt;\n&lt;base href=\"/\" /&gt;\n&lt;!--\n      https://www.w3.org/TR/trace-context/\n      Set the `traceparent` in the server's HTML template code. It should be\n      dynamically generated server side to have the server's request trace Id,\n      a parent span Id that was set on the server's request span, and the trace\n      flags to indicate the server's sampling decision\n      (01 = sampled, 00 = notsampled).\n      '{version}-{traceId}-{spanId}-{sampleDecision}'\n    --&gt;\n&lt;meta\nname=\"traceparent\"\ncontent=\"00-ab42124a3c573678d4d8b21ba52df3bf-d21f7bc17caa5aba-01\"\n/&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    Example of using Web Tracer with document load instrumentation with console\n    exporter and collector exporter\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/browser/#_3","title":"\u5b89\u88c5","text":"<p>To create traces in the browser, you will need <code>@opentelemetry/sdk-trace-web</code>, and the instrumentation <code>@opentelemetry/instrumentation-document-load</code>:</p> <pre><code>npm init -y\nnpm install @opentelemetry/api \\\n@opentelemetry/sdk-trace-web \\\n@opentelemetry/instrumentation-document-load \\\n@opentelemetry/context-zone\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/browser/#_4","title":"\u521d\u59cb\u5316\u4e0e\u914d\u7f6e","text":"<p>If you are coding in TypeScript, then run the following command:</p> <pre><code>tsc --init\n</code></pre> <p>Then acquire parcel, which will (among other things) let you work in Typescript.</p> <pre><code>npm install --save-dev parcel\n</code></pre> <p>Create an empty code file named <code>document-load</code> with a <code>.ts</code> or <code>.js</code> extension, as appropriate, based on the language you've chosen to write your app in. Add the following code to your HTML right before the <code>&lt;/body&gt;</code> closing tag:</p> <p>{{&lt; tabpane lang=html persistLang=false &gt;}} {{&lt; tab TypeScript &gt;}}</p> <p>{{&lt; /tab &gt;}} {{&lt; tab JavaScript &gt;}}</p> <p>{{&lt; /tab &gt;}} {{&lt; /tabpane &gt;}}</p> <p>We will add some code that will trace the document load timings and output those as OpenTelemetry Spans.</p>"},{"location":"docs/instrumentation/js/getting-started/browser/#_5","title":"\u521b\u5efa\u8ddf\u8e2a\u7a0b\u5e8f\u63d0\u4f9b\u7a0b\u5e8f","text":"<p>Add the following code to the <code>document-load.ts|js</code> to create a tracer provider, which brings the instrumentation to trace document load:</p> <pre><code>/* document-load.ts|js file - the code snippet is the same for both the languages */\nimport { WebTracerProvider } from '@opentelemetry/sdk-trace-web';\nimport { DocumentLoadInstrumentation } from '@opentelemetry/instrumentation-document-load';\nimport { ZoneContextManager } from '@opentelemetry/context-zone';\nimport { registerInstrumentations } from '@opentelemetry/instrumentation';\nconst provider = new WebTracerProvider();\nprovider.register({\n// Changing default contextManager to use ZoneContextManager - supports asynchronous operations - optional\ncontextManager: new ZoneContextManager(),\n});\n// Registering instrumentations\nregisterInstrumentations({\ninstrumentations: [new DocumentLoadInstrumentation()],\n});\n</code></pre> <p>Now build the app with parcel:</p> <pre><code>npx parcel index.html\n</code></pre> <p>and open the development webserver (e.g. at <code>http://localhost:1234</code>) to see if your code works.</p> <p>There will be no output of traces yet, for this we need to add an exporter.</p>"},{"location":"docs/instrumentation/js/getting-started/browser/#_6","title":"\u521b\u5efa\u5bfc\u51fa\u5668","text":"<p>In the following example, we will use the <code>ConsoleSpanExporter</code> which prints all spans to the console.</p> <p>In order to visualize and analyze your traces, you will need to export them to a tracing backend. Follow these instructions for setting up a backend and exporter.</p> <p>You may also want to use the <code>BatchSpanProcessor</code> to export spans in batches in order to more efficiently use resources.</p> <p>To export traces to the console, modify <code>document-load.ts|js</code> so that it matches the following code snippet:</p> <pre><code>/* document-load.ts|js file - the code is the same for both the languages */\nimport {\nConsoleSpanExporter,\nSimpleSpanProcessor,\n} from '@opentelemetry/sdk-trace-base';\nimport { WebTracerProvider } from '@opentelemetry/sdk-trace-web';\nimport { DocumentLoadInstrumentation } from '@opentelemetry/instrumentation-document-load';\nimport { ZoneContextManager } from '@opentelemetry/context-zone';\nimport { registerInstrumentations } from '@opentelemetry/instrumentation';\nconst provider = new WebTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporter()));\nprovider.register({\n// Changing default contextManager to use ZoneContextManager - supports asynchronous operations - optional\ncontextManager: new ZoneContextManager(),\n});\n// Registering instrumentations\nregisterInstrumentations({\ninstrumentations: [new DocumentLoadInstrumentation()],\n});\n</code></pre> <p>\u73b0\u5728\uff0c\u91cd\u65b0\u6784\u5efa\u5e94\u7528\u7a0b\u5e8f\u5e76\u518d\u6b21\u6253\u5f00\u6d4f\u89c8\u5668\u3002 \u5728\u5f00\u53d1\u4eba\u5458\u5de5\u5177\u680f\u7684\u63a7\u5236\u53f0\u4e2d\uff0c\u60a8\u5e94\u8be5\u770b\u5230\u6b63\u5728\u5bfc\u51fa\u4e00\u4e9b\u8ddf\u8e2a:</p> <pre><code>{\n\"traceId\": \"ab42124a3c573678d4d8b21ba52df3bf\",\n\"parentId\": \"cfb565047957cb0d\",\n\"name\": \"documentFetch\",\n\"id\": \"5123fc802ffb5255\",\n\"kind\": 0,\n\"timestamp\": 1606814247811266,\n\"duration\": 9390,\n\"attributes\": {\n\"component\": \"document-load\",\n\"http.response_content_length\": 905\n},\n\"status\": {\n\"code\": 0\n},\n\"events\": [\n{\n\"name\": \"fetchStart\",\n\"time\": [1606814247, 811266158]\n},\n{\n\"name\": \"domainLookupStart\",\n\"time\": [1606814247, 811266158]\n},\n{\n\"name\": \"domainLookupEnd\",\n\"time\": [1606814247, 811266158]\n},\n{\n\"name\": \"connectStart\",\n\"time\": [1606814247, 811266158]\n},\n{\n\"name\": \"connectEnd\",\n\"time\": [1606814247, 811266158]\n},\n{\n\"name\": \"requestStart\",\n\"time\": [1606814247, 819101158]\n},\n{\n\"name\": \"responseStart\",\n\"time\": [1606814247, 819791158]\n},\n{\n\"name\": \"responseEnd\",\n\"time\": [1606814247, 820656158]\n}\n]\n}\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/browser/#_7","title":"\u6dfb\u52a0\u7684\u8bbe\u5907","text":"<p>\u5982\u679c\u4f60\u60f3\u68c0\u6d4bAJAX\u8bf7\u6c42\u3001\u7528\u6237\u4ea4\u4e92\u548c\u5176\u4ed6\uff0c\u4f60\u53ef\u4ee5\u4e3a\u5b83\u4eec\u6ce8\u518c\u989d\u5916\u7684\u68c0\u6d4b:</p> <pre><code>registerInstrumentations({\ninstrumentations: [\nnew UserInteractionInstrumentation(),\nnew XMLHttpRequestInstrumentation(),\n],\n});\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/browser/#web","title":"\u7528\u4e8eWeb\u7684\u5143\u5305","text":"<p>\u8981\u5229\u7528\u6700\u5e38\u89c1\u7684\u4eea\u5668\u90fd\u5728\u4e00\u4e2a\u4f60\u53ef\u4ee5\u7b80\u5355\u5730\u4f7f\u7528\u5f00\u653e\u9065\u6d4b\u5143\u5305\u7684web</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/","title":"Node.js","text":"<p>\u672c\u9875\u5c06\u5411\u60a8\u5c55\u793a\u5982\u4f55\u5728 Node.js \u4e2d\u5f00\u59cb\u4f7f\u7528 OpenTelemetry\u3002</p> <p>\u60a8\u5c06\u5b66\u4e60\u5982\u4f55\u81ea\u52a8\u68c0\u6d4b\u4e00\u4e2a\u7b80\u5355\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u4e00\u79cd \u5c06traces\u3001metrics\u548clogs\u53d1\u9001\u5230\u63a7\u5236\u53f0\u7684\u65b9\u5f0f\u3002</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_1","title":"\u5148\u51b3\u6761\u4ef6","text":"<p>\u786e\u4fdd\u5728\u672c\u5730\u5b89\u88c5\u4e86\u4ee5\u4e0b\u8f6f\u4ef6:</p> <ul> <li>Node.js</li> <li>TypeScript, \u5982\u679c\u4f60\u8981\u4f7f\u7528   TypeScript\u3002</li> </ul>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_2","title":"\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f","text":"<p>\u4e0b\u9762\u7684\u793a\u4f8b\u4f7f\u7528\u4e00\u4e2a\u57fa\u672c\u7684Express\u5e94\u7528\u7a0b\u5e8f\u3002\u5982\u679c\u4f60\u4e0d\u4f7f\u7528 Express\uff0c\u6ca1\u5173\u7cfb\u2014\u2014\u4f60\u4e5f\u53ef\u4ee5\u5c06 OpenTelemetry JavaScript \u4e0e\u5176\u4ed6 web \u6846\u67b6\u4e00\u8d77\u4f7f\u7528\uff0c\u6bd4 \u5982 Koa \u548c Nest.JS\u3002\u8981\u83b7\u5f97\u652f\u6301\u6846\u67b6\u7684\u5b8c\u6574\u5e93\u5217\u8868\uff0c\u8bf7\u53c2 \u89c1registry\u3002</p> <p>\u6709\u5173\u66f4\u8be6\u7ec6\u7684\u793a\u4f8b\uff0c\u8bf7\u53c2\u89c1\u793a\u4f8b.</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_3","title":"\u4f9d\u8d56\u5173\u7cfb","text":"<p>\u9996\u5148\uff0c\u5728\u65b0\u76ee\u5f55\u4e2d\u8bbe\u7f6e\u4e00\u4e2a\u7a7a\u7684<code>package.json</code>:</p> <pre><code>npm init -y\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u5b89\u88c5 Express \u4f9d\u8d56\u9879\u3002</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} npm install typescript \\   ts-node \\   types/node \\   express \\   types/express {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} npm install express {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#http","title":"\u521b\u5efa\u5e76\u542f\u52a8 HTTP \u670d\u52a1\u5668","text":"<p>Create a file named <code>app.ts</code> (or <code>app.js</code> if not using typescript) and add the following code to it:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>/*app.ts*/\nimport express, { Express } from \"express\";\nconst PORT: number = parseInt(process.env.PORT || \"8080\");\nconst app: Express = express();\nfunction getRandomNumber(min: number, max: number) {\nreturn Math.floor(Math.random() * (max - min) + min);\n}\napp.get(\"/rolldice\", (req, res) =&gt; {\nres.send(getRandomNumber(1, 6).toString());\n});\napp.listen(PORT, () =&gt; {\nconsole.log(`Listening for requests on http://localhost:${PORT}`);\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>/*app.js*/\nconst express = require(\"express\");\nconst PORT = parseInt(process.env.PORT || \"8080\");\nconst app = express();\nfunction getRandomNumber(min, max) {\nreturn Math.floor(Math.random() * (max - min) + min);\n}\napp.get(\"/rolldice\", (req, res) =&gt; {\nres.send(getRandomNumber(1, 6).toString());\n});\napp.listen(PORT, () =&gt; {\nconsole.log(`Listening for requests on http://localhost:${PORT}`);\n});\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane&gt;}}</p> <p>Run the application with the following command and open http://localhost:8080/rolldice in your web browser to ensure it is working.</p> <p>{{&lt; tabpane lang=console persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>$ npx ts-node app.ts\nListening for requests on http://localhost:8080\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>$ node app.js\nListening for requests on http://localhost:8080\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_4","title":"\u63d2\u88c5","text":"<p>The following shows how to install, initialize, and run an application instrumented with OpenTelemetry.</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_5","title":"\u4f9d\u8d56","text":"<p>First, install the Node SDK and autoinstrumentations package.</p> <p>The Node SDK lets you initialize OpenTelemetry with several configuration defaults that are correct for the majority of use cases.</p> <p>The <code>auto-instrumentations-node</code> package installs instrumentation packages that will automatically create spans corresponding to code called in libraries. In this case, it provides instrumentation for Express, letting the example app automatically create spans for each incoming request.</p> <pre><code>npm install @opentelemetry/sdk-node \\\n@opentelemetry/api \\\n@opentelemetry/auto-instrumentations-node \\\n@opentelemetry/sdk-metrics\n</code></pre> <p>To find all autoinstrumentation modules, you can look at the registry.</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#_6","title":"\u8bbe\u7f6e","text":"<p>The instrumentation setup and configuration must be run before your application code. One tool commonly used for this task is the --require flag.</p> <p>Create a file named <code>instrumentation.ts</code> (or <code>instrumentation.js</code> if not using typescript) , which will contain your instrumentation setup code.</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>/*instrumentation.ts*/\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { ConsoleSpanExporter } from '@opentelemetry/sdk-trace-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\nimport { PeriodicExportingMetricReader, ConsoleMetricExporter } from '@opentelemetry/sdk-metrics';\nconst sdk = new NodeSDK({\ntraceExporter: new ConsoleSpanExporter(),\nmetricReader: new PeriodicExportingMetricReader({\nexporter: new ConsoleMetricExporter()\n}),\ninstrumentations: [getNodeAutoInstrumentations()]\n});\nsdk\n.start()\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>/*instrumentation.js*/\n// Require dependencies\nconst { NodeSDK } = require('@opentelemetry/sdk-node');\nconst { ConsoleSpanExporter } = require('@opentelemetry/sdk-trace-node');\nconst { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');\nconst { PeriodicExportingMetricReader, ConsoleMetricExporter } = require('@opentelemetry/sdk-metrics');\nconst sdk = new NodeSDK({\ntraceExporter: new ConsoleSpanExporter(),\nmetricReader: new PeriodicExportingMetricReader({\nexporter: new ConsoleMetricExporter()\n}),\ninstrumentations: [getNodeAutoInstrumentations()]\n});\nsdk\n.start()\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#run-the-instrumented-app","title":"Run the instrumented app","text":"<p>Now you can run your application as you normally would, but you can use the <code>--require</code> flag to load the instrumentation before the application code.</p> <p>{{&lt; tabpane lang=console persistLang=false &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>$ npx ts-node --require ./instrumentation.ts app.ts\nListening for requests on http://localhost:8080\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>$ node --require ./instrumentation.js app.js\nListening for requests on http://localhost:8080\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <p>Open http://localhost:8080/rolldice in your web browser and reload the page a few times. After a while you should see the spans printed in the console by the <code>ConsoleSpanExporter</code>.</p> View example output <pre><code>{\n\"traceId\": \"3f1fe6256ea46d19ec3ca97b3409ad6d\",\n\"parentId\": \"f0b7b340dd6e08a7\",\n\"name\": \"middleware - query\",\n\"id\": \"41a27f331c7bfed3\",\n\"kind\": 0,\n\"timestamp\": 1624982589722992,\n\"duration\": 417,\n\"attributes\": {\n\"http.route\": \"/\",\n\"express.name\": \"query\",\n\"express.type\": \"middleware\"\n},\n\"status\": { \"code\": 0 },\n\"events\": []\n}\n{\n\"traceId\": \"3f1fe6256ea46d19ec3ca97b3409ad6d\",\n\"parentId\": \"f0b7b340dd6e08a7\",\n\"name\": \"middleware - expressInit\",\n\"id\": \"e0ed537a699f652a\",\n\"kind\": 0,\n\"timestamp\": 1624982589725778,\n\"duration\": 673,\n\"attributes\": {\n\"http.route\": \"/\",\n\"express.name\": \"expressInit\",\n\"express.type\": \"middleware\"\n},\n\"status\": { code: 0 },\n\"events\": []\n}\n{\n\"traceId\": \"3f1fe6256ea46d19ec3ca97b3409ad6d\",\n\"parentId\": \"f0b7b340dd6e08a7\",\n\"name\": \"request handler - /\",\n\"id\": \"8614a81e1847b7ef\",\n\"kind\": 0,\n\"timestamp\": 1624982589726941,\n\"duration\": 21,\n\"attributes\": {\n\"http.route\": \"/\",\n\"express.name\": \"/\",\n\"express.type\": \"request_handler\"\n},\n\"status\": { code: 0 },\n\"events\": []\n}\n{\n\"traceId\": \"3f1fe6256ea46d19ec3ca97b3409ad6d\",\n\"parentId\": undefined,\n\"name\": \"GET /\",\n\"id\": \"f0b7b340dd6e08a7\",\n\"kind\": 1,\n\"timestamp\": 1624982589720260,\n\"duration\": 11380,\n\"attributes\": {\n\"http.url\": \"http://localhost:8080/\",\n\"http.host\": \"localhost:8080\",\n\"net.host.name\": \"localhost\",\n\"http.method\": \"GET\",\n\"http.route\": \"\",\n\"http.target\": \"/\",\n\"http.user_agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\",\n\"http.flavor\": \"1.1\",\n\"net.transport\": \"ip_tcp\",\n\"net.host.ip\": \"::1\",\n\"net.host.port\": 8080,\n\"net.peer.ip\": \"::1\",\n\"net.peer.port\": 61520,\n\"http.status_code\": 304,\n\"http.status_text\": \"NOT MODIFIED\"\n},\n\"status\": { \"code\": 1 },\n\"events\": []\n}\n</code></pre> <p>The generated span tracks the lifetime of a request to the <code>/rolldice</code> route.</p> <p>Send a few more requests to the endpoint. After a moment, you'll see metrics in the console output, such as the following:</p> View example output <pre><code>{\n  descriptor: {\n    name: 'http.server.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the inbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: [\n{\n      attributes: [Object],\n      startTime: [Array],\n      endTime: [Array],\n      value: [Object]\n}\n]\n}\n{\n  descriptor: {\n    name: 'http.client.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the outbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: []\n}\n{\n  descriptor: {\n    name: 'db.client.connections.usage',\n    type: 'UP_DOWN_COUNTER',\n    description: 'The number of connections that are currently in the state referenced by the attribute \"state\".',\n    unit: '{connections}',\n    valueType: 1\n},\n  dataPointType: 3,\n  dataPoints: []\n}\n{\n  descriptor: {\n    name: 'http.server.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the inbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: [\n{\n      attributes: [Object],\n      startTime: [Array],\n      endTime: [Array],\n      value: [Object]\n}\n]\n}\n{\n  descriptor: {\n    name: 'http.client.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the outbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: []\n}\n{\n  descriptor: {\n    name: 'db.client.connections.usage',\n    type: 'UP_DOWN_COUNTER',\n    description: 'The number of connections that are currently in the state referenced by the attribute \"state\".',\n    unit: '{connections}',\n    valueType: 1\n},\n  dataPointType: 3,\n  dataPoints: []\n}\n{\n  descriptor: {\n    name: 'http.server.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the inbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: [\n{\n      attributes: [Object],\n      startTime: [Array],\n      endTime: [Array],\n      value: [Object]\n}\n]\n}\n{\n  descriptor: {\n    name: 'http.client.duration',\n    type: 'HISTOGRAM',\n    description: 'measures the duration of the outbound HTTP requests',\n    unit: 'ms',\n    valueType: 1\n},\n  dataPointType: 0,\n  dataPoints: []\n}\n{\n  descriptor: {\n    name: 'db.client.connections.usage',\n    type: 'UP_DOWN_COUNTER',\n    description: 'The number of connections that are currently in the state referenced by the attribute \"state\".',\n    unit: '{connections}',\n    valueType: 1\n},\n  dataPointType: 3,\n  dataPoints: []\n}\n</code></pre>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#next-steps","title":"Next Steps","text":"<p>Enrich your instrumentation generated automatically with manual instrumentation of your own codebase. This gets you customized observability data.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/js/getting-started/nodejs/#troubleshooting","title":"Troubleshooting","text":"<p>Did something go wrong? You can enable diagnostic logging to validate that OpenTelemetry is initialized correctly:</p> <p>{{&lt; tabpane langEqualsHeader=true &gt;}}</p> <p>{{&lt; tab TypeScript &gt;}} <pre><code>/*instrumentation.ts*/\nimport { diag, DiagConsoleLogger, DiagLogLevel } from '@opentelemetry/api';\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.INFO);\n// const sdk = new NodeSDK({...\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; tab JavaScript &gt;}} <pre><code>/*instrumentation.js*/\n// Require dependencies\nconst { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api');\n// For troubleshooting, set the log level to DiagLogLevel.DEBUG\ndiag.setLogger(new DiagConsoleLogger(), DiagLogLevel.INFO);\n// const sdk = new NodeSDK({...\n</code></pre> {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p>"},{"location":"docs/instrumentation/net/","title":".NET","text":"<p>{{% lang_instrumentation_index_head dotnet %}}</p> <p>* While the OpenTelemetryLoggerProvider (i.e integration with ILogger) is stable, the OTLP Log Exporter is still non-stable.</p> <p>{{% /lang_instrumentation_index_head %}}</p>"},{"location":"docs/instrumentation/net/#version-support","title":"Version Support","text":"<p>OpenTelemetry for .NET supports all officially supported versions of .NET Core and .NET Framework except for .NET Framework 3.5 SP1.</p>"},{"location":"docs/instrumentation/net/#repositories","title":"Repositories","text":"<p>OpenTelemetry .NET consists of the following repositories:</p> <ul> <li>OpenTelemetry .NET</li> <li>OpenTelemetry .NET Contrib</li> <li>OpenTelemetry .NET Automatic Instrumentation</li> </ul>"},{"location":"docs/instrumentation/net/automatic/","title":"Automatic Instrumentation","text":"<p>You can use automatic instrumentation to initialize signal providers and generate telemetry data for supported instrumented libraries without modifying the application's source code.</p> <p>Here you can find the latest release of OpenTelemetry .NET Automatic Instrumentation.</p> <p>You can find the documentation in the project's repository.</p>"},{"location":"docs/instrumentation/net/automatic/#next-steps","title":"Next steps","text":"<p>After you have set up the automatic instrumentation, you may want to add manual instrumentation to collect custom telemetry data.</p> <p>If you do not want to rely on automatic instrumentation, which is currently in beta, you may want to use instrumentation libraries explicitly instead.</p>"},{"location":"docs/instrumentation/net/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your traces and metrics, you will need to export them to a backend.</p>"},{"location":"docs/instrumentation/net/exporters/#console-exporter","title":"Console exporter","text":"<p>The console exporter is useful for development and debugging tasks, and is the simplest to set up.</p> <pre><code>dotnet add package OpenTelemetry.Exporter.Console\ndotnet add package OpenTelemetry.Extensions.Hosting\n</code></pre> <p>If you're using ASP.NET Core, configure the exporter in your ASP.NET Core services:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry().WithTracing(b =&gt;\n{\nb.AddConsoleExporter()\n// The rest of your setup code goes here too\n});\n</code></pre> <p>Otherwise, configure the exporter when creating a tracer provider:</p> <pre><code>using var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddConsoleExporter()\n// Other setup code, like setting a resource goes here too\n.Build();\n</code></pre>"},{"location":"docs/instrumentation/net/exporters/#otlp-endpoint","title":"OTLP endpoint","text":"<p>To send trace data to an OTLP endpoint (like the collector or Jaeger) you'll want to configure an OTLP exporter that sends to your endpoint.</p> <pre><code>dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol\ndotnet add package OpenTelemetry.Extensions.Hosting\n</code></pre> <p>If you're using ASP.NET Core, configure the exporter in your ASP.NET Core services:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(b =&gt;\n{\nb.AddOtlpExporter()\n// The rest of your setup code goes here too\n});\n</code></pre> <p>This will, by default, send traces using gRPC to http://localhost:4317, to customize this to use HTTP and the protobuf format, you can add options like this:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(b =&gt;\n{\nb\n.AddOtlpExporter(opt =&gt;\n{\nopt.Endpoint = new Uri(\"your-endpoint-here\");\nopt.Protocol = OtlpExportProtocol.HttpProtobuf;\n})\n// The rest of your setup code goes here too\n});\n</code></pre> <p>Otherwise, configure the exporter when creating a tracer provider:</p> <pre><code>using var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddOtlpExporter(opt =&gt;\n{\nopt.Endpoint = new Uri(\"your-endpoint-here\");\nopt.Protocol = OtlpExportProtocol.HttpProtobuf;\n})\n// Other setup code, like setting a resource goes here too\n.Build();\n</code></pre> <p>Use environment variables to set values like headers and an endpoint URL for production.</p>"},{"location":"docs/instrumentation/net/exporters/#note-for-net-core-31-and-below-and-grpc","title":"Note for .NET Core 3.1 and below and gRPC","text":"<p>Note: Versions below .NET 6 are not officially supported by opentelemetry-dotnet, therefore this section is here to help, but may not work as the library progresses.</p> <p>If you're not using ASP.NET Core gRPC and you are running on .NET Core 3.x, you'll need to add the following at application startup</p> <pre><code>AppContext.SetSwitch(\"System.Net.Http.SocketsHttpHandler.Http2UnencryptedSupport\", true);\n</code></pre> <p>If you are using .NET 5 or higher, the previous code sample is not required.</p>"},{"location":"docs/instrumentation/net/exporters/#jaeger","title":"Jaeger","text":"<p>To try out the OTLP exporter, you can run Jaeger as an OTLP endpoint and for trace visualization in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 16686:16686 \\\n-p 4317:4317 \\\n-p 4318:4318 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/net/exporters/#zipkin","title":"Zipkin","text":"<p>If you are using Zipkin to visualize trace data, you'll need to set it up first. This is how to run it in a docker container:</p> <pre><code>docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n</code></pre> <p>Next, install the Zipkin exporter package:</p> <pre><code>dotnet add package OpenTelemetry.Exporter.Zipkin\ndotnet add package OpenTelemetry.Extensions.Hosting\n</code></pre> <p>If you're using ASP.NET Core, configure the exporter in your ASP.NET Core services:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(b =&gt;\n{\nb.AddZipkinExporter(o =&gt;\n{\no.Endpoint = new Uri(\"your-zipkin-uri-here\");\n})\n// The rest of your setup code goes here too\n});\n</code></pre> <p>Otherwise, configure the exporter when creating a tracer provider:</p> <pre><code>using var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddZipkinExporter(o =&gt;\n{\no.Endpoint = new Uri(\"your-zipkin-uri-here\");\n})\n// Other setup code, like setting a resource goes here too\n.Build();\n</code></pre>"},{"location":"docs/instrumentation/net/exporters/#prometheus-experimental","title":"Prometheus (Experimental)","text":"<p>*Note: this is experimental and dependent on the OpenTelemetry specification to be made stable before it will be a released package. For now, we recommend using the OTLP exporter and using the OpenTelemetry Collector to send metrics to Prometheus*</p> <p>If you're using Prometheus to visualize metrics data, you'll need to set it up first. Here's how to do it using a docker container:</p> <p>First, you'll need a <code>prometheus.yml</code> file to configure your Prometheus backend, such as the following:</p> <pre><code>global:\n  scrape_interval: 1s\n  evaluation_interval: 1s\n\nscrape_configs:\n  - job_name: prometheus\n    static_configs:\n      - targets: [localhost:9090]\n</code></pre> <p>Next, run the following docker command to set up Prometheus:</p> <pre><code>docker run \\\n-p 9090:9090 \\\n-v ${PWD}/prometheus.yml:/etc/prometheus/prometheus.yml \\\nprom/prometheus\n</code></pre> <p>Next, install the Prometheus exporter:</p>"},{"location":"docs/instrumentation/net/exporters/#aspnet","title":"ASP.NET","text":"<pre><code>dotnet add package OpenTelemetry.Exporter.Prometheus.AspNetCore --version 1.4.0-rc.4\ndotnet add package OpenTelemetry.Extensions.Hosting\n</code></pre> <p>If you're using ASP.NET Core, configure the exporter in your ASP.NET Core services:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry()\n.WithMetrics(b =&gt; b.AddPrometheusExporter());\n</code></pre> <p>You'll then need to add the endpoint so that Prometheus can scrape your site. You can do this using the <code>IAppBuilder</code> extension like this:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\n// .. Setup\nvar app = builder.Build();\napp.UseOpenTelemetryPrometheusScrapingEndpoint();\nawait app.RunAsync();\n</code></pre>"},{"location":"docs/instrumentation/net/exporters/#non-aspnet-core","title":"Non-ASP.NET Core","text":"<p>For applications not using ASP.NET Core, you can use the <code>HttpListener</code> version which is available in a different package:</p> <pre><code>dotnet add package OpenTelemetry.Exporter.Prometheus.HttpListener --version 1.4.0-rc.4\n</code></pre> <p>Then this is setup directly on the <code>MeterProviderBuilder</code>:</p> <pre><code>var meterProvider = Sdk.CreateMeterProviderBuilder()\n.AddMeter(MyMeter.Name)\n.AddPrometheusHttpListener(\noptions =&gt; options.UriPrefixes = new string[] { \"http://localhost:9090/\" })\n.Build();\n</code></pre> <p>Finally, register the Prometheus scraping middleware using the <code>UseOpenTelemetryPrometheusScrapingEndpoint</code> extension method on <code>IApplicationBuilder</code> :</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\nvar app = builder.Build();\napp.UseOpenTelemetryPrometheusScrapingEndpoint();\n</code></pre> <p>Further details on configuring the Prometheus exporter can be found here.</p>"},{"location":"docs/instrumentation/net/exporters/#next-steps","title":"Next steps","text":"<p>To ensure you're getting the most data as easily as possible, install instrumentation libraries to generate observability data.</p> <p>Additionally, enriching your codebase with manual instrumentation gives you customized observability data.</p> <p>You can also check the automatic instrumentation for .NET, which is currently in beta.</p>"},{"location":"docs/instrumentation/net/getting-started/","title":"Getting Started","text":"<p>OpenTelemetry for .NET is unique among OpenTelemetry implementations, as it is integrated with the .NET <code>System.Diagnostics</code> library. At a high level, you can think of OpenTelemetry for .NET as a bridge between the telemetry available through <code>System.Diagnostics</code> and the greater OpenTelemetry ecosystem, such as OpenTelemetry Protocol (OTLP) and the OpenTelemetry Collector.</p>"},{"location":"docs/instrumentation/net/getting-started/#aspnet-core","title":"ASP.NET Core","text":"<p>The following example demonstrates using Instrumentation Libraries and manual instrumentation via an ASP.NET Core app.</p> <p>First, create your basic ASP.NET Core site:</p> <pre><code>dotnet new mvc\n</code></pre> <p>Next, Add the Core OpenTelemetry packages</p> <pre><code>dotnet add package OpenTelemetry.Exporter.Console\ndotnet add package OpenTelemetry.Extensions.Hosting\n</code></pre> <p>Now let's add the instrumentation packages for ASP.NET Core. This will give us some automatic spans for each HTTP request to our app.</p> <pre><code>dotnet add package OpenTelemetry.Instrumentation.AspNetCore --prerelease\n</code></pre> <p>Note that as the Semantic Conventions for attribute names are not currently stable the instrumentation package is currently not in a released state. That doesn't mean that the functionality itself is not stable. This means that you need to use the <code>--prerelease</code> flag, or install a specific version of the package</p>"},{"location":"docs/instrumentation/net/getting-started/#setup","title":"Setup","text":"<p>Next, we need to add OpenTelemetry to our Service Collection in <code>Program.cs</code> so that its listening correctly.</p> <pre><code>using System.Diagnostics;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nvar builder = WebApplication.CreateBuilder(args);\n// .. other setup\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(tracerProviderBuilder =&gt;\ntracerProviderBuilder\n.AddSource(DiagnosticsConfig.ActivitySource.Name)\n.ConfigureResource(resource =&gt; resource\n.AddService(DiagnosticsConfig.ServiceName))\n.AddAspNetCoreInstrumentation()\n.AddConsoleExporter());\n// ... other setup\npublic static class DiagnosticsConfig\n{\npublic const string ServiceName = \"MyService\";\npublic static ActivitySource ActivitySource = new ActivitySource(ServiceName);\n}\n</code></pre> <p>At this stage, you should be able to run your site, and see a Console output similar to this:</p> <p>Note: an <code>Activity</code> in .NET is analogous to a Span in OpenTelemetry terminology</p> View example output <pre><code>Activity.TraceId:            54d084eba205a7a39398df4642be8f4a\nActivity.SpanId:             aca5e39a86a17d59\nActivity.TraceFlags:         Recorded\nActivity.ActivitySourceName: Microsoft.AspNetCore\nActivity.DisplayName:        /\nActivity.Kind:               Server\nActivity.StartTime:          2023-02-21T12:19:28.2499974Z\nActivity.Duration:           00:00:00.3106744\nActivity.Tags:\n    net.host.name: localhost\n    net.host.port: 5123\n    http.method: GET\n    http.scheme: http\n    http.target: /\n    http.url: http://localhost:5123/\n    http.flavor: 1.1\n    http.user_agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36 Edg/110.0.1587.50\n    http.status_code: 200\nResource associated with Activity:\n    service.name: MyService\n    service.instance.id: 2c7ca153-e460-4643-b550-7c08487a4c0c\n</code></pre>"},{"location":"docs/instrumentation/net/getting-started/#manual-instrumentation","title":"Manual Instrumentation","text":"<p>Next, add tracing via the <code>System.Diagnostics</code> API.</p> <p>Paste the following code into your <code>HomeController</code>'s <code>Index</code> action:</p> <pre><code>public IActionResult Index()\n{\n// Track work inside of the request\nusing var activity = DiagnosticsConfig.ActivitySource.StartActivity(\"SayHello\");\nactivity?.SetTag(\"foo\", 1);\nactivity?.SetTag(\"bar\", \"Hello, World!\");\nactivity?.SetTag(\"baz\", new int[] { 1, 2, 3 });\nreturn View();\n}\n</code></pre> <p>When you run the app and navigate to the <code>/</code> route, you'll see output about spans similar to the following:</p> View example output <pre><code>Activity.TraceId:            47d25efc8b5e9184ce57e692f5f65465\nActivity.SpanId:             bb864adcf4592f54\nActivity.TraceFlags:         Recorded\nActivity.ParentSpanId:       acbff23f5ad721ff\nActivity.ActivitySourceName: MyService\nActivity.DisplayName:        SayHello\nActivity.Kind:               Internal\nActivity.StartTime:          2023-02-21T12:27:41.9596458Z\nActivity.Duration:           00:00:00.0005683\nActivity.Tags:\n    foo: 1\n    bar: Hello, World!\n    baz: [1,2,3]\nResource associated with Activity:\n    service.name: MyService\n    service.instance.id: 2b07a9ca-29c4-4e01-b0ed-929184b32192\n</code></pre> <p>You'll notice the <code>Activity</code> objects from ASP.NET Core alongside the <code>Activity</code> we created manually in our controller action.</p>"},{"location":"docs/instrumentation/net/getting-started/#metrics","title":"Metrics","text":"<p>Next we'll add the ASP.NET Core automatically generated metrics to the app.</p> <pre><code>using OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nusing OpenTelemetry.Metrics;\nvar builder = WebApplication.CreateBuilder(args);\n// .. other setup\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(/*  .. tracing setup */ )\n.WithMetrics(metricsProviderBuilder =&gt;\nmetricsProviderBuilder\n.ConfigureResource(resource =&gt; resource\n.AddService(DiagnosticsConfig.ServiceName))\n.AddAspNetCoreInstrumentation()\n.AddConsoleExporter());\n// .. other setup\n</code></pre> <p>If you run your application now, you'll see a series of metrics output to the console. like this.</p> View example output <pre><code>Export http.server.duration, Measures the duration of inbound HTTP requests., Unit: ms, Meter: OpenTelemetry.Instrumentation.AspNetCore/1.0.0.0\n(2023-02-21T12:38:57.0187781Z, 2023-02-21T12:44:16.9651349Z] http.flavor: 1.1 http.method: GET http.route: {controller=Home}/{action=Index}/{id?} http.scheme: http http.status_code: 200 net.host.name: localhost net.host.port: 5123 Histogram\nValue: Sum: 373.4504 Count: 1 Min: 373.4504 Max: 373.4504\n(-Infinity,0]:0\n(0,5]:0\n(5,10]:0\n(10,25]:0\n(25,50]:0\n(50,75]:0\n(75,100]:0\n(100,250]:0\n(250,500]:1\n(500,750]:0\n(750,1000]:0\n(1000,2500]:0\n(2500,5000]:0\n(5000,7500]:0\n(7500,10000]:0\n(10000,+Infinity]:0\n</code></pre>"},{"location":"docs/instrumentation/net/getting-started/#manual-metrics","title":"Manual Metrics","text":"<p>Next, add some manual metrics to the app. This will initialize a Meter to create a counter in code.</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\n// .. other setup\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(/*  .. tracing setup */ )\n.WithMetrics(metricsProviderBuilder =&gt;\nmetricsProviderBuilder\n.AddMeter(DiagnosticsConfig.Meter.Name)\n// .. more metrics\n);\npublic static class DiagnosticsConfig\n{\npublic const string ServiceName = \"MyService\";\n// .. other config\npublic static Meter Meter = new(ServiceName);\npublic static Counter&lt;long&gt; RequestCounter =\nMeter.CreateCounter&lt;long&gt;(\"app.request_counter\");\n}\n</code></pre> <p>Now we can increment the counter in our <code>Index</code> action.</p> <pre><code>    public IActionResult Index()\n{\n// do other stuff\nDiagnosticsConfig.RequestCounter.Add(1,\nnew(\"Action\", nameof(Index)),\nnew(\"Controller\", nameof(HomeController)));\nreturn View();\n}\n</code></pre> <p>You'll notice here that we're also adding Tags (OpenTelemetry Attributes) to our request counter that distinguishes it from other requests. You should now see an output like this.</p> View example output <pre><code>Export app.request_counter, Meter: MyService\n(2023-02-21T13:11:28.7265324Z, 2023-02-21T13:11:48.7074259Z] Action: Index Controller: HomeController LongSum\nValue: 1\n</code></pre> <p>Tip: if you comment out the <code>.AddAspNetCoreInstrumentation()</code> line in <code>Program.cs</code> you'll be able to see the output better.</p>"},{"location":"docs/instrumentation/net/getting-started/#send-data-to-a-collector","title":"Send data to a collector","text":"<p>The OpenTelemetry Collector is a vital component of most production deployments. A collector is most beneficial in the following situations, among others:</p> <ul> <li>A single telemetry sink shared by multiple services, to reduce overhead of   switching exporters</li> <li>Aggregate traces across multiple services, running on multiple hosts</li> <li>A central place to process traces prior to exporting them to a backend</li> </ul>"},{"location":"docs/instrumentation/net/getting-started/#configure-and-run-a-local-collector","title":"Configure and run a local collector","text":"<p>First, save the following collector configuration code to a file in the <code>/tmp/</code> directory:</p> <pre><code># /tmp/otel-collector-config.yaml\nreceivers:\notlp:\nprotocols:\nhttp:\ngrpc:\nexporters:\nlogging:\nloglevel: debug\nprocessors:\nbatch:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [logging]\nprocessors: [batch]\nmetrics:\nreceivers: [otlp]\nexporters: [logging]\nprocessors: [batch]\n</code></pre> <p>Then run the docker command to acquire and run the collector based on this configuration:</p> <pre><code>docker run -p 4317:4317 \\\n-v /tmp/otel-collector-config.yaml:/etc/otel-collector-config.yaml \\\notel/opentelemetry-collector:latest \\\n--config=/etc/otel-collector-config.yaml\n</code></pre> <p>You will now have an collector instance running locally.</p>"},{"location":"docs/instrumentation/net/getting-started/#modify-the-code-to-export-spans-via-otlp","title":"Modify the code to export spans via OTLP","text":"<p>The next step is to modify the code to send spans to the collector via OTLP instead of the console.</p> <p>First, add the following package:</p> <pre><code>dotnet add package OpenTelemetry.Exporter.OpenTelemetryProtocol\n</code></pre> <p>Next, using the ASP.NET Core code from earlier, replace the console exporter with an OTLP exporter:</p> <pre><code>builder.Services.AddOpenTelemetry()\n.WithTracing(tracerProviderBuilder =&gt;\ntracerProviderBuilder\n// .. other config\n.AddOtlpExporter())\n.WithMetrics(metricsProviderBuilder =&gt;\nmetricsProviderBuilder\n// .. other config\n.AddOtlpExporter());\n</code></pre> <p>By default, it will send spans to <code>localhost:4317</code>, which is what the collector is listening on if you've followed the step above.</p>"},{"location":"docs/instrumentation/net/getting-started/#run-the-application","title":"Run the application","text":"<p>Run the application like before:</p> <pre><code>dotnet run\n</code></pre> <p>Now, telemetry will be output by the collector process.</p>"},{"location":"docs/instrumentation/net/getting-started/#next-steps","title":"Next steps","text":"<p>To ensure you're getting the most data as easily as possible, install instrumentation libraries to generate observability data.</p> <p>Additionally, enriching your codebase with manual instrumentation gives you customized observability data.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p> <p>You can also check the automatic instrumentation for .NET, which is currently in beta.</p>"},{"location":"docs/instrumentation/net/libraries/","title":"Using instrumentation libraries","text":"<p>You can use instrumentation libraries in order to generate telemetry data for a particular instrumented library.</p> <p>For example, the instrumentation library for ASP.NET Core will automatically create spans and metrics based on the inbound HTTP requests.</p>"},{"location":"docs/instrumentation/net/libraries/#setup","title":"Setup","text":"<p>Each instrumentation library is a NuGet package, and installing them is typically done like so:</p> <pre><code>dotnet add package OpenTelemetry.Instrumentation.{library-name-or-type}\n</code></pre> <p>It is typically then registered at application startup time, such as when creating a TracerProvider.</p>"},{"location":"docs/instrumentation/net/libraries/#note-on-versioning","title":"Note on Versioning","text":"<p>The Semantic Conventions (Standards) for attribute names are not currently stable therefore the instrumentation package is currently not in a released state. That doesn't mean that the functionality itself is not stable, only that the names of some of the attributes may change in the future, some may be added, some may be removed. This means that you need to use the <code>--prerelease</code> flag, or install a specific version of the package</p>"},{"location":"docs/instrumentation/net/libraries/#example-with-aspnet-core-and-httpclient","title":"Example with ASP.NET Core and HttpClient","text":"<p>As an example, here's how you can instrument inbound and output requests from an ASP.NET Core app.</p> <p>First, get the appropriate packages of OpenTelemetry Core:</p> <pre><code>dotnet add package OpenTelemetry\ndotnet add package OpenTelemetry.Extensions.Hosting\ndotnet add package OpenTelemetry.Exporter.Console\n</code></pre> <p>Then you can install the Instrumentation packages</p> <pre><code>dotnet add package OpenTelemetry.Instrumentation.AspNetCore --prerelease\ndotnet add package OpenTelemetry.Instrumentation.Http --prerelease\n</code></pre> <p>Next, configure each instrumentation library at startup and use them!</p> <pre><code>using OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\nvar builder = WebApplication.CreateBuilder(args);\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(b =&gt;\n{\nb\n.AddHttpClientInstrumentation()\n.AddAspNetCoreInstrumentation();\n});\nvar app = builder.Build();\nvar httpClient = new HttpClient();\napp.MapGet(\"/hello\", async () =&gt;\n{\nvar html = await httpClient.GetStringAsync(\"https://example.com/\");\nif (string.IsNullOrWhiteSpace(html))\n{\nreturn \"Hello, World!\";\n}\nelse\n{\nreturn \"Hello, World!\";\n}\n});\napp.Run();\n</code></pre> <p>When you run this code and access the <code>/hello</code> endpoint, the instrumentation libraries will:</p> <ul> <li>Start a new trace</li> <li>Generate a span representing the request made to the endpoint</li> <li>Generate a child span representing the HTTP GET made to <code>https://example.com/</code></li> </ul> <p>If you add more instrumentation libraries, then you get more spans for each of those.</p>"},{"location":"docs/instrumentation/net/libraries/#available-instrumentation-libraries","title":"Available instrumentation libraries","text":"<p>A full list of instrumentation libraries produced by OpenTelemetry is available from the opentelemetry-dotnet repository.</p> <p>You can also find more instrumentations available in the registry.</p>"},{"location":"docs/instrumentation/net/libraries/#next-steps","title":"Next steps","text":"<p>After you have set up instrumentation libraries, you may want to add manual instrumentation to collect custom telemetry data.</p> <p>If you are using .NET Framework 4.x instead of modern .NET, refer to the .NET Framework docs to configure OpenTelemetry and instrumentation libraries on .NET Framework.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p> <p>You can also check the automatic instrumentation for .NET, which is currently in beta.</p>"},{"location":"docs/instrumentation/net/manual/","title":"Manual Instrumentation","text":"<p>Manual instrumentation is the process of adding observability code to your application.</p>"},{"location":"docs/instrumentation/net/manual/#a-note-on-terminology","title":"A note on terminology","text":"<p>.NET is different from other languages/runtimes that support OpenTelemetry. The Tracing API is implemented by the System.Diagnostics API, repurposing existing constructs like <code>ActivitySource</code> and <code>Activity</code> to be OpenTelemetry-compliant under the covers.</p> <p>However, there are parts of the OpenTelemetry API and terminology that .NET developers must still know to be able to instrument their applications, which are covered here as well as the <code>System.Diagnostics</code> API.</p> <p>If you prefer to use OpenTelemetry APIs instead of <code>System.Diagnostics</code> APIs, you can refer to the OpenTelemetry API Shim docs for tracing.</p>"},{"location":"docs/instrumentation/net/manual/#initializing-tracing","title":"Initializing tracing","text":"<p>There are two main ways to initialize tracing, depending on whether you're using a console app or something that's ASP.NET Core-based.</p>"},{"location":"docs/instrumentation/net/manual/#console-app","title":"Console app","text":"<p>To start tracing in a console app, you need to create a tracer provider.</p> <p>First, ensure that you have the right packages:</p> <pre><code>dotnet add package OpenTelemetry\ndotnet add package OpenTelemetry.Exporter.Console\n</code></pre> <p>And then use code like this at the beginning of your program, during any important startup operations.</p> <pre><code>using OpenTelemetry;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\n// ...\nvar serviceName = \"MyServiceName\";\nvar serviceVersion = \"1.0.0\";\nusing var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddSource(serviceName)\n.ConfigureResource(resource =&gt;\nresource.AddService(\nserviceName: serviceName,\nserviceVersion: serviceVersion))\n.AddConsoleExporter()\n.Build();\n// ...\n</code></pre> <p>This is also where you can configure instrumentation libraries.</p> <p>Note that this sample uses the Console Exporter. If you are exporting to another endpoint, you'll have to use a different exporter.</p>"},{"location":"docs/instrumentation/net/manual/#aspnet-core","title":"ASP.NET Core","text":"<p>To start tracing in an ASP.NET Core-based app, use the OpenTelemetry extensions for ASP.NET Core setup.</p> <p>First, ensure that you have the right packages:</p> <pre><code>dotnet add package OpenTelemetry\ndotnet add package OpenTelemetry.Extensions.Hosting\ndotnet add package OpenTelemetry.Exporter.Console\n</code></pre> <p>Then you can install the Instrumentation package</p> <pre><code>dotnet add package OpenTelemetry.Instrumentation.AspNetCore --prerelease\n</code></pre> <p>Note that the <code>--prerelease</code> flag is required for all instrumentation packages because they are all dependent on naming conventions for attributes/labels (Semantic Conventions) that aren't yet classed as stable.</p> <p>Next, configure it in your ASP.NET Core startup routine where you have access to an <code>IServiceCollection</code>.</p> <pre><code>using OpenTelemetry;\nusing OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\n// Define some important constants and the activity source.\n// These can come from a config file, constants file, etc.\nvar serviceName = \"MyCompany.MyProduct.MyService\";\nvar serviceVersion = \"1.0.0\";\nvar builder = WebApplication.CreateBuilder(args);\n// Configure important OpenTelemetry settings, the console exporter\nbuilder.Services.AddOpenTelemetry()\n.WithTracing(b =&gt;\n{\nb\n.AddConsoleExporter()\n.AddSource(serviceName)\n.ConfigureResource(resource =&gt;\nresource.AddService(\nserviceName: serviceName,\nserviceVersion: serviceVersion))\n});\n</code></pre> <p>This is also where you can configure instrumentation libraries.</p> <p>Note that this sample uses the Console Exporter. If you are exporting to another endpoint, you'll have to use a different exporter.</p>"},{"location":"docs/instrumentation/net/manual/#setting-up-an-activitysource","title":"Setting up an ActivitySource","text":"<p>Once tracing is initialized, you can configure an <code>ActivitySource</code>, which will be how you trace operations with <code>Activity</code>s.</p> <p>Typically, an <code>ActivitySource</code> is instantiated once per app/service that is being instrumented, so it's a good idea to instantiate it once in a shared location. It is also typically named the same as the Service Name.</p> <pre><code>using System.Diagnostics;\npublic static class Telemetry\n{\n//...\n// Name it after the service name for your app.\n// It can come from a config file, constants file, etc.\npublic static readonly ActivitySource MyActivitySource = new(TelemetryConstants.ServiceName);\n//...\n}\n</code></pre> <p>You can instantiate several <code>ActivitySource</code>s if that suits your scenario, although it is generally sufficient to just have one defined per service.</p>"},{"location":"docs/instrumentation/net/manual/#creating-activities","title":"Creating Activities","text":"<p>To create an <code>Activity</code>, give it a name and create it from your <code>ActivitySource</code>.</p> <pre><code>using var myActivity = MyActivitySource.StartActivity(\"SayHello\");\n// do work that 'myActivity' will now track\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#creating-nested-activities","title":"Creating nested Activities","text":"<p>If you have a distinct sub-operation you'd like to track as a part of another one, you can create activities to represent the relationship.</p> <pre><code>public static void ParentOperation()\n{\nusing var parentActivity = MyActivitySource.StartActivity(\"ParentActivity\");\n// Do some work tracked by parentActivity\nChildOperation();\n// Finish up work tracked by parentActivity again\n}\npublic static void ChildOperation()\n{\nusing var childActivity = MyActivitySource.StartActivity(\"ChildActivity\");\n// Track work in ChildOperation with childActivity\n}\n</code></pre> <p>When you view spans in a trace visualization tool, <code>ChildActivity</code> will be tracked as a nested operation under <code>ParentActivity</code>.</p>"},{"location":"docs/instrumentation/net/manual/#nested-activities-in-the-same-scope","title":"Nested Activities in the same scope","text":"<p>You may wish to create a parent-child relationship in the same scope. Although possible, this is generally not recommended because you need to be careful to end any nested <code>Activity</code> when you expect it to end.</p> <pre><code>public static void DoWork()\n{\nusing var parentActivity = MyActivitySource.StartActivity(\"ParentActivity\");\n// Do some work tracked by parentActivity\nusing (var childActivity = MyActivitySource.StartActivity(\"ChildActivity\"))\n{\n// Do some \"child\" work in the same function\n}\n// Finish up work tracked by parentActivity again\n}\n</code></pre> <p>In the preceding example, <code>childActivity</code> is ended because the scope of the <code>using</code> block is explicitly defined, rather than scoped to <code>DoWork</code> itself like <code>parentActivity</code>.</p>"},{"location":"docs/instrumentation/net/manual/#creating-independent-activities","title":"Creating independent Activities","text":"<p>The previous examples showed how to create Activities that follow a nested hierarchy. In some cases, you'll want to create independent Activities that are siblings of the same root rather than being nested.</p> <pre><code>public static void DoWork()\n{\nusing var parent = MyActivitySource.StartActivity(\"parent\");\nusing (var child1 = DemoSource.StartActivity(\"child1\"))\n{\n// Do some work that 'child1' tracks\n}\nusing (var child2 = DemoSource.StartActivity(\"child2\"))\n{\n// Do some work that 'child2' tracks\n}\n// 'child1' and 'child2' both share 'parent' as a parent, but are independent\n// from one another\n}\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#creating-new-root-activities","title":"Creating new root Activities","text":"<p>If you wish to create a new root Activity, you'll need to \"de-parent\" from the current activity.</p> <pre><code>public static void DoWork()\n{\nvar previous = Activity.Current;\nActivity.Current = null;\nvar newRoot = source.StartActivity(\"NewRoot\");\n// Re-set the previous Current Activity so the trace isn't messed up\nActivity.Current = previous;\n}\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#get-the-current-activity","title":"Get the current Activity","text":"<p>Sometimes it's helpful to access whatever the current <code>Activity</code> is at a point in time so you can enrich it with more information.</p> <pre><code>var activity = Activity.Current;\n// may be null if there is none\n</code></pre> <p>Note that <code>using</code> is not used in the prior example. Doing so will end current <code>Activity</code>, which is not likely to be desired.</p>"},{"location":"docs/instrumentation/net/manual/#add-tags-to-an-activity","title":"Add tags to an Activity","text":"<p>Tags (the equivalent of <code>Attributes</code> in OpenTelemetry) let you attach key/value pairs to an <code>Activity</code> so it carries more information about the current operation that it's tracking.</p> <pre><code>using var myActivity = MyActivitySource.StartActivity(\"SayHello\");\nactivity?.SetTag(\"operation.value\", 1);\nactivity?.SetTag(\"operation.name\", \"Saying hello!\");\nactivity?.SetTag(\"operation.other-stuff\", new int[] { 1, 2, 3 });\n</code></pre> <p>We recommend that all Tag names are defined in constants rather than defined inline as this provides both consistency and also discoverability.</p>"},{"location":"docs/instrumentation/net/manual/#adding-events","title":"Adding events","text":"<p>An event is a human-readable message on an <code>Activity</code> that represents \"something happening\" during its lifetime.</p> <pre><code>using var myActivity = MyActivitySource.StartActivity(\"SayHello\");\n// ...\nmyActivity?.AddEvent(new(\"Gonna try it!\"));\n// ...\nmyActivity?.AddEvent(new(\"Did it!\"));\n</code></pre> <p>Events can also be created with a timestamp and a collection of Tags.</p> <pre><code>using var myActivity = MyActivitySource.StartActivity(\"SayHello\");\n// ...\nmyActivity?.AddEvent(new(\"Gonna try it!\", DateTimeOffset.Now));\n// ...\nvar eventTags = new Dictionary&lt;string, object?&gt;\n{\n{ \"foo\", 1 },\n{ \"bar\", \"Hello, World!\" },\n{ \"baz\", new int[] { 1, 2, 3 } }\n};\nmyActivity?.AddEvent(new(\"Gonna try it!\", DateTimeOffset.Now, new(eventTags)));\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#adding-links","title":"Adding links","text":"<p>An <code>Activity</code> can be created with zero or more <code>ActivityLink</code>s that are causally related.</p> <pre><code>// Get a context from somewhere, perhaps it's passed in as a parameter\nvar activityContext = Activity.Current!.Context;\nvar links = new List&lt;ActivityLink&gt;\n{\nnew ActivityLink(activityContext)\n};\nusing var anotherActivity =\nMyActivitySource.StartActivity(\nActivityKind.Internal,\nname: \"anotherActivity\",\nlinks: links);\n// do some work\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#set-activity-status","title":"Set Activity status","text":"<p>A status can be set on an activity, typically used to specify that an activity has not completed successfully - <code>ActivityStatusCode.Error</code>. In rare scenarios, you could override the <code>Error</code> status with <code>Ok</code>, but don't set <code>Ok</code> on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>using var myActivity = MyActivitySource.StartActivity(\"SayHello\");\ntry\n{\n// do something\n}\ncatch (Exception ex)\n{\nmyActivity.SetStatus(ActivityStatusCode.Error, \"Something bad happened!\");\n}\n</code></pre>"},{"location":"docs/instrumentation/net/manual/#next-steps","title":"Next steps","text":"<p>After you've set up manual instrumentation, you may want to use instrumentation libraries. As the name suggests, they will instrument relevant libraries you're using and generate spans (activities) for things like inbound and outbound HTTP requests and more.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p> <p>You can also check the automatic instrumentation for .NET, which is currently in beta.</p>"},{"location":"docs/instrumentation/net/netframework/","title":".NET Framework instrumentation configuration","text":"<p>OpenTelemetry supports both .NET and .NET Framework (an older Windows-based .NET implementation).</p> <p>If you're already using the modern, cross-platform implementation of .NET, you can skip this article.</p>"},{"location":"docs/instrumentation/net/netframework/#aspnet-initialization","title":"ASP.NET Initialization","text":"<p>Initialization for ASP.NET is a little different than for ASP.NET Core.</p> <p>First, install the following NuGet packages:</p> <ul> <li>OpenTelemetry.Instrumentation.AspNet</li> <li>OpenTelemetry.Extensions.Hosting</li> </ul> <p>Next, modify your <code>Web.Config</code> file to add a required HttpModule:</p> <pre><code>&lt;system.webServer&gt;\n&lt;modules&gt;\n&lt;add\nname=\"TelemetryHttpModule\"\ntype=\"OpenTelemetry.Instrumentation.AspNet.TelemetryHttpModule,\n                OpenTelemetry.Instrumentation.AspNet.TelemetryHttpModule\"\npreCondition=\"integratedMode,managedHandler\" /&gt;\n&lt;/modules&gt;\n&lt;/system.webServer&gt;\n</code></pre> <p>Finally, initialize ASP.NET instrumentation in your <code>Global.asax.cs</code> file along with other OpenTelemetry initialization:</p> <pre><code>using OpenTelemetry;\nusing OpenTelemetry.Trace;\npublic class WebApiApplication : HttpApplication\n{\nprivate TracerProvider _tracerProvider;\nprotected void Application_Start()\n{\n_tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddAspNetInstrumentation()\n// Other configuration, like adding an exporter and setting resources\n.AddConsoleExporter()\n.AddSource(\"my-service-name\")\n.SetResourceBuilder(\nResourceBuilder.CreateDefault()\n.AddService(serviceName: \"my-service-name\", serviceVersion: \"1.0.0\"))\n.Build();\n}\nprotected void Application_End()\n{\n_tracerProvider?.Dispose();\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/net/netframework/#advanced-aspnet-configuration","title":"Advanced ASP.NET configuration","text":"<p>ASP.NET instrumentation can be configured to change the default behavior.</p>"},{"location":"docs/instrumentation/net/netframework/#filter","title":"Filter","text":"<p>ASP.NET instrumentation collects all incoming HTTP requests by default. However, you can filter incoming requests by using the <code>Filter</code> method in <code>AspNetInstrumentationOptions</code>. This works similar to a LINQ <code>Where</code> clause, where only requests that match a condition will be collected.</p> <p>The following code snippet shows how to use <code>Filter</code> to only allow GET requests.</p> <pre><code>this.tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddAspNetInstrumentation(\n(options) =&gt; options.Filter =\n(httpContext) =&gt;\n{\n// only collect telemetry about HTTP GET requests\nreturn httpContext.Request.HttpMethod.Equals(\"GET\");\n})\n.Build();\n</code></pre> <p>Filtering happens at an early stage, and is different from Sampling, which occurs after data has been collected. Filtering will limit what gets collected in the first place.</p>"},{"location":"docs/instrumentation/net/netframework/#enrich","title":"Enrich","text":"<p>If you have data that you'd like to have added to every <code>Activity</code> that's generated by OpenTelemetry, you can use the <code>Enrich</code> method.</p> <p>The <code>Enrich</code> action is called only when <code>activity.IsAllDataRequested</code> is <code>true</code>. It contains the <code>Activity</code> created, the name of the event, and the raw object</p> <p>The following code snippet shows how to add additional tags using <code>Enrich</code>.</p> <pre><code>this.tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddAspNetInstrumentation((options) =&gt; options.Enrich\n= (activity, eventName, rawObject) =&gt;\n{\nif (eventName.Equals(\"OnStartActivity\"))\n{\nif (rawObject is HttpRequest httpRequest)\n{\nactivity?.SetTag(\"physicalPath\", httpRequest.PhysicalPath);\n}\n}\nelse if (eventName.Equals(\"OnStopActivity\"))\n{\nif (rawObject is HttpResponse httpResponse)\n{\nactivity?.SetTag(\"responseType\", httpResponse.ContentType);\n}\n}\n})\n.Build();\n</code></pre> <p>See Add tags to an Activity for annotating trace data more generally.</p>"},{"location":"docs/instrumentation/net/netframework/#recordexception","title":"RecordException","text":"<p>ASP.NET instrumentation automatically sets a given <code>Activity</code>'s status to <code>Error</code> if an unhandled exception is thrown.</p> <p>You can also set the <code>RecordException</code> property to <code>true</code>, which will store an exception on the <code>Activity</code> itself as an <code>ActivityEvent</code>.</p>"},{"location":"docs/instrumentation/net/netframework/#next-steps","title":"Next steps","text":"<p>After you have observability generated automatically with instrumentation libraries, you may want to add manual instrumentation to collect custom telemetry data.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p> <p>You can also check the automatic instrumentation for .NET, which is currently in beta.</p>"},{"location":"docs/instrumentation/net/resources/","title":"Resources","text":"<p>A resource represents the entity producing telemetry as resource attributes. For example, a process producing telemetry that is running in a container on Kubernetes has a Pod name, a namespace, and possibly a deployment name. All three of these attributes can be included in the resource.</p> <p>In your observability backend, you can use resource information to better investigate interesting behavior. For example, if your trace or metrics data indicate latency in your system, you can narrow it down to a specific container, pod, or Kubernetes deployment.</p>"},{"location":"docs/instrumentation/net/resources/#setup","title":"Setup","text":"<p>Follow the instructions in the Getting Started, so that you have a running .NET app exporting data to the console.</p>"},{"location":"docs/instrumentation/net/resources/#adding-resources-with-environment-variables","title":"Adding resources with environment variables","text":"<p>You can use the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable to inject resources into your application. The .NET SDK will automatically detect these resources.</p> <p>The following example adds Service, Host and OS resource attributes via environment variables, running unix programs like <code>uname</code> to generate the resource data.</p> <pre><code>$ env OTEL_RESOURCE_ATTRIBUTES=\"service.name=resource-tutorial-dotnet,service.namespace=tutorial,service.version=1.0,service.instance.id=`uuidgen`,host.name=`HOSTNAME`,host.type=`uname -m`,os.name=`uname -s`,os.version=`uname -r`\" dotnet run\n\nActivity.TraceId:          d1cbb7787440cc95b325835cb2ff8018\nActivity.SpanId:           2ca007300fcb3068\nActivity.TraceFlags:           Recorded\nActivity.ActivitySourceName: tutorial-dotnet\nActivity.DisplayName: SayHello\nActivity.Kind:        Internal\nActivity.StartTime:   2022-10-02T13:31:12.0175090Z\nActivity.Duration:    00:00:00.0003920\nActivity.Tags:\n    foo: 1\n    bar: Hello, World!\n    baz: [1,2,3]\nResource associated with Activity:\n    service.name: resource-tutorial-dotnet\n    service.namespace: tutorial\n    service.version: 1.0\n    service.instance.id: 93B14BAD-813D-48EE-9FB1-2ADFD07C5E78\n    host.name: myhost\n    host.type: arm64\n    os.name: Darwin\n    os.version: 21.6.0\n</code></pre>"},{"location":"docs/instrumentation/net/resources/#adding-resources-in-code","title":"Adding resources in code","text":"<p>You can also add custom resources in code by attaching them to a <code>ResourceBuilder</code>.</p> <p>The following example builds on the getting started sample and adds two custom resources, <code>environment.name</code> and <code>team.name</code> in code:</p> <pre><code>using System.Diagnostics;\nusing System.Collections.Generic;\nusing OpenTelemetry;\nusing OpenTelemetry.Trace;\nusing OpenTelemetry.Resources;\nvar serviceName = \"resource-tutorial-dotnet\";\nvar serviceVersion = \"1.0\";\nvar resourceBuilder =\nResourceBuilder\n.CreateDefault()\n.AddService(serviceName: serviceName, serviceVersion: serviceVersion)\n.AddAttributes(new Dictionary&lt;string, object&gt;\n{\n[\"environment.name\"] = \"production\",\n[\"team.name\"] = \"backend\"\n});\nvar sourceName = \"tutorial-dotnet\";\nusing var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddSource(sourceName)\n.SetResourceBuilder(resourceBuilder)\n.AddConsoleExporter()\n.Build();\nvar MyActivitySource = new ActivitySource(sourceName);\nusing var activity = MyActivitySource.StartActivity(\"SayHello\");\nactivity?.SetTag(\"foo\", 1);\nactivity?.SetTag(\"bar\", \"Hello, World!\");\nactivity?.SetTag(\"baz\", new int[] { 1, 2, 3 });\n</code></pre> <p>In this example, the <code>service.name</code> and <code>service.version</code> values are set in code as well. Additionally, <code>service.instance.id</code> gets a default value.</p> <p>If you run the same command as in Adding resources with environment variables, but this time without <code>service.name</code> <code>service.version</code>, and <code>service.instance.id</code>, you'll see the <code>environment.name</code> and <code>team.name</code> resources in the resource list:</p> <pre><code>$ env OTEL_RESOURCE_ATTRIBUTES=\"service.namespace=tutorial,host.name=`HOSTNAME`,host.type=`uname -m`,os.name=`uname -s`,os.version=`uname -r`\" dotnet run\n\nActivity.TraceId:          d1cbb7787440cc95b325835cb2ff8018\nActivity.SpanId:           2ca007300fcb3068\nActivity.TraceFlags:           Recorded\nActivity.ActivitySourceName: tutorial-dotnet\nActivity.DisplayName: SayHello\nActivity.Kind:        Internal\nActivity.StartTime:   2022-10-02T13:31:12.0175090Z\nActivity.Duration:    00:00:00.0003920\nActivity.Tags:\n    foo: 1\n    bar: Hello, World!\n    baz: [1,2,3]\nResource associated with Activity:\n    environment.name: production\n    team.name: backend\n    service.name: resource-tutorial-dotnet\n    service.namespace: tutorial\n    service.version: 1.0\n    service.instance.id: 28976A1C-BF02-43CA-BAE0-6E0564431462\n    host.name: pcarter\n    host.type: arm64\n    os.name: Darwin\n    os.version: 21.6.0\n</code></pre> <p>Note: If you set resource attributes with both environment variables and code, the values in code take precedence.</p>"},{"location":"docs/instrumentation/net/resources/#next-steps","title":"Next steps","text":"<p>There are more resource detectors you can add to your configuration, for example to get details about your Cloud environment or Deployment.</p>"},{"location":"docs/instrumentation/net/shim/","title":"OpenTelemetry Tracing Shim","text":"<p>.NET is different from other languages/runtimes that support OpenTelemetry. Tracing is implemented by the System.Diagnostics API, repurposing older constructs like <code>ActivitySource</code> and <code>Activity</code> to be OpenTelemetry-compliant under the covers.</p> <p>OpenTelemetry for .NET also provides an API shim on top of the System.Diagnostics- based implementation. This shim is helpful if you're working with other languages and OpenTelemetry in the same codebase, or if you prefer to use terminology consistent with the OpenTelemetry spec.</p>"},{"location":"docs/instrumentation/net/shim/#initializing-tracing","title":"Initializing tracing","text":"<p>There are two main ways to initialize tracing, depending on whether you're using a console app or something that's ASP.NET Core-based.</p>"},{"location":"docs/instrumentation/net/shim/#console-app","title":"Console app","text":"<p>To start tracing in a console app, you need to create a tracer provider.</p> <p>First, ensure that you have the right packages:</p> <pre><code>dotnet add package OpenTelemetry\ndotnet add package OpenTelemetry.Exporter.Console\n</code></pre> <p>And then use code like this at the beginning of your program, during any important startup operations.</p> <pre><code>using OpenTelemetry;\nusing OpenTelemetry.Trace;\nusing OpenTelemetry.Resources;\n// ...\nvar serviceName = \"MyServiceName\";\nvar serviceVersion = \"1.0.0\";\nusing var tracerProvider = Sdk.CreateTracerProviderBuilder()\n.AddSource(serviceName)\n.SetResourceBuilder(\nResourceBuilder.CreateDefault()\n.AddService(serviceName: serviceName, serviceVersion: serviceVersion))\n.AddConsoleExporter()\n.Build();\n//...\n</code></pre> <p>This is also where you can configure instrumentation libraries.</p> <p>Note that this sample uses the Console Exporter. If you are exporting to another endpoint, you'll have to use a different exporter.</p>"},{"location":"docs/instrumentation/net/shim/#aspnet-core","title":"ASP.NET Core","text":"<p>To start tracing in an ASP.NET Core-based app, use the OpenTelemetry extensions for ASP.NET Core setup.</p> <p>First, ensure that you have the right packages:</p> <pre><code>dotnet add package OpenTelemetry --prerelease\ndotnet add package OpenTelemetry.Instrumentation.AspNetCore --prerelease\ndotnet add package OpenTelemetry.Extensions.Hosting --prerelease\ndotnet add package OpenTelemetry.Exporter.Console --prerelease\n</code></pre> <p>And then configure it in your ASP.NET Core startup routine where you have access to an <code>IServiceCollection</code>.</p> <pre><code>using OpenTelemetry.Resources;\nusing OpenTelemetry.Trace;\n// These can come from a config file, constants file, etc.\nvar serviceName = \"MyCompany.MyProduct.MyService\";\nvar serviceVersion = \"1.0.0\";\nvar builder = WebApplication.CreateBuilder(args);\n// Configure important OpenTelemetry settings, the console exporter, and instrumentation library\nbuilder.Services.AddOpenTelemetry().WithTracing(tcb =&gt;\n{\ntcb\n.AddSource(serviceName)\n.SetResourceBuilder(\nResourceBuilder.CreateDefault()\n.AddService(serviceName: serviceName, serviceVersion: serviceVersion))\n.AddAspNetCoreInstrumentation()\n.AddConsoleExporter();\n});\n</code></pre> <p>In the preceding example, a <code>Tracer</code> corresponding to the service is injected during setup. This lets you get access to an instance in your endpoint mapping (or controllers if you're using an older version of .NET).</p> <p>It's not required to inject a service-level tracer, nor does it improve performance either. You will need to decide where you'll want your tracer instance to live, though.</p> <p>This is also where you can configure instrumentation libraries.</p> <p>Note that this sample uses the Console Exporter. If you are exporting to another endpoint, you'll have to use a different exporter.</p>"},{"location":"docs/instrumentation/net/shim/#setting-up-a-tracer","title":"Setting up a tracer","text":"<p>Once tracing is initialized, you can configure a <code>Tracer</code>, which will be how you trace operations with <code>Span</code>s.</p> <p>Typically, a <code>Tracer</code> is instantiated once per app/service that is being instrumented, so it's a good idea to instantiate it once in a shared location. It is also typically named the same as the Service Name.</p>"},{"location":"docs/instrumentation/net/shim/#injecting-a-tracer-with-aspnet-core","title":"Injecting a tracer with ASP.NET Core","text":"<p>ASP.NET Core generally encourages injecting instances of long-lived objects like <code>Tracer</code>s during setup.</p> <pre><code>using OpenTelemetry.Trace;\nvar builder = WebApplication.CreateBuilder(args);\n// ...\nbuilder.Services.AddSingleton(TracerProvider.Default.GetTracer(serviceName));\n// ...\nvar app = builder.Build();\n// ...\napp.MapGet(\"/hello\", (Tracer tracer) =&gt;\n{\nusing var span = tracer.StartActiveSpan(\"hello-span\");\n// do stuff\n});\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#acquiring-a-tracer-from-a-tracerprovider","title":"Acquiring a tracer from a TracerProvider","text":"<p>If you're not using ASP.NET Core or would rather not inject an instance of a <code>Tracer</code>, create one from your instantiated <code>TracerProvider</code>:</p> <pre><code>// ...\nvar tracer = tracerProvider.GetTracer(serviceName);\n// Assign it somewhere globally\n//...\n</code></pre> <p>You'll likely want to assign this <code>Tracer</code> instance to a variable in a central location so that you have access to it throughout your service.</p> <p>You can instantiate as many <code>Tracer</code>s as you'd like per service, although it's generally sufficient to just have one defined per service.</p>"},{"location":"docs/instrumentation/net/shim/#creating-spans","title":"Creating Spans","text":"<p>To create a span, give it a name and create it from your <code>Tracer</code>.</p> <pre><code>using var span = MyTracer.StartActiveSpan(\"SayHello\");\n// do work that 'span' will now track\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#creating-nested-spans","title":"Creating nested Spans","text":"<p>If you have a distinct sub-operation you'd like to track as a part of another one, you can create spans to represent the relationship.</p> <pre><code>public static void ParentOperation(Tracer tracer)\n{\nusing var parentSpan = tracer.StartActiveSpan(\"parent-span\");\n// Do some work tracked by parentSpan\nChildOperation(tracer);\n// Finish up work tracked by parentSpan again\n}\npublic static void ChildOperation(Tracer tracer)\n{\nusing var childSpan = tracer.StartActiveSpan(\"child-span\");\n// Track work in ChildOperation with childSpan\n}\n</code></pre> <p>When you view spans in a trace visualization tool, <code>child-span</code> will be tracked as a nested operation under <code>parent-span\"</code>.</p>"},{"location":"docs/instrumentation/net/shim/#nested-spans-in-the-same-scope","title":"Nested Spans in the same scope","text":"<p>You may wish to create a parent-child relationship in the same scope. Although possible, this is generally not recommended because you need to be careful to end any nested <code>TelemetrySpan</code> when you expect it to end.</p> <pre><code>public static void DoWork(Tracer tracer)\n{\nusing var parentSpan = tracer.StartActiveSpan(\"parent-span\");\n// Do some work tracked by parentSpan\nusing (var childSpan = tracer.StartActiveSpan(\"child-span\"))\n{\n// Do some \"child\" work in the same function\n}\n// Finish up work tracked by parentSpan again\n}\n</code></pre> <p>In the preceding example, <code>childSpan</code> is ended because the scope of the <code>using</code> block is explicitly defined, rather than scoped to <code>DoWork</code> itself like <code>parentSpan</code>.</p>"},{"location":"docs/instrumentation/net/shim/#creating-independent-spans","title":"Creating independent Spans","text":"<p>The previous examples showed how to create Spans that follow a nested hierarchy. In some cases, you'll want to create independent Spans that are siblings of the same root rather than being nested.</p> <pre><code>public static void DoWork(Tracer tracer)\n{\nusing var parent = tracer.StartSpan(\"parent\");\n// 'parent' will be the shared parent of both 'child1' and 'child2'\nusing (var child1 = tracer.StartSpan(\"child1\"))\n{\n// do some work that 'child1' tracks\n}\nusing (var child2 = tracer.StartSpan(\"child2\"))\n{\n// do some work that 'child2' tracks\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#creating-new-root-spans","title":"Creating new root Spans","text":"<p>You can also create new root spans that are completely detached from the current trace.</p> <pre><code>public static void DoWork(Tracer tracer)\n{\nusing var newRoot = tracer.StartRootSpan(\"newRoot\");\n}\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#get-the-current-span","title":"Get the current Span","text":"<p>Sometimes it's helpful to access whatever the current <code>TelemetrySpan</code> is at a point in time so you can enrich it with more information.</p> <pre><code>var span = Tracer.CurrentSpan;\n// do cool stuff!\n</code></pre> <p>Note that <code>using</code> is not used in the prior example. Doing so will end current <code>TelemetrySpan</code> when it goes out of scope, which is unlikely to be desired behavior.</p>"},{"location":"docs/instrumentation/net/shim/#add-attributes-to-a-span","title":"Add Attributes to a Span","text":"<p>Attributes let you attach key/value pairs to a <code>TelemetrySpan</code> so it carries more information about the current operation that it's tracking.</p> <pre><code>using var span = tracer.StartActiveSpan(\"SayHello\");\nspan.SetAttribute(\"operation.value\", 1);\nspan.SetAttribute(\"operation.name\", \"Saying hello!\");\nspan.SetAttribute(\"operation.other-stuff\", new int[] { 1, 2, 3 });\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#adding-events","title":"Adding events","text":"<p>An event is a human-readable message on an <code>TelemetrySpan</code> that represents \"something happening\" during its lifetime. You can think of it like a primitive log.</p> <pre><code>using var span = tracer.StartActiveSpan(\"SayHello\");\n// ...\nspan.AddEvent(\"Doing something...\");\n// ...\nspan.AddEvent(\"Dit it!\");\n</code></pre> <p>Events can also be created with a timestamp and a collection of attributes.</p> <pre><code>using var span = tracer.StartActiveSpan(\"SayHello\");\n// ...\nspan.AddEvent(\"event-message\");\nspan.AddEvent(\"event-message2\", DateTimeOffset.Now);\n// ...\nvar attributeData = new Dictionary&lt;string, object&gt;\n{\n{\"foo\", 1 },\n{ \"bar\", \"Hello, World!\" },\n{ \"baz\", new int[] { 1, 2, 3 } }\n};\nspan.AddEvent(\"asdf\", DateTimeOffset.Now, new(attributeData));\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#adding-links","title":"Adding links","text":"<p>A <code>TelemetrySpan</code> can be created with zero or more <code>Link</code>s that are causally related.</p> <pre><code>// Get a context from somewhere, perhaps it's passed in as a parameter\nvar ctx = span.Context;\nvar links = new List&lt;Link&gt;\n{\nnew(ctx)\n};\nusing var span = tracer.StartActiveSpan(\"another-span\", links: links);\n// do some work\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#set-span-status","title":"Set span status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>Status.Error</code>. In rare scenarios, you could override the <code>Error</code> status with <code>Ok</code>, but don't set <code>Ok</code> on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>using var span = tracer.StartActiveSpan(\"SayHello\");\ntry\n{\n// do something\n}\ncatch (Exception ex)\n{\nspan.SetStatus(Status.Error, \"Something bad happened!\");\n}\n</code></pre>"},{"location":"docs/instrumentation/net/shim/#record-exceptions-in-spans","title":"Record exceptions in spans","text":"<p>It can be a good idea to record exceptions when they happen. It's recommended to do this in conjunction with setting span status.</p> <pre><code>using var span = tracer.StartActiveSpan(\"SayHello\");\ntry\n{\n// do something\n}\ncatch (Exception ex)\n{\nspan.SetStatus(Status.Error, \"Something bad happened!\");\nspan.RecordException(ex)\n}\n</code></pre> <p>This will capture things like the current stack trace as attributes in the span.</p>"},{"location":"docs/instrumentation/net/shim/#next-steps","title":"Next steps","text":"<p>After you've setup manual instrumentation, you may want to use instrumentation libraries. Instrumentation libraries will instrument relevant libraries you're using and generate data for things like inbound and outbound HTTP requests and more.</p> <p>You'll also want to configure an appropriate exporter to export your telemetry data to one or more telemetry backends.</p>"},{"location":"docs/instrumentation/other/","title":"Other languages","text":"<p>Implementing the OpenTelemetry specification is not limited to the languages you will find in our documentation. OpenTelemetry is designed in a way that it is possible to implement it in every language you like.</p> <p>For some languages, unofficial implementations exist -- you can find them in the registry. If you know about an implementation not listed there, please add it to the registry.</p> <p>The OpenTelemetry community is open to maintain implementations for additional languages and with that make them \"official\" parts of the OpenTelemetry projects. For that you can raise an issue and gauge interest in the formation of a special interest group for your language.</p> <p>For the following languages a request to create a SIG exists:</p> <ul> <li>Lua</li> <li>Perl</li> <li>Julia</li> </ul>"},{"location":"docs/instrumentation/php/","title":"PHP","text":"<p>{{% lang_instrumentation_index_head php /%}}</p>"},{"location":"docs/instrumentation/php/#further-reading","title":"Further Reading","text":"<ul> <li>OpenTelemetry for PHP on GitHub</li> <li>Installation</li> <li>Examples</li> </ul>"},{"location":"docs/instrumentation/php/#requirements","title":"Requirements","text":"<p>OpenTelemetry for PHP requires a minimum PHP version of 7.4, and auto-instrumentation requires version 8.0+.</p>"},{"location":"docs/instrumentation/php/#dependencies","title":"Dependencies","text":"<p>Some of the <code>SDK</code> and <code>Contrib</code> packages have a dependency on both a HTTP Factories (PSR17) and a php-http/async-client implementation. You can find appropriate composer packages implementing given standards on packagist.org.</p> <p>See http-factory-implementations to find a <code>PSR17 (HTTP factories)</code> implementation, and async-client-implementations to find a <code>php-http/async-client</code> implementation.</p>"},{"location":"docs/instrumentation/php/#optional-php-extensions","title":"Optional PHP extensions","text":"Extension Purpose ext-grpc Required to use gRPC as a transport for the OTLP exporter ext-mbstring More performant than the fallback, <code>symfony/polyfill-mbstring</code> ext-zlib If you want to compress exported data ext-ffi Fiber-based context storage ext-protobuf Significant performance improvement for otlp+protobuf exporting"},{"location":"docs/instrumentation/php/#ext-ffi","title":"ext-ffi","text":"<p>Fibers support can be enabled by setting the <code>OTEL_PHP_FIBERS_ENABLED</code> environment variable to <code>true</code>. Using fibers with non-<code>CLI</code> SAPIs may require preloading of bindings. One way to achieve this is setting <code>ffi.preload</code> to <code>src/Context/fiber/zend_observer_fiber.h</code> and setting <code>opcache.preload</code> to <code>vendor/autoload.php</code>.</p>"},{"location":"docs/instrumentation/php/#ext-protobuf","title":"ext-protobuf","text":"<p>The native protobuf library is significantly slower than the extension. We strongly encourage the use of the extension.</p>"},{"location":"docs/instrumentation/php/#setup","title":"Setup","text":"<p>OpenTelemetry for PHP is distributed via packagist, in a number of packages. We recommend that you install only the packages that you need, which as a minimum is usually <code>API</code>, <code>Context</code>, <code>SDK</code> and an exporter.</p> <p>We strongly encourage that your code only depend on classes and interfaces in the <code>API</code> package.</p>"},{"location":"docs/instrumentation/php/automatic/","title":"Automatic Instrumentation","text":"<p>Automatic instrumentation with PHP requires at least PHP 8.0, and the OpenTelemetry PHP extension. The extension allows developers code to hook into classes and methods, and execute userland code before and after the hooked method runs.</p>"},{"location":"docs/instrumentation/php/automatic/#example","title":"Example","text":"<pre><code>&lt;?php\nOpenTelemetry\\Instrumentation\\hook(\n'class': DemoClass::class,\n'function': 'run',\n'pre': static function (DemoClass $demo, array $params, string $class, string $function, ?string $filename, ?int $lineno) {\nstatic $instrumentation;\n$instrumentation ??= new CachedInstrumentation('example');\n$span = $instrumentation-&gt;tracer()-&gt;spanBuilder($class)-&gt;startSpan();\nContext::storage()-&gt;attach($span-&gt;storeInContext(Context::getCurrent()));\n},\n'post': static function (DemoClass $demo, array $params, $returnValue, ?Throwable $exception) {\n$scope = Context::storage()-&gt;scope();\n$scope-&gt;detach();\n$span = Span::fromContext($scope-&gt;context());\nif ($exception) {\n$span-&gt;recordException($exception);\n$span-&gt;setStatus(StatusCode::STATUS_ERROR);\n}\n$span-&gt;end();\n}\n);\n$demo = new DemoClass();\n$demo-&gt;run();\n</code></pre> <p>Here, we provide <code>pre</code> and <code>post</code> functions, which are executed before and after <code>DemoClass::run</code>. The <code>pre</code> function starts and activates a span, and the <code>post</code> function ends it. If an exception was thrown by <code>DemoClass::run()</code>, the <code>post</code> function will record it, without affecting exception propagation.</p>"},{"location":"docs/instrumentation/php/automatic/#installation","title":"Installation","text":"<p>The extension can be installed via pecl, pickle or php-extension-installer (docker specific).</p> <ol> <li>Setup development environment. Installing from source requires proper    development environment and some dependencies:</li> </ol> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab \"Linux (apt)\" &gt;}}sudo apt-get install gcc make autoconf{{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"MacOS (homebrew)\" &gt;}}brew install gcc make autoconf{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <ol> <li>Build/install the extension. With your environment set up you can install the    extension:</li> </ol> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab pecl &gt;}}pecl install opentelemetry-beta{{&lt; /tab &gt;}}</p> <p>{{&lt; tab pickle &gt;}} php pickle.phar install --source https://github.com/open-telemetry/opentelemetry-php-instrumentation.git#1.0.0beta2 {{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"php-extension-installer (docker)\" &gt;}} install-php-extensions opentelemetry {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <ol> <li>Add the extension to your <code>php.ini</code> file:</li> </ol> <pre><code>[opentelemetry]\nextension=opentelemetry.so\n</code></pre> <ol> <li>Verify that the extension is installed and enabled:</li> </ol> <pre><code>php -m | grep opentelemetry\n</code></pre>"},{"location":"docs/instrumentation/php/automatic/#zero-code-configuration-for-automatic-instrumentation","title":"Zero-code configuration for automatic instrumentation","text":"<p>When used in conjunction with the OpenTelemetry SDK, you can use environment variables or <code>php.ini</code> to configure auto-instrumentation:</p> <pre><code>OTEL_PHP_AUTOLOAD_ENABLED=true \\\nOTEL_SERVICE_NAME=your-service-name \\\nOTEL_TRACES_EXPORTER=otlp \\\nOTEL_EXPORTER_OTLP_PROTOCOL=grpc \\\nOTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4317 \\\nOTEL_PROPAGATORS=baggage,tracecontext \\\nphp myapp.php\n</code></pre>"},{"location":"docs/instrumentation/php/automatic/#manual-setup-for-automatic-instrumentation","title":"Manual setup for automatic instrumentation","text":"<pre><code>&lt;?php\nOpenTelemetry\\API\\Common\\Instrumentation\\Globals::registerInitializer(function (Configurator $configurator) {\n$propagator = TraceContextPropagator::getInstance();\n$spanProcessor = new BatchSpanProcessor(/*params*/);\n$tracerProvider = (new TracerProviderBuilder())\n-&gt;addSpanProcessor($spanProcessor)\n-&gt;setSampler(new ParentBased(new AlwaysOnSampler()))\n-&gt;build();\nShutdownHandler::register([$tracerProvider, 'shutdown']);\nreturn $configurator\n-&gt;withTracerProvider($tracerProvider)\n-&gt;withPropagator($propagator);\n});\n//instrumentation libraries can access the configured providers (or a no-op implementation) via `Globals`\n$tracer = Globals::tracerProvider()-&gt;getTracer('example');\n//or, via CachedInstrumentation\n$instrumentation = new CachedInstrumentation('example');\n$tracerProvider = $instrumentation-&gt;tracer();\n</code></pre>"},{"location":"docs/instrumentation/php/automatic/#supported-libraries-and-frameworks","title":"Supported libraries and frameworks","text":"<p>Automatic Instrumentation comes with a number of instrumentation libraries for commonly used PHP libraries. For the full list, see instrumentation libraries on packagist.</p>"},{"location":"docs/instrumentation/php/automatic/#next-steps","title":"Next steps","text":"<p>After you have automatic instrumentation configured for your app or service, you might want to add manual instrumentation to collect custom telemetry data.</p>"},{"location":"docs/instrumentation/php/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your telemetry, you will need to export it to a backend. OpenTelemetry PHP provides exporters for some common open source backends.</p>"},{"location":"docs/instrumentation/php/exporters/#otlp","title":"OTLP","text":"<p>To send trace data to a OTLP endpoint (like the collector or Jaeger) you'll need to use the <code>open-telemetry/exporter-otlp</code> package:</p> <pre><code>composer require open-telemetry/exporter-otlp\n</code></pre> <p>If you use gRPC, you will also need to install the <code>open-telemetry/transport-grpc</code> package:</p> <pre><code>composer require open-telemetry/transport-grpc\n</code></pre> <p>Next, configure the exporter with an OTLP endpoint. For example, you can update <code>GettingStarted.php</code> from Getting Started like the following:</p> <p>{{&lt; tabpane &gt;}} {{&lt; tab gRPC &gt;}} use OpenTelemetry\\API\\Common\\Signal\\Signals; use OpenTelemetry\\Contrib\\Grpc\\GrpcTransportFactory; use OpenTelemetry\\Contrib\\Otlp\\OtlpUtil; use OpenTelemetry\\Contrib\\Otlp\\SpanExporter; use OpenTelemetry\\SDK\\Trace\\SpanProcessor\\SimpleSpanProcessor; use OpenTelemetry\\SDK\\Trace\\TracerProvider;</p> <p>$transport = (new GrpcTransportFactory())-&gt;create('http://collector:4317' . OtlpUtil::method(Signals::TRACE)); \\(exporter = new SpanExporter(\\)transport); {{&lt; /tab &gt;}} {{&lt; tab protobuf &gt;}} use OpenTelemetry\\Contrib\\Otlp\\OtlpHttpTransportFactory; use OpenTelemetry\\Contrib\\Otlp\\SpanExporter; use OpenTelemetry\\SDK\\Trace\\SpanProcessor\\SimpleSpanProcessor; use OpenTelemetry\\SDK\\Trace\\TracerProvider;</p> <p>$transport = (new OtlpHttpTransportFactory())-&gt;create('http://collector:4318/v1/traces', 'application/x-protobuf'); \\(exporter = new SpanExporter(\\)transport); {{&lt; /tab&gt;}} {{&lt; tab json &gt;}} use OpenTelemetry\\Contrib\\Otlp\\OtlpHttpTransportFactory; use OpenTelemetry\\Contrib\\Otlp\\SpanExporter; use OpenTelemetry\\SDK\\Trace\\SpanProcessor\\SimpleSpanProcessor; use OpenTelemetry\\SDK\\Trace\\TracerProvider;</p> <p>$transport = (new OtlpHttpTransportFactory())-&gt;create('http://collector:4318/v1/traces', 'application/json'); \\(exporter = new SpanExporter(\\)transport); {{&lt; /tab &gt;}} {{&lt; tab nd-json &gt;}} /* newline-delimited JSON */ use OpenTelemetry\\Contrib\\Otlp\\OtlpHttpTransportFactory; use OpenTelemetry\\Contrib\\Otlp\\SpanExporter; use OpenTelemetry\\SDK\\Trace\\SpanProcessor\\SimpleSpanProcessor; use OpenTelemetry\\SDK\\Trace\\TracerProvider;</p> <p>$transport = (new OtlpHttpTransportFactory())-&gt;create('http://collector:4318/v1/traces', 'application/x-ndjson'); \\(exporter = new SpanExporter(\\)transport); {{&lt; /tab &gt;}} {{&lt; /tabpane &gt;}}</p> <p>Then, register the exporter in a tracer provider:</p> <pre><code>$tracerProvider =  new TracerProvider(\n   new SimpleSpanProcessor($exporter)\n);\n$tracer = $tracerProvider-&gt;getTracer('io.opentelemetry.contrib.php');\n</code></pre> <p>To try out the example locally, you can run Jaeger in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 4317:4317 \\\n-p 4318:4318 \\\n-p 14250:14250 \\\n-p 14268:14268 \\\n-p 14269:14269 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/php/exporters/#zipkin","title":"Zipkin","text":"<p>If you're using Zipkin to visualize traces, you'll need to set it up first. Here's how to run it locally in a docker container.</p> <pre><code>docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n</code></pre> <p>Install the exporter package as a dependency for your application:</p> <pre><code>composer require open-telemetry/exporter-zipkin\n</code></pre> <p>Update the example to use the Zipkin exporter and to send data to your zipkin backend:</p> <pre><code>$transport = PsrTransportFactory::discover()-&gt;create('http://zipkin:9411/api/v2/spans', 'application/json');\n$zipkinExporter = new ZipkinExporter($transport);\n$tracerProvider =  new TracerProvider(\n    new SimpleSpanProcessor($zipkinExporter)\n);\n$tracer = $tracerProvider-&gt;getTracer('io.opentelemetry.contrib.php');\n</code></pre>"},{"location":"docs/instrumentation/php/exporters/#minimizing-export-delays","title":"Minimizing export delays","text":"<p>Most PHP runtimes are synchronous and blocking. Sending telemetry data can delay HTTP responses being received by your users.</p> <p>If you are using <code>fastcgi</code>, you could issue a call to <code>fastcgi_finish_request()</code> after sending a user response, which means that delays in sending telemetry data will not hold up request processing.</p> <p>To minimize the impact of slow transport of telemetry data, particularly for external or cloud-based backends, you should consider using a local OpenTelemetry Collector. A local collector can quickly accept, then batch and send all of your telemetry to the backend. Such a setup will make your system more robust and scalable.</p>"},{"location":"docs/instrumentation/php/getting-started/","title":"Getting Started","text":"<p>This page will show you how to get started with OpenTelemetry in PHP.</p> <p>You will learn how you can instrument a simple PHP application automatically, in such a way that traces, metrics and logs are emitted to the console.</p>"},{"location":"docs/instrumentation/php/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have the following installed locally: open source</p> <ul> <li>PHP 8.0+</li> <li>PECL</li> <li>composer</li> </ul> <p>Before you get started make sure that you have both available in your shell:</p> <pre><code>$ php -v\n$ composer -v\n</code></pre>"},{"location":"docs/instrumentation/php/getting-started/#example-application","title":"Example Application","text":"<p>The following example uses a basic Slim application. If you are not using Slim, that's ok \u2014 you can use OpenTelemetry PHP with other web frameworks as well, such as Wordpress, Symfony and Laravel. For a complete list of libraries for supported frameworks, see the registry.</p>"},{"location":"docs/instrumentation/php/getting-started/#dependencies","title":"Dependencies","text":"<p>In an empty directory create a minimal <code>composer.json</code> file in your directory:</p> <pre><code>echo '{\"require\": {}, \"minimum-stability\": \"beta\", \"config\": {\"allow-plugins\": {\"php-http/discovery\": true}}}' &gt; composer.json\n</code></pre> <p>In an empty directory create a minimal <code>composer.json</code> file in your directory:</p> <pre><code>composer require slim/slim:\"4.*\"\ncomposer require slim/psr7\n</code></pre>"},{"location":"docs/instrumentation/php/getting-started/#create-and-launch-an-http-server","title":"Create and launch an HTTP Server","text":"<p>In that same directory, create a file called <code>index.php</code></p>"},{"location":"docs/instrumentation/php/getting-started/#export-to-console","title":"Export to Console","text":"<p>In your directory create a file called <code>GettingStarted.php</code> with the following content:</p> <pre><code>&lt;?php\nuse Psr\\Http\\Message\\ResponseInterface as Response;\nuse Psr\\Http\\Message\\ServerRequestInterface as Request;\nuse Slim\\Factory\\AppFactory;\nrequire __DIR__ . '/vendor/autoload.php';\n$app = AppFactory::create();\n$app-&gt;get('/rolldice', function (Request $request, Response $response, $args) {\n$response-&gt;getBody()-&gt;write(strval(random_int(1,6)));\nreturn $response;\n});\n$app-&gt;run();\n</code></pre> <p>Run the application with the following command and open http://localhost:8080/rolldice in your web browser to ensure it is working.</p> <pre><code>php -S localhost:8080\n</code></pre>"},{"location":"docs/instrumentation/php/getting-started/#instrumentation","title":"Instrumentation","text":"<p>Next, you\u2019ll use the OpenTelemetry PHP extension to automatically instrument the application at launch time.</p> <ol> <li>Since the extension is built from source, you need to setup some build tools</li> </ol> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab \"Linux (apt)\" &gt;}}sudo apt-get install gcc make autoconf{{&lt; /tab &gt;}}</p> <p>{{&lt; tab \"MacOS (homebrew)\" &gt;}}brew install gcc make autoconf{{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane &gt;}}</p> <ol> <li>Build the extension with <code>PECL</code>:</li> </ol> <pre><code>$ pecl install opentelemetry-beta\n</code></pre> <p>{{% alert title=\"Note\" color=\"warning\" %}}If you want to pickle or the    docker-specific use php-extension-installer to build/install the extension,    read the instructions here. {{% /alert %}}</p> <ol> <li>Add the extension to your <code>php.ini</code> file:</li> </ol> <pre><code>[opentelemetry]\nextension=opentelemetry.so\n</code></pre> <ol> <li>Verify that the extension is installed and enabled:</li> </ol> <pre><code>$ php -m | grep opentelemetry\n</code></pre> <ol> <li>Add additional dependencies to your application, which are required for the    automatic instrumentation of your code:</li> </ol> <pre><code>$ composer require php-http/guzzle7-adapter open-telemetry/sdk open-telemetry/opentelemetry-auto-slim\n</code></pre> <p>With the OpenTelemetry PHP extension set up you can now run your application and automatically instrument it at launch time:</p> <pre><code>env OTEL_PHP_AUTOLOAD_ENABLED=true  OTEL_TRACES_EXPORTER=console OTEL_METRICS_EXPORTER=none OTEL_LOGS_EXPORTER=none php -S localhost:8080\n</code></pre> <p>Open http://localhost:8080/rolldice in your web browser and reload the page a few times. After a while you should see the spans printed to your console:</p> View example output <pre><code>[\n{\n\"name\": \"GET /rolldice\",\n\"context\": {\n\"trace_id\": \"16d7c6da7c021c574205736527816eb7\",\n\"span_id\": \"268e52331de62e33\",\n\"trace_state\": \"\"\n},\n\"resource\": {\n\"service.name\": \"__root__\",\n\"service.version\": \"1.0.0+no-version-set\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.language\": \"php\",\n\"telemetry.sdk.version\": \"1.0.0beta10\",\n\"telemetry.auto.version\": \"1.0.0beta5\",\n\"process.runtime.name\": \"cli-server\",\n\"process.runtime.version\": \"8.2.6\",\n\"process.pid\": 24435,\n\"process.executable.path\": \"/bin/php\",\n\"process.owner\": \"php\",\n\"os.type\": \"darwin\",\n\"os.description\": \"22.4.0\",\n\"os.name\": \"Darwin\",\n\"os.version\": \"Darwin Kernel Version 22.4.0: Mon Mar  6 20:59:28 PST 2023; root:xnu-8796.101.5~3/RELEASE_ARM64_T6000\",\n\"host.name\": \"OPENTELEMETRY-PHP\",\n\"host.arch\": \"arm64\"\n},\n\"parent_span_id\": \"\",\n\"kind\": \"KIND_SERVER\",\n\"start\": 1684749478068582482,\n\"end\": 1684749478072715774,\n\"attributes\": {\n\"code.function\": \"handle\",\n\"code.namespace\": \"Slim\\\\App\",\n\"code.filepath\": \"/vendor/slim/slim/Slim/App.php\",\n\"code.lineno\": 197,\n\"http.url\": \"http://localhost:8080/rolldice\",\n\"http.method\": \"GET\",\n\"http.request_content_length\": \"\",\n\"http.scheme\": \"http\",\n\"http.status_code\": 200,\n\"http.flavor\": \"1.1\",\n\"http.response_content_length\": \"\"\n},\n\"status\": {\n\"code\": \"Unset\",\n\"description\": \"\"\n},\n\"events\": [],\n\"links\": []\n}\n]\n</code></pre>"},{"location":"docs/instrumentation/php/getting-started/#whats-next","title":"What's next?","text":"<p>For more:</p> <ul> <li>Run this example with another exporter for telemetry data.</li> <li>Try automatic instrumentation on one of your own apps.</li> <li>Learn about manual instrumentation and try out more   examples.</li> </ul>"},{"location":"docs/instrumentation/php/manual/","title":"Manual Instrumentation","text":"<p>Libraries that want to export telemetry data using OpenTelemetry MUST only depend on the <code>opentelemetry-api</code> package and should never configure or depend on the OpenTelemetry SDK.</p> <p>The SDK configuration must be provided by Applications which should also depend on the <code>opentelemetry-sdk</code> package, or any other implementation of the OpenTelemetry API. This way, libraries will obtain a real implementation only if the user application is configured for it.</p>"},{"location":"docs/instrumentation/php/manual/#installation","title":"Installation","text":"<p>The following shows how to install, initialize, and run an application instrumented with OpenTelemetry.</p> <p>To use the OpenTelemetry SDK for PHP you need packages that satisfy the dependencies for <code>php-http/async-client-implementation</code> and <code>psr/http-factory-implementation</code>, for example the Guzzle 7 HTTP Adapter satisfies both:</p> <pre><code>composer require \"php-http/guzzle7-adapter\"\n</code></pre> <p>Now you can install the OpenTelemetry SDK:</p> <pre><code>composer require open-telemetry/sdk\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/php/manual/#setup","title":"Setup","text":"<p>The first step is to get a handle to an instance of the <code>OpenTelemetry</code> interface.</p> <p>If you are an application developer, you need to configure an instance of the <code>OpenTelemetry SDK</code> as early as possible in your application. This can be done using the <code>Sdk::builder()</code> method. The returned <code>SdkBuilder</code> instance gets the providers related to the signals, tracing and metrics, in order to build the <code>OpenTelemetry</code> instance.</p> <p>You can build the providers by using the <code>TracerProvider::builder()</code> and <code>MeterProvider::builder()</code> methods. It is also strongly recommended to define a <code>Resource</code> instance as a representation of the entity producing the telemetry; in particular the <code>service.name</code> attribute is the most important piece of telemetry source-identifying info.</p>"},{"location":"docs/instrumentation/php/manual/#example","title":"Example","text":"<pre><code>&lt;?php\n$resource = ResourceInfoFactory::defaultResource();\n$transport = (new GrpcTransportFactory())-&gt;create('http://collector:4317' . OtlpUtil::method(Signals::TRACE));\n$exporter = new SpanExporter($transport);\n$reader = new ExportingReader(\nnew MetricExporter(\nPsrTransportFactory::discover()-&gt;create('http://collector:4318/v1/metrics', 'application/x-protobuf')\n),\nClockFactory::getDefault()\n);\n$meterProvider = MeterProvider::builder()\n-&gt;setResource($resource)\n-&gt;addReader($reader)\n-&gt;build();\n$tracerProvider = TracerProvider::builder()\n-&gt;addSpanProcessor(\n(new BatchSpanProcessorBuilder($spanExporter))\n-&gt;setMeterProvider($meterProvider)\n-&gt;build()\n)\n-&gt;setResource($resource)\n-&gt;setSampler(new ParentBased(new AlwaysOnSampler()))\n-&gt;build();\nSdk::builder()\n-&gt;setTracerProvider($tracerProvider)\n-&gt;setMeterProvider($meterProvider)\n-&gt;setPropagator(TraceContextPropagator::getInstance())\n-&gt;setAutoShutdown(true)\n-&gt;buildAndRegisterGlobal();\n$instrumentation = new CachedInstrumentation('example');\n$tracer = $instrumentation-&gt;tracer();\n</code></pre> <p>It's important to run the tracer provider's <code>shutdown()</code> method when the PHP process ends, to enable flushing of any enqueued telemetry. The shutdown process is blocking, so consider running it in an async process. Otherwise, you can use the <code>ShutdownHandler</code> to register the shutdown function as part of PHP's shutdown process:</p> <pre><code>\\OpenTelemetry\\SDK\\Common\\Util\\ShutdownHandler::register([$tracerProvider, 'shutdown']);\n\\OpenTelemetry\\SDK\\Common\\Util\\ShutdownHandler::register([$meterProvider, 'shutdown']);\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#acquiring-a-tracer","title":"Acquiring a Tracer","text":"<p>To do Tracing you'll need to acquire a <code>Tracer</code>.</p> <p>Note: Methods of the OpenTelemetry SDK should never be called.</p> <p>First, a <code>Tracer</code> must be acquired, which is responsible for creating spans and interacting with the Context. A tracer is acquired by using the OpenTelemetry API specifying the name and version of the library instrumenting the instrumented library or application to be monitored. More information is available in the specification chapter Obtaining a Tracer.</p> <pre><code>$tracer = Globals::tracerProvider()-&gt;getTracer('instrumentation-library-name', '1.0.0');\n</code></pre> <p>Important: the \"name\" and optional version of the tracer are purely informational. All <code>Tracer</code>s that are created by a single <code>OpenTelemetry</code> instance will interoperate, regardless of name.</p>"},{"location":"docs/instrumentation/php/manual/#create-spans","title":"Create Spans","text":"<p>To create Spans, you only need to specify the name of the span. The start and end time of the span is automatically set by the OpenTelemetry SDK.</p> <pre><code>$span = $tracer-&gt;spanBuilder(\"my span\")-&gt;startSpan();\n// Make the span the current span\ntry {\n  $scope = $span-&gt;activate();\n  // In this scope, the span is the current/active span\n} finally {\n    $span-&gt;end();\n    $scope-&gt;detach();\n}\n</code></pre> <p>It's required to call <code>end()</code> to end the span, and you must <code>detach</code> the active scope if you have activated it.</p>"},{"location":"docs/instrumentation/php/manual/#create-nested-spans","title":"Create nested Spans","text":"<p>Most of the time, we want to correlate spans for nested operations. OpenTelemetry supports tracing within processes and across remote processes. For more details how to share context between remote processes, see Context Propagation.</p> <p>For a method <code>a</code> calling a method <code>b</code>, the spans could be manually linked in the following way:</p> <pre><code>  $parentSpan = $tracer-&gt;spanBuilder(\"parent\")-&gt;startSpan();\n  $scope = $parentSpan-&gt;activate();\n  try {\n    $child = $tracer-&gt;spanBuilder(\"child\")-&gt;startSpan();\n    //do stuff\n    $child-&gt;end();\n  } finally {\n    $parentSpan-&gt;end();\n    $scope-&gt;detach();\n  }\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#get-the-current-span","title":"Get the current span","text":"<p>Sometimes it's helpful to do something with the current/active span at a particular point in program execution.</p> <pre><code>$span = OpenTelemetry\\API\\Trace\\Span::getCurrent();\n</code></pre> <p>And if you want the current span for a particular <code>Context</code> object:</p> <pre><code>$span = OpenTelemetry\\API\\Trace\\Span::fromContext($context);\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#span-attributes","title":"Span Attributes","text":"<p>In OpenTelemetry spans can be created freely and it's up to the implementor to annotate them with attributes specific to the represented operation. Attributes provide additional context on a span about the specific operation it tracks, such as results or operation properties.</p> <pre><code>$span = $tracer-&gt;spanBuilder(\"/resource/path\")-&gt;setSpanKind(SpanKind::CLIENT)-&gt;startSpan();\n$span-&gt;setAttribute(\"http.method\", \"GET\");\n$span-&gt;setAttribute(\"http.url\", (string) $url);\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#create-spans-with-events","title":"Create Spans with events","text":"<p>Spans can be annotated with named events (called Span Events) that can carry zero or more Span Attributes, each of which itself is a key:value map paired automatically with a timestamp.</p> <pre><code>$span-&gt;addEvent(\"Init\");\n...\n$span-&gt;addEvent(\"End\");\n</code></pre> <pre><code>$eventAttributes = Attributes::create([\n    \"key\" =&gt; \"value\",\n    \"result\" =&gt; 3.14159;\n]);\n$span-&gt;addEvent(\"End Computation\", $eventAttributes);\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#create-spans-with-links","title":"Create Spans with links","text":"<p>A Span may be linked to zero or more other Spans that are causally related via a Span Link. Links can be used to represent batched operations where a Span was initiated by multiple initiating Spans, each representing a single incoming item being processed in the batch.</p> <pre><code>$span = $tracer-&gt;spanBuilder(\"span-with-links\")\n        -&gt;addLink($parentSpan1-&gt;getContext())\n        -&gt;addLink($parentSpan2-&gt;getContext())\n        -&gt;addLink($parentSpan3-&gt;getContext())\n        -&gt;addLink($remoteSpanContext)\n    -&gt;startSpan();\n</code></pre> <p>For more details how to read context from remote processes, see Context Propagation.</p>"},{"location":"docs/instrumentation/php/manual/#set-span-status-and-record-exceptions","title":"Set span status and record exceptions","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>SpanStatus::ERROR</code>. In rare scenarios, you could override the <code>Error</code> status with <code>Ok</code>, but don't set <code>Ok</code> on successfully-completed spans.</p> <p>It can be a good idea to record exceptions when they happen. It's recommended to do this in conjunction with setting span status.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>$span = $tracer-&gt;spanBuilder(\"my-span\")-&gt;startSpan();\n$scope = $span-&gt;activate();\ntry {\n  // do something\n} catch (Throwable $t) {\n  $span-&gt;setStatus(StatusCode::STATUS_ERROR, \"Something bad happened!\");\n  $span-&gt;recordException($t); //This will capture things like the current stack trace in the span.\n  throw $t;\n} finally {\n  $span-&gt;end(); // Cannot modify span after this call\n  $scope-&gt;detach();\n}\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#sampler","title":"Sampler","text":"<p>It is not always feasible to trace and export every user request in an application. In order to strike a balance between observability and expenses, traces can be sampled.</p> <p>The OpenTelemetry SDK offers four samplers out of the box:</p> <ul> <li><code>AlwaysOnSampler</code> which samples every trace regardless of upstream sampling   decisions.</li> <li><code>AlwaysOffSampler</code> which doesn't sample any trace, regardless of upstream   sampling decisions.</li> <li><code>ParentBased</code> which uses the parent span to make sampling decisions, if   present.</li> <li><code>TraceIdRatioBased</code> which samples a configurable percentage of traces, and   additionally samples any trace that was sampled upstream.</li> </ul> <pre><code>$tracerProvider = TracerProvider::builder()\n  -&gt;setSampler(new AlwaysOnSampler())\n  //or\n  -&gt;setSampler(new AlwaysOffSampler())\n  //or\n  -&gt;setSampler(new TraceIdRatioBasedSampler(0.5))\n  -&gt;build();\n</code></pre> <p>Additional samplers can be provided by implementing <code>OpenTelemetry\\SDK\\Trace\\SamplerInterface</code>. An example of doing so would be to make sampling decisions based on attributes set at span creation time.</p>"},{"location":"docs/instrumentation/php/manual/#span-processor","title":"Span Processor","text":"<p>Different Span processors are offered by OpenTelemetry. The <code>SimpleSpanProcessor</code> immediately forwards ended spans to the exporter, while the <code>BatchSpanProcessor</code> batches them and sends them in bulk.</p> <pre><code>$tracerProvider = TracerProvider::builder()\n  -&gt;addSpanProcessor(new SimpleSpanProcessor(new ConsoleSpanExporterFactory()-&gt;create()))\n  -&gt;build();\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#transports","title":"Transports","text":"<p>All exporters require a <code>Transport</code>, which is responsible for the sending of telemetry data from an exporter:</p> <ul> <li><code>PsrTransport</code> - uses a PSR18 client to send data over HTTP</li> <li><code>StreamTransport</code> - uses a stream to send data</li> <li><code>GrpcTransport</code> - uses gRPC to send protobuf-encoded data</li> </ul>"},{"location":"docs/instrumentation/php/manual/#exporter","title":"Exporter","text":"<p>Span processors are initialized with an exporter which is responsible for sending the telemetry data to a particular backend:</p> <ul> <li><code>InMemory</code>: keeps the data in memory, useful for testing and debugging.</li> <li><code>Console</code>: sends the data to a stream such as <code>stdout</code> or <code>stderr</code></li> <li><code>Zipkin</code>: prepares and sends the collected telemetry data to a Zipkin backend   via the Zipkin APIs.</li> <li>Logging Exporter: saves the telemetry data into log streams.</li> <li>OpenTelemetry Protocol Exporter: sends the data in OTLP format to the   OpenTelemetry Collector or other OTLP receivers. The   underlying <code>Transport</code> can send:</li> <li>protobuf over HTTP</li> <li>protobuf over gRPC</li> <li>JSON over HTTP</li> </ul>"},{"location":"docs/instrumentation/php/manual/#logging-and-error-handling","title":"Logging and Error Handling","text":"<p>OpenTelemetry can be configured to use a PSR-3 logger to log information about OpenTelemetry, including errors and warnings about misconfigurations or failures exporting data:</p> <pre><code>$logger = new Psr3Logger(LogLevel::INFO);\nLoggerHolder::set($logger);\n</code></pre> <p>If no PSR-3 logger is provided, error messages will instead be recorded via <code>trigger_error</code> (at a level no higher than <code>E_USER_WARNING</code>).</p> <p>For more fine-grained control and special case handling, custom handlers and filters can be applied to the logger (if the logger offers this ability).</p>"},{"location":"docs/instrumentation/php/manual/#metrics","title":"Metrics","text":"<p>OpenTelemetry can be used to measure and record different types of metrics from an application, which can then be pushed to a metrics service such as the OpenTelemetry collector:</p> <ul> <li>counter</li> <li>async counter</li> <li>histogram</li> <li>async gauge</li> <li>up/down counter</li> <li>async up/down counter</li> </ul> <p>Meter types and usage are explained in the metrics concepts documentation.</p>"},{"location":"docs/instrumentation/php/manual/#setup_1","title":"Setup","text":"<p>First, create a <code>MeterProvider</code>:</p> <pre><code>$reader = new ExportingReader((new ConsoleMetricExporterFactory())-&gt;create());\n$meterProvider = MeterProvider::builder()\n    -&gt;addReader($reader)\n    -&gt;build();\n</code></pre> <p>You can now use the meter provider to retrieve meters.</p>"},{"location":"docs/instrumentation/php/manual/#synchronous-meters","title":"Synchronous meters","text":"<p>A synchronous meter must be manually adjusted as data changes:</p> <pre><code>$up_down = $meterProvider\n    -&gt;getMeter('my_up_down')\n    -&gt;createUpDownCounter('queued', 'jobs', 'The number of jobs enqueued');\n//jobs come in\n$up_down-&gt;add(2);\n//job completed\n$up_down-&gt;add(-1);\n//more jobs come in\n$up_down-&gt;add(2);\n$meterProvider-&gt;forceFlush();\n</code></pre> <p>Synchronous metrics are exported when <code>forceFlush()</code> and/or <code>shutdown()</code> are called on the meter provider.</p>"},{"location":"docs/instrumentation/php/manual/#asynchronous-meters","title":"Asynchronous meters","text":"<p>Async meters are <code>observable</code>, eg <code>ObservableGauge</code>. When registering an observable/async meter, you provide one or more callback functions. The callback functions will be called by a periodic exporting metric reader, for example based on an event-loop timer. The callback(s) are responsible for returning the latest data for the meter.</p> <p>In this example, the callbacks are executed when <code>$reader-&gt;collect()</code> is executed:</p> <pre><code>$queue = [\n    'job1',\n    'job2',\n    'job3',\n];\n$meterProvider\n    -&gt;getMeter('my_gauge')\n    -&gt;createObservableGauge('queued', 'jobs', 'The number of jobs enqueued')\n    -&gt;observe(static function (ObserverInterface $observer) use ($queue): void {\n        $observer-&gt;observe(count($queue));\n    });\n$reader-&gt;collect();\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#readers","title":"Readers","text":"<p>Currently we only have an <code>ExportingReader</code>, which is an implementation of the periodic exporting metric reader. When its <code>collect()</code> method is called, all associated asynchronous meters are observed, and metrics pushed to the exporter.</p>"},{"location":"docs/instrumentation/php/manual/#logging","title":"Logging","text":"<p>As logging is a mature and well-established function, the OpenTelemetry approach is a little different for this signal.</p> <p>The OpenTelemetry logger is not designed to be used directly, but rather to be integrated into existing logging libraries as a handler. In this way, you can choose to have some or all of your application logs sent to an OpenTelemetry-compatible service such as the collector.</p>"},{"location":"docs/instrumentation/php/manual/#setup_2","title":"Setup","text":"<p>You get a logger from a <code>LoggerProvider</code>. Log records get emitted via an <code>EventLogger</code>:</p> <pre><code>&lt;?php\n$loggerProvider = new LoggerProvider(\nnew SimpleLogsProcessor(\nnew ConsoleExporter()\n)\n);\n$logger = $loggerProvider-&gt;getLogger('demo', '1.0', 'http://schema.url', [/*attributes*/]);\n$eventLogger = new EventLogger($logger, 'my-domain');\n</code></pre> <p>Once configured, a <code>LogRecord</code> can be created and sent via the event logger's <code>logEvent</code>method:</p> <pre><code>$record = (new LogRecord('hello world'))\n    -&gt;setSeverityText('INFO')\n    -&gt;setAttributes([/*attributes*/]);\n$eventLogger-&gt;logEvent('foo', $record);\n</code></pre>"},{"location":"docs/instrumentation/php/manual/#integrations-for-3rd-party-logging-libraries","title":"Integrations for 3rd-party logging libraries","text":""},{"location":"docs/instrumentation/php/manual/#monolog","title":"Monolog","text":"<p>You can use the monolog handler to send monolog logs to an OpenTelemetry-capable receiver:</p> <pre><code>composer require open-telemetry/opentelemetry-logger-monolog\n</code></pre> <pre><code>$loggerProvider = new LoggerProvider(/*params*/);\n$handler = new \\OpenTelemetry\\Contrib\\Logs\\Monolog\\Handler(\n    $loggerProvider,\n    \\Psr\\Log\\LogLevel::ERROR,\n);\n$logger = new \\Monolog\\Logger('example', [$handler]);\n$logger-&gt;info('hello, world');\n$logger-&gt;error('oh no', [\n    'foo' =&gt; 'bar',\n    'exception' =&gt; new \\Exception('something went wrong'),\n]);\n$loggerProvider-&gt;shutdown();\n</code></pre>"},{"location":"docs/instrumentation/php/propagation/","title":"Propagation","text":"<p>Propagation is the mechanism that moves data between services and processes. Although not limited to tracing, it is what allows traces to build causal information about a system across services that are arbitrarily distributed across process and network boundaries.</p> <p>OpenTelemetry provides a text-based approach to propagate context to remote services using the W3C Trace Context HTTP headers.</p>"},{"location":"docs/instrumentation/php/propagation/#context-propagation-with-frameworks-and-libraries","title":"Context propagation with frameworks and libraries","text":"<p>Auto-instrumentation exists for some popular PHP frameworks (eg Symfony, Laravel, Slim) and HTTP libraries propagate context for incoming and outgoing HTTP requests.</p> <p>We highly recommend that you use auto-instrumentation or instrumentation libraries to propagate context. Although it is possible to propagate context manually, the PHP auto-instrumentation and instrumentation libraries are well-tested and easier to use.</p>"},{"location":"docs/instrumentation/php/propagation/#incoming","title":"Incoming","text":"<p>Auto-instrumentation for frameworks which implement the PSR-15 <code>RequestHandlerInterface</code> will automatically extract W3C tracecontext headers, create a root span, and set a remote parent for the root span.</p> <pre><code>composer require open-telemetry/opentelemetry-auto-psr15\n</code></pre>"},{"location":"docs/instrumentation/php/propagation/#outgoing","title":"Outgoing","text":"<p>PSR-18 auto-instrumentation will automatically apply W3C tracecontext headers to outgoing HTTP requests for any library which implements the PSR-18 interface.</p> <pre><code>open-telemetry/opentelemetry-auto-psr18\n</code></pre>"},{"location":"docs/instrumentation/php/propagation/#manual-w3c-trace-context-propagation","title":"Manual W3C Trace Context Propagation","text":"<p>In some cases, it is not possible to propagate context with an instrumentation library. There may not be an instrumentation library that matches a library you're using to have services communicate with one another. Or you many have requirements that instrumentation libraries cannot fulfill, even if they exist.</p> <p>When you must propagate context manually, you can use the context api.</p> <p>The following presents an example of an outgoing HTTP request:</p> <pre><code>$request = new Request('GET', 'http://localhost:8080/resource');\n$outgoing = $tracer-&gt;spanBuilder('/resource')-&gt;setSpanKind(SpanKind::CLIENT)-&gt;startSpan();\n$outgoing-&gt;setAttribute(TraceAttributes::HTTP_METHOD, $request-&gt;getMethod());\n$outgoing-&gt;setAttribute(TraceAttributes::HTTP_URL, (string) $request-&gt;getUri());\n$carrier = [];\nTraceContextPropagator::getInstance()-&gt;inject($carrier);\nforeach ($carrier as $name =&gt; $value) {\n    $request = $request-&gt;withAddedHeader($name, $value);\n}\ntry {\n    $response = $client-&gt;send($request);\n} finally {\n    $outgoing-&gt;end();\n}\n</code></pre> <p>Similarly, the text-based approach can be used to read the W3C Trace Context from incoming requests. The following presents an example of processing an incoming HTTP request:</p> <pre><code>$request = ServerRequestCreator::createFromGlobals();\n$context = TraceContextPropagator::getInstance()-&gt;extract($request-&gt;getHeaders());\n$root = $tracer-&gt;spanBuilder('HTTP ' . $request-&gt;getMethod())\n    -&gt;setStartTimestamp((int) ($request-&gt;getServerParams()['REQUEST_TIME_FLOAT'] * 1e9))\n    -&gt;setParent($context)\n    -&gt;setSpanKind(SpanKind::KIND_SERVER)\n    -&gt;startSpan();\n$scope = $root-&gt;activate();\ntry {\n    /* do stuff */\n} finally {\n    $root-&gt;end();\n    $scope-&gt;detach();\n}\n</code></pre>"},{"location":"docs/instrumentation/php/resources/","title":"Resources","text":"<p>A resource represents the entity producing telemetry as resource attributes. For example, a process producing telemetry that is running in a container on Kubernetes has a Pod name, a namespace, and possibly a deployment name. All three of these attributes can be included in the resource.</p> <p>In your observability backend, you can use resource information to better investigate interesting behavior. For example, if your trace or metrics data indicate latency in your system, you can narrow it down to a specific container, pod, or Kubernetes deployment.</p>"},{"location":"docs/instrumentation/php/resources/#resource-detection","title":"Resource Detection","text":"<p>The PHP SDK detects resources from a variety of sources, and by default will use all available resource detectors:</p> <ul> <li>environment (<code>OTEL_RESOURCE_ATTRIBUTES</code>, <code>OTEL_SERVICE_NAME</code>)</li> <li>host information</li> <li>host operating system</li> <li>current process</li> <li>runtime</li> </ul>"},{"location":"docs/instrumentation/php/resources/#disabling-resource-detection","title":"Disabling resource detection","text":"<p>By default, all SDK resource detectors are used, but you can use the environment variable <code>OTEL_PHP_RESOURCE_DETECTORS</code> to enable only certain detectors, or completely disable them:</p> <ul> <li><code>env</code></li> <li><code>host</code></li> <li><code>os</code></li> <li><code>process</code></li> <li><code>process_runtime</code></li> <li><code>sdk</code></li> <li><code>sdk_provided</code></li> <li><code>all</code> - enable all resource detectors</li> <li><code>none</code> - disable resource detection</li> </ul> <p>For example, to enable only the <code>env</code>, <code>host</code> and <code>sdk</code> detectors:</p> <pre><code>env OTEL_PHP_RESOURCE_DETECTORS=env,host,sdk \\\nphp example.php\n</code></pre>"},{"location":"docs/instrumentation/php/resources/#custom-resource-detectors","title":"Custom resource detectors","text":"<p>Resource detectors for generic platforms or vendor-specific environments can be installed as composer packages.</p> <p>For example, to install and enable the <code>container</code> resource detector:</p> <pre><code>composer require open-telemetry/detector-container\nenv OTEL_PHP_RESOURCE_DETECTORS=container \\\nphp example.php\n</code></pre> <p>Note that installed detectors are automatically included in the default <code>all</code> resource detector list.</p>"},{"location":"docs/instrumentation/php/resources/#adding-resources-with-environment-variables","title":"Adding resources with environment variables","text":"<p>If there is not an SDK detector for the resource you need, you can add arbitrary resources via the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable, which is interpreted by the <code>env</code> detector. This variable takes a comma-separated list of key=value pairs, for example:</p> <pre><code>env OTEL_RESOURCE_ATTRIBUTES=\"service.name=my_service,service.namespace=demo,service.version=1.0,deployment.environment=development\" \\\nphp example.php\n</code></pre>"},{"location":"docs/instrumentation/php/resources/#adding-resources-in-code","title":"Adding resources in code","text":"<p>Custom resources can also be configured in your code. Here, the default resources (detected as described above) are merged with custom resources. The resources are then passed to the tracer provider, where they will be associated with all generated spans.</p> <pre><code>$resource = ResourceInfoFactory::defaultResource()-&gt;merge(ResourceInfo::create(Attributes::create([\n    ResourceAttributes::SERVICE_NAMESPACE =&gt; 'foo',\n    ResourceAttributes::SERVICE_NAME =&gt; 'bar',\n    ResourceAttributes::SERVICE_INSTANCE_ID =&gt; 1,\n    ResourceAttributes::SERVICE_VERSION =&gt; '0.1',\n    ResourceAttributes::DEPLOYMENT_ENVIRONMENT =&gt; 'development',\n])));\n$tracerProvider =  new TracerProvider(\n    new SimpleSpanProcessor(\n        (new ConsoleSpanExporterFactory())-&gt;create()\n    ),\n    null,\n    $resource\n);\n</code></pre>"},{"location":"docs/instrumentation/php/sdk/","title":"SDK","text":"<p>The OpenTelemetry SDK provides a working implementation of the API, and can be set up and configured in a number of ways.</p>"},{"location":"docs/instrumentation/php/sdk/#manual-setup","title":"Manual setup","text":"<p>Setting up an SDK manually gives you the most control over the SDK's configuration:</p> <pre><code>&lt;?php\n$exporter = new InMemoryExporter();\n$meterProvider = new NoopMeterProvider();\n$tracerProvider =  new TracerProvider(\nnew BatchSpanProcessor(\n$exporter,\nClockFactory::getDefault(),\n2048, //max queue size\n5000, //export timeout\n1024, //max batch size\ntrue, //autoflush\n$meterProvider\n)\n);\n</code></pre>"},{"location":"docs/instrumentation/php/sdk/#sdk-builder","title":"SDK Builder","text":"<p>The SDK builder provides a convenient interface to configure parts of the SDK. However, it doesn't support all of the features that manual setup does.</p> <pre><code>&lt;?php\n$spanExporter = new InMemoryExporter(); //mock exporter for demonstration purposes\n$meterProvider = MeterProvider::builder()\n-&gt;registerMetricReader(\nnew ExportingReader(new MetricExporter((new StreamTransportFactory())-&gt;create(STDOUT, 'application/x-ndjson'), /*Temporality::CUMULATIVE*/))\n)\n-&gt;build();\n$tracerProvider = TracerProvider::builder()\n-&gt;addSpanProcessor(\n(new BatchSpanProcessorBuilder($spanExporter))\n-&gt;setMeterProvider($meterProvider)\n-&gt;build()\n)\n-&gt;build();\n$loggerProvider = LoggerProvider::builder()\n-&gt;addLogRecordProcessor(\nnew SimpleLogsProcessor(\n(new ConsoleExporterFactory())-&gt;create()\n)\n)\n-&gt;setResource(ResourceInfo::create(Attributes::create(['foo' =&gt; 'bar'])))\n-&gt;build();\nSdk::builder()\n-&gt;setTracerProvider($tracerProvider)\n-&gt;setLoggerProvider($loggerProvider)\n-&gt;setMeterProvider($meterProvider)\n-&gt;setPropagator(TraceContextPropagator::getInstance())\n-&gt;setAutoShutdown(true)\n-&gt;buildAndRegisterGlobal();\n</code></pre>"},{"location":"docs/instrumentation/php/sdk/#autoloading","title":"Autoloading","text":"<p>If all configuration comes from environment variables (or <code>php.ini</code>), you can use SDK autoloading to automatically configure and globally register an SDK. The only requirement for this is that you set <code>OTEL_PHP_AUTOLOAD_ENABLED=true</code>, and provide any required/non-standard configuration as set out in sdk-configuration.</p> <p>For example:</p> <pre><code>OTEL_PHP_AUTOLOAD_ENABLED=true \\\nOTEL_EXPORTER_OTLP_PROTOCOL=grpc \\\nOTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4317 \\\nphp example.php\n</code></pre> <pre><code>&lt;?php\nrequire 'vendor/autoload.php'; //sdk autoloading happens as part of composer initialization\n$tracer = OpenTelemetry\\API\\Common\\Instrumentation\\Globals::tracerProvider()-&gt;getTracer('name', 'version', 'schema.url', [/*attributes*/]);\n$meter = OpenTelemetry\\API\\Common\\Instrumentation\\Globals::meterProvider()-&gt;getTracer('name', 'version', 'schema.url', [/*attributes*/]);\n</code></pre> <p>SDK autoloading happens as part of the composer autoloader.</p> <p>{{% alert title=\"Important\" color=\"warning\" %}}The batch span and log processors emit metrics about their internal state, so ensure that you have a correctly configured metrics exporter. Alternatively, you can disable metrics by setting <code>OTEL_METRICS_EXPORTER=none</code>}</p>"},{"location":"docs/instrumentation/php/sdk/#configuration","title":"Configuration","text":"<p>The PHP SDK supports most of the available configurations. Our conformance to the specification is listed in the spec compliance matrix.</p> <p>There are also a number of PHP-specific configurations:</p> Name Default value Values Example Description OTEL_PHP_TRACES_PROCESSOR batch batch, simple simple Span processor selection OTEL_PHP_DETECTORS all env, host, os, process, process_runtime, sdk, sdk_provided, container env,os,process Resource detector selection OTEL_PHP_AUTOLOAD_ENABLED false true, false true Enable/disable SDK autoloading OTEL_PHP_DISABLED_INSTRUMENTATIONS [] Instrumentation name(s) psr15,psr18 Disable one or more installed auto-instrumentations <p>Configurations can be provided as environment variables, or via <code>php.ini</code> (or a file included by <code>php.ini</code>)</p>"},{"location":"docs/instrumentation/python/","title":"Python","text":"<p>{{% lang_instrumentation_index_head python /%}}</p>"},{"location":"docs/instrumentation/python/#version-support","title":"Version support","text":"<p>OpenTelemetry-Python supports Python 3.6 and higher.</p>"},{"location":"docs/instrumentation/python/#installation","title":"Installation","text":"<p>The API and SDK packages are available on PyPI, and can be installed via pip:</p> <pre><code>pip install opentelemetry-api\npip install opentelemetry-sdk\n</code></pre> <p>In addition, there are several extension packages which can be installed separately as:</p> <pre><code>pip install opentelemetry-exporter-{exporter}\npip install opentelemetry-instrumentation-{instrumentation}\n</code></pre> <p>These are for exporter and instrumentation packages respectively. The Jaeger, Zipkin, Prometheus, OTLP and OpenCensus Exporters can be found in the exporter directory of the repository. Instrumentations and additional exporters can be found in the contrib repo instrumentation and exporter directories.</p>"},{"location":"docs/instrumentation/python/#extensions","title":"Extensions","text":"<p>To find related projects like exporters, instrumentation libraries, tracer implementations, etc., visit the Registry.</p>"},{"location":"docs/instrumentation/python/#installing-cutting-edge-packages","title":"Installing Cutting-edge Packages","text":"<p>There is some functionality that has not yet been released to PyPI. In that situation, you may want to install the packages directly from the repo. This can be done by cloning the repository and doing an editable install:</p> <pre><code>git clone https://github.com/open-telemetry/opentelemetry-python.git\ncd opentelemetry-python\npip install -e ./opentelemetry-api\npip install -e ./opentelemetry-sdk\n</code></pre>"},{"location":"docs/instrumentation/python/#repositories-and-benchmarks","title":"Repositories and benchmarks","text":"<ul> <li>Main repo: opentelemetry-python</li> <li>Contrib repo: opentelemetry-python-contrib</li> </ul>"},{"location":"docs/instrumentation/python/cookbook/","title":"Cookbook","text":"<p>This page is a cookbook for common scenarios.</p>"},{"location":"docs/instrumentation/python/cookbook/#create-a-new-span","title":"Create a new span","text":"<pre><code>from opentelemetry import trace\ntracer = trace.get_tracer(\"my.tracer\")\nwith tracer.start_as_current_span(\"print\") as span:\nprint(\"foo\")\nspan.set_attribute(\"printed_string\", \"foo\")\n</code></pre>"},{"location":"docs/instrumentation/python/cookbook/#getting-and-modifying-a-span","title":"Getting and modifying a span","text":"<pre><code>from opentelemetry import trace\ncurrent_span = trace.get_current_span()\ncurrent_span.set_attribute(\"hometown\", \"seattle\")\n</code></pre>"},{"location":"docs/instrumentation/python/cookbook/#create-a-nested-span","title":"Create a nested span","text":"<pre><code>from opentelemetry import trace\nimport time\ntracer = trace.get_tracer(\"my.tracer\")\n# Create a new span to track some work\nwith tracer.start_as_current_span(\"parent\"):\ntime.sleep(1)\n# Create a nested span to track nested work\nwith tracer.start_as_current_span(\"child\"):\ntime.sleep(2)\n# the nested span is closed when it's out of scope\n# Now the parent span is the current span again\ntime.sleep(1)\n# This span is also closed when it goes out of scope\n</code></pre>"},{"location":"docs/instrumentation/python/cookbook/#capturing-baggage-at-different-contexts","title":"Capturing baggage at different contexts","text":"<pre><code>from opentelemetry import trace, baggage\ntracer = trace.get_tracer(\"my.tracer\")\nwith tracer.start_as_current_span(name=\"root span\") as root_span:\nparent_ctx = baggage.set_baggage(\"context\", \"parent\")\nwith tracer.start_as_current_span(\nname=\"child span\", context=parent_ctx\n) as child_span:\nchild_ctx = baggage.set_baggage(\"context\", \"child\")\nprint(baggage.get_baggage(\"context\", parent_ctx))\nprint(baggage.get_baggage(\"context\", child_ctx))\n</code></pre>"},{"location":"docs/instrumentation/python/cookbook/#manually-setting-span-context","title":"Manually setting span context","text":"<p>Usually your application or serving framework will take care of propagating your trace context for you. But in some cases, you may need to save your trace context (with <code>.inject</code>) and restore it elsewhere (with <code>.extract</code>) yourself.</p> <pre><code>from opentelemetry import trace, context\nfrom opentelemetry.trace import NonRecordingSpan, SpanContext, TraceFlags\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\nfrom opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator\n# Set up a simple processor to write spans out to the console so we can see what's happening.\ntrace.set_tracer_provider(TracerProvider())\ntrace.get_tracer_provider().add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\ntracer = trace.get_tracer(\"my.tracer\")\n# A TextMapPropagator works with any dict-like object as its Carrier by default. You can also implement custom getters and setters.\nwith tracer.start_as_current_span('first-trace'):\ncarrier = {}\n# Write the current context into the carrier.\nTraceContextTextMapPropagator().inject(carrier)\n# The below might be in a different thread, on a different machine, etc.\n# As a typical example, it would be on a different microservice and the carrier would\n# have been forwarded via HTTP headers.\n# Extract the trace context from the carrier.\n# Here's what a typical carrier might look like, as it would have been injected above.\ncarrier = {'traceparent': '00-a9c3b99a95cc045e573e163c3ac80a77-d99d251a8caecd06-01'}\n# Then we use a propagator to get a context from it.\nctx = TraceContextTextMapPropagator().extract(carrier=carrier)\n# Instead of extracting the trace context from the carrier, if you have a SpanContext\n# object already you can get a trace context from it like this.\nspan_context = SpanContext(\ntrace_id=2604504634922341076776623263868986797,\nspan_id=5213367945872657620,\nis_remote=True,\ntrace_flags=TraceFlags(0x01)\n)\nctx = trace.set_span_in_context(NonRecordingSpan(span_context))\n# Now there are a few ways to make use of the trace context.\n# You can pass the context object when starting a span.\nwith tracer.start_as_current_span('child', context=ctx) as span:\nspan.set_attribute('primes', [2, 3, 5, 7])\n# Or you can make it the current context, and then the next span will pick it up.\n# The returned token lets you restore the previous context.\ntoken = context.attach(ctx)\ntry:\nwith tracer.start_as_current_span('child') as span:\nspan.set_attribute('evens', [2, 4, 6, 8])\nfinally:\ncontext.detach(token)\n</code></pre>"},{"location":"docs/instrumentation/python/cookbook/#using-multiple-tracer-providers-with-different-resource","title":"Using multiple tracer providers with different Resource","text":"<pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n# Global tracer provider which can be set only once\ntrace.set_tracer_provider(\nTracerProvider(resource=Resource.create({\"service.name\": \"service1\"}))\n)\ntrace.get_tracer_provider().add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\ntracer = trace.get_tracer(\"tracer.one\")\nwith tracer.start_as_current_span(\"some-name\") as span:\nspan.set_attribute(\"key\", \"value\")\nanother_tracer_provider = TracerProvider(\nresource=Resource.create({\"service.name\": \"service2\"})\n)\nanother_tracer_provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))\nanother_tracer = trace.get_tracer(\"tracer.two\", tracer_provider=another_tracer_provider)\nwith another_tracer.start_as_current_span(\"name-here\") as span:\nspan.set_attribute(\"another-key\", \"another-value\")\n</code></pre>"},{"location":"docs/instrumentation/python/distro/","title":"OpenTelemetry Distro","text":"<p>In order to make using OpenTelemetry and auto-instrumentation as quick as possible without sacrificing flexibility, OpenTelemetry distros provide a mechanism to automatically configure some of the more common options for users. By harnessing their power, users of OpenTelemetry can configure the components as they need. The <code>opentelemetry-distro</code> package provides some defaults to users looking to get started, it configures:</p> <ul> <li>the SDK TracerProvider</li> <li>a BatchSpanProcessor</li> <li>the OTLP <code>SpanExporter</code> to send data to an OpenTelemetry collector</li> </ul> <p>The package also provides a starting point for anyone interested in producing an alternative distro. The interfaces implemented by the package are loaded by the auto-instrumentation via the <code>opentelemetry_distro</code> and <code>opentelemetry_configurator</code> entry points to configure the application before any other code is executed.</p> <p>In order to automatically export data from OpenTelemetry to the OpenTelemetry collector, installing the package will set up all the required entry points.</p> <pre><code>$ pip install opentelemetry-distro[otlp] opentelemetry-instrumentation\n</code></pre> <p>Start the Collector locally to see data being exported. Write the following file:</p> <pre><code># /tmp/otel-collector-config.yaml\nreceivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nexporters:\nlogging:\nloglevel: debug\nprocessors:\nbatch:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [logging]\nprocessors: [batch]\n</code></pre> <p>Then start the Docker container:</p> <pre><code>docker run -p 4317:4317 \\\n-v /tmp/otel-collector-config.yaml:/etc/otel-collector-config.yaml \\\notel/opentelemetry-collector:latest \\\n--config=/etc/otel-collector-config.yaml\n</code></pre> <p>The following code will create a span with no configuration.</p> <pre><code># no_configuration.py\nfrom opentelemetry import trace\nwith trace.get_tracer(\"my.tracer\").start_as_current_span(\"foo\"):\nwith trace.get_tracer(\"my.tracer\").start_as_current_span(\"bar\"):\nprint(\"baz\")\n</code></pre> <p>Lastly, run the <code>no_configuration.py</code> with the auto-instrumentation:</p> <pre><code>$ opentelemetry-instrument python no_configuration.py\n</code></pre> <p>The resulting span will appear in the output from the collector and look similar to this:</p> <pre><code>Resource labels:\n     -&gt; telemetry.sdk.language: STRING(python)\n     -&gt; telemetry.sdk.name: STRING(opentelemetry)\n     -&gt; telemetry.sdk.version: STRING(1.1.0)\n     -&gt; service.name: STRING(unknown_service)\nInstrumentationLibrarySpans #0\nInstrumentationLibrary __main__\nSpan #0\n    Trace ID       : db3c99e5bfc50ef8be1773c3765e8845\n    Parent ID      : 0677126a4d110cb8\n    ID             : 3163b3022808ed1b\n    Name           : bar\n    Kind           : SPAN_KIND_INTERNAL\n    Start time     : 2021-05-06 22:54:51.23063 +0000 UTC\n    End time       : 2021-05-06 22:54:51.230684 +0000 UTC\n    Status code    : STATUS_CODE_UNSET\n    Status message :\nSpan #1\n    Trace ID       : db3c99e5bfc50ef8be1773c3765e8845\n    Parent ID      :\n    ID             : 0677126a4d110cb8\n    Name           : foo\n    Kind           : SPAN_KIND_INTERNAL\n    Start time     : 2021-05-06 22:54:51.230549 +0000 UTC\n    End time       : 2021-05-06 22:54:51.230706 +0000 UTC\n    Status code    : STATUS_CODE_UNSET\n    Status message :\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your telemetry you will need to use an exporter.</p>"},{"location":"docs/instrumentation/python/exporters/#console-exporter","title":"Console exporter","text":"<p>The console exporter is useful for development and debugging tasks, and is the simplest to set up.</p>"},{"location":"docs/instrumentation/python/exporters/#trace","title":"Trace","text":"<pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\n# Service name is required for most backends,\n# and although it's not necessary for console export,\n# it's good to set service name anyways.\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\nprovider = TracerProvider(resource=resource)\nprocessor = BatchSpanProcessor(ConsoleSpanExporter())\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n# Merrily go about tracing!\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#metrics","title":"Metrics","text":"<p>Use a <code>PeriodicExportingMetricReader</code> to periodically print metrics to the console. <code>PeriodicExportingMetricReader</code> can be configured to export at a different interval, change the temporality for each instrument kind, or change the default aggregation for each instrument kind.</p>"},{"location":"docs/instrumentation/python/exporters/#temporality-presets","title":"Temporality Presets","text":"<p>There are temporality presets for each instrumentation kind. These presets can be set with the environment variable <code>OTEL_EXPORTER_METRICS_TEMPORALITY_PREFERENCE</code>, for example:</p> <pre><code>$ export OTEL_EXPORTER_METRICS_TEMPORALITY_PREFERENCE=\"DELTA\"\n</code></pre> <p>The default value for <code>OTEL_EXPORTER_METRICS_TEMPORALITY_PREFERENCE</code> is <code>\"CUMULATIVE\"</code>.</p> <p>The available values and their corresponding settings for this environment variable are:</p> <ul> <li> <p><code>CUMULATIVE</code></p> </li> <li> <p><code>Counter</code>: <code>CUMULATIVE</code></p> </li> <li><code>UpDownCounter</code>: <code>CUMULATIVE</code></li> <li><code>Histogram</code>: <code>CUMULATIVE</code></li> <li><code>ObservableCounter</code>: <code>CUMULATIVE</code></li> <li><code>ObservableUpDownCounter</code>: <code>CUMULATIVE</code></li> <li> <p><code>ObservableGauge</code>: <code>CUMULATIVE</code></p> </li> <li> <p><code>DELTA</code></p> </li> <li> <p><code>Counter</code>: <code>DELTA</code></p> </li> <li><code>UpDownCounter</code>: <code>CUMULATIVE</code></li> <li><code>Histogram</code>: <code>DELTA</code></li> <li><code>ObservableCounter</code>: <code>DELTA</code></li> <li><code>ObservableUpDownCounter</code>: <code>CUMULATIVE</code></li> <li> <p><code>ObservableGauge</code>: <code>CUMULATIVE</code></p> </li> <li> <p><code>LOWMEMORY</code></p> </li> <li><code>Counter</code>: <code>DELTA</code></li> <li><code>UpDownCounter</code>: <code>CUMULATIVE</code></li> <li><code>Histogram</code>: <code>DELTA</code></li> <li><code>ObservableCounter</code>: <code>CUMULATIVE</code></li> <li><code>ObservableUpDownCounter</code>: <code>CUMULATIVE</code></li> <li><code>ObservableGauge</code>: <code>CUMULATIVE</code></li> </ul> <p>Setting <code>OTEL_EXPORTER_METRICS_TEMPORALITY_PREFERENCE</code> to any other value than <code>CUMULATIVE</code>, <code>DELTA</code> or <code>LOWMEMORY</code> will log a warning and set this environment variable to <code>CUMULATIVE</code>.</p> <pre><code>from opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader, ConsoleMetricExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\n# Service name is required for most backends,\n# and although it's not necessary for console export,\n# it's good to set service name anyways.\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\nreader = PeriodicExportingMetricReader(ConsoleMetricExporter())\nprovider = MeterProvider(resource=resource, metric_readers=[reader])\nmetrics.set_meter_provider(provider)\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#otlp-endpoint-or-collector","title":"OTLP endpoint or Collector","text":"<p>To send data to an OTLP endpoint or the OpenTelemetry Collector, you'll want to configure an OTLP exporter that sends to your endpoint.</p> <p>First, install an OTLP exporter:</p> <pre><code>$ pip install opentelemetry-exporter-otlp-proto-grpc\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#trace_1","title":"Trace","text":"<pre><code>from opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n# Service name is required for most backends\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\nprovider = TracerProvider(resource=resource)\nprocessor = BatchSpanProcessor(OTLPSpanExporter(endpoint=\"your-endpoint-here\"))\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n# Merrily go about tracing!\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#metrics_1","title":"Metrics","text":"<pre><code>from opentelemetry import metrics\nfrom opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\n# Service name is required for most backends\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\nreader = PeriodicExportingMetricReader(\nOTLPMetricExporter(endpoint=\"localhost:5555\")\n)\nprovider = MeterProvider(resource=resource, metric_readers=[reader])\nmetrics.set_meter_provider(provider)\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#using-http","title":"Using HTTP","text":"<p>If you'd prefer to use OTLP/HTTP with the binary-encoded protobuf format, you can install the package:</p> <pre><code>$ pip install opentelemetry-exporter-otlp-proto-http\n</code></pre> <p>Next, replace the import declarations with the following:</p> <pre><code>from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n</code></pre> <p>Finally, update your exporter endpoint if you're specifying it in code:</p> <pre><code>OTLPSpanExporter(endpoint=\"&lt;traces-endpoint&gt;/v1/traces\")\n</code></pre> <p>There is not currently an OTLP/HTTP metric exporter.</p>"},{"location":"docs/instrumentation/python/exporters/#jaeger","title":"Jaeger","text":"<p>If you are using Jaeger to visualize trace data, you'll need to set it up first. This is how to run it in a docker container:</p> <pre><code>$ docker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n-p 5775:5775/udp \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 14268:14268 \\\n-p 14250:14250 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n</code></pre> <p>Next, install the Jaeger exporter package:</p> <pre><code>$ pip install opentelemetry-exporter-jaeger\n</code></pre> <p>This will install packages for both:</p> <ul> <li><code>opentelemetry-exporter-jaeger-thrift</code></li> <li><code>opentelemetry-exporter-jaeger-proto-grpc</code></li> </ul> <p>You can use either to export your traces to Jaeger.</p> <p>Once the package is installed, you can configure the exporter when initializing tracing:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\njaeger_exporter = JaegerExporter(\nagent_host_name=\"localhost\",\nagent_port=6831,\n)\nprovider = TracerProvider(resource=resource)\nprocessor = BatchSpanProcessor(jaeger_exporter)\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n# Merrily go about tracing!\n</code></pre> <p>The previous example uses thrift. To use protobuf, change the import declaration to:</p> <pre><code>from opentelemetry.exporter.jaeger.proto.grpc import JaegerExporter\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#zipkin","title":"Zipkin","text":"<p>If you are using Zipkin to visualize trace data, you'll need to set it up first. This is how to run it in a docker container:</p> <pre><code>$ docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n</code></pre> <p>Next, install the Zipkin exporter package:</p> <pre><code>$ pip install opentelemetry-exporter-zipkin-proto-http\n</code></pre> <p>Then you can configure the exporter when initializing tracing:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.exporter.zipkin.proto.http import ZipkinExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\nzipkin_exporter = ZipkinExporter(endpoint=\"http://localhost:9411/api/v2/spans\")\nprovider = TracerProvider(resource=resource)\nprocessor = BatchSpanProcessor(zipkin_exporter)\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n# merrily go about tracing!\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#using-json","title":"Using JSON","text":"<p>If you'd prefer to use Thrift as the protocol, you can install the package:</p> <pre><code>$ pip install opentelemetry-exporter-zipkin-json\n</code></pre> <p>And replace the <code>ZipkinExporter</code> import declaration with the following:</p> <pre><code>from opentelemetry.exporter.zipkin.json import ZipkinExporter\n</code></pre>"},{"location":"docs/instrumentation/python/exporters/#prometheus","title":"Prometheus","text":"<p>If you are using Prometheus to collect metrics data, you'll need to set it up first.</p> <p>First create a config file:</p> <pre><code>$ cat &gt; prometheus.yml &lt;&lt;EOF\nscrape_configs:\n  - job_name: 'otel-python-demo'\n    scrape_interval: 5s\n    static_configs:\n      - targets: ['localhost:8000']\nEOF\n</code></pre> <p>Then start the Prometheus server in Docker:</p> <pre><code>$ docker run -d --rm \\\n--network=host \\\n-v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml \\\nprom/prometheus\n</code></pre> <p>Next, install the Prometheus exporter package:</p> <pre><code>$ pip install opentelemetry-exporter-prometheus\n</code></pre> <p>Then you can configure the exporter when initializing metrics:</p> <pre><code>from prometheus_client import start_http_server\nfrom opentelemetry import metrics\nfrom opentelemetry.exporter.prometheus import PrometheusMetricReader\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\n# Service name is required for most backends\nresource = Resource(attributes={\nSERVICE_NAME: \"your-service-name\"\n})\n# Start Prometheus client\nstart_http_server(port=8000, addr=\"localhost\")\n# Initialize PrometheusMetricReader which pulls metrics from the SDK\n# on-demand to respond to scrape requests\nreader = PrometheusMetricReader()\nprovider = MeterProvider(resource=resource, metric_readers=[reader])\nmetrics.set_meter_provider(provider)\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/","title":"Getting Started","text":"<p>This page will show you how to get started with OpenTelemetry in Python.</p> <p>You will learn how you can instrument a simple application automatically, in such a way that traces, metrics and logs are emitted to the console.</p>"},{"location":"docs/instrumentation/python/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have the following installed locally:</p> <ul> <li>Python 3</li> </ul>"},{"location":"docs/instrumentation/python/getting-started/#example-application","title":"Example Application","text":"<p>The following example uses a basic Flask application. If you are not using Flask, that's ok \u2014 you can use OpenTelemetry Python with other web frameworks as well, such as Django and FastAPI. For a complete list of libraries for supported frameworks, see the registry.</p> <p>For more elaborate examples, see examples.</p>"},{"location":"docs/instrumentation/python/getting-started/#installation","title":"Installation","text":"<p>To begin, set up an environment in a new directory:</p> <pre><code>mkdir otel-getting-started\ncd otel-getting-started\npython3 -m venv .\nsource ./bin/activate\n</code></pre> <p>Now install Flask:</p> <pre><code>pip install flask\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/#create-and-launch-an-http-server","title":"Create and launch an HTTP Server","text":"<p>Create a file <code>app.py</code> and add the following code to it:</p> <pre><code>from random import randint\nfrom flask import Flask\napp = Flask(__name__)\n@app.route(\"/rolldice\")\ndef roll_dice():\nreturn str(do_roll())\ndef do_roll():\nreturn randint(1, 6)\n</code></pre> <p>Run the application with the following command and open http://localhost:8080/rolldice in your web browser to ensure it is working.</p> <pre><code>flask run -p 8080\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/#instrumentation","title":"Instrumentation","text":"<p>Automatic instrumentation will generate telemetry data on your behalf. There are several options you can take, covered in more detail in Automatic Instrumentation. Here we'll use the <code>opentelemetry-instrument</code> agent.</p> <p>Install the <code>opentelemetry-distro</code> package, which contains the OpenTelemetry API, SDK and also the tools <code>opentelemetry-bootstrap</code> and <code>opentelemetry-instrument</code> you will use below.</p> <pre><code>pip install opentelemetry-distro\n</code></pre> <p>Run the <code>opentelemetry-bootstrap</code> command:</p> <pre><code>opentelemetry-bootstrap -a install\n</code></pre> <p>This will install Flask instrumentation.</p>"},{"location":"docs/instrumentation/python/getting-started/#run-the-instrumented-app","title":"Run the instrumented app","text":"<p>You can now run your instrumented app with <code>opentelemetry-instrument</code> and have it print to the console for now:</p> <pre><code>opentelemetry-instrument \\\n--traces_exporter console \\\n--metrics_exporter console \\\n--logs_exporter console \\\nflask run -p 8080\n</code></pre> <p>Open http://localhost:8080/rolldice in your web browser and reload the page a few times. After a while you should see the spans printed in the console, such as the following:</p> View example output <pre><code>{\n\"name\": \"/rolldice\",\n\"context\": {\n\"trace_id\": \"0xdcd253b9501348b63369d83219da0b14\",\n\"span_id\": \"0x886c05bc23d2250e\",\n\"trace_state\": \"[]\"\n},\n\"kind\": \"SpanKind.SERVER\",\n\"parent_id\": null,\n\"start_time\": \"2022-04-27T23:53:11.533109Z\",\n\"end_time\": \"2022-04-27T23:53:11.534097Z\",\n\"status\": {\n\"status_code\": \"UNSET\"\n},\n\"attributes\": {\n\"http.method\": \"GET\",\n\"http.server_name\": \"127.0.0.1\",\n\"http.scheme\": \"http\",\n\"net.host.port\": 5000,\n\"http.host\": \"localhost:5000\",\n\"http.target\": \"/rolldice\",\n\"net.peer.ip\": \"127.0.0.1\",\n\"http.user_agent\": \"curl/7.68.0\",\n\"net.peer.port\": 52538,\n\"http.flavor\": \"1.1\",\n\"http.route\": \"/rolldice\",\n\"http.status_code\": 200\n},\n\"events\": [],\n\"links\": [],\n\"resource\": {\n\"attributes\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"1.14.0\",\n\"telemetry.auto.version\": \"0.35b0\",\n\"service.name\": \"unknown_service\"\n},\n\"schema_url\": \"\"\n}\n}\n</code></pre> <p>The generated span tracks the lifetime of a request to the <code>/rolldice</code> route.</p> <p>Send a few more requests to the endpoint, and then either wait for a little bit or terminate the app and you'll see metrics in the console output, such as the following:</p> View example output <pre><code>{\n\"resource_metrics\": [\n{\n\"resource\": {\n\"attributes\": {\n\"service.name\": \"unknown_service\",\n\"telemetry.auto.version\": \"0.34b0\",\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"1.13.0\"\n},\n\"schema_url\": \"\"\n},\n\"schema_url\": \"\",\n\"scope_metrics\": [\n{\n\"metrics\": [\n{\n\"data\": {\n\"aggregation_temporality\": 2,\n\"data_points\": [\n{\n\"attributes\": {\n\"http.flavor\": \"1.1\",\n\"http.host\": \"localhost:5000\",\n\"http.method\": \"GET\",\n\"http.scheme\": \"http\",\n\"http.server_name\": \"127.0.0.1\"\n},\n\"start_time_unix_nano\": 1666077040061693305,\n\"time_unix_nano\": 1666077098181107419,\n\"value\": 0\n}\n],\n\"is_monotonic\": false\n},\n\"description\": \"measures the number of concurrent HTTP requests that are currently in-flight\",\n\"name\": \"http.server.active_requests\",\n\"unit\": \"requests\"\n},\n{\n\"data\": {\n\"aggregation_temporality\": 2,\n\"data_points\": [\n{\n\"attributes\": {\n\"http.flavor\": \"1.1\",\n\"http.host\": \"localhost:5000\",\n\"http.method\": \"GET\",\n\"http.scheme\": \"http\",\n\"http.server_name\": \"127.0.0.1\",\n\"http.status_code\": 200,\n\"net.host.port\": 5000\n},\n\"bucket_counts\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n\"count\": 1,\n\"explicit_bounds\": [\n0, 5, 10, 25, 50, 75, 100, 250, 500, 1000\n],\n\"max\": 1,\n\"min\": 1,\n\"start_time_unix_nano\": 1666077040063027610,\n\"sum\": 1,\n\"time_unix_nano\": 1666077098181107419\n}\n]\n},\n\"description\": \"measures the duration of the inbound HTTP request\",\n\"name\": \"http.server.duration\",\n\"unit\": \"ms\"\n}\n],\n\"schema_url\": \"\",\n\"scope\": {\n\"name\": \"opentelemetry.instrumentation.flask\",\n\"schema_url\": \"\",\n\"version\": \"0.34b0\"\n}\n}\n]\n}\n]\n}\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/#add-manual-instrumentation-to-automatic-instrumentation","title":"Add manual instrumentation to automatic instrumentation","text":"<p>Automatic instrumentation captures telemetry at the edges of your systems, such as inbound and outbound HTTP requests, but it doesn't capture what's going on in your application. For that you'll need to write some manual instrumentation. Here's how you can easily link up manual instrumentation with automatic instrumentation.</p>"},{"location":"docs/instrumentation/python/getting-started/#traces","title":"Traces","text":"<p>First, modify <code>app.py</code> to include code that initializes a tracer and uses it to create a trace that's a child of the one that's automatically generated:</p> <pre><code># These are the necessary import declarations\nfrom opentelemetry import trace\nfrom random import randint\nfrom flask import Flask\n# Acquire a tracer\ntracer = trace.get_tracer(\"diceroller.tracer\")\napp = Flask(__name__)\n@app.route(\"/rolldice\")\ndef roll_dice():\nreturn str(do_roll())\ndef do_roll():\n# This creates a new span that's the child of the current one\nwith tracer.start_as_current_span(\"do_roll\") as rollspan:\nres = randint(1, 6)\nrollspan.set_attribute(\"roll.value\", res)\nreturn res\n</code></pre> <p>Now run the app again:</p> <pre><code>opentelemetry-instrument \\\n--traces_exporter console \\\n--metrics_exporter console \\\nflask run -p 8080\n</code></pre> <p>When you send a request to the server, you'll see two spans in the trace emitted to the console, and the one called <code>do_roll</code> registers its parent as the automatically created one:</p> View example output <pre><code>{\n\"name\": \"do_roll\",\n\"context\": {\n\"trace_id\": \"0x48da59d77e13beadd1a961dc8fcaa74e\",\n\"span_id\": \"0x40c38b50bc8da6b7\",\n\"trace_state\": \"[]\"\n},\n\"kind\": \"SpanKind.INTERNAL\",\n\"parent_id\": \"0x84f8c5d92970d94f\",\n\"start_time\": \"2022-04-28T00:07:55.892307Z\",\n\"end_time\": \"2022-04-28T00:07:55.892331Z\",\n\"status\": {\n\"status_code\": \"UNSET\"\n},\n\"attributes\": {\n\"roll.value\": 4\n},\n\"events\": [],\n\"links\": [],\n\"resource\": {\n\"attributes\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"1.14.0\",\n\"telemetry.auto.version\": \"0.35b0\",\n\"service.name\": \"unknown_service\"\n},\n\"schema_url\": \"\"\n}\n}\n{\n\"name\": \"/rolldice\",\n\"context\": {\n\"trace_id\": \"0x48da59d77e13beadd1a961dc8fcaa74e\",\n\"span_id\": \"0x84f8c5d92970d94f\",\n\"trace_state\": \"[]\"\n},\n\"kind\": \"SpanKind.SERVER\",\n\"parent_id\": null,\n\"start_time\": \"2022-04-28T00:07:55.891500Z\",\n\"end_time\": \"2022-04-28T00:07:55.892552Z\",\n\"status\": {\n\"status_code\": \"UNSET\"\n},\n\"attributes\": {\n\"http.method\": \"GET\",\n\"http.server_name\": \"127.0.0.1\",\n\"http.scheme\": \"http\",\n\"net.host.port\": 5000,\n\"http.host\": \"localhost:5000\",\n\"http.target\": \"/rolldice\",\n\"net.peer.ip\": \"127.0.0.1\",\n\"http.user_agent\": \"curl/7.68.0\",\n\"net.peer.port\": 53824,\n\"http.flavor\": \"1.1\",\n\"http.route\": \"/rolldice\",\n\"http.status_code\": 200\n},\n\"events\": [],\n\"links\": [],\n\"resource\": {\n\"attributes\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"1.14.0\",\n\"telemetry.auto.version\": \"0.35b0\",\n\"service.name\": \"unknown_service\"\n},\n\"schema_url\": \"\"\n}\n}\n</code></pre> <p>The <code>parent_id</code> of <code>do_roll</code> is the same is the <code>span_id</code> for <code>/rolldice</code>, indicating a parent-child relationship!</p>"},{"location":"docs/instrumentation/python/getting-started/#metrics","title":"Metrics","text":"<p>Now modify <code>app.py</code> to include code that initializes a meter and uses it to create a counter instrument which counts the number of rolls for each possible roll value:</p> <pre><code># These are the necessary import declarations\nfrom opentelemetry import trace\nfrom opentelemetry import metrics\nfrom random import randint\nfrom flask import Flask\ntracer = trace.get_tracer(\"diceroller.tracer\")\n# Acquire a meter.\nmeter = metrics.get_meter(\"diceroller.meter\")\n# Now create a counter instrument to make measurements with\nroll_counter = meter.create_counter(\n\"roll_counter\",\ndescription=\"The number of rolls by roll value\",\n)\napp = Flask(__name__)\n@app.route(\"/rolldice\")\ndef roll_dice():\nreturn str(do_roll())\ndef do_roll():\nwith tracer.start_as_current_span(\"do_roll\") as rollspan:\nres = randint(1, 6)\nrollspan.set_attribute(\"roll.value\", res)\n# This adds 1 to the counter for the given roll value\nroll_counter.add(1, {\"roll.value\": res})\nreturn res\n</code></pre> <p>Now run the app again:</p> <pre><code>opentelemetry-instrument \\\n--traces_exporter console \\\n--metrics_exporter console \\\nflask run -p 8080\n</code></pre> <p>When you send a request to the server, you'll see the roll counter metric emitted to the console, with separate counts for each roll value:</p> View example output <pre><code>{\n\"resource_metrics\": [\n{\n\"resource\": {\n\"attributes\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"1.12.0rc1\",\n\"telemetry.auto.version\": \"0.31b0\",\n\"service.name\": \"unknown_service\"\n},\n\"schema_url\": \"\"\n},\n\"scope_metrics\": [\n{\n\"scope\": {\n\"name\": \"app\",\n\"version\": \"\",\n\"schema_url\": null\n},\n\"metrics\": [\n{\n\"name\": \"roll_counter\",\n\"description\": \"The number of rolls by roll value\",\n\"unit\": \"\",\n\"data\": {\n\"data_points\": [\n{\n\"attributes\": {\n\"roll.value\": 4\n},\n\"start_time_unix_nano\": 1654790325350232600,\n\"time_unix_nano\": 1654790332211598800,\n\"value\": 3\n},\n{\n\"attributes\": {\n\"roll.value\": 6\n},\n\"start_time_unix_nano\": 1654790325350232600,\n\"time_unix_nano\": 1654790332211598800,\n\"value\": 4\n},\n{\n\"attributes\": {\n\"roll.value\": 5\n},\n\"start_time_unix_nano\": 1654790325350232600,\n\"time_unix_nano\": 1654790332211598800,\n\"value\": 1\n},\n{\n\"attributes\": {\n\"roll.value\": 1\n},\n\"start_time_unix_nano\": 1654790325350232600,\n\"time_unix_nano\": 1654790332211598800,\n\"value\": 2\n},\n{\n\"attributes\": {\n\"roll.value\": 3\n},\n\"start_time_unix_nano\": 1654790325350232600,\n\"time_unix_nano\": 1654790332211598800,\n\"value\": 1\n}\n],\n\"aggregation_temporality\": 2,\n\"is_monotonic\": true\n}\n}\n],\n\"schema_url\": null\n}\n],\n\"schema_url\": \"\"\n}\n]\n}\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/#send-telemetry-to-an-opentelemetry-collector","title":"Send telemetry to an OpenTelemetry Collector","text":"<p>The OpenTelemetry Collector is a critical component of most production deployments. Some examples of when it's beneficial to use a collector:</p> <ul> <li>A single telemetry sink shared by multiple services, to reduce overhead of   switching exporters</li> <li>Aggregating traces across multiple services, running on multiple hosts</li> <li>A central place to process traces prior to exporting them to a backend</li> </ul> <p>Unless you have just a single service or are experimenting, you'll want to use a collector in production deployments.</p>"},{"location":"docs/instrumentation/python/getting-started/#configure-and-run-a-local-collector","title":"Configure and run a local collector","text":"<p>First, save the following collector configuration code to a file in the <code>/tmp/</code> directory:</p> <pre><code># /tmp/otel-collector-config.yaml\nreceivers:\notlp:\nprotocols:\ngrpc:\nexporters:\nlogging:\nloglevel: debug\nprocessors:\nbatch:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nexporters: [logging]\nprocessors: [batch]\nmetrics:\nreceivers: [otlp]\nexporters: [logging]\nprocessors: [batch]\n</code></pre> <p>Then run the docker command to acquire and run the collector based on this configuration:</p> <pre><code>docker run -p 4317:4317 \\\n-v /tmp/otel-collector-config.yaml:/etc/otel-collector-config.yaml \\\notel/opentelemetry-collector:latest \\\n--config=/etc/otel-collector-config.yaml\n</code></pre> <p>You will now have an collector instance running locally, listening on port 4317.</p>"},{"location":"docs/instrumentation/python/getting-started/#modify-the-command-to-export-spans-and-metrics-via-otlp","title":"Modify the command to export spans and metrics via OTLP","text":"<p>The next step is to modify the command to send spans and metrics to the collector via OTLP instead of the console.</p> <p>To do this, install the OTLP exporter package:</p> <pre><code>pip install opentelemetry-exporter-otlp\n</code></pre> <p>The <code>opentelemetry-instrument</code> agent will detect the package you just installed and default to OTLP export when it's run next.</p>"},{"location":"docs/instrumentation/python/getting-started/#run-the-application","title":"Run the application","text":"<p>Run the application like before, but don't export to the console:</p> <pre><code>opentelemetry-instrument flask run -p 8080\n</code></pre> <p>By default, <code>opentelemetry-instrument</code> exports traces and metrics over OTLP/gRPC and will send them to <code>localhost:4317</code>, which is what the collector is listening on.</p> <p>When you access the <code>/rolldice</code> route now, you'll see output in the collector process instead of the flask process, which should look something like this:</p> View example output <pre><code>2022-06-09T20:43:39.915Z        DEBUG   loggingexporter/logging_exporter.go:51  ResourceSpans #0\nResource labels:\n     -&gt; telemetry.sdk.language: STRING(python)\n     -&gt; telemetry.sdk.name: STRING(opentelemetry)\n     -&gt; telemetry.sdk.version: STRING(1.12.0rc1)\n     -&gt; telemetry.auto.version: STRING(0.31b0)\n     -&gt; service.name: STRING(unknown_service)\nInstrumentationLibrarySpans #0\nInstrumentationLibrary app\nSpan #0\n    Trace ID       : 7d4047189ac3d5f96d590f974bbec20a\n    Parent ID      : 0b21630539446c31\n    ID             : 4d18cee9463a79ba\n    Name           : do_roll\n    Kind           : SPAN_KIND_INTERNAL\n    Start time     : 2022-06-09 20:43:37.390134089 +0000 UTC\n    End time       : 2022-06-09 20:43:37.390327687 +0000 UTC\n    Status code    : STATUS_CODE_UNSET\n    Status message :\nAttributes:\n     -&gt; roll.value: INT(5)\nInstrumentationLibrarySpans #1\nInstrumentationLibrary opentelemetry.instrumentation.flask 0.31b0\nSpan #0\n    Trace ID       : 7d4047189ac3d5f96d590f974bbec20a\n    Parent ID      :\n    ID             : 0b21630539446c31\n    Name           : /rolldice\n    Kind           : SPAN_KIND_SERVER\n    Start time     : 2022-06-09 20:43:37.388733595 +0000 UTC\n    End time       : 2022-06-09 20:43:37.390723792 +0000 UTC\n    Status code    : STATUS_CODE_UNSET\n    Status message :\nAttributes:\n     -&gt; http.method: STRING(GET)\n     -&gt; http.server_name: STRING(127.0.0.1)\n     -&gt; http.scheme: STRING(http)\n     -&gt; net.host.port: INT(5000)\n     -&gt; http.host: STRING(localhost:5000)\n     -&gt; http.target: STRING(/rolldice)\n     -&gt; net.peer.ip: STRING(127.0.0.1)\n     -&gt; http.user_agent: STRING(curl/7.82.0)\n     -&gt; net.peer.port: INT(53878)\n     -&gt; http.flavor: STRING(1.1)\n     -&gt; http.route: STRING(/rolldice)\n     -&gt; http.status_code: INT(200)\n\n2022-06-09T20:43:40.025Z        INFO    loggingexporter/logging_exporter.go:56  MetricsExporter {\"#metrics\": 1}\n2022-06-09T20:43:40.025Z        DEBUG   loggingexporter/logging_exporter.go:66  ResourceMetrics #0\nResource labels:\n     -&gt; telemetry.sdk.language: STRING(python)\n     -&gt; telemetry.sdk.name: STRING(opentelemetry)\n     -&gt; telemetry.sdk.version: STRING(1.12.0rc1)\n     -&gt; telemetry.auto.version: STRING(0.31b0)\n     -&gt; service.name: STRING(unknown_service)\nInstrumentationLibraryMetrics #0\nInstrumentationLibrary app\nMetric #0\nDescriptor:\n     -&gt; Name: roll_counter\n     -&gt; Description: The number of rolls by roll value\n     -&gt; Unit:\n     -&gt; DataType: Sum\n     -&gt; IsMonotonic: true\n     -&gt; AggregationTemporality: AGGREGATION_TEMPORALITY_CUMULATIVE\nNumberDataPoints #0\nData point attributes:\n     -&gt; roll.value: INT(5)\nStartTimestamp: 2022-06-09 20:43:37.390226915 +0000 UTC\nTimestamp: 2022-06-09 20:43:39.848587966 +0000 UTC\nValue: 1\n</code></pre>"},{"location":"docs/instrumentation/python/getting-started/#next-steps","title":"Next steps","text":"<p>There are several options available for automatic instrumentation and Python. See Automatic Instrumentation to learn about them and how to configure them.</p> <p>There's a lot more to manual instrumentation than just creating a child span. To learn details about initializing manual instrumentation and many more parts of the OpenTelemetry API you can use, see Manual Instrumentation.</p> <p>Finally, there are several options for exporting your telemetry data with OpenTelemetry. To learn how to export your data to a preferred backend, see Exporters.</p>"},{"location":"docs/instrumentation/python/manual/","title":"Manual Instrumentation","text":"<p>Manual instrumentation is the process of adding observability code to your application.</p>"},{"location":"docs/instrumentation/python/manual/#initializing-the-sdk","title":"Initializing the SDK","text":"<p>First, ensure you have the API and SDK packages:</p> <pre><code>pip install opentelemetry-api\npip install opentelemetry-sdk\n</code></pre> <p>To start tracing, you'll need to initialize a <code>TracerProvider</code> and optionally set it as the global default.</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import (\nBatchSpanProcessor,\nConsoleSpanExporter,\n)\nprovider = TracerProvider()\nprocessor = BatchSpanProcessor(ConsoleSpanExporter())\nprovider.add_span_processor(processor)\n# Sets the global default tracer provider\ntrace.set_tracer_provider(provider)\n# Creates a tracer from the global tracer provider\ntracer = trace.get_tracer(\"my.tracer.name\")\n</code></pre> <p>To start collecting metrics, you'll need to initialize a <code>MeterProvider</code> and optionally set it as the global default.</p> <pre><code>from opentelemetry import metrics\nfrom opentelemetry.sdk.metrics import MeterProvider\nfrom opentelemetry.sdk.metrics.export import (\nConsoleMetricExporter,\nPeriodicExportingMetricReader,\n)\nmetric_reader = PeriodicExportingMetricReader(ConsoleMetricExporter())\nprovider = MeterProvider(metric_readers=[metric_reader])\n# Sets the global default meter provider\nmetrics.set_meter_provider(provider)\n# Creates a meter from the global meter provider\nmeter = metrics.get_meter(\"my.meter.name\")\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/python/manual/#creating-spans","title":"Creating spans","text":"<p>To create a span, you'll typically want it to be started as the current span.</p> <pre><code>def do_work():\nwith tracer.start_as_current_span(\"span-name\") as span:\n# do some work that 'span' will track\nprint(\"doing some work...\")\n# When the 'with' block goes out of scope, 'span' is closed for you\n</code></pre> <p>You can also use <code>start_span</code> to create a span without making it the current span. This is usually done to track concurrent or asynchronous operations.</p>"},{"location":"docs/instrumentation/python/manual/#creating-nested-spans","title":"Creating nested spans","text":"<p>If you have a distinct sub-operation you'd like to track as a part of another one, you can create spans to represent the relationship:</p> <pre><code>def do_work():\nwith tracer.start_as_current_span(\"parent\") as parent:\n# do some work that 'parent' tracks\nprint(\"doing some work...\")\n# Create a nested span to track nested work\nwith tracer.start_as_current_span(\"child\") as child:\n# do some work that 'child' tracks\nprint(\"doing some nested work...\")\n# the nested span is closed when it's out of scope\n# This span is also closed when it goes out of scope\n</code></pre> <p>When you view spans in a trace visualization tool, <code>child</code> will be tracked as a nested span under <code>parent</code>.</p>"},{"location":"docs/instrumentation/python/manual/#creating-spans-with-decorators","title":"Creating spans with decorators","text":"<p>It's common to have a single span track the execution of an entire function. In that scenario, there is a decorator you can use to reduce code:</p> <pre><code>@tracer.start_as_current_span(\"do_work\")\ndef do_work():\nprint(\"doing some work...\")\n</code></pre> <p>Use of the decorator is equivalent to creating the span inside <code>do_work()</code> and ending it when <code>do_work()</code> is finished.</p> <p>To use the decorator, you must have a <code>tracer</code> instance available global to your function declaration.</p> <p>If you need to add attributes, events, or links then it's less convenient to use a decorator.</p>"},{"location":"docs/instrumentation/python/manual/#get-the-current-span","title":"Get the current span","text":"<p>Sometimes it's helpful to access whatever the current span is at a point in time so that you can enrich it with more information.</p> <pre><code>from opentelemetry import trace\ncurrent_span = trace.get_current_span()\n# enrich 'current_span' with some information\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#add-attributes-to-a-span","title":"Add attributes to a span","text":"<p>Attributes let you attach key/value pairs to a span so it carries more information about the current operation that it's tracking.</p> <pre><code>from opentelemetry import trace\ncurrent_span = trace.get_current_span()\ncurrent_span.set_attribute(\"operation.value\", 1)\ncurrent_span.set_attribute(\"operation.name\", \"Saying hello!\")\ncurrent_span.set_attribute(\"operation.other-stuff\", [1, 2, 3])\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#add-semantic-attributes","title":"Add semantic attributes","text":"<p>Semantic Attributes are pre-defined Attributes that are well-known naming conventions for common kinds of data. Using Semantic Attributes lets you normalize this kind of information across your systems.</p> <p>To use Semantic Attributes in Python, ensure you have the semantic conventions package:</p> <pre><code>pip install opentelemetry-semantic-conventions\n</code></pre> <p>Then you can use it in code:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.semconv.trace import SpanAttributes\n// ...\ncurrent_span = trace.get_current_span()\ncurrent_span.set_attribute(SpanAttributes.HTTP_METHOD, \"GET\")\ncurrent_span.set_attribute(SpanAttributes.HTTP_URL, \"https://opentelemetry.io/\")\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#adding-events","title":"Adding events","text":"<p>An event is a human-readable message on a span that represents \"something happening\" during its lifetime. You can think of it as a primitive log.</p> <pre><code>from opentelemetry import trace\ncurrent_span = trace.get_current_span()\ncurrent_span.add_event(\"Gonna try it!\")\n# Do the thing\ncurrent_span.add_event(\"Did it!\")\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#adding-links","title":"Adding links","text":"<p>A span can be created with zero or more span links that causally link it to another span. A link needs a span context to be created.</p> <pre><code>from opentelemetry import trace\ntracer = trace.get_tracer(__name__)\nwith tracer.start_as_current_span(\"span-1\"):\n# Do something that 'span-1' tracks.\nctx = trace.get_current_span().get_span_context()\nlink_from_span_1 = trace.Link(ctx)\nwith tracer.start_as_current_span(\"span-2\", links=[link_from_span_1]):\n# Do something that 'span-2' tracks.\n# The link in 'span-2' is casually associated it with the 'span-1',\n# but it is not a child span.\npass\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#set-span-status","title":"Set span status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>StatusCode.ERROR</code>. In rare scenarios, you could override the Error status with <code>StatusCode.OK</code>, but don\u2019t set <code>StatusCode.OK</code> on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\ncurrent_span = trace.get_current_span()\ntry:\n# something that might fail\nexcept:\ncurrent_span.set_status(Status(StatusCode.ERROR))\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#record-exceptions-in-spans","title":"Record exceptions in spans","text":"<p>It can be a good idea to record exceptions when they happen. It\u2019s recommended to do this in conjunction with setting span status.</p> <pre><code>from opentelemetry import trace\nfrom opentelemetry.trace import Status, StatusCode\ncurrent_span = trace.get_current_span()\ntry:\n# something that might fail\n# Consider catching a more specific exception in your code\nexcept Exception as ex:\ncurrent_span.set_status(Status(StatusCode.ERROR))\ncurrent_span.record_exception(ex)\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#change-the-default-propagation-format","title":"Change the default propagation format","text":"<p>By default, OpenTelemetry Python will use the following propagation formats:</p> <ul> <li>W3C Trace Context</li> <li>W3C Baggage</li> </ul> <p>If you have a need to change the defaults, you can do so either via environment variables or in code:</p>"},{"location":"docs/instrumentation/python/manual/#using-environment-variables","title":"Using Environment Variables","text":"<p>You can set the <code>OTEL_PROPAGATORS</code> environment variable with a comma-separated list. Accepted values are:</p> <ul> <li><code>\"tracecontext\"</code>: W3C Trace Context</li> <li><code>\"baggage\"</code>: W3C Baggage</li> <li><code>\"b3\"</code>: B3 Single</li> <li><code>\"b3multi\"</code>: B3 Multi</li> <li><code>\"jaeger\"</code>: Jaeger</li> <li><code>\"xray\"</code>: AWS X-Ray (third party)</li> <li><code>\"ottrace\"</code>: OT Trace (third party)</li> <li><code>\"none\"</code>: No automatically configured propagator.</li> </ul> <p>The default configuration is equivalent to <code>OTEL_PROPAGATORS=\"tracecontext,baggage\"</code>.</p>"},{"location":"docs/instrumentation/python/manual/#using-sdk-apis","title":"Using SDK APIs","text":"<p>Alternatively, you can change the format in code.</p> <p>For example, if you need to use Zipkin's B3 propagation format instead, you can install the B3 package:</p> <pre><code>pip install opentelemetry-propagator-b3\n</code></pre> <p>And then set the B3 propagator in your tracing initialization code:</p> <pre><code>from opentelemetry.propagate import set_global_textmap\nfrom opentelemetry.propagators.b3 import B3Format\nset_global_textmap(B3Format())\n</code></pre> <p>Note that environment variables will override what's configured in code.</p>"},{"location":"docs/instrumentation/python/manual/#metrics","title":"Metrics","text":""},{"location":"docs/instrumentation/python/manual/#creating-and-using-synchronous-instruments","title":"Creating and using synchronous instruments","text":"<p>Instruments are used to make measurements of your application. Synchronous instruments are used inline with application/business processing logic, like when handling a request or calling another service.</p> <p>First, create your instrument. Instruments are generally created once at the module or class level and then used inline with business logic. This example uses a Counter instrument to count the number of work items completed:</p> <pre><code>work_counter = meter.create_counter(\n\"work.counter\", unit=\"1\", description=\"Counts the amount of work done\"\n)\n</code></pre> <p>Using the Counter's add operation, the code below increments the count by one, using the work item's type as an attribute.</p> <pre><code>def do_work(work_item):\n# count the work being doing\nwork_counter.add(1, {\"work.type\": work_item.work_type})\nprint(\"doing some work...\")\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#creating-and-using-asynchronous-instruments","title":"Creating and using asynchronous instruments","text":"<p>Asynchronous instruments give the user a way to register callback functions, which are invoked on demand to make measurements. This is useful to periodically measure a value that cannot be instrumented directly. Async instruments are created with zero or more callbacks which will be invoked during metric collection. Each callback accepts options from the SDK and returns its observations.</p> <p>This example uses an Asynchronous Gauge instrument to report the current config version provided by a configuration server by scraping an HTTP endpoint. First, write a callback to make observations:</p> <pre><code>from typing import Iterable\nfrom opentelemetry.metrics import CallbackOptions, Observation\ndef scrape_config_versions(options: CallbackOptions) -&gt; Iterable[Observation]:\nr = requests.get(\n\"http://configserver/version_metadata\", timeout=options.timeout_millis / 10**3\n)\nfor metadata in r.json():\nyield Observation(\nmetadata[\"version_num\"], {\"config.name\": metadata[\"version_num\"]}\n)\n</code></pre> <p>Note that OpenTelemetry will pass options to your callback containing a timeout. Callbacks should respect this timeout to avoid blocking indefinitely. Finally, create the instrument with the callback to register it:</p> <pre><code>meter.create_observable_gauge(\n\"config.version\",\ncallbacks=[scrape_config_versions],\ndescription=\"The active config version for each configuration\",\n)\n</code></pre>"},{"location":"docs/instrumentation/python/manual/#additional-references","title":"Additional References","text":"<ul> <li>Trace</li> <li>Trace Concepts</li> <li>Trace Specification</li> <li>Python Trace API Documentation</li> <li>Python Trace SDK Documentation</li> <li>Metrics</li> <li>Metrics Concepts</li> <li>Metrics Specification</li> <li>Python Metrics API Documentation</li> <li>Python Metrics SDK Documentation</li> </ul>"},{"location":"docs/instrumentation/python/mypy/","title":"Using mypy","text":"<p>If you're using mypy, you'll need to turn on namespace packages, otherwise <code>mypy</code> won't be able to run correctly.</p> <p>To turn on namespace packages, do one of the following:</p> <p>Add the following to your project configuration file:</p> <pre><code>[tool.mypy]\nnamespace_packages = true\n</code></pre> <p>Or, use a command-line switch:</p> <pre><code>mypy --namespace-packages\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/","title":"Automatic Instrumentation","text":"<p>Automatic instrumentation with Python uses a Python agent that can be attached to any Python application. It dynamically injects bytecode to capture telemetry from many popular libraries and frameworks.</p>"},{"location":"docs/instrumentation/python/automatic/#setup","title":"Setup","text":"<p>Run the following commands to install the appropriate packages.</p> <pre><code>pip install opentelemetry-distro \\\n    opentelemetry-exporter-otlp\nopentelemetry-bootstrap -a install\n</code></pre> <p>The <code>opentelemetry-distro</code> package installs the API, SDK, and the <code>opentelemetry-bootstrap</code> and <code>opentelemetry-instrument</code> tools.</p> <p>The <code>opentelemetry-bootstrap -a install</code> command reads through the list of packages installed in your active <code>site-packages</code> folder, and installs the corresponding instrumentation libraries for these packages, if applicable. For example, if you already installed the <code>flask</code> package, running <code>opentelemetry-bootstrap -a install</code> will install <code>opentelemetry-instrumentation-flask</code> for you.</p> <p>NOTE: If you leave out <code>-a install</code>, the command will simply list out the recommended auto-instrumentation packages to be installed. More information can be found here.</p>"},{"location":"docs/instrumentation/python/automatic/#configuring-the-agent","title":"Configuring the agent","text":"<p>The agent is highly configurable.</p> <p>One option is to configure the agent by way of configuration properties from the CLI:</p> <pre><code>opentelemetry-instrument \\\n    --traces_exporter console,otlp \\\n    --metrics_exporter console \\\n    --service_name your-service-name \\\n    --exporter_otlp_endpoint 0.0.0.0:4317 \\\n    python myapp.py\n</code></pre> <p>Alternatively, you can use environment variables to configure the agent:</p> <pre><code>OTEL_SERVICE_NAME=your-service-name \\\nOTEL_TRACES_EXPORTER=console,otlp \\\nOTEL_METRICS_EXPORTER=console \\\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=0.0.0.0:4317\nopentelemetry-instrument \\\n    python myapp.py\n</code></pre> <p>To see the full range of configuration options, see Agent Configuration.</p>"},{"location":"docs/instrumentation/python/automatic/#supported-libraries-and-frameworks","title":"Supported libraries and frameworks","text":"<p>A number of popular Python libraries are auto-instrumented, including Flask and Django. You can find the full list here.</p>"},{"location":"docs/instrumentation/python/automatic/#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/instrumentation/python/automatic/#python-package-installation-failure","title":"Python package installation failure","text":"<p>The Python package installs require <code>gcc</code> and <code>gcc-c++</code>, which you may need to install if you\u2019re running a slim version of Linux (e.g., CentOS).</p> <p>CentOS:</p> <pre><code>yum -y install python3-devel\nyum -y install gcc-c++\n</code></pre> <p>Debian/Ubuntu:</p> <pre><code>apt install -y python3-dev\napt install -y build-essential\n</code></pre> <p>Alpine:</p> <pre><code>apk add python3-dev\napk add build-base\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/#grpc-connectivity","title":"gRPC Connectivity","text":"<p>To debug Python gRPC connectivity issues, set the following gRPC debug environment variables:</p> <pre><code>export GRPC_VERBOSITY=debug\nexport GRPC_TRACE=http,call_error,connectivity_state\nopentelemetry-instrument python &lt;your_app&gt;.py\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/agent-config/","title":"Agent Configuration","text":"<p>The agent is highly configurable, either by:</p> <ul> <li>Passing configuration properties from the CLI</li> <li>Setting   environment variables</li> </ul>"},{"location":"docs/instrumentation/python/automatic/agent-config/#configuration-properties","title":"Configuration properties","text":"<p>Here's an example of agent configuration via configuration properties:</p> <pre><code>opentelemetry-instrument \\\n--traces_exporter console,otlp \\\n--metrics_exporter console \\\n--service_name your-service-name \\\n--exporter_otlp_endpoint 0.0.0.0:4317 \\\npython myapp.py\n</code></pre> <p>Here's an explanation of what each configuration does:</p> <ul> <li><code>traces_exporter</code> specifies which traces exporter to use. In this case, traces   are being exported to <code>console</code> (stdout) and with <code>otlp</code>. The <code>otlp</code> option   tells <code>opentelemetry-instrument</code> to send the traces to an endpoint that   accepts OTLP via gRPC. In order to use HTTP instead of gRPC, add   <code>--exporter_otlp_protocol http</code>. The full list of available options for   traces_exporter, see the Python contrib   OpenTelemetry Instrumentation.</li> <li><code>metrics_exporter</code> specifies which metrics exporter to use. In this case,   metrics are being exported to <code>console</code> (stdout). It is currently required for   your to specify a metrics exporter. If you aren't exporting metrics, specify   <code>none</code> as the value instead.</li> <li><code>service_name</code> sets the name of the service associated with your telemetry,   and is sent to your Observability backend.</li> <li><code>exporter_otlp_endpoint</code> sets the endpoint where telemetry is exported to. If   omitted, the default Collector endpoint will be used,   which is <code>0.0.0.0:4317</code> for gRPC and <code>0.0.0.0:4318</code> for HTTP.</li> <li><code>exporter_otlp_headers</code> is required depending on your chosen Observability   backend. More info exporter OTLP headers be found   here.</li> </ul>"},{"location":"docs/instrumentation/python/automatic/agent-config/#environment-variables","title":"Environment Variables","text":"<p>In some cases, configuring via environment variables is more preferred. Any setting configurable with a command-line argument can also be configured with an Environment Variable.</p> <p>You can apply the following steps to determine the correct name mapping of the desired configuration property:</p> <ul> <li>Convert the configuration property to uppercase.</li> <li>Prefix environment variable with <code>OTEL_</code></li> </ul> <p>For example, <code>exporter_otlp_endpoint</code> would convert to <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code>.</p>"},{"location":"docs/instrumentation/python/automatic/agent-config/#python-specific-configuration","title":"Python-specific Configuration","text":"<p>There are some python specific configuration options you can set by prefixing environment variables with <code>OTEL_PYTHON_</code>.</p>"},{"location":"docs/instrumentation/python/automatic/agent-config/#excluded-urls","title":"Excluded URLs","text":"<p>Comma-separated regexes representing which URLs to exclude across all instrumentations:</p> <ul> <li><code>OTEL_PYTHON_EXCLUDED_URLS</code></li> </ul> <p>You can also exclude URLs for specific instrumentations by using a variable <code>OTEL_PYTHON_&lt;library&gt;_EXCLUDED_URLS</code>, where library is the uppercase version of one of the following: Django, Falcon, FastAPI, Flask, Pyramid, Requests, Starlette, Tornado, urllib, urllib3.</p> <p>Examples:</p> <pre><code>export OTEL_PYTHON_EXCLUDED_URLS=\"client/.*/info,healthcheck\"\nexport OTEL_PYTHON_URLLIB3_EXCLUDED_URLS=\"client/.*/info\"\nexport OTEL_PYTHON_REQUESTS_EXCLUDED_URLS=\"healthcheck\"\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/agent-config/#request-attribute-names","title":"Request Attribute Names","text":"<p>Comma-separated list of names that will be extracted from the request object and set as attributes on spans.</p> <ul> <li><code>OTEL_PYTHON_DJANGO_TRACED_REQUEST_ATTRS</code></li> <li><code>OTEL_PYTHON_FALCON_TRACED_REQUEST_ATTRS</code></li> <li><code>OTEL_PYTHON_TORNADO_TRACED_REQUEST_ATTRS</code></li> </ul> <p>Examples:</p> <pre><code>export OTEL_PYTHON_DJANGO_TRACED_REQUEST_ATTRS='path_info,content_type'\nexport OTEL_PYTHON_FALCON_TRACED_REQUEST_ATTRS='query_string,uri_template'\nexport OTEL_PYTHON_TORNADO_TRACED_REQUEST_ATTRS='uri,query'\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/agent-config/#logging","title":"Logging","text":"<p>There are some configuration options used to control the logs that are outputted.</p> <ul> <li><code>OTEL_PYTHON_LOG_CORRELATION</code>: to enable trace context injection into logs   (true, false)</li> <li><code>OTEL_PYTHON_LOG_FORMAT</code>: to instruct the instrumentation to use a custom   logging format</li> <li><code>OTEL_PYTHON_LOG_LEVEL</code>: to set a custom log level (info, error, debug,   warning)</li> </ul> <p>Examples:</p> <pre><code>export OTEL_PYTHON_LOG_CORRELATION=true\nexport OTEL_PYTHON_LOG_FORMAT=\"%(msg)s [span_id=%(span_id)s]\"\nexport OTEL_PYTHON_LOG_LEVEL=debug\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/agent-config/#other","title":"Other","text":"<p>There are some more configuration options that can be set that don't fall into a specific category.</p> <ul> <li><code>OTEL_PYTHON_DJANGO_INSTRUMENT</code>: set to <code>false</code> to disable the default enabled   state for the Django instrumentation</li> <li><code>OTEL_PYTHON_ELASTICSEARCH_NAME_PREFIX</code>: changes the default prefixes for   Elasticsearch operation names from \"Elasticsearch\" to whatever is used here</li> <li><code>OTEL_PYTHON_GRPC_EXCLUDED_SERVICES</code>: comma-separated list of specific   services to exclude for the gRPC instrumentation</li> <li><code>OTEL_PYTHON_ID_GENERATOR</code>: to specify which IDs generator to use for the   global Tracer Provider</li> <li><code>OTEL_PYTHON_INSTRUMENTATION_SANITIZE_REDIS</code>: to enable query sanitization</li> </ul> <p>Examples:</p> <pre><code>export OTEL_PYTHON_DJANGO_INSTRUMENT=false\nexport OTEL_PYTHON_ELASTICSEARCH_NAME_PREFIX=my-custom-prefix\nexport OTEL_PYTHON_GRPC_EXCLUDED_SERVICES=\"GRPCTestServer,GRPCHealthServer\"\nexport OTEL_PYTHON_ID_GENERATOR=xray\nexport OTEL_PYTHON_INSTRUMENTATION_SANITIZE_REDIS=true\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/agent-config/#disabling-specific-instrumentations","title":"Disabling Specific Instrumentations","text":"<p>The Python agent by default will detect a python program's packages and instrument any packages it can. This makes instrumentation easy, but can result in too much or unwanted data.</p> <p>You can omit specific packages from instrumentation by using the <code>OTEL_PYTHON_DISABLED_INSTRUMENTATIONS</code> environment variable. The environment variable can be set to a comma-separated list of package names to exclude from instrumentation.</p> <p>For example, if your Python program uses the <code>redis</code> and <code>kafka-python</code> packages, by default the agent will use the <code>opentelemetry-instrumentation-redis</code> and <code>opentelemetry-instrumentation-kafka-python</code> packages to instrument them. To disable this, you can set <code>OTEL_PYTHON_DISABLED_INSTRUMENTATIONS=redis,kafka-python</code>.</p>"},{"location":"docs/instrumentation/python/automatic/example/","title":"Auto-Instrumentation Example","text":"<p>This page demonstrates how to use Python auto-instrumentation in OpenTelemetry. The example is based on an OpenTracing example. You can download or view the source files used in this page from the <code>opentelemetry-python</code> repo.</p> <p>This example uses three different scripts. The main difference between them is how they are instrumented:</p> <ol> <li><code>server_manual.py</code> is instrumented manually.</li> <li><code>server_automatic.py</code> is instrumented automatically.</li> <li><code>server_programmatic.py</code> is instrumented programmatically.</li> </ol> <p>Programmatic instrumentation is a kind of instrumentation that requires minimal instrumentation code to be added to the application. Only some instrumentation libraries offer additional capabilities that give you greater control over the instrumentation process when used programmatically.</p> <p>Run the first script without the automatic instrumentation agent and second with the agent. They should both produce the same results, demonstrating that the automatic instrumentation agent does exactly the same thing as manual instrumentation.</p> <p>Automatic instrumentation utilizes monkey-patching to dynamically rewrite methods and classes at runtime through instrumentation libraries. This reduces the amount of work required to integrate OpenTelemetry into your application code. Below, you will see the difference between a Flask route instrumented manually, automatically and programmatically.</p>"},{"location":"docs/instrumentation/python/automatic/example/#manually-instrumented-server","title":"Manually instrumented server","text":"<p><code>server_manual.py</code></p> <pre><code>@app.route(\"/server_request\")\ndef server_request():\nwith tracer.start_as_current_span(\n\"server_request\",\ncontext=extract(request.headers),\nkind=trace.SpanKind.SERVER,\nattributes=collect_request_attributes(request.environ),\n):\nprint(request.args.get(\"param\"))\nreturn \"served\"\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#automatically-instrumented-server","title":"Automatically-instrumented server","text":"<p><code>server_automatic.py</code></p> <pre><code>@app.route(\"/server_request\")\ndef server_request():\nprint(request.args.get(\"param\"))\nreturn \"served\"\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#programmatically-instrumented-server","title":"Programmatically-instrumented server","text":"<p><code>server_programmatic.py</code></p> <pre><code>instrumentor = FlaskInstrumentor()\napp = Flask(__name__)\ninstrumentor.instrument_app(app)\n# instrumentor.instrument_app(app, excluded_urls=\"/server_request\")\n@app.route(\"/server_request\")\ndef server_request():\nprint(request.args.get(\"param\"))\nreturn \"served\"\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#prepare","title":"Prepare","text":"<p>Execute the following example in a separate virtual environment. Run the following commands to prepare for auto-instrumentation:</p> <pre><code>$ mkdir auto_instrumentation\n$ virtualenv auto_instrumentation\n$ source auto_instrumentation/bin/activate\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#install","title":"Install","text":"<p>Run the following commands to install the appropriate packages. The <code>opentelemetry-distro</code> package depends on a few others, like <code>opentelemetry-sdk</code> for custom instrumentation of your own code and <code>opentelemetry-instrumentation</code> which provides several commands that help automatically instrument a program.</p> <pre><code>$ pip install opentelemetry-distro\n$ pip install opentelemetry-instrumentation-flask\n$ pip install flask\n$ pip install requests\n</code></pre> <p>The examples that follow send instrumentation results to the console. Learn more about installing and configuring the OpenTelemetry Distro to send telemetry to other destinations, like an OpenTelemetry Collector.</p> <p>Note: To use automatic instrumentation through <code>opentelemetry-instrument</code>, you must configure it via environment variables or the command line. The agent creates a telemetry pipeline that cannot be modified other than through these means. If you need more customization for your telemetry pipelines, then you need to forego the agent and import the OpenTelemetry SDK and instrumentation libraries into your code and configure them there. You may also extend automatic instrumentation by importing the OpenTelemetry API. For more details, see the API reference.</p>"},{"location":"docs/instrumentation/python/automatic/example/#execute","title":"Execute","text":"<p>This section guides you through the manual process of instrumenting a server as well as the process of executing an automatically instrumented server.</p>"},{"location":"docs/instrumentation/python/automatic/example/#execute-the-manually-instrumented-server","title":"Execute the manually instrumented server","text":"<p>Execute the server in two separate consoles, one to run each of the scripts that make up this example:</p> <pre><code>$ source auto_instrumentation/bin/activate\n$ python server_manual.py\n</code></pre> <pre><code>$ source auto_instrumentation/bin/activate\n$ python client.py testing\n</code></pre> <p>The console running <code>server_manual.py</code> will display the spans generated by instrumentation as JSON. The spans should appear similar to the following example:</p> <pre><code>{\n\"name\": \"server_request\",\n\"context\": {\n\"trace_id\": \"0xfa002aad260b5f7110db674a9ddfcd23\",\n\"span_id\": \"0x8b8bbaf3ca9c5131\",\n\"trace_state\": \"{}\"\n},\n\"kind\": \"SpanKind.SERVER\",\n\"parent_id\": null,\n\"start_time\": \"2020-04-30T17:28:57.886397Z\",\n\"end_time\": \"2020-04-30T17:28:57.886490Z\",\n\"status\": {\n\"status_code\": \"OK\"\n},\n\"attributes\": {\n\"http.method\": \"GET\",\n\"http.server_name\": \"127.0.0.1\",\n\"http.scheme\": \"http\",\n\"host.port\": 8082,\n\"http.host\": \"localhost:8082\",\n\"http.target\": \"/server_request?param=testing\",\n\"net.peer.ip\": \"127.0.0.1\",\n\"net.peer.port\": 52872,\n\"http.flavor\": \"1.1\"\n},\n\"events\": [],\n\"links\": [],\n\"resource\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"0.16b1\"\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#execute-the-automatically-instrumented-server","title":"Execute the automatically-instrumented server","text":"<p>Stop the execution of <code>server_manual.py</code> by pressing Control+C and run the following command instead:</p> <pre><code>$ opentelemetry-instrument --traces_exporter console --metrics_exporter none python server_automatic.py\n</code></pre> <p>In the console where you previously executed <code>client.py</code>, run the following command again:</p> <pre><code>$ python client.py testing\n</code></pre> <p>The console running <code>server_automatic.py</code> will display the spans generated by instrumentation as JSON. The spans should appear similar to the following example:</p> <pre><code>{\n\"name\": \"server_request\",\n\"context\": {\n\"trace_id\": \"0x9f528e0b76189f539d9c21b1a7a2fc24\",\n\"span_id\": \"0xd79760685cd4c269\",\n\"trace_state\": \"{}\"\n},\n\"kind\": \"SpanKind.SERVER\",\n\"parent_id\": \"0xb4fb7eee22ef78e4\",\n\"start_time\": \"2020-04-30T17:10:02.400604Z\",\n\"end_time\": \"2020-04-30T17:10:02.401858Z\",\n\"status\": {\n\"status_code\": \"OK\"\n},\n\"attributes\": {\n\"http.method\": \"GET\",\n\"http.server_name\": \"127.0.0.1\",\n\"http.scheme\": \"http\",\n\"host.port\": 8082,\n\"http.host\": \"localhost:8082\",\n\"http.target\": \"/server_request?param=testing\",\n\"net.peer.ip\": \"127.0.0.1\",\n\"net.peer.port\": 48240,\n\"http.flavor\": \"1.1\",\n\"http.route\": \"/server_request\",\n\"http.status_text\": \"OK\",\n\"http.status_code\": 200\n},\n\"events\": [],\n\"links\": [],\n\"resource\": {\n\"telemetry.sdk.language\": \"python\",\n\"telemetry.sdk.name\": \"opentelemetry\",\n\"telemetry.sdk.version\": \"0.16b1\",\n\"service.name\": \"\"\n}\n}\n</code></pre> <p>You can see that both outputs are the same because automatic instrumentation does exactly what manual instrumentation does.</p>"},{"location":"docs/instrumentation/python/automatic/example/#execute-the-programmatically-instrumented-server","title":"Execute the programmatically-instrumented server","text":"<p>It is also possible to use the instrumentation libraries (such as <code>opentelemetry-instrumentation-flask</code>) by themselves which may have an advantage of customizing options. However, by choosing to do this it means you forego using auto-instrumentation by starting your application with <code>opentelemetry-instrument</code> as this is mutually exclusive.</p> <p>Execute the server just like you would do for manual instrumentation, in two separate consoles, one to run each of the scripts that make up this example:</p> <pre><code>source auto_instrumentation/bin/activate\npython server_programmatic.py\n</code></pre> <pre><code>source auto_instrumentation/bin/activate\npython client.py testing\n</code></pre> <p>The results should be the same as running with manual instrumentation.</p>"},{"location":"docs/instrumentation/python/automatic/example/#using-programmatic-instrumentation-features","title":"Using programmatic-instrumentation features","text":"<p>Some instrumentation libraries include features that allow for more precise control while instrumenting programmatically, the instrumentation library for Flask is one of them.</p> <p>This example has a line commented out, change it like this:</p> <pre><code># instrumentor.instrument_app(app)\ninstrumentor.instrument_app(app, excluded_urls=\"/server_request\")\n</code></pre> <p>After running the example again, no instrumentation should appear on the server side. This is because or the <code>excluded_urls</code> option passed to <code>instrument_app</code> that effectively stops the <code>server_request</code> function from being instrumented as its URL matches the regular expression passed to <code>excluded_urls</code>.</p>"},{"location":"docs/instrumentation/python/automatic/example/#instrumentation-while-debugging","title":"Instrumentation while debugging","text":"<p>The debug mode can be enabled in the Flask app like this:</p> <pre><code>if __name__ == \"__main__\":\napp.run(port=8082, debug=True)\n</code></pre> <p>The debug mode can break instrumentation from happening because it enables a reloader. To run instrumentation while the debug mode is enabled, set the <code>use_reloader</code> option to <code>False</code>:</p> <pre><code>if __name__ == \"__main__\":\napp.run(port=8082, debug=True, use_reloader=False)\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/example/#configure","title":"Configure","text":"<p>The auto instrumentation can consume configuration from environment variables.</p>"},{"location":"docs/instrumentation/python/automatic/example/#capture-http-request-and-response-headers","title":"Capture HTTP request and response headers","text":"<p>You can capture predefined HTTP headers as span attributes, according to the semantic convention.</p> <p>To define which HTTP headers you want to capture, provide a comma-separated list of HTTP header names via the environment variables <code>OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST</code> and <code>OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE</code>, e.g.:</p> <pre><code>$ export OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST=\"Accept-Encoding,User-Agent,Referer\"\n$ export OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE=\"Last-Modified,Content-Type\"\n$ opentelemetry-instrument --traces_exporter console --metrics_exporter none python app.py\n</code></pre> <p>These configuration options are supported by the following HTTP instrumentations:</p> <ul> <li>Django</li> <li>Falcon</li> <li>FastAPI</li> <li>Pyramid</li> <li>Starlette</li> <li>Tornado</li> <li>WSGI</li> </ul> <p>If those headers are available, they will be included in your span:</p> <pre><code>{\n\"attributes\": {\n\"http.request.header.user-agent\": [\n\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0)\"\n],\n\"http.request.header.accept_encoding\": [\"gzip, deflate, br\"],\n\"http.response.header.last_modified\": [\"2022-04-20 17:07:13.075765\"],\n\"http.response.header.content_type\": [\"text/html; charset=utf-8\"]\n}\n}\n</code></pre>"},{"location":"docs/instrumentation/python/automatic/operator/","title":"Using the OpenTelemetry Operator to Inject Auto-Instrumentation","text":"<p>If you run your Python service in Kubernetes, you can take advantage of the OpenTelemetry Operator to inject auto-instrumentation without having to modify each of your services directly. See the OpenTelemetry Operator Auto-instrumentation docs for more details.</p>"},{"location":"docs/instrumentation/ruby/","title":"Ruby","text":"<p>{{% lang_instrumentation_index_head ruby /%}}</p>"},{"location":"docs/instrumentation/ruby/#whos-using-opentelemetry-ruby","title":"Who's using OpenTelemetry Ruby?","text":"<p>OpenTelemetry Ruby is in use by a number of companies, including:</p> <ul> <li>Heroku</li> <li>GitHub</li> <li>Fulcrum</li> <li>Puppet</li> <li>Shopify</li> <li>TableCheck</li> <li>Dropbox DocSend</li> </ul> <p>If you would like to add your name to this list, submit a pull request.</p>"},{"location":"docs/instrumentation/ruby/#repository","title":"Repository","text":"<ul> <li>OpenTelemetry for Ruby repository</li> </ul>"},{"location":"docs/instrumentation/ruby/automatic/","title":"Automatic instrumentation","text":"<p>Automatic instrumentation in ruby is done via instrumentation packages, and most commonly, the <code>opentelemetry-instrumentation-all</code> package. These are called Instrumentation Libraries.</p> <p>For example, if you are using Rails and enable instrumentation, your running Rails app will automatically generate telemetry data for inbound requests to your controllers.</p>"},{"location":"docs/instrumentation/ruby/automatic/#configuring-all-instrumentation-libraries","title":"Configuring all instrumentation libraries","text":"<p>The recommended way to use instrumentation libraries is to use the <code>opentelemetry-instrumentation-all</code> package:</p> <pre><code>gem 'opentelemetry-sdk'\ngem 'opentelemetry-exporter-otlp'\ngem 'opentelemetry-instrumentation-all'\n</code></pre> <p>and configure it early in your application lifecycle. See the example below using a Rails initializer:</p> <pre><code># config/initializers/opentelemetry.rb\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/exporter/otlp'\nrequire 'opentelemetry/instrumentation/all'\nOpenTelemetry::SDK.configure do |c|\nc.service_name = '&lt;YOUR_SERVICE_NAME&gt;'\nc.use_all() # enables all instrumentation!\nend\n</code></pre> <p>This will install all instrumentation libraries and enable the ones that match up to libraries you're using in your app.</p>"},{"location":"docs/instrumentation/ruby/automatic/#overriding-configuration-for-specific-instrumentation-libraries","title":"Overriding configuration for specific instrumentation libraries","text":"<p>If you are enabling all instrumentation but want to override the configuration for a specific one, call <code>use_all</code> with a configuration map parameter, where the key represents the library, and the value is its specific configuration parameter.</p> <p>For example, here's how you can install all instrumentations except the <code>Redis</code> instrumentation into your app:</p> <pre><code>require 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nOpenTelemetry::SDK.configure do |c|\nconfig = {'OpenTelemetry::Instrumentation::Redis' =&gt; { enabled: false }}\nc.use_all(config)\nend\n</code></pre> <p>To override more instrumentation, add another entry in the <code>config</code> map.</p>"},{"location":"docs/instrumentation/ruby/automatic/#configuring-specific-instrumentation-libraries","title":"Configuring specific instrumentation libraries","text":"<p>If you prefer more selectively installing and using only specific instrumentation libraries, you can do that too. For example, here's how to use only <code>Sinatra</code> and <code>Faraday</code>, with <code>Faraday</code> being configured with an additional configuration parameter.</p> <p>First, install the specific instrumentation libraries you know you want to use:</p> <pre><code>gem install opentelemetry-instrumentation-sinatra\ngem install opentelemetry-instrumentation-faraday\n</code></pre> <p>Then configure them:</p> <pre><code>require 'opentelemetry/sdk'\n# install all compatible instrumentation with default configuration\nOpenTelemetry::SDK.configure do |c|\nc.use 'OpenTelemetry::Instrumentation::Sinatra'\nc.use 'OpenTelemetry::Instrumentation::Faraday', { opt: 'value' }\nend\n</code></pre>"},{"location":"docs/instrumentation/ruby/automatic/#next-steps","title":"Next steps","text":"<p>Instrumentation libraries are the easiest way to generate lots of useful telemetry data about your ruby apps. But they don't generate data specific to your application's logic! To do that, you'll need to enrich the automatic instrumentation from instrumentation libraries with manual instrumentation.</p>"},{"location":"docs/instrumentation/ruby/exporters/","title":"Exporters","text":"<p>In order to visualize and analyze your traces, you will need to export them to a backend such as Jaeger or Zipkin. OpenTelemetry Ruby provides exporters for some common open source backends.</p> <p>Below you will find some introductions on how to set up backends and the matching exporters.</p>"},{"location":"docs/instrumentation/ruby/exporters/#otlp-endpoint","title":"OTLP endpoint","text":"<p>To send trace data to a OTLP endpoint (like the collector or Jaeger) you'll want to use an exporter package, such as <code>opentelemetry-exporter-otlp</code>:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab bundler &gt;}} bundle add opentelemetry-exporter-otlp {{&lt; /tab &gt;}}</p> <p>{{&lt; tab gem &gt;}} gem install opentelemetry-exporter-otlp {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane&gt;}}</p> <p>Next, configure the exporter to point at an OTLP endpoint. For example you can update <code>config/initializers/opentelemetry.rb</code> from the Getting Started by adding <code>require 'opentelemetry-exporter-otlp'</code> to the code:</p> <pre><code># config/initializers/opentelemetry.rb\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nrequire 'opentelemetry-exporter-otlp'\nOpenTelemetry::SDK.configure do |c|\nc.service_name = 'dice-ruby'\nc.use_all() # enables all instrumentation!\nend\n</code></pre> <p>If you now run your application it will use OTLP to export traces:</p> <pre><code>rails server -p 8080\n</code></pre> <p>By default traces are sent to an OTLP endpoint listening on localhost:4318. You can change the endpoint by setting the <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> accordingly:</p> <pre><code>env OTEL_EXPORTER_OTLP_ENDPOINT=\"http://localhost:4318/v1/traces\" rails server -p 8080\n</code></pre> <p>To try out the OTLP exporter quickly and see your traces visualized at the receiving end, you can run Jaeger in a docker container:</p> <pre><code>docker run -d --name jaeger \\\n-e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n-e COLLECTOR_OTLP_ENABLED=true \\\n-p 6831:6831/udp \\\n-p 6832:6832/udp \\\n-p 5778:5778 \\\n-p 16686:16686 \\\n-p 4317:4317 \\\n-p 4318:4318 \\\n-p 14250:14250 \\\n-p 14268:14268 \\\n-p 14269:14269 \\\n-p 9411:9411 \\\njaegertracing/all-in-one:latest\n</code></pre>"},{"location":"docs/instrumentation/ruby/exporters/#zipkin","title":"Zipkin","text":"<p>To set up Zipkin as quickly as possible, run it in a docker container:</p> <pre><code>docker run --rm -d -p 9411:9411 --name zipkin openzipkin/zipkin\n</code></pre> <p>Install the exporter package as a dependency for your application:</p> <p>{{&lt; tabpane lang=shell persistLang=false &gt;}}</p> <p>{{&lt; tab bundle &gt;}} bundle add opentelemetry-exporter-zipkin {{&lt; /tab &gt;}}</p> <p>{{&lt; tab gem &gt;}} gem install opentelemetry-exporter-zipkin {{&lt; /tab &gt;}}</p> <p>{{&lt; /tabpane&gt;}}</p> <p>Update your OpenTelemetry configuration to use the exporter and to send data to your Zipkin backend:</p> <pre><code># config/initializers/opentelemetry.rb\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nrequire 'opentelemetry-exporter-zipkin'\nOpenTelemetry::SDK.configure do |c|\nc.service_name = 'dice-ruby'\nc.use_all() # enables all instrumentation!\nend\n</code></pre> <p>If you now run your application, set the environment variable <code>OTEL_TRACES_EXPORTER</code> to zipkin:</p> <pre><code>env OTEL_TRACES_EXPORTER=zipkin rails server\n</code></pre> <p>By default traces are sent to a Zipkin endpoint listening on port localhost:9411. You can change the endpoint by setting the <code>OTEL_EXPORTER_ZIPKIN_ENDPOINT</code> accordingly:</p> <pre><code>env OTEL_EXPORTER_OTLP_ENDPOINT=\"http://localhost:9411\" rails server\n</code></pre>"},{"location":"docs/instrumentation/ruby/getting-started/","title":"Getting Started","text":"<p>This page will show you how to get started with OpenTelemetry in Ruby.</p> <p>You will learn how you can instrument a simple application automatically, in such a way that traces, metrics and logs are emitted to the console.</p>"},{"location":"docs/instrumentation/ruby/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have the following installed locally:</p> <ul> <li>MRI Ruby &gt;= <code>3.0</code>, jruby &gt;= <code>9.3.2.0</code>, or truffleruby &gt;= 22.1</li> <li>Bundler</li> </ul> <p>{{% alert  title=\"Warning\" color=\"warning\" %}} <code>jruby</code> only targets compatibility with MRI Ruby 2.6.8, which is EOL. This project does not officially support MRI Ruby 2.6.8, and provides <code>jruby</code> support on a best-effort basis until the <code>jruby</code> project supports compatibility with more modern Ruby runtimes.</p> <p>While tested, support for <code>truffleruby</code> is on a best-effort basis at this time. {{% /alert %}}</p>"},{"location":"docs/instrumentation/ruby/getting-started/#example-application","title":"Example Application","text":"<p>The following example uses a basic Rails application. If you are not using Rails, that's ok \u2014 you can use OpenTelemetry Ruby with other web frameworks as well, such as Sinatra and Rack. For a complete list of libraries for supported frameworks, see the registry.</p> <p>For more elaborate examples, see examples.</p>"},{"location":"docs/instrumentation/ruby/getting-started/#dependencies","title":"Dependencies","text":"<p>To begin, install rails:</p> <pre><code>gem install rails\n</code></pre>"},{"location":"docs/instrumentation/ruby/getting-started/#create-the-application","title":"Create the application","text":"<p>Create a new api-only application called <code>dice-ruby</code> and change into the newly created folder <code>dice-ruby</code></p> <pre><code>rails new --api dice-ruby\ncd dice-ruby\n</code></pre> <p>Create a controller for rolling a dice:</p> <pre><code>rails generate controller dice\n</code></pre> <p>This will create a file called <code>app/controllers/dice_controller.rb</code>. Open that file in your preferred editor and update it with the following code:</p> <pre><code>class DiceController &lt; ApplicationController\ndef roll\nrender json: (rand(6) + 1).to_s\nend\nend\n</code></pre> <p>Next, open the <code>config/routes.rb</code> file and add the following code:</p> <pre><code>Rails.application.routes.draw do\nget 'rolldice', to: 'dice#roll'\nend\n</code></pre> <p>Run the application with the following command and open http://localhost:8080/rolldice in your web browser to ensure it is working.</p> <pre><code>rails server -p 8080\n</code></pre> <p>If everything works fine you should see a number between 1 and 6 returned to you. You can now stop the application and instrument it using OpenTelemetry.</p>"},{"location":"docs/instrumentation/ruby/getting-started/#instrumentation","title":"Instrumentation","text":"<p>Install the <code>opentelemetry-sdk</code> and <code>opentelemetry-instrumentation-all</code> packages:</p> <pre><code>bundle add opentelemetry-sdk opentelemetry-instrumentation-all\n</code></pre> <p>The inclusion of <code>opentelemetry-instrumentation-all</code> provides instrumentations for Rails, Sinatra, several HTTP libraries, and more.</p> <p>For Rails applications, the usual way to initialize OpenTelemetry is in a Rails initializer. For other Ruby services, perform this initialization as early as possible in the start-up process.</p> <p>Create a file named <code>config/initializers/opentelemetry.rb</code> with the following code:</p> <pre><code># config/initializers/opentelemetry.rb\nrequire 'opentelemetry/sdk'\nrequire 'opentelemetry/instrumentation/all'\nOpenTelemetry::SDK.configure do |c|\nc.service_name = 'dice-ruby'\nc.use_all() # enables all instrumentation!\nend\n</code></pre> <p>The call <code>c.use_all()</code> enables all instrumentations in the <code>instrumentation/all</code> package. If you have more advanced configuration needs, see configuring specific instrumentation libraries.</p>"},{"location":"docs/instrumentation/ruby/getting-started/#run-the-instrumented-app","title":"Run the instrumented app","text":"<p>You can now run your instrumented app and have it print to the console for now:</p> <pre><code>env OTEL_TRACES_EXPORTER=console rails server -p 8080\n</code></pre> <p>Open http://localhost:8080/rolldice in your web browser and reload the page a few times. You should see the spans printed in the console, such as the following:</p> <pre><code>#&lt;struct OpenTelemetry::SDK::Trace::SpanData\nname=\"DiceController#roll\",\nkind=:server,\nstatus=#&lt;OpenTelemetry::Trace::Status:0x000000010587fc48 @code=1, @description=\"\"&gt;,\nparent_span_id=\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\",\ntotal_recorded_attributes=8,\ntotal_recorded_events=0,\ntotal_recorded_links=0,\nstart_timestamp=1683555544407294000,\nend_timestamp=1683555544464308000,\nattributes=\n{\"http.method\"=&gt;\"GET\",\n\"http.host\"=&gt;\"localhost:8080\",\n\"http.scheme\"=&gt;\"http\",\n\"http.target\"=&gt;\"/rolldice\",\n\"http.user_agent\"=&gt;\"curl/7.87.0\",\n\"code.namespace\"=&gt;\"DiceController\",\n\"code.function\"=&gt;\"roll\",\n\"http.status_code\"=&gt;200},\nlinks=nil,\nevents=nil,\nresource=\n#&lt;OpenTelemetry::SDK::Resources::Resource:0x000000010511d1f8\n@attributes=\n{\"service.name\"=&gt;\"&lt;YOUR_SERVICE_NAME&gt;\",\n\"process.pid\"=&gt;83900,\n\"process.command\"=&gt;\"bin/rails\",\n\"process.runtime.name\"=&gt;\"ruby\",\n\"process.runtime.version\"=&gt;\"3.2.2\",\n\"process.runtime.description\"=&gt;\"ruby 3.2.2 (2023-03-30 revision e51014f9c0) [arm64-darwin22]\",\n\"telemetry.sdk.name\"=&gt;\"opentelemetry\",\n\"telemetry.sdk.language\"=&gt;\"ruby\",\n\"telemetry.sdk.version\"=&gt;\"1.2.0\"}&gt;,\ninstrumentation_scope=#&lt;struct OpenTelemetry::SDK::InstrumentationScope name=\"OpenTelemetry::Instrumentation::Rack\", version=\"0.23.0\"&gt;,\nspan_id=\"\\xA7\\xF0\\x9B#\\b[\\xE4I\",\ntrace_id=\"\\xF3\\xDC\\b8\\x91h\\xB0\\xDF\\xDEn*CH\\x9Blf\",\ntrace_flags=#&lt;OpenTelemetry::Trace::TraceFlags:0x00000001057b7b08 @flags=1&gt;,\ntracestate=#&lt;OpenTelemetry::Trace::Tracestate:0x00000001057b67f8 @hash={}&gt;&gt;\n</code></pre>"},{"location":"docs/instrumentation/ruby/getting-started/#what-next","title":"What next?","text":"<p>Adding tracing to a single service is a great first step. OpenTelemetry provides a few more features that will allow you gain even deeper insights!</p> <ul> <li>Exporters allow you to export your data to a preferred backend.</li> <li>Context propagation is perhaps one of the most powerful concepts in   OpenTelemetry because it will upgrade your single service trace into a   distributed trace, which makes it possible for OpenTelemetry vendors to   visualize a request from end-to-end across process and network boundaries.</li> <li>Span events allow you to add a human-readable message on a span that   represents \"something happening\" during its lifetime.</li> <li>Manual instrumentation will give provide you the ability to enrich   your traces with domain specific data.</li> </ul>"},{"location":"docs/instrumentation/ruby/manual/","title":"Manual Instrumentation","text":"<p>Auto-instrumentation is the easiest way to get started with instrumenting your code, but in order to get the most insight into your system, you should add manual instrumentation where appropriate. To do this, use the OpenTelemetry SDK to access the currently executing span and add attributes to it, and/or to create new spans.</p>"},{"location":"docs/instrumentation/ruby/manual/#initializing-the-sdk","title":"Initializing the SDK","text":"<p>First, ensure you have the SDK package installed:</p> <pre><code>gem install opentelemetry-sdk\n</code></pre> <p>Then include configuration code that runs when your program initializes. Make sure that <code>service.name</code> is set by configuring a service name.</p>"},{"location":"docs/instrumentation/ruby/manual/#acquiring-a-tracer","title":"Acquiring a Tracer","text":"<p>To begin tracing, you will need to ensure you have an initialized <code>Tracer</code> that comes from a <code>TracerProvider</code>.</p> <p>The easiest and most common way to do this is to use the globally-registered TracerProvider. If you are using instrumentation libraries, such as in a Rails app, then one will be registered for you.</p> <pre><code># If in a rails app, this lives in config/initializers/opentelemetry.rb\nrequire \"opentelemetry/sdk\"\nOpenTelemetry::SDK.configure do |c|\nc.service_name = '&lt;YOUR_SERVICE_NAME&gt;'\nend\n# 'Tracer' can be used throughout your code now\nMyAppTracer = OpenTelemetry.tracer_provider.tracer('&lt;YOUR_TRACER_NAME&gt;')\n</code></pre> <p>With a <code>Tracer</code> acquired, you can manually trace code.</p>"},{"location":"docs/instrumentation/ruby/manual/#tracing","title":"Tracing","text":""},{"location":"docs/instrumentation/ruby/manual/#get-the-current-span","title":"Get the current span","text":"<p>It's very common to add information to the current span somewhere within your program. To do so, you can get the current span and add attributes to it.</p> <pre><code>require \"opentelemetry/sdk\"\ndef track_extended_warranty(extended_warranty)\n# Get the current span\ncurrent_span = OpenTelemetry::Trace.current_span\n# And add useful stuff to it!\ncurrent_span.add_attributes({\n\"com.extended_warranty.id\" =&gt; extended_warranty.id,\n\"com.extended_warranty.timestamp\" =&gt; extended_warranty.timestamp\n})\nend\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#creating-new-spans","title":"Creating New Spans","text":"<p>To create a span, you\u2019ll need a configured <code>Tracer</code>.</p> <p>Typically when you create a new span, you'll want it to be the active/current span. To do that, use <code>in_span</code>:</p> <pre><code>require \"opentelemetry/sdk\"\ndef do_work\nMyAppTracer.in_span(\"do_work\") do |span|\n# do some work that the 'do_work' span tracks!\nend\nend\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#creating-nested-spans","title":"Creating nested spans","text":"<p>If you have a distinct sub-operation you\u2019d like to track as a part of another one, you can create nested spans to represent the relationship:</p> <pre><code>require \"opentelemetry/sdk\"\ndef parent_work\nMyAppTracer.in_span(\"parent\") do |span|\n# do some work that the 'parent' span tracks!\nchild_work\n# do some more work afterwards\nend\nend\ndef child_work\nMyAppTracer.in_span(\"child\") do |span|\n# do some work that the 'child' span tracks!\nend\nend\n</code></pre> <p>In the preceding example, two spans are created - named <code>parent</code> and <code>child</code> - with <code>child</code> nested under <code>parent</code>. If you view a trace with these spans in a trace visualization tool, <code>child</code> will be nested under <code>parent</code>.</p>"},{"location":"docs/instrumentation/ruby/manual/#add-attributes-to-a-span","title":"Add attributes to a span","text":"<p>Attributes let you attach key/value pairs to a span so it carries more information about the current operation that it\u2019s tracking.</p> <p>You can use <code>set_attribute</code> to add a single attribute to a span:</p> <pre><code>require \"opentelemetry/sdk\"\ncurrent_span = OpenTelemetry::Trace.current_span\ncurrent_span.set_attribute(\"animals\", [\"elephant\", \"tiger\"])\n</code></pre> <p>You can use <code>add_attributes</code> to add a map of attributes:</p> <pre><code>require \"opentelemetry/sdk\"\ncurrent_span = OpenTelemetry::Trace.current_span\ncurrent_span.add_attributes({\n\"my.cool.attribute\" =&gt; \"a value\",\n\"my.first.name\" =&gt; \"Oscar\"\n})\n</code></pre> <p>You can also add attributes to a span as it's being created:</p> <pre><code>require \"opentelemetry/sdk\"\nMyAppTracer.in_span('foo', attributes: { \"hello\" =&gt; \"world\", \"some.number\" =&gt; 1024 }) do |span|\n#  do stuff with the span\nend\n</code></pre> <p>\u26a0 Spans are thread safe data structures that require locks when they are mutated. You should therefore avoid calling <code>set_attribute</code> multiple times and instead assign attributes in bulk with a Hash, either during span creation or with <code>add_attributes</code> on an existing span.</p> <p>\u26a0 Sampling decisions happen at the moment of span creation. If your sampler considers span attributes when deciding to sample a span, then you must pass those attributes as part of span creation. Any attributes added after creation will not be seen by the sampler, because the sampling decision has already been made.</p>"},{"location":"docs/instrumentation/ruby/manual/#add-semantic-attributes","title":"Add semantic attributes","text":"<p>Semantic Attributes are pre-defined Attributes that are well-known naming conventions for common kinds of data. Using Semantic Attributes lets you normalize this kind of information across your systems.</p> <p>To use Semantic Attributes in Ruby, add the appropriate gem:</p> <pre><code>gem install opentelemetry-semantic_conventions\n</code></pre> <p>Then you can use it in code:</p> <pre><code>require 'opentelemetry/sdk'\nrequire 'opentelemetry/semantic_conventions'\ncurrent_span = OpenTelemetry::Trace.current_span\ncurrent_span.add_attributes({\nOpenTelemetry::SemanticConventions::Trace::HTTP_METHOD =&gt; \"GET\",\nOpenTelemetry::SemanticConventions::Trace::HTTP_URL =&gt; \"https://opentelemetry.io/\",\n})\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#add-span-events","title":"Add Span Events","text":"<p>A span event is a human-readable message on a span that represents \"something happening\" during it's lifetime. For example, imagine a function that requires exclusive access to a resource that is under a mutex. An event could be created at two points - once, when we try to gain access to the resource, and another when we acquire the mutex.</p> <pre><code>require \"opentelemetry/sdk\"\nspan = OpenTelemetry::Trace.current_span\nspan.add_event(\"Acquiring lock\")\nif mutex.try_lock\nspan.add_event(\"Got lock, doing work...\")\n# some code here\nspan.add_event(\"Releasing lock\")\nelse\nspan.add_event(\"Lock already in use\")\nend\n</code></pre> <p>A useful characteristic of events is that their timestamps are displayed as offsets from the beginning of the span, allowing you to easily see how much time elapsed between them.</p> <p>Events can also have attributes of their own e.g.</p> <pre><code>require \"opentelemetry/sdk\"\nspan.add_event(\"Cancelled wait due to external signal\", attributes: {\n\"pid\" =&gt; 4328,\n\"signal\" =&gt; \"SIGHUP\"\n})\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#add-span-links","title":"Add Span Links","text":"<p>A span can be created with zero or more span links that causally link it to another span. A link needs a span context to be created.</p> <pre><code>require \"opentelemetry/sdk\"\nspan_to_link_from = OpenTelemetry::Trace.current_span\nlink = OpenTelemetry::Trace::Link.new(span_to_link_from.context)\nMyAppTracer.in_span(\"new-span\", links: [link])\n# do something that 'new_span' tracks\n# The link in 'new_span' casually associated it with the span it's linked from,\n# but it is not necessarily a child span.\nend\n</code></pre> <p>Span Links are often used to link together different traces that are related in some way, such as a long-running task that calls into sub-tasks asynchronously.</p> <p>Links can also be created with additional attributes:</p> <pre><code>link = OpenTelemetry::Trace::Link.new(span_to_link_from.context, attributes: { \"some.attribute\" =&gt; 12 })\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#set-span-status","title":"Set span status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - StatusCode.ERROR. In rare scenarios, you could override the Error status with StatusCode.OK, but don\u2019t set StatusCode.OK on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>require \"opentelemetry/sdk\"\ncurrent_span = OpenTelemetry::Trace.current_span\nbegin\n1/0 # something that obviously fails\nrescue\ncurrent_span.status = OpenTelemetry::Trace::Status.error(\"error message here!\")\nend\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#record-exceptions-in-spans","title":"Record exceptions in spans","text":"<p>It can be a good idea to record exceptions when they happen. It\u2019s recommended to do this in conjunction with setting span status.</p> <pre><code>require \"opentelemetry/sdk\"\ncurrent_span = OpenTelemetry::Trace.current_span\nbegin\n1/0 # something that obviously fails\nrescue Exception =&gt; e\ncurrent_span.status = OpenTelemetry::Trace::Status.error(\"error message here!\")\ncurrent_span.record_exception(e)\nend\n</code></pre> <p>Recording an exception creates a Span Event on the current span with a stack trace as an attribute on the span event.</p> <p>Exceptions can also be recorded with additional attributes:</p> <pre><code>current_span.record_exception(ex, attributes: { \"some.attribute\" =&gt; 12 })\n</code></pre>"},{"location":"docs/instrumentation/ruby/manual/#context-propagation","title":"Context Propagation","text":"<p>Distributed Tracing tracks the progression of a single Request, called a Trace, as it is handled by Services that make up an Application. A Distributed Trace transverses process, network and security boundaries. Glossary</p> <p>This requires context propagation, a mechanism where identifiers for a trace are sent to remote processes.</p> <p>\u2139 The OpenTelemetry Ruby SDK will take care of context propagation as long as your service is leveraging auto-instrumented libraries. Please refer to the README for more details.</p> <p>In order to propagate trace context over the wire, a propagator must be registered with the OpenTelemetry SDK. The W3 TraceContext and Baggage propagators are configured by default. Operators may override this value by setting <code>OTEL_PROPAGATORS</code> environment variable to a comma separated list of propagators. For example, to add B3 propagation, set <code>OTEL_PROPAGATORS</code> to the complete list of propagation formats you wish to support:</p> <pre><code>export OTEL_PROPAGATORS=tracecontext,baggage,b3\n</code></pre> <p>Propagators other than <code>tracecontext</code> and <code>baggage</code> must be added as gem dependencies to your Gemfile, e.g.:</p> <pre><code>gem 'opentelemetry-propagator-b3'\n</code></pre>"},{"location":"docs/instrumentation/ruby/sampling/","title":"Sampling","text":"<p>Sampling is a process that restricts the amount of traces that are generated by a system. The Ruby SDK offers several head samplers.</p>"},{"location":"docs/instrumentation/ruby/sampling/#default-behavior","title":"Default behavior","text":"<p>By default, all spans are sampled, and thus, 100% of traces are sampled. If you do not need to manage data volume, don't bother setting a sampler.</p> <p>Specifically, the default sampler is a composite of ParentBased and ALWAYS_ON that ensures the root span in a trace is always sampled, and that all child spans respect use their parent's sampling flag to make a sampling decision. This guarantees that all spans in a trace are sampled by default.</p>"},{"location":"docs/instrumentation/ruby/sampling/#traceidratiobased-sampler","title":"TraceIDRatioBased Sampler","text":"<p>The most common head sampler to use is the TraceIdRatioBased sampler. It deterministically samples a percentage of traces that you pass in as a parameter.</p>"},{"location":"docs/instrumentation/ruby/sampling/#environment-variables","title":"Environment Variables","text":"<p>You can configure a <code>TraceIdRatioBased</code> sampler with environment variables:</p> <pre><code>export OTEL_TRACES_SAMPLER=\"traceidratio\"\nexport OTEL_TRACES_SAMPLER_ARG=\"0.1\"\n</code></pre> <p>This tells the SDK to sample spans such that only 10% of traces get exported.</p>"},{"location":"docs/instrumentation/ruby/sampling/#configuration-in-code","title":"Configuration in Code","text":"<p>Although it is possible to configure a <code>TraceIdRatioBased</code> sampler in code, it's not recommended. Doing so requires you to manually set up a Tracer Provider with all the right configuration options, which is hard to get right compared to just using <code>OpenTelemetry::SDK.configure</code>.</p>"},{"location":"docs/instrumentation/rust/","title":"Rust","text":"<p>{{% lang_instrumentation_index_head rust /%}}</p>"},{"location":"docs/instrumentation/rust/#crates","title":"Crates","text":"<p>OpenTelemetry for Rust publishes the following crates:</p> <ul> <li><code>opentelemetry</code></li> <li><code>opentelemetry-api</code></li> <li><code>opentelemetry-sdk</code></li> <li><code>opentelemetry-aws</code></li> <li><code>opentelemetry-contrib</code></li> <li><code>opentelemetry-datadog</code></li> <li><code>opentelemetry-dynatrace</code></li> <li><code>opentelemetry-http</code></li> <li><code>opentelemetry-jaeger</code></li> <li><code>opentelemetry-otlp</code></li> <li><code>opentelemetry-prometheus</code></li> <li><code>opentelemetry-semantic-conventions</code></li> <li><code>opentelemetry-stackdriver</code></li> <li><code>opentelemetry-zipkin</code></li> </ul>"},{"location":"docs/instrumentation/rust/#further-reading","title":"Further Reading","text":"<ul> <li>Docs for Rust API &amp; SDK</li> <li>Examples</li> <li>Ecosystem</li> </ul>"},{"location":"docs/instrumentation/swift/","title":"Swift","text":"<p>{{% lang_instrumentation_index_head swift /%}}</p>"},{"location":"docs/instrumentation/swift/#further-reading","title":"Further Reading","text":"<ul> <li>OpenTelemetry for Swift on GitHub</li> <li>Installation</li> <li>Examples</li> </ul>"},{"location":"docs/instrumentation/swift/libraries/","title":"Instrumentation Libraries","text":"<p>OpenTelemetry-Swift provides several instrumentation libraries that generate instrumentation for you when they're installed and initialized.</p>"},{"location":"docs/instrumentation/swift/libraries/#sdkresourceextension","title":"<code>SDKResourceExtension</code>","text":"<p><code>SDKResourceExtension</code> provides details about the device as a Resource.</p>"},{"location":"docs/instrumentation/swift/libraries/#usage","title":"Usage","text":"<p>Use <code>DefaultResource.get()</code> to generate an all-in-one resource object. This resource can be added to a <code>TracerProvider</code> or <code>MetricProvider</code>.</p> <pre><code>OpenTelemetry.registerTracerProvider(traceProvider: TracerProviderBuilder()\n.with(resource: DefaultResource.get())\n.build())\n</code></pre>"},{"location":"docs/instrumentation/swift/libraries/#details","title":"Details","text":"<p><code>SDKResourceExtension</code> provides attributes in a resource object with details about the iOS device, OS details, and application details. It applies these values to the appropriate semantic attributes.</p>"},{"location":"docs/instrumentation/swift/libraries/#application-info","title":"Application Info","text":"Attribute Value example Description <code>service.name</code> <code>MyApplication</code> <code>CFBundleName</code>; The application name defined in the App's info.plist. <code>service.version</code> <code>1.0 (1234)</code> <code>CFBundleShortVersion</code> &amp; (<code>CFBundleVersion</code>); The application version as defined in the App's info.plist <code>service.namespace</code> <code>com.myCompany.myApplication</code> <code>CFBundleIdentifier</code>"},{"location":"docs/instrumentation/swift/libraries/#device-info","title":"Device Info","text":"Attribute Value example Description <code>device.model.identifier</code> <code>iphone13,3</code> fetched from <code>sysctl</code> depending on device type <code>device.id</code> <code>00000000-0000-0000000</code> <code>identifierForVendor</code> uuid string"},{"location":"docs/instrumentation/swift/libraries/#operating-system-info","title":"Operating System Info","text":"Attributes Value example Description <code>os.type</code> <code>darwin</code> predefined in <code>ResourceAttributes</code> <code>os.name</code> <code>iOS</code>, <code>watchOS</code>, <code>macOS</code> <code>UIDevice.current.systemName</code> or dependent on platform <code>os.version</code> <code>15.4.0</code> <code>ProcessInfo.processInfo.operatingSystemVersion</code> <code>os.description</code> <code>iOS Version 15.4 (Build 19E240)</code> A combination of os name, version and build."},{"location":"docs/instrumentation/swift/libraries/#nsurlsession-instrumentation","title":"<code>NSURLSession</code> Instrumentation","text":"<p>This instrumentation creates spans for all network requests made with NSURLSessions. It also injects distributed tracing headers in instrumented network requests. <code>NetworkStatus</code> is a dependency of this package, which provides network status attributes on network spans.</p> <p>Note: The NSURLSession instrumentation relies on the global tracer provider in the OpenTelemetry object. Custom tracer providers must be configured and set as the global provider prior to this instrumentation.</p>"},{"location":"docs/instrumentation/swift/libraries/#usage_1","title":"Usage","text":"<p>Initialize the class with <code>URLSessionInstrumentation(configuration: URLSessionInstrumentationConfiguration())</code> to automatically capture all network calls.</p> <p>This behavior can be modified or augmented by using the optional callbacks defined in <code>URLSessionInstrumentationConfiguration</code>:</p> <ul> <li><code>shouldInstrument: ((URLRequest) -&gt; (Bool)?)?</code></li> </ul> <p>Filter which requests you want to instrument, all by default.</p> <ul> <li><code>shouldRecordPayload: ((URLSession) -&gt; (Bool)?)?</code></li> </ul> <p>Implement if you want the session to record payload data, false by default.</p> <ul> <li><code>shouldInjectTracingHeaders: ((URLRequest) -&gt; (Bool)?)?</code></li> </ul> <p>Allow filtering which requests you want to inject headers to follow the trace,   true by default. You must also return true if you want to inject custom   headers.</p> <ul> <li><code>injectCustomHeaders: ((inout URLRequest, Span?) -&gt; Void)?</code></li> </ul> <p>Implement this callback to inject custom headers or modify the request in any   other way.</p> <ul> <li><code>nameSpan: ((URLRequest) -&gt; (String)?)?</code></li> </ul> <p>Modify the name for the given request instead of standard OpenTelemetry name.</p> <ul> <li><code>createdRequest: ((URLRequest, Span) -&gt; Void)?</code></li> </ul> <p>Called after request is created, it allows to add extra information to the   Span.</p> <ul> <li><code>receivedResponse: ((URLResponse, DataOrFile?, Span) -&gt; Void)?</code></li> </ul> <p>Called after response is received, it allows to add extra information to the   Span.</p> <ul> <li><code>receivedError: ((Error, DataOrFile?, HTTPStatus, Span) -&gt; Void)?</code></li> </ul> <p>Called after an error is received, it allows to add extra information to the   Span.</p> <p>Below is an example of initialization. <code>URLSessionInstrumentationConfiguration</code>'s construction can be passed the parameters defined above to suit the needs of the application.</p> <pre><code>let sessionInstrumentation = URLSessionInstrumentation(configuration: URLSessionInstrumentationConfiguration())\n</code></pre>"},{"location":"docs/instrumentation/swift/libraries/#details_1","title":"Details","text":"<p><code>NSURLSession</code> instrumentation also provides additional attributes providing details about the network state of the device at the time of network requests.</p> Attribute Value example Description <code>net.host.connection.type</code> <code>wifi</code>, <code>cell</code>, <code>unavailable</code> The type of connection utilized by the device at the time of the request. <code>net.host.connection.subtype</code> <code>EDGE</code> <code>LTE</code>, etc They type of cellular connection. Only populated if the connection type is <code>cell</code>. <code>net.host.carrier.name</code> <code>T-Mobile</code>, <code>Verizon</code>, etc The cellular carrier name. Only populated for cellular connection types. <code>net.host.carrier.icc</code> <code>DE</code> The ISO 3166-1 alpha-2 2-character country code associated with the mobile carrier network. <code>net.host.carrier.mcc</code> <code>310</code> Mobile Country Code <code>net.host.carrier.mnc</code> <code>001</code> Mobile network code"},{"location":"docs/instrumentation/swift/libraries/#signpostintegration","title":"<code>SignpostIntegration</code>","text":"<p>This package creates <code>os_signpost</code> <code>begin</code> and <code>end</code> calls when spans are started or ended. It allows automatic integration of applications instrumented with OpenTelemetry to show their spans in a profiling app like <code>Instruments</code>. It also exports the <code>OSLog</code> it uses for posting so the user can add extra signpost events. This functionality is shown in <code>Simple Exporter</code> example.</p>"},{"location":"docs/instrumentation/swift/libraries/#usage_2","title":"Usage","text":"<p>Just add SignpostIntegration as any other Span Processor (see the manual instrumentation) docs for details on configuring your providers:</p> <pre><code>OpenTelemetry.instance.tracerProvider.addSpanProcessor(SignPostIntegration())\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/","title":"Manual Instrumentation","text":""},{"location":"docs/instrumentation/swift/manual/#setup","title":"Setup","text":"<p>OpenTelemetry Swift provides limited functionality in its default configuration. For more useful functionality, some configuration is required.</p> <p>The default registered <code>TracerProvider</code> and <code>MetricProvider</code> are not configured with an exporter. There are several exporters available depending on your needs. Below we will explore configuring the OTLP exporter, which can be used for sending data to the collector.</p> <pre><code>import GRPC\nimport OpenTelemetryApi\nimport OpenTelemetrySdk\nimport OpenTelemetryProtocolExporter\n// initialize the OtlpTraceExporter\nlet otlpConfiguration = OtlpConfiguration(timeout: OtlpConfiguration.DefaultTimeoutInterval)\nlet grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1))\n.connect(host: &lt;collector host&gt;, port: &lt;collector port&gt;)\nlet traceExporter = OtlpTraceExporter(channel: grpcChannel,\nconfig: otlpConfiguration)\n// build &amp; register the Tracer Provider using the built otlp trace exporter\nOpenTelemetry.registerTracerProvider(tracerProvider: TracerProviderBuilder()\n.add(spanProcessor:SimpleSpanProcessor(spanExporter: traceExporter))\n.with(resource: Resource())\n.build())\n</code></pre> <p>A similar pattern is used for the OtlpMetricExporter:</p> <pre><code>// otlpConfiguration &amp; grpcChannel can be reused\nOpenTelemetry.registerMeterProvider(meterProvider: MeterProviderBuilder()\n.with(processor: MetricProcessorSdk())\n.with(exporter: OtlpMetricExporter(channel: channel, config: otlpConfiguration))\n.with(resource: Resource())\n.build())\n</code></pre> <p>After configuring the MeterProvider &amp; TracerProvider all subsequently initialized instrumentation will be exporting using this OTLP exporter.</p>"},{"location":"docs/instrumentation/swift/manual/#acquiring-a-tracer","title":"Acquiring a Tracer","text":"<p>To do tracing, you will need a tracer. A tracer is acquired through the tracer provider and is responsible for creating spans. The OpenTelemetry manages the tracer provider as we defined and registered above. A tracer requires an instrumentation name, and an optional version to be created:</p> <pre><code>let  tracer = OpenTelemetry.instance.tracerProvider.get(instrumentationName: \"instrumentation-library-name\", instrumentationVersion: \"1.0.0\")\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#creating-spans","title":"Creating Spans","text":"<p>A span represents a unit of work or operation. Spans are the building blocks of Traces. To create a span use the span builder associated with the tracer:</p> <pre><code>let span =  let builder = tracer.spanBuilder(spanName: \"\\(name)\").startSpan()\n...\nspan.end()\n</code></pre> <p>It is required to call <code>end()</code> to end the span.</p>"},{"location":"docs/instrumentation/swift/manual/#creating-nested-spans","title":"Creating Nested Spans","text":"<p>Spans are used to build relationship between operations. Below is an example of how we can manually build relationship between spans.</p> <p>Below we have <code>parent()</code> calling <code>child()</code> and how to manually link spans of each of these methods.</p> <pre><code>func parent() {\nlet parentSpan = someTracer.spanBuilder(spanName: \"parent span\").startSpan()\nchild(span: parentSpan)\nparentSpan.end()\n}\nfunc child(parentSpan: Span) {\nlet childSpan = someTracer.spanBuilder(spanName: \"child span\")\n.setParent(parentSpan)\n.startSpan()\n// do work\nchildSpan.end()\n}\n</code></pre> <p>The parent-child relationship will be automatically linked if <code>activeSpan</code> is used:</p> <pre><code>func parent() {\nlet parentSpan = someTracer.spanBuilder(spanName: \"parent span\")\n.setActive(true) // automatically sets context\n.startSpan()\nchild()\nparentSpan.end()\n}\nfunc child() {\nlet childSpan = someTracer.spanBuilder(spanName: \"child span\")\n.startSpan() //automatically captures `active span` as parent\n// do work\nchildSpan.end()\n}\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#getting-the-current-span","title":"Getting the Current Span","text":"<p>Sometimes it's useful to do something with the current/active span. Here's how to access the current span from an arbitrary point in your code.</p> <pre><code>  let currentSpan = OpenTelemetry.instance.contextProvider.activeSpan\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#span-attributes","title":"Span Attributes","text":"<p>Spans can also be annotated with additional attributes. All spans will be automatically annotated with the <code>Resource</code> attributes attached to the tracer provider. The Opentelemetry-swift sdk already provides instrumentation of common attributes in the <code>SDKResourceExtension</code> instrumentation. In this example a span for a network request capturing details about that request using existing semantic conventions.</p> <pre><code>let span = tracer.spanBuilder(\"/resource/path\").startSpan()\nspan.setAttribute(\"http.method\", \"GET\");\nspan.setAttribute(\"http.url\", url.toString());\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#creating-span-events","title":"Creating Span Events","text":"<p>A Span Event can be thought of as a structured log message (or annotation) on a Span, typically used to denote a meaningful, singular point in time during the Span\u2019s duration.</p> <pre><code>            let attributes = [\n\"key\" : AttributeValue.string(\"value\"),\n\"result\" : AttributeValue.int(100)\n]\nspan.addEvent(name: \"computation complete\", attributes: attributes)\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#setting-span-status","title":"Setting Span Status","text":"<p>A status can be set on a span, typically used to specify that a span has not completed successfully - <code>SpanStatus.Error</code>. In rare scenarios, you could override the Error status with OK, but don\u2019t set OK on successfully-completed spans.</p> <p>The status can be set at any time before the span is finished:</p> <pre><code>func myFunction() {\nlet span = someTracer.spanBuilder(spanName: \"my span\").startSpan()\ndefer {\nspan.end()\n}\nguard let criticalData = get() else {\nspan.status = .error(description: \"something bad happened\")\nreturn\n}\n// do something\n}\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#recording-exceptions-in-spans","title":"Recording exceptions in Spans","text":"<p>Semantic conventions provide special demarcation for events that record exceptions:</p> <pre><code>let span = someTracer.spanBuilder(spanName: \"my span\").startSpan()\ndo {\ntry throwingFunction()\n} catch {\nspan.addEvent(name: SemanticAttributes.exception.rawValue,\nattributes: [SemanticAttributes.exceptionType.rawValue: AttributeValue.string(String(describing: type(of: error))),\nSemanticAttributes.exceptionEscaped.rawValue: AttributeValue.bool(false),\nSemanticAttributes.exceptionMessage.rawValue: AttributeValue.string(error.localizedDescription)])\n})\nspan.status = .error(description: error.localizedDescription)\n}\nspan.end()\n</code></pre>"},{"location":"docs/instrumentation/swift/manual/#sdk-configuration","title":"SDK Configuration","text":""},{"location":"docs/instrumentation/swift/manual/#processors","title":"Processors","text":"<p>Different Span processors are offered by OpenTelemetry-swift. The <code>SimpleSpanProcessor</code> immediately forwards ended spans to the exporter, while the <code>BatchSpanProcessor</code> batches them and sends them in bulk. Multiple Span processors can be configured to be active at the same time using the <code>MultiSpanProcessor</code>. For example, you may create a <code>SimpleSpanProcessor</code> that exports to a logger, and a <code>BatchSpanProcesssor</code> that exports to a OpenTelemetry Collector:</p> <pre><code>let otlpConfiguration = OtlpConfiguration(timeout: OtlpConfiguration.DefaultTimeoutInterval)\nlet grpcChannel = ClientConnection.usingPlatformAppropriateTLS(for: MultiThreadedEventLoopGroup(numberOfThreads:1))\n.connect(host: &lt;collector host&gt;, port: &lt;collector port&gt;)\nlet traceExporter = OtlpTraceExporter(channel: grpcChannel\nconfig: otlpConfiguration)\n// build &amp; register the Tracer Provider using the built otlp trace exporter\nOpenTelemetry.registerTracerProvider(tracerProvider: TracerProviderBuilder()\n.add(spanProcessor:BatchSpanProcessor(spanExporter: traceExporter))\n.add(spanProcessor:SimpleSpanProcessor(spanExporter: StdoutExporter))\n.with(resource: Resource())\n.build())\n</code></pre> <p>The batch span processor allows for a variety of parameters for customization including.</p>"},{"location":"docs/instrumentation/swift/manual/#exporters","title":"Exporters","text":"<p>OpenTelemetry-Swift provides the following exporters:</p> <ul> <li><code>InMemoryExporter</code>: Keeps the span data in memory. This is useful for testing   and debugging.</li> <li><code>DatadogExporter</code>: Converts OpenTelemetry span data to Datadog traces &amp; span   Events to Datadog logs.</li> <li><code>JaegerExporter</code>: Converts OpenTelemetry span data to Jaeger format and   exports to a Jaeger endpoint.</li> <li>Persistence exporter: An exporter decorator that provides data persistence to   existing metric and trace exporters.</li> <li><code>PrometheusExporter</code>: Converts metric data to Prometheus format and exports to   a Prometheus endpoint.</li> <li><code>StdoutExporter</code>: Exports span data to Stdout. Useful for debugging.</li> <li><code>ZipkinTraceExporter</code>: Exports span data to Zipkin format to a Zipkin   endpoint.</li> </ul>"},{"location":"docs/k8s/","title":"Kubernetes \u5f00\u653e\u9065\u6d4b Operator","text":""},{"location":"docs/k8s/#_1","title":"\u4ecb\u7ecd","text":"<p>OpenTelemetry Operator \u662fKubernetes Operator\u7684 \u4e00\u4e2a\u5b9e\u73b0\u3002</p> <p>Operator \u7ba1\u7406:</p> <ul> <li>OpenTelemetry \u6536\u96c6\u5668</li> <li>\u4f7f\u7528 OpenTelemetry \u5de5\u5177\u5e93\u81ea\u52a8\u68c0\u6d4b\u5de5\u4f5c\u8d1f\u8f7d</li> </ul>"},{"location":"docs/k8s/#_2","title":"\u5f00\u59cb","text":"<p>\u8981\u5728\u73b0\u6709\u96c6\u7fa4\u4e2d\u5b89\u88c5 Operator\uff0c\u8bf7\u786e\u4fdd\u5b89\u88c5\u4e86<code>cert-manager</code>\u5e76\u8fd0\u884c:</p> <pre><code>$ kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml\n</code></pre> <p>\u4e00\u65e6<code>opentelemetry-operator</code>\u90e8\u7f72\u5c31\u7eea\uff0c\u521b\u5efa\u4e00\u4e2a\u5f00\u653e\u9065\u6d4b\u91c7\u96c6\u5668(otelcol)\u5b9e\u4f8b\uff0c\u5982\u4e0b \u6240\u793a:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: simplest\nspec:\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n          http:\n    processors:\n    exporters:\n      logging:\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp]\n          processors: []\n          exporters: [logging]\nEOF\n</code></pre> <p>\u8981\u4e86\u89e3\u66f4\u591a\u914d\u7f6e\u9009\u9879\uff0c\u4ee5\u53ca\u4f7f\u7528 OpenTelemetry \u5de5\u5177\u5e93\u8bbe\u7f6e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u52a8\u68c0\u6d4b\u6ce8\u5165\uff0c\u8bf7 \u7ee7\u7eed\u9605 \u8bfb\u8fd9\u91cc.</p>"},{"location":"docs/k8s/collector/","title":"OpenTelemetry Collector","text":"<p>OpenTelemetry Collector \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0e\u4f9b\u5e94\u5546\u65e0\u5173\u7684\u5b9e\u73b0\uff0c\u7528\u4e8e\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u9065\u6d4b\u6570 \u636e\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u652f\u6301\u5f00\u6e90\u9065\u6d4b\u6570\u636e\u683c\u5f0f(\u4f8b\u5982 Jaeger\u3001Prometheus \u7b49)\u5230\u591a\u4e2a\u5f00\u6e90\u6216\u5546\u4e1a\u540e \u7aef\uff0c\u5b83\u6d88\u9664\u4e86\u8fd0\u884c\u3001\u64cd\u4f5c\u548c\u7ef4\u62a4\u591a\u4e2a\u4ee3\u7406/\u6536\u96c6\u5668\u7684\u9700\u8981\u3002</p> <p>\u76ee\u7684:</p> <ul> <li>\u53ef\u7528\u6027: \u5408\u7406\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u652f\u6301\u6d41\u884c\u7684\u534f\u8bae\uff0c\u5f00\u7bb1\u5373\u7528\u7684\u8fd0\u884c\u548c\u6536\u96c6\u3002</li> <li>\u9ad8\u6027\u80fd: \u5728\u5404\u79cd\u8d1f\u8f7d\u548c\u914d\u7f6e\u4e0b\u90fd\u975e\u5e38\u7a33\u5b9a\u548c\u9ad8\u6027\u80fd\u3002</li> <li>\u53ef\u89c2\u6d4b: \u4e00\u4e2a\u53ef\u89c2\u5bdf\u670d\u52a1\u7684\u8303\u4f8b\u3002</li> <li>\u53ef\u6269\u5c55: \u65e0\u9700\u89e6\u53ca\u6838\u5fc3\u4ee3\u7801\u5373\u53ef\u81ea\u5b9a\u4e49\u3002</li> <li>\u7edf\u4e00\u6027: \u5355\u4e2a\u4ee3\u7801\u5e93\uff0c\u53ef\u4f5c\u4e3a\u652f\u6301\u8ddf\u8e2a\u3001\u5ea6\u91cf\u548c\u65e5\u5fd7\u7684\u4ee3\u7406\u6216\u6536\u96c6\u5668\u8fdb\u884c\u90e8\u7f72\u3002</li> </ul>"},{"location":"docs/k8s/collector/#_1","title":"\u7a33\u5b9a\u7684\u6c34\u5e73","text":"<p>\u6536\u96c6\u5668\u7ec4\u4ef6\u548c\u5b9e\u73b0\u5904\u4e8e\u7a33\u5b9a\u6027\u7684\u4e0d\u540c\u9636\u6bb5\uff0c\u901a\u5e38\u5206\u4e3a\u529f\u80fd\u548c\u914d\u7f6e\u4e24\u90e8\u5206\u3002\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u72b6\u6001\u5728 \u8be5\u7ec4\u4ef6\u7684 README \u6587\u4ef6\u4e2d\u53ef\u7528\u3002\u867d\u7136\u6211\u4eec\u6253\u7b97\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u7ec4\u4ef6\u4f5c\u4e3a\u8be5\u5b58\u50a8\u5e93\u7684\u4e00\u90e8\u5206\uff0c\u4f46 \u6211\u4eec\u627f\u8ba4\uff0c\u5e76\u975e\u6240\u6709\u7ec4\u4ef6\u90fd\u5df2\u51c6\u5907\u5c31\u7eea\u3002\u56e0\u6b64\uff0c\u6bcf\u4e2a\u7ec4\u4ef6\u5e94\u6839\u636e\u4ee5\u4e0b\u5b9a\u4e49\u5217\u51fa\u6bcf\u4e2a\u9065\u6d4b\u4fe1\u53f7 \u7684\u5f53\u524d\u7a33\u5b9a\u7ea7\u522b:</p>"},{"location":"docs/k8s/collector/#development","title":"Development","text":"<p>\u5e76\u975e\u8be5\u7ec4\u4ef6\u7684\u6240\u6709\u90e8\u5206\u90fd\u5df2\u5230\u4f4d\uff0c\u800c\u4e14\u5b83\u53ef\u80fd\u8fd8\u4e0d\u80fd\u4f5c\u4e3a\u4efb\u4f55\u53d1\u884c\u7248\u7684\u4e00\u90e8\u5206\u4f7f\u7528\u3002\u5e94\u8be5\u62a5 \u544a\u9519\u8bef\u548c\u6027\u80fd\u95ee\u9898\uff0c\u4f46\u7ec4\u4ef6\u6240\u6709\u8005\u53ef\u80fd\u4e0d\u4f1a\u7ed9\u4e88\u5b83\u4eec\u592a\u591a\u5173\u6ce8\u3002\u4f60\u7684\u53cd\u9988\u4ecd\u7136\u662f\u9700\u8981\u7684\uff0c\u7279 \u522b\u662f\u5f53\u5b83\u6d89\u53ca\u5230\u7528\u6237\u4f53\u9a8c(\u914d\u7f6e\u9009\u9879\uff0c\u7ec4\u4ef6\u53ef\u89c2\u5bdf\u6027\uff0c\u6280\u672f\u5b9e\u73b0\u7ec6\u8282\uff0c\u2026)\u3002\u6839\u636e\u60c5\u51b5\u7684\u53d1\u5c55 \uff0c\u914d\u7f6e\u9009\u9879\u53ef\u80fd\u4f1a\u7ecf\u5e38\u5931\u6548\u3002\u8be5\u7ec4\u4ef6\u4e0d\u5e94\u5728\u751f\u4ea7\u4e2d\u4f7f\u7528\u3002</p>"},{"location":"docs/k8s/collector/#alpha","title":"Alpha","text":"<p>The component is ready to be used for limited non-critical workloads and the authors of this component would welcome your feedback. Bugs and performance problems should be reported, but component owners might not work on them right away. The configuration options might change often without backwards compatibility guarantees.</p>"},{"location":"docs/k8s/collector/#beta","title":"Beta","text":"<p>Same as Alpha, but the configuration options are deemed stable. While there might be breaking changes between releases, component owners should try to minimize them. A component at this stage is expected to have had exposure to non-critical production workloads already during its Alpha phase, making it suitable for broader usage.</p>"},{"location":"docs/k8s/collector/#stable","title":"Stable","text":"<p>The component is ready for general availability. Bugs and performance problems should be reported and there's an expectation that the component owners will work on them. Breaking changes, including configuration options and the component's output are not expected to happen without prior notice, unless under special circumstances.</p>"},{"location":"docs/k8s/collector/#deprecated","title":"Deprecated","text":"<p>The component is planned to be removed in a future version and no further support will be provided. Note that new issues will likely not be worked on. When a component enters \"deprecated\" mode, it is expected to exist for at least two minor releases. See the component's readme file for more details on when a component will cease to exist.</p>"},{"location":"docs/k8s/collector/#unmaintained","title":"Unmaintained","text":"<p>A component identified as unmaintained does not have an active code owner. Such component may have never been assigned a code owner or a previously active code owner has not responded to requests for feedback within 6 weeks of being contacted. Issues and pull requests for unmaintained components will be labelled as such. After 6 months of being unmaintained, these components will be removed from official distribution. Components that are unmaintained are actively seeking contributors to become code owners.</p>"},{"location":"docs/k8s/collector/#_2","title":"\u517c\u5bb9\u6027","text":"<p>\u5f53\u7528\u4f5c\u5e93\u65f6\uff0cOpenTelemetry Collector \u5c1d\u8bd5\u8ddf\u8e2a\u5f53\u524d\u652f\u6301\u7684 Go \u7248\u672c\uff0c \u5982\u7531 Go \u56e2\u961f\u5b9a\u4e49\u3002\u79fb\u9664\u5bf9\u4e0d\u53d7\u652f\u6301\u7684 Go \u7248\u672c\u7684\u652f\u6301\u5e76\u4e0d\u4f1a\u88ab\u8ba4\u4e3a\u662f\u7834\u574f\u6027\u7684\u6539\u53d8\u3002</p> <p>\u4ece Go 1.18 \u53d1\u5e03\u5f00\u59cb\uff0cOpenTelemetry Collector \u5bf9 Go \u7248\u672c\u7684\u652f\u6301\u5c06\u66f4\u65b0\u5982\u4e0b:</p> <ol> <li>\u65b0\u7684 Go \u5c0f\u7248\u672c\u201cN\u201d\u53d1\u5e03\u540e\u7684\u7b2c\u4e00\u4e2a\u7248\u672c\u5c06\u4e3a\u65b0\u7684 Go \u5c0f\u7248\u672c\u6dfb\u52a0\u6784\u5efa\u548c\u6d4b\u8bd5\u6b65\u9aa4\u3002</li> <li>Go \u5c0f\u7248\u672c\u201cN\u201d\u53d1\u5e03\u540e\u7684\u7b2c\u4e00\u4e2a\u7248\u672c\u5c06\u53d6\u6d88\u5bf9 Go \u5c0f\u7248\u672c\u201cN-2\u201d\u7684\u652f\u6301\u3002</li> </ol> <p>\u5b98\u65b9 OpenTelemetry Collector \u53d1\u884c\u7248\u4e8c\u8fdb\u5236\u6587\u4ef6\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u53d7\u652f\u6301\u7684 Go \u7248\u672c\u6784\u5efa\u3002</p>"},{"location":"docs/k8s/collector/design/","title":"\u6536\u96c6\u5668\u67b6\u6784","text":"<p>\u672c\u6587\u6863\u63cf\u8ff0\u4e86 OpenTelemetry Collector \u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u5b9e\u73b0\u3002</p>"},{"location":"docs/k8s/collector/design/#_2","title":"\u603b\u7ed3","text":"<p>OpenTelemetry Collector \u662f\u4e00\u4e2a\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5b83\u53ef\u4ee5\u63a5\u6536\u9065\u6d4b\u6570\u636e\uff0c\u53ef\u9009\u5730\u5904\u7406\u5b83\uff0c\u5e76\u8fdb \u4e00\u6b65\u5bfc\u51fa\u5b83\u3002</p> <p>Collector \u652f\u6301\u51e0\u4e2a\u6d41\u884c\u7684\u5f00\u6e90\u534f\u8bae\u6765\u63a5\u6536\u548c\u53d1\u9001\u9065\u6d4b\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u53ef\u63d2\u5165\u7684\u67b6\u6784\u6765\u6dfb \u52a0\u66f4\u591a\u7684\u534f\u8bae\u3002</p> <p>\u6570\u636e\u63a5\u6536\u3001\u5904\u7406\u548c\u5bfc\u51fa\u4f7f\u7528Pipelines\u5b8c\u6210\u3002\u53ef\u4ee5\u5c06 Collector \u914d\u7f6e\u4e3a\u5177\u6709 \u4e00\u4e2a\u6216\u591a\u4e2a\u7ba1\u9053\u3002\u6bcf\u4e2a\u7ba1\u9053\u5305\u62ec:</p> <ul> <li>\u63a5\u6536\u6570\u636e\u7684\u4e00\u7ec4\u63a5\u6536\u5668</li> <li>\u4e00\u7cfb\u5217\u53ef\u9009\u7684\u5904\u7406\u5668\uff0c\u5b83\u4eec\u4ece\u63a5\u6536\u7aef\u83b7\u53d6\u6570\u636e\u5e76\u8fdb\u884c\u5904\u7406</li> <li>\u4e00\u7ec4\u5bfc\u51fa\u5668\u4ece\u5904\u7406\u5668\u83b7\u53d6\u6570\u636e\u5e76\u5c06\u5176\u53d1\u9001\u5230\u6536\u96c6\u5668\u4e4b\u5916\u3002</li> </ul> <p>\u540c\u4e00\u4e2a\u63a5\u6536\u5668\u53ef\u4ee5\u5305\u542b\u5728\u591a\u4e2a\u7ba1\u9053\u4e2d\uff0c\u591a\u4e2a\u7ba1\u9053\u53ef\u4ee5\u5305\u542b\u540c\u4e00\u4e2a\u5bfc\u51fa\u5668\u3002</p>"},{"location":"docs/k8s/collector/design/#_3","title":"\u7ba1\u9053","text":"<p>\u7ba1\u9053\u5b9a\u4e49\u4e86\u6570\u636e\u5728\u6536\u96c6\u5668\u4e2d\u9075\u5faa\u7684\u8def\u5f84\uff0c\u4ece\u63a5\u6536\u5668\u5f00\u59cb\uff0c\u7136\u540e\u8fdb\u4e00\u6b65\u5904\u7406\u6216\u4fee\u6539\uff0c\u6700\u540e\u901a\u8fc7 \u5bfc\u51fa\u5668\u9000\u51fa\u6536\u96c6\u5668\u3002</p> <p>\u7ba1\u9053\u53ef\u4ee5\u5728 3 \u79cd\u9065\u6d4b\u6570\u636e\u7c7b\u578b\u4e0a\u8fd0\u884c:\u8f68\u8ff9\u3001\u5ea6\u91cf\u548c\u65e5\u5fd7\u3002\u6570\u636e\u7c7b\u578b\u662f\u7531\u5176\u914d\u7f6e\u5b9a\u4e49\u7684\u7ba1\u9053 \u7684\u5c5e\u6027\u3002\u7ba1\u9053\u4e2d\u4f7f\u7528\u7684\u63a5\u6536\u5668\u3001\u5904\u7406\u5668\u548c\u5bfc\u51fa\u5668\u5fc5\u987b\u652f\u6301\u7279\u5b9a\u7684\u6570\u636e\u7c7b\u578b\uff0c\u5426\u5219\u5728\u52a0\u8f7d\u914d\u7f6e \u65f6\u5c06\u62a5\u544a<code>ErrDataTypeIsNotSupported</code>\u3002\u7ba1\u9053\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u65b9\u5f0f\u6765\u63cf\u8ff0:</p> <p></p> <p>\u7ba1\u9053\u4e2d\u53ef\u4ee5\u6709\u4e00\u4e2a\u6216\u591a\u4e2a\u63a5\u6536\u5668\u3002\u6765\u81ea\u6240\u6709\u63a5\u6536\u5668\u7684\u6570\u636e\u88ab\u63a8\u9001\u5230\u7b2c\u4e00\u4e2a\u5904\u7406\u5668\uff0c\u8be5\u5904\u7406\u5668 \u5bf9\u5176\u6267\u884c\u5904\u7406\uff0c\u7136\u540e\u5c06\u5176\u63a8\u9001\u5230\u4e0b\u4e00\u4e2a\u5904\u7406\u5668(\u6216\u8005\u5b83\u53ef\u80fd\u4e22\u5f03\u6570\u636e\uff0c\u4f8b\u5982\uff0c\u5982\u679c\u5b83\u662f\u4e00\u4e2a\u201c \u91c7\u6837\u201d\u5904\u7406\u5668)\uff0c\u7b49\u7b49\uff0c\u76f4\u5230\u7ba1\u9053\u4e2d\u7684\u6700\u540e\u4e00\u4e2a\u5904\u7406\u5668\u5c06\u6570\u636e\u63a8\u9001\u5230\u5bfc\u51fa\u5668\u3002\u6bcf\u4e2a\u5bfc\u51fa\u5668\u83b7\u5f97 \u6bcf\u4e2a\u6570\u636e\u5143\u7d20\u7684\u526f\u672c\u3002\u6700\u540e\u4e00\u4e2a\u5904\u7406\u5668\u4f7f\u7528<code>fanoutconsumer</code>\u5c06\u6570\u636e\u5206\u6563\u5230\u591a\u4e2a\u5bfc\u51fa\u5668\u3002</p> <p>\u7ba1\u9053\u662f\u5728 Collector \u542f\u52a8\u671f\u95f4\u6839\u636e\u914d\u7f6e\u4e2d\u7684\u7ba1\u9053\u5b9a\u4e49\u6784\u5efa\u7684\u3002</p> <p>\u7ba1\u9053\u914d\u7f6e\u901a\u5e38\u662f\u8fd9\u6837\u7684:</p> <pre><code>service:\npipelines: # section that can contain multiple subsections, one per pipeline\ntraces: # type of the pipeline\nreceivers: [otlp, jaeger, zipkin]\nprocessors: [memory_limiter, batch]\nexporters: [otlp, jaeger, zipkin]\n</code></pre> <p>\u4e0a\u9762\u7684\u4f8b\u5b50\u4e3a\u201c\u8ddf\u8e2a\u201d\u7c7b\u578b\u7684\u9065\u6d4b\u6570\u636e\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7ba1\u9053\uff0c\u6709 3 \u4e2a\u63a5\u6536\u5668\uff0c2 \u4e2a\u5904\u7406\u5668\u548c 3 \u4e2a \u8f93\u51fa\u5668\u3002</p> <p>\u6709\u5173\u914d\u7f6e\u6587\u4ef6\u683c\u5f0f\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2 \u9605\u672c\u6587\u6863.</p>"},{"location":"docs/k8s/collector/design/#-receivers","title":"\u63a5\u6536\u5668 - Receivers","text":"<p>\u63a5\u6536\u5668\u901a\u5e38\u76d1\u542c\u7f51\u7edc\u7aef\u53e3\u5e76\u63a5\u6536\u9065\u6d4b\u6570\u636e\u3002\u901a\u5e38\u5c06\u4e00\u4e2a\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u5c06\u63a5\u6536\u5230\u7684\u6570\u636e\u53d1\u9001\u5230 \u4e00\u4e2a\u7ba1\u9053\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u5c06\u540c\u4e00\u4e2a\u63a5\u6536\u5668\u914d\u7f6e\u4e3a\u5c06\u63a5\u6536\u5230\u7684\u76f8\u540c\u6570\u636e\u53d1\u9001\u5230\u591a\u4e2a\u7ba1\u9053\u3002\u8fd9\u53ef\u4ee5 \u901a\u8fc7\u7b80\u5355\u5730\u5728\u51e0\u4e2a\u7ba1\u9053\u7684 <code>receiver</code> \u952e\u4e2d\u5217\u51fa\u76f8\u540c\u7684\u63a5\u6536\u5668\u6765\u5b9e\u73b0:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nendpoint: localhost:4317\nservice:\npipelines:\ntraces: # a pipeline of \u201ctraces\u201d type\nreceivers: [otlp]\nprocessors: [memory_limiter, batch]\nexporters: [jaeger]\ntraces/2: # another pipeline of \u201ctraces\u201d type\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [opencensus]\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c<code>otlp</code>\u63a5\u6536\u5668\u5c06\u53d1\u9001\u76f8\u540c\u7684\u6570\u636e\u5230\u7ba1\u9053<code>traces</code>\u548c\u7ba1\u9053<code>traces/2</code>\u3002 (\u6ce8 \u610f:\u914d\u7f6e\u4f7f\u7528\u590d\u5408\u952e\u540d\u7684\u5f62\u5f0f\u4e3a<code>type[/name]</code>\uff0c\u5b9a\u4e49 \u89c1\u672c\u6587\u6863).</p> <p>\u5f53 Collector \u52a0\u8f7d\u6b64\u914d\u7f6e\u65f6\uff0c\u7ed3\u679c\u5c06\u5982\u4e0b\u6240\u793a(\u4e3a\u7b80\u6d01\u8d77\u89c1\uff0c\u56fe\u4e2d\u7701\u7565\u4e86\u90e8\u5206\u5904\u7406\u5668\u548c\u5bfc\u51fa \u5668):</p> <p></p> <p>\u91cd\u8981\u63d0\u793a</p> <p>\u5f53\u540c\u4e00\u4e2a\u63a5\u6536\u5668\u5728\u591a\u4e2a\u7ba1\u9053\u4e2d\u88ab\u5f15\u7528\u65f6\uff0c\u6536\u96c6\u5668\u5c06\u5728\u8fd0\u884c\u65f6\u4ec5\u521b\u5efa\u4e00\u4e2a\u63a5\u6536\u5668\u5b9e\u4f8b\uff0c\u8be5\u5b9e\u4f8b\u5c06\u6570\u636e\u53d1\u9001\u7ed9\u6247\u51fa\u6d88\u8d39\u8005\uff0c\u540e\u8005\u5c06\u6570\u636e\u53d1\u9001\u7ed9\u6bcf\u4e2a\u7ba1\u9053\u7684\u7b2c\u4e00\u4e2a\u5904\u7406\u5668\u3002 \u6570\u636e\u4ece\u63a5\u6536\u5668\u5230\u6247\u5f62\u6d88\u8d39\u8005\uff0c\u518d\u5230\u5904\u7406\u5668\u7684\u4f20\u64ad\u662f\u901a\u8fc7\u540c\u6b65\u51fd\u6570\u8c03\u7528\u5b8c\u6210\u7684\u3002 \u8fd9\u610f\u5473\u7740\uff0c\u5982\u679c\u4e00\u4e2a\u5904\u7406\u5668\u963b\u585e\u4e86\u8c03\u7528\uff0c\u5219\u9644\u52a0\u5230\u6b64\u63a5\u6536\u5668\u7684\u5176\u4ed6\u7ba1\u9053\u5c06\u88ab\u963b\u6b62\u63a5\u6536\u76f8\u540c\u7684\u6570\u636e\uff0c\u5e76\u4e14\u63a5\u6536\u5668\u672c\u8eab\u5c06\u505c\u6b62\u5904\u7406\u548c\u8f6c\u53d1\u65b0\u63a5\u6536\u5230\u7684\u6570\u636e\u3002</p>"},{"location":"docs/k8s/collector/design/#-exporters","title":"\u5bfc\u51fa\u5668 - Exporters","text":"<p>\u5bfc\u51fa\u5668\u901a\u5e38\u5c06\u6570\u636e\u8f6c\u53d1\u5230\u7f51\u7edc\u4e0a\u7684\u4e00\u4e2a\u76ee\u7684\u5730(\u4f46\u4ed6\u4eec\u4e5f\u53ef\u4ee5\u5c06\u5176\u53d1\u9001\u5230\u5176\u4ed6\u5730\u65b9\uff0c\u4f8b \u5982<code>logging</code>\u5bfc\u51fa\u5668\u5c06\u9065\u6d4b\u6570\u636e\u5199\u5165\u65e5\u5fd7\u76ee\u7684\u5730)\u3002</p> <p>\u8be5\u914d\u7f6e\u5141\u8bb8\u62e5\u6709\u76f8\u540c\u7c7b\u578b\u7684\u591a\u4e2a\u5bfc\u51fa\u5668\uff0c\u751a\u81f3\u5728\u540c\u4e00\u7ba1\u9053\u4e2d\u4e5f\u662f\u5982\u6b64\u3002\u4f8b\u5982\uff0c\u53ef\u4ee5\u5b9a\u4e49 2 \u4e2a<code>otlp</code>\u5bfc\u51fa\u5668\uff0c\u6bcf\u4e2a\u5bfc\u51fa\u5668\u53d1\u9001\u5230\u4e0d\u540c\u7684 OTLP \u7aef\u70b9\uff0c\u4f8b\u5982:</p> <pre><code>exporters:\notlp/1:\nendpoint: example.com:4317\notlp/2:\nendpoint: localhost:14317\n</code></pre> <p>\u901a\u5e38\u4e00\u4e2a\u5bfc\u51fa\u5668\u4ece\u4e00\u4e2a\u7ba1\u9053\u83b7\u53d6\u6570\u636e\uff0c\u4f46\u662f\u4e5f\u53ef\u4ee5\u914d\u7f6e\u591a\u4e2a\u7ba1\u9053\u5411\u540c\u4e00\u4e2a\u5bfc\u51fa\u5668\u53d1\u9001\u6570\u636e\uff0c \u4f8b\u5982:</p> <pre><code>exporters:\njaeger:\nprotocols:\ngrpc:\nendpoint: localhost:14250\nservice:\npipelines:\ntraces: # a pipeline of \u201ctraces\u201d type\nreceivers: [zipkin]\nprocessors: [memory_limiter]\nexporters: [jaeger]\ntraces/2: # another pipeline of \u201ctraces\u201d type\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [jaeger]\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c<code>jaeger</code>\u5bfc\u51fa\u5668\u5c06\u4ece\u7ba1\u9053<code>traces</code>\u548c\u7ba1\u9053<code>traces/2</code>\u4e2d\u83b7\u53d6\u6570\u636e\u3002\u5f53 Collector \u52a0\u8f7d\u6b64\u914d\u7f6e\u65f6\uff0c\u7ed3\u679c\u5c06\u5982\u4e0b\u6240\u793a(\u4e3a\u7b80\u6d01\u8d77\u89c1\uff0c\u56fe\u4e2d\u7701\u7565\u4e86\u90e8\u5206\u5904\u7406\u5668\u548c\u63a5\u6536\u5668 ):</p> <p></p>"},{"location":"docs/k8s/collector/design/#-processors","title":"\u5904\u7406\u5668 - Processors","text":"<p>\u7ba1\u9053\u53ef\u4ee5\u5305\u542b\u987a\u5e8f\u8fde\u63a5\u7684\u5904\u7406\u5668\u3002\u7b2c\u4e00\u4e2a\u5904\u7406\u5668\u4ece\u4e3a\u7ba1\u9053\u914d\u7f6e\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u63a5\u6536\u5668\u83b7\u53d6\u6570\u636e \uff0c\u6700\u540e\u4e00\u4e2a\u5904\u7406\u5668\u5c06\u6570\u636e\u53d1\u9001\u5230\u4e3a\u7ba1\u9053\u914d\u7f6e\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u5bfc\u51fa\u5668\u3002\u7b2c\u4e00\u5904\u7406\u5668\u548c\u6700\u540e\u5904\u7406\u5668 \u4e4b\u95f4\u7684\u6240\u6709\u5904\u7406\u5668\u4e25\u683c\u5730\u4ec5\u4ece\u524d\u9762\u7684\u4e00\u4e2a\u5904\u7406\u5668\u63a5\u6536\u6570\u636e\uff0c\u5e76\u4e25\u683c\u5730\u4ec5\u5411\u540e\u9762\u7684\u5904\u7406\u5668\u53d1\u9001 \u6570\u636e\u3002</p> <p>\u5904\u7406\u5668\u53ef\u4ee5\u5728\u8f6c\u53d1\u6570\u636e\u4e4b\u524d\u5bf9\u5176\u8fdb\u884c\u8f6c\u6362(\u5373\u5728\u8de8\u5ea6\u4e2d\u6dfb\u52a0\u6216\u5220\u9664\u5c5e\u6027)\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u51b3\u5b9a \u4e0d\u8f6c\u53d1\u6570\u636e\u6765\u5220\u9664\u6570\u636e(\u4f8b\u5982\uff0c<code>probabilisticsampler</code>\u5904\u7406\u5668\u5c31\u662f\u8fd9\u6837\u5de5\u4f5c\u7684)\uff0c\u5b83\u4eec\u4e5f\u53ef \u4ee5\u751f\u6210\u65b0\u6570\u636e\u3002\u8fd9\u5c31\u662f<code>spanmetrics</code>\u5904\u7406\u5668\u4e3a\u7ba1\u9053\u5904\u7406\u7684\u8de8\u5ea6\u751f\u6210\u6307\u6807\u7684\u65b9\u5f0f\u3002</p> <p>\u5904\u7406\u5668\u7684\u76f8\u540c\u540d\u79f0\u53ef\u4ee5\u5728\u591a\u4e2a\u7ba1\u9053\u7684<code>processors</code>\u952e\u4e2d\u5f15\u7528\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6bcf\u4e2a\u5904\u7406\u5668\u5c06 \u4f7f\u7528\u76f8\u540c\u7684\u914d\u7f6e\uff0c\u4f46\u662f\u6bcf\u4e2a\u7ba1\u9053\u5c06\u59cb\u7ec8\u83b7\u5f97\u81ea\u5df1\u7684\u5904\u7406\u5668\u5b9e\u4f8b\u3002\u6bcf\u4e2a\u5904\u7406\u5668\u90fd\u6709\u81ea\u5df1\u7684\u72b6\u6001 \uff0c\u8fd9\u4e9b\u5904\u7406\u5668\u6c38\u8fdc\u4e0d\u4f1a\u5728\u7ba1\u9053\u4e4b\u95f4\u5171\u4eab\u3002\u4f8b\u5982\uff0c\u5982\u679c\u5728\u591a\u4e2a\u7ba1\u9053\u4e2d\u4f7f\u7528\u201c\u6279\u5904\u7406\u201d\u5904\u7406\u5668\uff0c\u6bcf \u4e2a\u7ba1\u9053\u5c06\u6709\u81ea\u5df1\u7684\u6279\u5904\u7406\u5904\u7406\u5668(\u5c3d\u7ba1\u5982\u679c\u6bcf\u4e2a\u6279\u5904\u7406\u5904\u7406\u5668\u5728\u914d\u7f6e\u4e2d\u5f15\u7528\u76f8\u540c\u7684\u952e\uff0c\u5219\u5b83 \u4eec\u5c06\u4ee5\u5b8c\u5168\u76f8\u540c\u7684\u65b9\u5f0f\u914d\u7f6e)\u3002\u4ee5\u5982\u4e0b\u914d\u7f6e\u4e3a\u4f8b:</p> <pre><code>processors:\nbatch:\nsend_batch_size: 10000\ntimeout: 10s\nservice:\npipelines:\ntraces: # a pipeline of \u201ctraces\u201d type\nreceivers: [zipkin]\nprocessors: [batch]\nexporters: [jaeger]\ntraces/2: # another pipeline of \u201ctraces\u201d type\nreceivers: [otlp]\nprocessors: [batch]\nexporters: [otlp]\n</code></pre> <p>\u5f53 Collector \u52a0\u8f7d\u6b64\u914d\u7f6e\u65f6\uff0c\u7ed3\u679c\u5c06\u5982\u4e0b\u6240\u793a:</p> <p></p> <p>\u8bf7\u6ce8\u610f\uff0c\u6bcf\u4e2a<code>batch</code>\u5904\u7406\u5668\u90fd\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u5b9e\u4f8b\uff0c\u5c3d\u7ba1\u4e24\u8005\u90fd\u4ee5\u76f8\u540c\u7684\u65b9\u5f0f\u914d\u7f6e\uff0c\u5373\u6bcf\u4e2a \u90fd\u6709\u4e00\u4e2a 10000 \u7684<code>send_batch_size</code>\u3002</p> <p>\u540c\u4e00\u4e2a\u5904\u7406\u5668\u7684\u540d\u5b57\u4e0d\u80fd\u5728\u4e00\u4e2a\u7ba1\u9053\u7684<code>processors</code>\u952e\u4e2d\u88ab\u591a\u6b21\u5f15\u7528\u3002</p> <p></p>"},{"location":"docs/k8s/collector/design/#_4","title":"\u4f5c\u4e3a\u4ee3\u7406\u8fd0\u884c","text":"<p>\u5728\u5178\u578b\u7684 VM/\u5bb9\u5668\u4e0a\uff0c\u6709\u4e00\u4e9b\u7528\u6237\u5e94\u7528\u7a0b\u5e8f\u8fd0\u884c\u5728\u4e00\u4e9b\u5e26\u6709 OpenTelemetry Library (Library)\u7684\u8fdb\u7a0b/pod \u4e2d\u3002\u4ee5\u524d\uff0cLibrary \u5bf9\u8ddf\u8e2a/\u6307\u6807/\u65e5\u5fd7\u8fdb\u884c\u6240\u6709\u7684\u8bb0\u5f55\u3001\u6536\u96c6\u3001\u91c7\u6837 \u548c\u805a\u5408\uff0c\u5e76\u901a\u8fc7 Library \u5bfc\u51fa\u5668\u5c06\u5b83\u4eec\u5bfc\u51fa\u5230\u5176\u4ed6\u6301\u4e45\u5b58\u50a8\u540e\u7aef\uff0c\u6216\u8005\u5728\u672c\u5730\u9875\u9762\u4e0a\u663e\u793a \u5b83\u4eec\u3002\u8fd9\u79cd\u6a21\u5f0f\u6709\u51e0\u4e2a\u7f3a\u70b9\uff0c\u4f8b\u5982:</p> <ol> <li>\u5bf9\u4e8e\u6bcf\u4e2a OpenTelemetry Library\uff0c\u51fa\u53e3\u5546/zpages \u9700\u8981\u7528\u672c\u5730\u8bed\u8a00\u91cd\u65b0\u5b9e\u73b0\u3002</li> <li>\u5728\u67d0\u4e9b\u7f16\u7a0b\u8bed\u8a00(\u5982 Ruby\u3001PHP)\u4e2d\uff0c\u5f88\u96be\u5728\u8fdb\u7a0b\u4e2d\u8fdb\u884c\u7edf\u8ba1\u805a\u5408\u3002</li> <li>\u8981\u542f\u7528\u5bfc\u51fa OpenTelemetry span /stats/metrics\uff0c\u5e94\u7528\u7a0b\u5e8f\u7528\u6237\u9700\u8981\u624b\u52a8\u6dfb\u52a0\u5e93\u5bfc\u51fa    \u5668\u5e76\u91cd\u65b0\u90e8\u7f72\u5b83\u4eec\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002\u5f53\u5df2\u7ecf\u53d1\u751f\u4e86\u4e8b\u4ef6\uff0c\u5e76\u4e14\u7528\u6237\u60f3\u8981\u4f7f\u7528    OpenTelemetry \u7acb\u5373\u8c03\u67e5\u53d1\u751f\u4e86\u4ec0\u4e48\u4e8b\u60c5\u65f6\uff0c\u8fd9\u4e00\u70b9\u5c24\u5176\u56f0\u96be\u3002</li> <li>\u5e94\u7528\u7a0b\u5e8f\u7528\u6237\u9700\u8981\u8d1f\u8d23\u914d\u7f6e\u548c\u521d\u59cb\u5316\u5bfc\u51fa\u7a0b\u5e8f\u3002\u8fd9\u662f\u5bb9\u6613\u51fa\u9519\u7684(\u4f8b\u5982\uff0c\u4ed6\u4eec\u53ef\u80fd\u6ca1\u6709\u8bbe    \u7f6e\u6b63\u786e\u7684\u51ed\u636e/\u88ab\u76d1\u89c6\u7684\u8d44\u6e90)\uff0c\u5e76\u4e14\u7528\u6237\u53ef\u80fd\u4e0d\u613f\u610f\u7528 OpenTelemetry\u201c\u6c61\u67d3\u201d\u4ed6\u4eec\u7684\u4ee3    \u7801\u3002</li> </ol> <p>\u8981\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u60a8\u53ef\u4ee5\u5c06 OpenTelemetry Collector \u4f5c\u4e3a\u4ee3\u7406\u8fd0\u884c\u3002 Agent \u4f5c\u4e3a\u865a\u62df\u673a /\u5bb9\u5668\u4e2d\u7684\u5b88\u62a4\u8fdb\u7a0b\u8fd0\u884c\uff0c\u53ef\u4ee5\u72ec\u7acb\u4e8e Library \u8fdb\u884c\u90e8\u7f72\u3002\u4e00\u65e6 Agent \u90e8\u7f72\u5e76\u8fd0\u884c\uff0c\u5b83\u5e94 \u8be5\u80fd\u591f\u4ece Library \u68c0\u7d22\u8ddf\u8e2a/\u6307\u6807/\u65e5\u5fd7\uff0c\u5e76\u5c06\u5b83\u4eec\u5bfc\u51fa\u5230\u5176\u4ed6\u540e\u7aef\u3002\u6211\u4eec\u4e5f\u53ef\u4ee5\u7ed9 Agent \u63a8\u9001\u914d\u7f6e(\u4f8b\u5982\u91c7\u6837\u6982\u7387)\u5230\u5e93\u7684\u80fd\u529b\u3002\u5bf9\u4e8e\u90a3\u4e9b\u4e0d\u80fd\u5728\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u7edf\u8ba1\u805a\u5408\u7684\u8bed\u8a00\uff0c\u5b83\u4eec\u4e5f \u5e94\u8be5\u80fd\u591f\u53d1\u9001\u539f\u59cb\u5ea6\u91cf\u5e76\u8ba9 Agent \u8fdb\u884c\u805a\u5408\u3002</p> <p></p> <p>\u5bf9\u4e8e\u5176\u4ed6\u5e93\u7684\u5f00\u53d1\u4eba\u5458/\u7ef4\u62a4\u8005: Agent \u8fd8\u53ef\u4ee5\u63a5\u53d7\u6765\u81ea\u5176\u4ed6\u8ddf\u8e2a/\u76d1\u63a7\u5e93\u7684\u8ddf\u8e2a/\u6307\u6807/\u65e5\u5fd7 \uff0c\u5982 Zipkin,Prometheus \u7b49\u3002\u8fd9\u662f\u901a\u8fc7\u6dfb\u52a0\u7279\u5b9a\u7684\u63a5\u6536\u5668\u6765\u5b9e\u73b0\u7684\u3002\u8be6\u60c5\u8bf7\u53c2 \u89c1receiver\u3002</p> <p></p>"},{"location":"docs/k8s/collector/design/#_5","title":"\u4f5c\u4e3a\u7f51\u5173\u8fd0\u884c","text":"<p>OpenTelemetry Collector \u53ef\u4ee5\u4f5c\u4e3a Gateway \u5b9e\u4f8b\u8fd0\u884c\uff0c\u5e76\u63a5\u6536\u7531\u4e00\u4e2a\u6216\u591a\u4e2a\u4ee3\u7406\u6216\u5e93\u5bfc \u51fa\u7684\u8de8\u5ea6\u548c\u6307\u6807\uff0c\u6216\u8005\u7531\u5728\u53d7\u652f\u6301\u7684\u534f\u8bae\u4e4b\u4e00\u4e2d\u53d1\u51fa\u7684\u4efb\u52a1/\u4ee3\u7406\u5bfc\u51fa\u7684\u8de8\u5ea6\u548c\u6307\u6807\u3002\u6536\u96c6 \u5668\u88ab\u914d\u7f6e\u4e3a\u5c06\u6570\u636e\u53d1\u9001\u5230\u5df2\u914d\u7f6e\u7684\u5bfc\u51fa\u5668\u3002\u4e0b\u56fe\u603b\u7ed3\u4e86\u90e8\u7f72\u67b6\u6784:</p> <p></p> <p>OpenTelemetry Collector \u8fd8\u53ef\u4ee5\u90e8\u7f72\u5728\u5176\u4ed6\u914d\u7f6e\u4e2d\uff0c\u4f8b\u5982\u4ee5\u5176\u63a5\u6536\u5668\u652f\u6301\u7684\u683c\u5f0f\u4e4b\u4e00\u4ece\u5176 \u4ed6\u4ee3\u7406\u6216\u5ba2\u6237\u7aef\u63a5\u6536\u6570\u636e\u3002</p>"},{"location":"docs/k8s/collector/ga-roadmap/","title":"\u91c7\u96c6\u5668 GA \u8def\u7ebf\u56fe","text":"<p>\u672c\u6587\u6863\u5b9a\u4e49\u4e86 OpenTelemetry Collector \u9075\u5faa\u7684\u8def\u7ebf\u56fe\uff0c\u4ee5\u53ca GA(\u7a33\u5b9a\u6027)\u7684\u6682\u5b9a\u65e5\u671f\u548c\u9700 \u6c42\u3002</p> <p>In this document, the term \u201cOpenTelemetry Collector packages\" refers to all the golang modules and packages that are part of the \u201cOpenTelemetry Collector\u201d ecosystem which include the core and contrib.</p> <p>In this document, the terms \"OpenTelemetry Collector\" and \"Collector\" both specifically refer to the entire OpenTelemetry Collector ecosystem\u2019s including core and contrib. These terms do not refer to the specification or the Client libraries in this document.</p>"},{"location":"docs/k8s/collector/ga-roadmap/#current-status","title":"Current Status","text":"<p>The OpenTelemetry Collector ecosystem right now has a lot of packages that are in different stages of stability (experimental, alpha, beta, etc.). All these packages have different public APIs/Interfaces (e.g. code API, configuration, etc.).</p> <p>A significant amount of legacy code was inherited from the Collector's ancestor OpenCensus Service, since then the Collector changed the internal data model and other significant changes were made.</p> <p>Trying to mark the entire ecosystem GA, at the same moment, will be a significant effort and will take a significant amount of time.</p>"},{"location":"docs/k8s/collector/ga-roadmap/#proposal","title":"Proposal","text":"<p>This document proposes a GA Roadmap based on multiple phases, where different parts of the collector will be released as stable at different moments of time.</p> <p>At this moment we are completely defining only the first two phases of the process, and the next phases will be defined at a later stage once the Collector maintainers will have better understanding of the process and implications.</p> <p>The primary focus is on the tracing parts. When other signal's data models (proto definition) will be marked as stable, the amount of work necessary to stabilize their APIs will be minimal: <code>pdata</code> is auto-generated so all changes that we do for trace will apply to all of them, <code>consumer</code> is minimal interface, <code>component</code> as well.</p> <p>Metrics components such as (<code>receiver/prometheus</code>, <code>exporter/prometheusremotewrite</code>) are explicitly left out of this roadmap document because metrics data model is not complete. When that work finishes, we can add them to the Phase 3, or later.</p>"},{"location":"docs/k8s/collector/ga-roadmap/#phase-1","title":"Phase 1","text":"<p>Key Results: At the end of this phase the Collector\u2019s core API will be marked as Stable.</p> <p>At the end of this phase we want to achieve core APIs stability. This will allow developers to implement custom components and extend the collector will be marked as stable. The complete list of the packages/modules will be finalized during the first action item of this phase, but the tentative list is:</p> <ul> <li><code>consumer</code></li> <li>Official internal data model <code>pdata</code>.</li> <li>Interfaces and utils to build a Consumer for (trace, metrics, logs).</li> <li><code>config</code></li> <li>Core <code>config</code> including service definition, component definition will be     stabilized.</li> <li>To be determined which config helpers will be marked as stable (e.g.     configgrpc, etc.).</li> <li><code>component</code></li> <li>Interfaces and utils to build a Collector component (receiver, processor,     exporter, extension).</li> <li><code>obsreport</code></li> <li>Focus on the public API of this package. It is out of scope to ensure     stability for the metrics emitted (focus in phase 2).</li> <li><code>service</code></li> <li>Public API to construct a OpenTelemetry Collector Service.</li> </ul> <p>Action Items:</p> <ul> <li>Create a new milestone for this phase, create issues for all the other action   items and add them to the milestone.</li> <li>Agreement on all packages/modules that will be marked as stable during this   phase.</li> <li>Write a version doc as per   version and stability document.</li> <li>Previously it was discussed that for the Collector it is fine to release     stable golang modules that contain APIs marked as experimental.</li> <li>Define status schema (experimental/stable), what are they applicable to     every module.</li> <li>Investigate if splitting into smaller, more granular, modules is possible.</li> <li>Define the modules schema, try to not break everyone. See     here.</li> <li>Investigate how can we release multiple golang modules from the same repo,     without asking people that consume them to use a replace statement. See     problem in contrib.</li> <li>Investigate how to release test-only utils.</li> <li>Review all public APIs and godocs for modules that we want to release in this   phase.</li> <li>Fix all critical issues, and remove unnecessary (\u201cWhen in doubt leave it     out\u201d) public APIs.</li> <li>Remove all already deprecated code from the stable modules.</li> <li>Transition to opentelemetry-go trace library from opencensus?</li> <li>Investigate if any config helper needs to be released as stable, if any do the   review of the public API, godoc and configuration.</li> <li>Investigate tools that check for API compatibility for go modules, enable them   for modules that we mark as stable.</li> </ul>"},{"location":"docs/k8s/collector/ga-roadmap/#phase-2","title":"Phase 2","text":"<p>Key Results: At the end of this phase the Collector\u2019s end-to-end support for OTLP traces only be marked as GA.</p> <p>At the end of this phase we want to ensure that the Collector can be run in production, it can receive OTLP trace traffic and emit OTLP trace traffic. The complete list of the packages/modules will be finalized during the first part of this phase, but the tentative list is:</p> <ul> <li><code>receiver</code></li> <li><code>receiverhelper</code> - without scraper utils in this phase.</li> <li><code>otlp</code></li> <li><code>processor</code></li> <li><code>processorhelper</code></li> <li><code>batch</code></li> <li><code>memory_limiter</code></li> <li><code>exporter</code></li> <li><code>exporterhelper</code></li> <li><code>otlp</code></li> <li><code>otlphttp</code></li> <li><code>extension</code></li> <li><code>extensionhelper</code></li> <li><code>healthcheck</code></li> <li><code>obsreport</code></li> <li>Stabilize the observability metrics (user public metrics).</li> </ul> <p>Action Items:</p> <ul> <li>Create a new milestone for this phase, create issues for all the other action   items and add them to the milestone.</li> <li>Agreement on all packages/modules that will be marked as stable during this   phase.</li> <li>Review all public APIs and godocs for modules that we want to release in this   phase.</li> <li>Fix all critical issues, and remove unnecessary (\u201cWhen in doubt leave it     out\u201d) public APIs.</li> <li>Remove all already deprecated code from the stable modules.</li> <li>Review all public configuration for all the modules, fix issues.</li> <li>Setup a proper loadtest environment and continuously publish results.</li> <li>Ensure correctness tests produce the expected results, improve until confident   that a binary that passes them is good to be shipped.</li> <li>Enable security checks on every PR (currently some are ignored like <code>codeql</code>).</li> </ul>"},{"location":"docs/k8s/collector/ga-roadmap/#phase-3","title":"Phase 3","text":"<p>Key Results: At the end of this phase all Collector\u2019s core components (receivers, processors, exporters, extensions) for traces only will be marked as GA.</p> <p>At the end of this phase we want to ensure that the Collector can be run in production, it can receive the trace traffic and emit OTLP trace traffic. The complete list of the packages/modules will be finalized during the first part of this phase, but the tentative list is:</p> <ul> <li><code>receiver</code></li> <li><code>jaeger</code></li> <li><code>opencensus</code></li> <li><code>zipkin</code></li> <li><code>processor</code></li> <li><code>spantransformer</code> - there are good reasons to merge <code>attributes</code> and <code>span</code>.</li> <li><code>resource</code></li> <li><code>filter</code> - we will consider offering a filter processor for all telemetry     signals not just for metrics</li> <li><code>exporter</code></li> <li><code>jaeger</code></li> <li><code>opencensus</code></li> <li><code>zipkin</code></li> <li><code>extension</code></li> <li><code>pprof</code></li> <li><code>zpages</code></li> </ul> <p>TODO: Add action items list.</p>"},{"location":"docs/k8s/collector/ga-roadmap/#phase-n","title":"Phase N","text":"<p>TODO: Add more phases if/when necessary.</p>"},{"location":"docs/k8s/collector/ga-roadmap/#alternatives","title":"Alternatives","text":"<p>One alternative proposal is to try to GA all packages at the same time. This proposal was rejected because of the complexity and size of the ecosystem that may force the GA process to take too much time.</p>"},{"location":"docs/k8s/collector/monitoring/","title":"\u76d1\u63a7","text":"<p>Collector \u4e3a\u5176\u76d1\u89c6\u63d0\u4f9b\u4e86\u8bb8\u591a\u6307\u6807\u3002\u4e0b\u9762\u5217\u51fa\u4e86\u4e00\u4e9b\u7528\u4e8e\u8b66\u62a5\u548c\u76d1\u89c6\u7684\u5173\u952e\u5efa\u8bae\u3002</p>"},{"location":"docs/k8s/collector/monitoring/#_2","title":"\u5173\u952e\u7684\u76d1\u63a7","text":""},{"location":"docs/k8s/collector/monitoring/#_3","title":"\u6570\u636e\u4e22\u5931","text":"<p>Use rate of <code>otelcol_processor_dropped_spans &gt; 0</code> and <code>otelcol_processor_dropped_metric_points &gt; 0</code> to detect data loss, depending on the requirements set up a minimal time window before alerting, avoiding notifications for small losses that are not considered outages or within the desired reliability level.</p>"},{"location":"docs/k8s/collector/monitoring/#cpu","title":"CPU \u8d44\u6e90\u4e0d\u8db3","text":"<p>This depends on the CPU metrics available on the deployment, eg.: <code>kube_pod_container_resource_limits_cpu_cores</code> for Kubernetes. Let's call it <code>available_cores</code> below. The idea here is to have an upper bound of the number of available cores, and the maximum expected ingestion rate considered safe, let's call it <code>safe_rate</code>, per core. This should trigger increase of resources/ instances (or raise an alert as appropriate) whenever <code>(actual_rate/available_cores) &lt; safe_rate</code>.</p> <p>The <code>safe_rate</code> depends on the specific configuration being used. // TODO: Provide reference <code>safe_rate</code> for a few selected configurations.</p>"},{"location":"docs/k8s/collector/monitoring/#_4","title":"\u4e8c\u7ea7\u76d1\u63a7","text":""},{"location":"docs/k8s/collector/monitoring/#_5","title":"\u961f\u5217\u957f\u5ea6","text":"<p>Most exporters offer a queue/retry mechanism that is recommended as the retry mechanism for the Collector and as such should be used in any production deployment.</p> <p>The <code>otelcol_exporter_queue_capacity</code> indicates the capacity of the retry queue (in batches). The <code>otelcol_exporter_queue_size</code> indicates the current size of retry queue. So you can use these two metrics to check if the queue capacity is enough for your workload.</p> <p>The <code>otelcol_exporter_enqueue_failed_spans</code>, <code>otelcol_exporter_enqueue_failed_metric_points</code> and <code>otelcol_exporter_enqueue_failed_log_records</code> indicate the number of span/metric points/log records failed to be added to the sending queue. This may be cause by a queue full of unsettled elements, so you may need to decrease your sending rate or horizontally scale collectors.</p> <p>The queue/retry mechanism also supports logging for monitoring. Check the logs for messages like <code>\"Dropping data because sending_queue is full\"</code>.</p>"},{"location":"docs/k8s/collector/monitoring/#_6","title":"\u63a5\u6536\u5931\u8d25","text":"<p>Sustained rates of <code>otelcol_receiver_refused_spans</code> and <code>otelcol_receiver_refused_metric_points</code> indicate too many errors returned to clients. Depending on the deployment and the client\u2019s resilience this may indicate data loss at the clients.</p> <p>Sustained rates of <code>otelcol_exporter_send_failed_spans</code> and <code>otelcol_exporter_send_failed_metric_points</code> indicate that the Collector is not able to export data as expected. It doesn't imply data loss per se since there could be retries but a high rate of failures could indicate issues with the network or backend receiving the data.</p>"},{"location":"docs/k8s/collector/monitoring/#_7","title":"\u6570\u636e\u6d41","text":""},{"location":"docs/k8s/collector/monitoring/#_8","title":"\u6570\u636e\u5bfc\u5165","text":"<p>The <code>otelcol_receiver_accepted_spans</code> and <code>otelcol_receiver_accepted_metric_points</code> metrics provide information about the data ingested by the Collector.</p>"},{"location":"docs/k8s/collector/monitoring/#_9","title":"\u51fa\u53e3\u6570\u636e","text":"<p>The <code>otecol_exporter_sent_spans</code> and <code>otelcol_exporter_sent_metric_points</code>metrics provide information about the data exported by the Collector.</p>"},{"location":"docs/k8s/collector/observability/","title":"\u6536\u96c6\u5668\u53ef\u89c2\u6d4b\u6027","text":""},{"location":"docs/k8s/collector/observability/#_2","title":"\u76ee\u6807","text":"<p>The goal of this document is to have a comprehensive description of observability of the Collector and changes needed to achieve observability part of our vision.</p>"},{"location":"docs/k8s/collector/observability/#_3","title":"\u4ec0\u4e48\u9700\u8981\u89c2\u5bdf","text":"<p>The following elements of the Collector need to be observable.</p>"},{"location":"docs/k8s/collector/observability/#_4","title":"\u5f53\u524d\u503c","text":"<ul> <li> <p>Resource consumption: CPU, RAM (in the future also IO - if we implement   persistent queues) and any other metrics that may be available to Go apps   (e.g. garbage size, etc).</p> </li> <li> <p>Receiving data rate, broken down by receivers and by data type   (traces/metrics).</p> </li> <li> <p>Exporting data rate, broken down by exporters and by data type   (traces/metrics).</p> </li> <li> <p>Data drop rate due to throttling, broken down by data type.</p> </li> <li> <p>Data drop rate due to invalid data received, broken down by data type.</p> </li> <li> <p>Current throttling state: Not Throttled/Throttled by Downstream/Internally   Saturated.</p> </li> <li> <p>Incoming connection count, broken down by receiver.</p> </li> <li> <p>Incoming connection rate (new connections per second), broken down by   receiver.</p> </li> <li> <p>In-memory queue size (in bytes and in units). Note: measurements in bytes may   be difficult / expensive to obtain and should be used cautiously.</p> </li> <li> <p>Persistent queue size (when supported).</p> </li> <li> <p>End-to-end latency (from receiver input to exporter output). Note that with   multiple receivers/exporters we potentially have NxM data paths, each with   different latency (plus different pipelines in the future), so realistically   we should likely expose the average of all data paths (perhaps broken down by   pipeline).</p> </li> <li> <p>Latency broken down by pipeline elements (including exporter network roundtrip   latency for request/response protocols).</p> </li> </ul> <p>\u201cRate\u201d values must reflect the average rate of the last 10 seconds. Rates must exposed in bytes/sec and units/sec (e.g. spans/sec).</p> <p>Note: some of the current values and rates may be calculated as derivatives of cumulative values in the backend, so it is an open question if we want to expose them separately or no.</p>"},{"location":"docs/k8s/collector/observability/#_5","title":"\u7d2f\u8ba1\u503c","text":"<ul> <li> <p>Total received data, broken down by receivers and by data type   (traces/metrics).</p> </li> <li> <p>Total exported data, broken down by exporters and by data type   (traces/metrics).</p> </li> <li> <p>Total dropped data due to throttling, broken down by data type.</p> </li> <li> <p>Total dropped data due to invalid data received, broken down by data type.</p> </li> <li> <p>Total incoming connection count, broken down by receiver.</p> </li> <li> <p>Uptime since start.</p> </li> </ul>"},{"location":"docs/k8s/collector/observability/#trace-or-log-on-events","title":"Trace or Log on Events","text":"<p>We want to generate the following events (log and/or send as a trace with additional data):</p> <ul> <li> <p>Collector started/stopped.</p> </li> <li> <p>Collector reconfigured (if we support on-the-fly reconfiguration).</p> </li> <li> <p>Begin dropping due to throttling (include throttling reason, e.g. local   saturation, downstream saturation, downstream unavailable, etc).</p> </li> <li> <p>Stop dropping due to throttling.</p> </li> <li> <p>Begin dropping due to invalid data (include sample/first invalid data).</p> </li> <li> <p>Stop dropping due to invalid data.</p> </li> <li> <p>Crash detected (differentiate clean stopping and crash, possibly include crash   data if available).</p> </li> </ul> <p>For begin/stop events we need to define an appropriate hysteresis to avoid generating too many events. Note that begin/stop events cannot be detected in the backend simply as derivatives of current rates, the events include additional data that is not present in the current value.</p>"},{"location":"docs/k8s/collector/observability/#host-metrics","title":"Host Metrics","text":"<p>The service should collect host resource metrics in addition to service's own process metrics. This may help to understand that the problem that we observe in the service is induced by a different process on the same host.</p>"},{"location":"docs/k8s/collector/observability/#how-we-expose-metricstraces","title":"How We Expose Metrics/Traces","text":"<p>Collector configuration must allow specifying the target for own metrics/traces (which can be different from the target of collected data). The metrics and traces must be clearly tagged to indicate that they are service\u2019s own metrics (to avoid conflating with collected data in the backend).</p>"},{"location":"docs/k8s/collector/observability/#impact","title":"Impact","text":"<p>We need to be able to assess the impact of these observability improvements on the core performance of the Collector.</p>"},{"location":"docs/k8s/collector/observability/#configurable-level-of-observability","title":"Configurable Level of Observability","text":"<p>Some of the metrics/traces can be high volume and may not be desirable to always observe. We should consider adding an observability verboseness \u201clevel\u201d that allows configuring the Collector to send more or less observability data (or even finer granularity to allow turning on/off specific metrics).</p> <p>The default level of observability must be defined in a way that has insignificant performance impact on the service.</p>"},{"location":"docs/k8s/collector/performance/","title":"\u6536\u96c6\u5668\u6027\u80fd","text":"<p>\u4e0b\u9762\u7684\u6027\u80fd\u6570\u5b57\u662f\u4f7f\u7528 OpenTelemetry Collector \u7684 0.1.3 \u7248\u672c\u751f\u6210\u7684\uff0c\u4e3b\u8981\u9002\u7528\u4e8e OpenTelemetry Collector\uff0c\u5e76\u4e14\u4ec5\u5bf9\u8ddf\u8e2a\u8fdb\u884c\u6d4b\u91cf\u3002\u5728\u672a\u6765\uff0c\u5c06\u6d4b\u8bd5\u66f4\u591a\u7684\u914d\u7f6e\u3002</p> <p>Note with the OpenTelemetry Agent you can expect as good if not better performance with lower resource utilization. This is because the OpenTelemetry Agent does not today support features such as batching or retries and will not support tail_sampling.</p> <p>It is important to note that the performance of the OpenTelemetry Collector depends on a variety of factors including:</p> <ul> <li>The receiving format: OpenTelemetry (55678), Jaeger thrift (14268) or Zipkin   v2 JSON (9411)</li> <li>The size of the spans (tests are based on number of attributes): 20</li> <li>Whether tail_sampling is enabled or not</li> <li>CPU / Memory allocation</li> <li>Operating System: Linux</li> </ul>"},{"location":"docs/k8s/collector/performance/#testing","title":"Testing","text":"<p>Testing was completed on Linux using the Synthetic Load Generator utility running for a minimum of one hour (i.e. sustained rate). You can reproduce these results in your own environment using the parameters described in this document. It is important to note that this utility has a few configurable parameters which can impact the results of the tests. The parameters used are defined below.</p> <ul> <li>FlushInterval(ms) [default: 1000]</li> <li>MaxQueueSize [default: 100]</li> <li>SubmissionRate(spans/sec): 100,000</li> </ul>"},{"location":"docs/k8s/collector/performance/#results-without-tail-based-sampling","title":"Results without tail-based sampling","text":"SpanFormat CPU(2+ GHz) RAM(GB) SustainedRate RecommendedMaximum OpenTelemetry 1 2 ~12K 10K OpenTelemetry 2 4 ~24K 20K Jaeger Thrift 1 2 ~14K 12K Jaeger Thrift 2 4 ~27.5K 24K Zipkin v2 JSON 1 2 ~10.5K 9K Zipkin v2 JSON 2 4 ~22K 18K <p>If you are NOT using tail-based sampling and you need higher rates then you can either:</p> <ul> <li>Divide traffic to different collector (e.g. by region)</li> <li>Scale-up by adding more resources (CPU/RAM)</li> <li>Scale-out by putting one or more collectors behind a load balancer or k8s   service</li> </ul>"},{"location":"docs/k8s/collector/performance/#results-with-tail-based-sampling","title":"Results with tail-based sampling","text":"<p>Note: Additional memory is required for tail-based sampling</p> SpanFormat CPU(2+ GHz) RAM(GB) SustainedRate RecommendedMaximum OpenTelemetry 1 2 ~9K 8K OpenTelemetry 2 4 ~18K 16K Jaeger Thrift 1 6 ~11.5K 10K Jaeger Thrift 2 8 ~23K 20K Zipkin v2 JSON 1 6 ~8.5K 7K Zipkin v2 JSON 2 8 ~16K 14K <p>If you are using tail-based sampling and you need higher rates then you can either:</p> <ul> <li>Scale-up by adding more resources (CPU/RAM)</li> <li>Scale-out by putting one or more collectors behind a load balancer or k8s   service, but the load balancer must support traceID-based routing (i.e. all   spans for a given traceID need to be received by the same collector instance)</li> </ul>"},{"location":"docs/k8s/collector/processing/","title":"\u6536\u96c6\u5668\u5904\u7406\u5668\u63a2\u7d22","text":"<p>Status: Draft</p>"},{"location":"docs/k8s/collector/processing/#_2","title":"\u5ba2\u89c2\u7684","text":"<p>\u63cf\u8ff0\u5728 OpenTelemetry \u6536\u96c6\u5668\u4e2d\u914d\u7f6e\u5904\u7406\u5668\u7684\u7528\u6237\u4f53\u9a8c\u548c\u7b56\u7565\u3002</p> <p>\u8fd9\u9879\u5de5\u4f5c\u7684\u539f\u578b\u662f\u5728\u5f00\u653e\u7535\u5b50-\u6536\u96c6\u5668-\u8d21\u732e\uff0c\u8bbe\u8ba1\u6587\u4ef6\u5728\u8fd9\u91cc\u8fdb\u884c\u66f4\u5e7f\u6cdb\u7684\u8ba8\u8bba\u3002</p>"},{"location":"docs/k8s/collector/processing/#summary","title":"Summary","text":"<p>The OpenTelemetry (OTel) collector is a tool to set up pipelines to receive telemetry from an application and export it to an observability backend. Part of the pipeline can include processing stages, which executes various business logic on incoming telemetry before it is exported.</p> <p>Over time, the collector has added various processors to satisfy different use cases, generally in an ad-hoc way to support each feature independently. We can improve the experience for users of the collector by consolidating processing patterns in terms of user experience, and this can be supported by defining a querying model for processors within the collector core, and likely also for use in SDKs, to simplify implementation and promote the consistent user experience and best practices.</p>"},{"location":"docs/k8s/collector/processing/#goals-and-non-goals","title":"Goals and non-goals","text":"<p>Goals:</p> <ul> <li>List out use cases for processing within the collector</li> <li>Consider what could be an ideal configuration experience for users</li> </ul> <p>Non-Goals:</p> <ul> <li>Merge every processor into one. Many use cases overlap and generalize, but not   all of them</li> <li>Technical design or implementation of configuration experience. Currently   focused on user experience.</li> </ul>"},{"location":"docs/k8s/collector/processing/#use-cases-for-processing","title":"Use cases for processing","text":""},{"location":"docs/k8s/collector/processing/#telemetry-mutation","title":"Telemetry mutation","text":"<p>Processors can be used to mutate the telemetry in the collector pipeline. OpenTelemetry SDKs collect detailed telemetry from applications, and it is common to have to mutate this into a way that is appropriate for an individual use case.</p> <p>Some types of mutation include</p> <ul> <li>Remove a forbidden attribute such as <code>http.request.header.authorization</code></li> <li>Reduce cardinality of an attribute such as translating <code>http.target</code> value of   <code>/user/123451/profile</code> to <code>/user/{userId}/profile</code></li> <li>Decrease the size of the telemetry payload by removing large resource   attributes such as <code>process.command_line</code></li> <li>Filtering out signals such as by removing all telemetry with a <code>http.target</code>   of <code>/health</code></li> <li>Attach information from resource into telemetry, for example adding certain   resource fields as metric dimensions</li> </ul> <p>The processors implementing this use case are <code>attributesprocessor</code>, <code>filterprocessor</code>, <code>metricstransformprocessor</code>, <code>resourceprocessor</code>, <code>spanprocessor</code>.</p>"},{"location":"docs/k8s/collector/processing/#metric-generation","title":"Metric generation","text":"<p>The collector may generate new metrics based on incoming telemetry. This can be for covering gaps in SDK coverage of metrics vs spans, or to create new metrics based on existing ones to model the data better for backend-specific expectations.</p> <ul> <li>Create new metrics based on information in spans, for example to create a   duration metric that is not implemented in the SDK yet</li> <li>Apply arithmetic between multiple incoming metrics to produce an output one,   for example divide an <code>amount</code> and a <code>capacity</code> to create a <code>utilization</code>   metric</li> </ul> <p>The processors implementing this use case are <code>metricsgenerationprocessor</code>, <code>spanmetricsprocessor</code>.</p>"},{"location":"docs/k8s/collector/processing/#grouping","title":"Grouping","text":"<p>Some processors are stateful, grouping telemetry over a window of time based on either a trace ID or an attribute value, or just general batching.</p> <ul> <li>Batch incoming telemetry before sending to exporters to reduce export requests</li> <li>Group spans by trace ID to allow doing tail sampling</li> <li>Group telemetry for the same path</li> </ul> <p>The processors implementing this use case are <code>batchprocessor</code>, <code>groupbyattrprocessor</code>, <code>groupbytraceprocessor</code>.</p>"},{"location":"docs/k8s/collector/processing/#metric-temporality","title":"Metric temporality","text":"<p>Two processors convert between the two types of temporality, cumulative and delta. The conversion is generally expected to happen as close to the source data as possible, for example within receivers themselves. The same configuration mechanism could be used for selecting metrics for temporality conversion as other cases, but it is expected that in practice configuration will be limited.</p> <p>The processors implementing this use case are <code>cumulativetodeltaprocessor</code>.</p>"},{"location":"docs/k8s/collector/processing/#telemetry-enrichment","title":"Telemetry enrichment","text":"<p>OpenTelemetry SDKs focus on collecting application specific data. They also may include resource detectors to populate environment specific data but the collector is commonly used to fill gaps in coverage of environment specific data.</p> <ul> <li>Add environment about a cloud provider to <code>Resource</code> of all incoming telemetry</li> </ul> <p>The processors implementing this use case are <code>k8sattributesprocessor</code>, <code>resourcedetectionprocessor</code>.</p>"},{"location":"docs/k8s/collector/processing/#opentelemetry-transformation-language","title":"OpenTelemetry Transformation Language","text":"<p>When looking at the use cases, there are certain common features for telemetry mutation and metric generation.</p> <ul> <li>Identify the type of signal (<code>span</code>, <code>metric</code>, <code>log</code>).</li> <li>Navigate to a path within the telemetry to operate on it</li> <li>Define an operation, and possibly operation arguments</li> </ul> <p>We can try to model these into a transformation language, in particular allowing the first two points to be shared among all processing operations, and only have implementation of individual types of processing need to implement operators that the user can use within an expression.</p> <p>Telemetry is modeled in the collector as <code>pdata</code> which is roughly a 1:1 mapping of the OTLP protocol. This data can be navigated using field expressions, which are fields within the protocol separated by dots. For example, the status message of a span is <code>status.message</code>. A map lookup can include the key as a string, for example <code>attributes[\"http.status_code\"]</code>.</p> <p>Operations are scoped to the type of a signal (<code>span</code>, <code>metric</code>, <code>log</code>), with all of the flattened points of that signal being part of a transformation space. Virtual fields are added to access data from a higher level before flattening, for <code>resource</code>, <code>library_info</code>. For metrics, the structure presented for processing is actual data points, e.g. <code>NumberDataPoint</code>, <code>HistogramDataPoint</code>, with the information from higher levels like <code>Metric</code> or the data type available as virtual fields.</p> <p>Virtual fields for all signals: <code>resource</code>, <code>library_info</code>. Virtual fields for metrics: <code>metric</code>, which contains <code>name</code>, <code>description</code>, <code>unit</code>, <code>type</code>, <code>aggregation_temporality</code>, and <code>is_monotonic</code>.</p> <p>Navigation can then be used with a simple expression language for identifying telemetry to operate on.</p> <pre><code>... where name = \"GET /cats\"\n</code></pre> <pre><code>... from span where attributes[\"http.target\"] = \"/health\"\n</code></pre> <pre><code>... where resource.attributes[\"deployment\"] = \"canary\"\n</code></pre> <pre><code>... from metric where metric.type = gauge\n</code></pre> <pre><code>... from metric where metric.name = \"http.active_requests\"\n</code></pre> <p>Fields should always be fully specified - for example <code>attributes</code> refers to the <code>attributes</code> field in the telemetry, not the <code>resource</code>. In the future, we may allow shorthand for accessing scoped information that is not ambiguous.</p> <p>Having selected telemetry to operate on, any needed operations can be defined as functions. Known useful functions should be implemented within the collector itself, provide registration from extension modules to allow customization with contrib components, and in the future can even allow user plugins possibly through WASM, similar to work in HTTP proxies. The arguments to operations will primarily be field expressions, allowing the operation to mutate telemetry as needed.</p> <p>There are times when the transformation language input and the underlying telemetry model do not translate cleanly. For example, a span ID is represented in pdata as a SpanID struct, but in the transformation language it is more natural to represent the span ID as a string or a byte array. The solution to this problem is Factories. Factories are functions that help translate between the transformation language input into the underlying pdata structure. These types of functions do not change the telemetry in any way. Instead, they manipulate the transformation language input into a form that will make working with the telemetry easier or more efficient.</p>"},{"location":"docs/k8s/collector/processing/#examples","title":"Examples","text":"<p>These examples contain a SQL-like declarative language. Applied statements interact with only one signal, but statements can be declared across multiple signals.</p> <p>Remove a forbidden attribute such as <code>http.request.header.authorization</code> from spans only</p> <pre><code>traces:\n  delete(attributes[\"http.request.header.authorization\"])\nmetrics:\n  delete(attributes[\"http.request.header.authorization\"])\nlogs:\n  delete(attributes[\"http.request.header.authorization\"])\n</code></pre> <p>Remove all attributes except for some</p> <pre><code>traces:\n  keep_keys(attributes, \"http.method\", \"http.status_code\")\nmetrics:\n  keep_keys(attributes, \"http.method\", \"http.status_code\")\nlogs:\n  keep_keys(attributes, \"http.method\", \"http.status_code\")\n</code></pre> <p>Reduce cardinality of an attribute</p> <pre><code>traces:\n  replace_match(attributes[\"http.target\"], \"/user/*/list/*\", \"/user/{userId}/list/{listId}\")\n</code></pre> <p>Reduce cardinality of a span name</p> <pre><code>traces:\n  replace_match(name, \"GET /user/*/list/*\", \"GET /user/{userId}/list/{listId}\")\n</code></pre> <p>Reduce cardinality of any matching attribute</p> <pre><code>traces:\n  replace_all_matches(attributes, \"/user/*/list/*\", \"/user/{userId}/list/{listId}\")\n</code></pre> <p>Decrease the size of the telemetry payload by removing large resource attributes</p> <pre><code>traces:\n  delete(resource.attributes[\"process.command_line\"])\nmetrics:\n  delete(resource.attributes[\"process.command_line\"])\nlogs:\n  delete(resource.attributes[\"process.command_line\"])\n</code></pre> <p>Filtering out signals such as by removing all metrics with a <code>http.target</code> of <code>/health</code></p> <pre><code>metrics:\n  drop() where attributes[\"http.target\"] = \"/health\"\n</code></pre> <p>Attach information from resource into telemetry, for example adding certain resource fields as metric attributes</p> <pre><code>metrics:\n  set(attributes[\"k8s_pod\"], resource.attributes[\"k8s.pod.name\"])\n</code></pre> <p>Group spans by trace ID</p> <pre><code>traces:\n  group_by(trace_id, 2m)\n</code></pre> <p>Update a spans ID</p> <pre><code>logs:\n  set(span_id, SpanID(0x0000000000000000))\ntraces:\n  set(span_id, SpanID(0x0000000000000000))\n</code></pre> <p>Create utilization metric from base metrics. Because navigation expressions only operate on a single piece of telemetry, helper functions for reading values from other metrics need to be provided.</p> <pre><code>metrics:\n  create_gauge(\"pod.cpu.utilized\", read_gauge(\"pod.cpu.usage\") / read_gauge(\"node.cpu.limit\")\n</code></pre> <p>A lot of processing. Queries are executed in order. While initially performance may degrade compared to more specialized processors, the expectation is that over time, the transform processor's engine would improve to be able to apply optimizations across queries, compile into machine code, etc.</p> <pre><code>receivers:\notlp:\nexporters:\notlp:\nprocessors:\ntransform:\n# Assuming group_by is defined in a contrib extension module, not baked into the \"transform\" processor\nextensions: [group_by]\ntraces:\nqueries:\n- drop() where attributes[\"http.target\"] = \"/health\"\n- delete(attributes[\"http.request.header.authorization\"])\n- replace_wildcards(\"/user/*/list/*\", \"/user/{userId}/list/{listId}\",\nattributes[\"http.target\"])\n- group_by(trace_id, 2m)\nmetrics:\nqueries:\n- drop() where attributes[\"http.target\"] = \"/health\"\n- delete(attributes[\"http.request.header.authorization\"])\n- replace_wildcards(\"/user/*/list/*\", \"/user/{userId}/list/{listId}\",\nattributes[\"http.target\"])\n- set(attributes[\"k8s_pod\"], resource.attributes[\"k8s.pod.name\"])\nlogs:\nqueries:\n- drop() where attributes[\"http.target\"] = \"/health\"\n- delete(attributes[\"http.request.header.authorization\"])\n- replace_wildcards(\"/user/*/list/*\", \"/user/{userId}/list/{listId}\",\nattributes[\"http.target\"])\npipelines:\n- receivers: [otlp]\nexporters: [otlp]\nprocessors: [transform]\n</code></pre> <p>The expressions would be executed in order, with each expression either mutating an input telemetry, dropping input telemetry, or adding additional telemetry (usually for stateful processors like batch processor which will drop telemetry for a window and then add them all at the same time). One caveat to note is that we would like to implement optimizations in the transform engine, for example to only apply filtering once for multiple operations with a shared filter. Functions with unknown side effects may cause issues with optimization we will need to explore.</p>"},{"location":"docs/k8s/collector/processing/#declarative-configuration","title":"Declarative configuration","text":"<p>The telemetry transformation language presents an SQL-like experience for defining telemetry transformations - it is made up of the three primary components described above, however, and can be presented declaratively instead depending on what makes sense as a user experience.</p> <pre><code>- type: span\nfilter:\nmatch:\npath: status.code\nvalue: OK\noperation:\nname: drop\n- type: all\noperation:\nname: delete\nargs:\n- attributes[\"http.request.header.authorization\"]\n</code></pre> <p>An implementation of the transformation language would likely parse expressions into this sort of structure so given an SQL-like implementation, it would likely be little overhead to support a YAML approach in addition.</p>"},{"location":"docs/k8s/collector/processing/#function-syntax","title":"Function syntax","text":"<p>Functions should be named and formatted according to the following standards.</p> <ul> <li>Function names MUST start with a verb unless it is a Factory.</li> <li>Factory functions MUST be UpperCamelCase and named based on the object being   created.</li> <li>Function names that contain multiple words MUST separate those words with <code>_</code>.</li> <li>Functions that interact with multiple items MUST have plurality in the name.   Ex: <code>truncate_all</code>, <code>keep_keys</code>, <code>replace_all_matches</code>.</li> <li>Functions that interact with a single item MUST NOT have plurality in the   name. If a function would interact with multiple items due to a condition,   like <code>where</code>, it is still considered singular. Ex: <code>set</code>, <code>delete</code>, <code>drop</code>,   <code>replace_match</code>.</li> <li>Functions that change a specific target MUST set the target as the first   parameter.</li> <li>Functions that take a list MUST set the list as the last parameter.</li> </ul>"},{"location":"docs/k8s/collector/processing/#implementing-a-processor-function","title":"Implementing a processor function","text":"<p>The <code>replace_match</code> function may look like this.</p> <pre><code>package replaceMatch\nimport \"regexp\"\nimport \"github.com/open-telemetry/opentelemetry/processors\"\n// Assuming this is not in \"core\"\nprocessors.register(\"replace_match\", replace_match)\nfunc replace_match(path processors.TelemetryPath, pattern regexp.Regexp, replacement string) processors.Result  {\nval := path.Get()\nif val == nil {\nreturn processors.CONTINUE\n}\n// replace finds placeholders in \"replacement\" and swaps them in for regex matched substrings.\nreplaced := replace(val, pattern, replacement)\npath.Set(replaced)\nreturn processors.CONTINUE\n}\n</code></pre> <p>Here, the processor framework recognizes the second parameter of the function is <code>regexp.Regexp</code> so will compile the string provided by the user in the config when processing it. Similarly for <code>path</code>, it recognizes properties of type <code>TelemetryPath</code> and will resolve it to the path within a matched telemetry during execution and pass it to the function. The path allows scalar operations on the field within the telemetry. The processor does not need to be aware of telemetry filtering, the <code>where ...</code> clause, as that will be handled by the framework before passing to the function.</p>"},{"location":"docs/k8s/collector/processing/#embedded-processors","title":"Embedded processors","text":"<p>The above describes a transformation language for configuring processing logic in the OpenTelemetry collector. There will be a single processor that exposes the processing logic into the collector config; however, the logic will be implemented within core packages rather than directly inside a processor. This is to ensure that where appropriate, processing can be embedded into other components, for example metric processing is often most appropriate to execute within a receiver based on receiver-specific requirements.</p>"},{"location":"docs/k8s/collector/processing/#limitations","title":"Limitations","text":"<p>There are some known issues and limitations that we hope to address while iterating on this idea.</p> <ul> <li>Handling array-typed attributes</li> <li>Working on a array of points, rather than a single point</li> <li>Metric alignment - for example defining an expression on two metrics, that may   not be at the same timestamp</li> <li>The collector has separate pipelines per signal - while the transformation   language could apply cross-signal, we will need to remain single-signal for   now</li> </ul>"},{"location":"docs/k8s/collector/release/","title":"\u6536\u96c6\u5668\u91ca\u653e\u7a0b\u5e8f","text":"<p>\u6536\u96c6\u5668\u7684\u6784\u5efa\u548c\u6d4b\u8bd5\u76ee\u524d\u662f\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u53d1\u5e03\uff0c\u4ecd\u7136\u9700\u8981\u624b\u52a8\u6267\u884c\u67d0\u4e9b\u64cd\u4f5c\u3002</p> <p>\u6211\u4eec\u4ee5\u76f8\u540c\u7684\u7248\u672c\u53d1\u5e03 core \u548c contrib \u6536\u96c6\u5668\uff0c\u5176\u4e2d contrib \u7248\u672c\u4f7f\u7528 core \u7248\u672c\u4f5c\u4e3a \u4f9d\u8d56\u3002\u6211\u4eec\u628a\u8fd9\u4e2a\u8fc7\u7a0b\u5206\u4e3a\u56db\u4e2a\u90e8\u5206\u3002\u53d1\u5e03\u5de5\u7a0b\u5e08\u5fc5\u987b\u53d1\u5e03:</p> <ol> <li>The Core collector, including the    collector builder CLI tool.</li> <li>The Contrib collector.</li> <li>The artifacts</li> </ol> <p>Important Note: You\u2019ll need to be able to sign git commits/tags in order to be able to release a collector version. Follow this guide to setup it up.</p> <p>Important Note: You\u2019ll need to be an approver for both the repos in order to be able to make the release. This is required as you\u2019ll need to push tags and commits directly to the following repositories:</p> <ul> <li>open-telemetry/opentelemetry-collector</li> <li>open-telemetry/opentelemetry-collector-contrib</li> <li>open-telemetry/opentelemetry-collector-releases</li> </ul>"},{"location":"docs/k8s/collector/release/#_2","title":"\u53d1\u5e03\u7ba1\u7406","text":"<p>A release manager is the person responsible for a specific release. While the manager might request help from other folks, they are ultimately responsible for the success of a release.</p> <p>In order to have more people comfortable with the release process, and in order to decrease the burden on a small number of volunteers, all core approvers are release managers from time to time, listed under the Release Schedule section. That table is updated at every release, with the current manager adding themselves to the bottom of the table, removing themselves from the top of the table.</p> <p>It is possible that a core approver isn't a contrib approver. In that case, the release manager should coordinate with a contrib approver for the steps requiring such role, like the publishing of tags.</p>"},{"location":"docs/k8s/collector/release/#releasing-opentelemetry-collector","title":"Releasing opentelemetry-collector","text":"<ol> <li> <p>Determine the version number that will be assigned to the release. During the    beta phase, we increment the minor version number and set the patch number    to 0. In this document, we are using <code>v0.55.0</code> as the version to be released,    following <code>v0.54.0</code>. Check if stable modules have any changes since the last    release by running    <code>make check-changes PREVIOUS_VERSION=v1.0.0-rc9 MODSET=stable</code>. If there are    no changes, there is no need to release new version for stable modules.</p> </li> <li> <p>Manually run the action    Automation - Prepare Release.    When prompted, enter the version numbers determined in Step 1, but do not    include <code>v</code>. This action will create an issue to track the progress of the    release and a pull request to update the changelog and version numbers in the    repo. While this PR is open all merging in Core should be haulted. Do    not move forward until this PR is merged.</p> </li> <li> <p>If the PR needs updated in any way you can make the changes in a fork and      PR those changes into the <code>prepare-release-prs/x</code> branch. You do not need      to wait for the CI to pass in this prep-to-prep PR.</p> </li> <li> <p>Update Contrib to use the latest in development version of Core. Run    <code>make update-otel</code> in Contrib root directory and if it results in any changes    submit a PR to Contrib with the changes as draft. This is to ensure that the    latest core does not break contrib in any way. We\u2019ll update it once more to    the final release number later.</p> </li> <li> <p>Create a branch named <code>release/&lt;release-series&gt;</code> (e.g. <code>release/v0.55.x</code>)    from the <code>Prepare Release</code> commit and push to    <code>open-telemetry/opentelemetry-collector</code>.</p> </li> <li> <p>Make sure you are on <code>release/&lt;release-series&gt;</code>. Tag the module groups with    the new release version by running:</p> </li> <li> <p><code>make push-tags MODSET=beta</code> for beta modules group,</p> </li> <li><code>make push-tags MODSET=stable</code> beta stable modules group, only if there      were changes since the last release.</li> </ol> <p>If you set your remote using <code>https</code> you need to include    <code>REMOTE=https://github.com/open-telemetry/opentelemetry-collector.git</code> in    each command. Wait for the new tag build to pass successfully.</p> <ol> <li> <p>The release script for the collector builder should create a new GitHub    release for the builder. This is a separate release from the core, but we    might join them in the future if it makes sense.</p> </li> <li> <p>A new <code>v0.55.0</code> release should be automatically created on Github by now.    Edit it and use the contents from the CHANGELOG.md as the release's    description.</p> </li> <li> <p>If you created a draft PR to Contrib in step 3, update the PR to use the    newly released Core version and set it to Ready for Review. Do not move    forward until this PR is merged.</p> </li> </ol>"},{"location":"docs/k8s/collector/release/#releasing-opentelemetry-collector-contrib","title":"Releasing opentelemetry-collector-contrib","text":"<ol> <li> <p>Manually run the action    Automation - Prepare Release.    When prompted, enter the version numbers determined in Step 1, but do not    include <code>v</code>. This action will a pull request to update the changelog and    version numbers in the repo. While this PR is open all merging in Contrib    should be haulted. Do not move forward until this PR is merged.</p> </li> <li> <p>If the PR needs updated in any way you can make the changes in a fork and      PR those changes into the <code>prepare-release-prs/x</code> branch. You do not need      to wait for the CI to pass in this prep-to-prep PR.</p> </li> <li> <p>Create a branch named <code>release/&lt;release-series&gt;</code> (e.g. <code>release/v0.55.x</code>) in    Contrib from the changelog update commit and push it to    <code>open-telemetry/opentelemetry-collector-contrib</code>.</p> </li> <li> <p>Make sure you are on <code>release/&lt;release-series&gt;</code>. Tag all the module groups    (<code>contrib-base</code>) with the new release version by running the    <code>make push-tags MODSET=contrib-base</code> command. If you set your remote using    <code>https</code> you need to include    <code>REMOTE=https://github.com/open-telemetry/opentelemetry-collector-contrib.git</code>    in each command. Wait for the new tag build to pass successfully.</p> </li> <li> <p>A new <code>v0.55.0</code> release should be automatically created on Github by now.    Edit it and use the contents from the CHANGELOG.md as the release's    description.</p> </li> </ol>"},{"location":"docs/k8s/collector/release/#producing-the-artifacts","title":"Producing the artifacts","text":"<p>The last step of the release process creates artifacts for the new version of the collector and publishes images to Dockerhub. The steps in this portion of the release are done in the opentelemetry-collector-releases repo.</p> <ol> <li> <p>Update the <code>./distribution/**/manifest.yaml</code> files to include the new release    version.</p> </li> <li> <p>Update the builder version in <code>OTELCOL_BUILDER_VERSION</code> to the new release in    the <code>Makefile</code>. While this might not be strictly necessary for every release,    this is a good practice.</p> </li> <li> <p>Create a pull request with the change and ensure the build completes    successfully. See    example.</p> </li> <li> <p>Tag with the new release version by running the <code>make push-tags TAG=v0.55.0</code>    command. If you set your remote using <code>https</code> you need to include    <code>REMOTE=https://github.com/open-telemetry/opentelemetry-collector-releases.git</code>    in each command. Wait for the new tag build to pass successfully.</p> </li> <li> <p>Ensure the \"Release\" action passes, this will</p> </li> <li> <p>push new container images to       https://hub.docker.com/repository/docker/otel/opentelemetry-collector</p> </li> <li> <p>create a Github release for the tag and push all the build artifacts to       the Github release. See       example.</p> </li> </ol>"},{"location":"docs/k8s/collector/release/#troubleshooting","title":"Troubleshooting","text":"<ol> <li><code>unknown revision internal/coreinternal/v0.55.0</code> -- This is typically an    indication that there's a dependency on a new module. You can fix it by    adding a new <code>replaces</code> entry to the <code>go.mod</code> for the affected module.</li> <li><code>commitChangesToNewBranch failed: invalid merge</code> -- This is a    known issue    with our release tooling. The current workaround is to clone a fresh copy of    the repository and try again. Note that you may need to set up a <code>fork</code>    remote pointing to your own fork for the release tooling to work properly.</li> <li><code>could not run Go Mod Tidy: go mod tidy failed</code> when running <code>multimod</code> --    This is a    known issue    with our release tooling. The current workaround is to run <code>make gotidy</code>    manually after the multimod tool fails and commit the result.</li> <li><code>Incorrect version \"X\" of \"go.opentelemetry.io/collector/component\" is included in \"X\"</code>    in CI after <code>make update-otel</code> -- It could be because the make target was run    too soon after updating Core and the goproxy hasn't updated yet. Try running    <code>export GOPROXY=direct</code> and then <code>make update-otel</code>.</li> <li><code>error: failed to push some refs to 'https://github.com/open-telemetry/opentelemetry-collector-contrib.git'</code>    during <code>make push-tags</code> -- If you encounter this error the <code>make push-tags</code>    target will terminate without pushing all the tags. Using the output of the    <code>make push-tags</code> target, save all the un-pushed the tags in <code>tags.txt</code> and    then use this make target to complete the push:</li> </ol> <pre><code>.PHONY: temp-push-tags\ntemp-push-tags:\n    for tag in `cat tags.txt`; do \\\necho \"pushing tag $${tag}\"; \\\ngit push ${REMOTE} $${tag}; \\\ndone;\n</code></pre>"},{"location":"docs/k8s/collector/release/#bugfix-releases","title":"Bugfix releases","text":""},{"location":"docs/k8s/collector/release/#bugfix-release-criteria","title":"Bugfix release criteria","text":"<p>Both <code>opentelemetry-collector</code> and <code>opentelemetry-collector-contrib</code> have very short 2 week release cycles. Because of this, we put a high bar when considering making a patch release, to avoid wasting engineering time unnecessarily.</p> <p>When considering making a bugfix release on the <code>v0.N.x</code> release cycle, the bug in question needs to fulfill the following criteria:</p> <ol> <li>The bug has no workaround or the workaround is significantly harder to put in    place than updating the version. Examples of simple workarounds are:</li> <li>Reverting a feature gate.</li> <li>Changing the configuration to an easy to find value.</li> <li>The bug happens in common setups. To gauge this, maintainers can consider the    following:</li> <li>The bug is not specific to an uncommon platform</li> <li>The bug happens with the default configuration or with a commonly used one      (e.g. has been reported by multiple people)</li> <li>The bug is sufficiently severe. For example (non-exhaustive list):</li> <li>The bug makes the Collector crash reliably</li> <li>The bug makes the Collector fail to start under an accepted configuration</li> <li>The bug produces significant data loss</li> <li>The bug makes the Collector negatively affect its environment (e.g.      significantly affects its host machine)</li> </ol> <p>We aim to provide a release that fixes security-related issues in at most 30 days since they are publicly announced; with the current release schedule this means security issues will typically not warrant a bugfix release. An exception is critical vulnerabilities (CVSSv3 score &gt;= 9.0), which will warrant a release within five business days.</p> <p>The OpenTelemetry Collector maintainers will ultimately have the responsibility to assess if a given bug or security issue fulfills all the necessary criteria and may grant exceptions in a case-by-case basis.</p>"},{"location":"docs/k8s/collector/release/#bugfix-release-procedure","title":"Bugfix release procedure","text":"<p>The following documents the procedure to release a bugfix</p> <ol> <li>Create a pull request against the <code>release/&lt;release-series&gt;</code> (e.g.    <code>release/v0.45.x</code>) branch to apply the fix.</li> <li>Create a pull request to update version number against the    <code>release/&lt;release-series&gt;</code> branch.</li> <li>Once those changes have been merged, create a pull request to the <code>main</code>    branch from the <code>release/&lt;release-series&gt;</code> branch.</li> <li>Enable the Merge pull request setting in the repository's Settings    tab.</li> <li>Tag all the modules with the new release version by running the    <code>make add-tag</code> command (e.g. <code>make add-tag TAG=v0.55.0</code>). Push them to    <code>open-telemetry/opentelemetry-collector</code> with <code>make push-tag TAG=v0.55.0</code>.    Wait for the new tag build to pass successfully.</li> <li>IMPORTANT: The pull request to bring the changes from the release branch    MUST be merged using the Merge pull request method, and NOT squashed    using \u201cSquash and merge\u201d. This is important as it allows us to ensure the    commit SHA from the release branch is also on the main branch. Not    following this step will cause much go dependency sadness.</li> <li>Once the branch has been merged, it will be auto-deleted. Restore the release    branch via GitHub.</li> <li>Once the patch is release, disable the Merge pull request setting.</li> </ol>"},{"location":"docs/k8s/collector/release/#release-schedule","title":"Release schedule","text":"Date Version Release manager 2023-06-19 v0.80.0 @Aneurysm9 2023-07-03 v0.81.0 @jpkrohling 2023-07-17 v0.82.0 @mx-psi 2023-07-31 v0.83.0 @djaglowski 2023-08-14 v0.84.0 @dmitryax 2023-08-28 v0.85.0 @codeboten 2023-09-11 v0.86.0 @codeboten 2023-09-25 v0.87.0 @bogdandrutu"},{"location":"docs/k8s/collector/roadmap/","title":"\u957f\u671f\u8def\u7ebf\u56fe","text":"<p>\u8fd9\u4efd\u957f\u671f\u8def\u7ebf\u56fe(\u8349\u6848)\u662f\u4e00\u4efd\u53cd\u6620\u6211\u4eec\u5f53\u524d\u613f\u671b\u7684\u613f\u666f\u6587\u4ef6\u3002\u5e76\u4e0d\u662f\u627f\u8bfa\u8981\u5b9e\u73b0\u6b64\u8def\u7ebf\u56fe\u4e2d \u5217\u51fa\u7684\u6240\u6709\u5185\u5bb9\u3002\u672c\u6587\u6863\u7684\u4e3b\u8981\u76ee\u7684\u662f\u786e\u4fdd\u6240\u6709\u8d21\u732e\u8005\u5de5\u4f5c\u4e00\u81f4\u3002\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u6211\u4eec\u7684 \u613f\u666f\u53d1\u751f\u4e86\u53d8\u5316\uff0c\u7ef4\u62a4\u4eba\u5458\u4fdd\u7559\u5728\u8def\u7ebf\u56fe\u4e2d\u6dfb\u52a0\u3001\u4fee\u6539\u548c\u5220\u9664\u9879\u76ee\u7684\u6743\u5229\u3002</p> Description Status Links \u6d4b\u8bd5 \u5ea6\u91cf\u6b63\u786e\u6027\u6d4b\u8bd5 Done #652 \u65b0\u683c\u5f0f \u5b8c\u6574\u7684 OTLP/HTTP \u652f\u6301 Done #882 \u4e3a\u6240\u6709\u4e3b\u6838\u5fc3\u5904\u7406\u5668(\u5c5e\u6027\u3001\u6279\u5904\u7406\u3001k8sattributes \u7b49)\u6dfb\u52a0\u65e5\u5fd7\u652f\u6301 Done 5 \u6700\u5c0f\u503c \u9488\u5bf9\u5927\u591a\u6570\u5e38\u89c1\u76ee\u6807\u7684\u53d1\u884c\u5305(\u4f8b\u5982 Docker\u3001RPM\u3001Windows \u7b49) Done https://github.com/open-telemetry/opentelemetry-collector-releases/releases \u68c0\u6d4b\u548c\u6536\u96c6 AWS \u4e0a\u7684\u73af\u5883\u6307\u6807\u548c\u6807\u7b7e Beta https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor k8s \u9065\u6d4b\u68c0\u6d4b\u4e0e\u91c7\u96c6 Beta https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/processor/k8sattributesprocessor \u4e3b\u673a\u5ea6\u91cf\u96c6\u5408 Beta https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/hostmetricsreceiver \u652f\u6301\u66f4\u591a\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u5ea6\u91cf\u96c6\u5408(\u4f8b\u5982 Kafka, Hadoop \u7b49) In Progress https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver \u5176\u4ed6\u529f\u80fd \u5b89\u5168\u505c\u673a(\u7ba1\u9053\u6392\u6c34) Done #483 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5f03\u7528\u961f\u5217\u91cd\u8bd5\u5904\u7406\u5668\u5e76\u542f\u7528\u6bcf\u4e2a\u5bfc\u51fa\u5668\u7684\u961f\u5217 Done #1721 <p>\u6b64\u65f6\uff0cOpenTelemetry Collector SIG \u7684\u5927\u90e8\u5206\u5de5\u4f5c\u90fd\u96c6\u4e2d\u5728\u8de8\u5404\u79cd\u5305\u5b9e\u73b0 GA \u72b6\u6001\u4e0a\u3002\u8bf7 \u53c2\u9605GA \u8def\u7ebf\u56fe\u6587\u6863\u4e2d\u7684\u5176\u4ed6\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"docs/k8s/collector/scraping-receivers/","title":"\u6293\u53d6\u6307\u6807\u63a5\u6536\u5668","text":"<p>Scraping metrics receivers are receivers that pull data from external sources at regular intervals and translate it into pdata which is sent further in the pipeline. The external source of metrics usually is a monitored system providing data about itself in some arbitrary format. There are two types of scraping metrics receivers:</p> <ul> <li> <p>Generic scraping metrics receivers: The set of metrics emitted by this   type of receiver fully depends on the state of the external source and/or the   user settings. Examples:</p> </li> <li> <p>Prometheus Receiver</p> </li> <li> <p>SQL Query Receiver</p> </li> <li> <p>Built-in scraping metrics receivers: Receivers of this type emit a   predefined set of metrics. However, the metrics themselves are configurable   via user settings. Examples of scrapings metrics receivers:</p> </li> <li> <p>Redis Receiver</p> </li> <li>Zookeeper Receiver</li> </ul> <p>This document covers built-in scraping metrics receivers. It defines which metrics these receivers can emit, defines stability guarantees and provides guidelines for metric updates.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#defining-emitted-metrics","title":"Defining emitted metrics","text":"<p>Each built-in scraping metrics receiver has a <code>metadata.yaml</code> file that MUST define all the metrics emitted by the receiver. The file is being used to generate an API for metrics recording, user settings to customize the emitted metrics and user documentation. The file schema is defined in https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/cmd/mdatagen/metric-metadata.yaml. Defining a metric in <code>metadata.yaml</code> DOES NOT guarantee that the metric will always be produced by the receiver. In some cases it may be impossible to fetch particular metrics from a system in a particular state.</p> <p>There are two categories of the metrics emitted by scraping receivers:</p> <ul> <li> <p>Default metrics: emitted by default, but can be disabled in user settings.</p> </li> <li> <p>Optional metrics: not emitted by default, but can be enabled in user   settings.</p> </li> </ul>"},{"location":"docs/k8s/collector/scraping-receivers/#how-to-identify-if-new-metric-should-be-default-or-optional","title":"How to identify if new metric should be default or optional?","text":"<p>There is no strict rule to differentiate default metrics from optional. As a rule of thumb, default metrics SHOULD be treated as metrics essential to determine healthiness of a particular monitored system. Optional metrics on the other hand SHOULD be treated as a source of additional information about the monitored system.</p> <p>Additionally, if any of the following conditions can be applied to a metric, it MUST be marked as optional:</p> <ul> <li>It is redundant with another metric. For example system CPU usage can be   emitted as two different metrics: <code>system.cpu.time</code> (CPU time reported as   cumulative sum in seconds) or <code>system.cpu.utilization</code> (fraction of CPU time   spent in different states reported as gauge), one of them has to be marked as   option.</li> <li>It creates a disproportionately high cardinality of resources and/or data   points.</li> <li>There is a notable expected performance impact.</li> <li>The source must be configured in an unusual way.</li> <li>It requires dedicated configuration in the receiver.</li> </ul>"},{"location":"docs/k8s/collector/scraping-receivers/#stability-levels-of-scraping-receivers","title":"Stability levels of scraping receivers","text":"<p>All the requirements defined for components in the Collector's README are applicable to the scraping receivers as well. In addition, the following rules applied specifically to scraping metrics receivers:</p>"},{"location":"docs/k8s/collector/scraping-receivers/#development","title":"Development","text":"<p>The receiver is not ready for use. All the metrics emitted by the receiver are not finalized and can change in any way.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#alpha","title":"Alpha","text":"<p>The receiver is ready for limited non-critical workloads. The list of emitted default metrics SHOULD be considered as complete, but any changes to the <code>metadata.yaml</code> still MAY be applied.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#beta","title":"Beta","text":"<p>The receiver is ready for non-critical production workloads. The list of emitted default metrics MUST be considered as complete. Breaking changes to the emitted metrics SHOULD be applied following the deprecation process.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#stable","title":"Stable","text":"<p>The receiver is ready for production workloads. Breaking changes to the emitted metrics SHOULD be avoided. Nevertheless, metrics that are emitted by default MUST be always kept up-to-date with the latest stable version of the monitored system. Given that, occasional breaking changes in the emitted metrics are expected even in the stable receivers. Any breaking change MUST be applied following the deprecation process.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#changing-the-emitted-metrics","title":"Changing the emitted metrics","text":"<p>Some changes are not considered breaking and can be applied to metrics emitted by scraping receivers of any stability level:</p> <ul> <li>Adding a new optional metric.</li> </ul> <p>Most of other changes to the emitted metrics are considered breaking and MUST be handled according to the stability level of the receiver. Each type of breaking change defines a set of steps that MUST (or SHOULD) be applied across several releases for a Stable (or Beta) components. At least 3 versions SHOULD be kept between the steps to give users time to prepare, e.g. if the first step is released in v0.62.0, the second step SHOULD be released not earlier than 0.65.0. Any warnings SHOULD include the version starting from which the next step will take effect. If a breaking change is more complicated and many metrics are involved in the change, feature gates SHOULD be used instead.</p>"},{"location":"docs/k8s/collector/scraping-receivers/#removing-an-optional-metric","title":"Removing an optional metric","text":"<p>Steps to remove an optional metric:</p> <ol> <li>Mark the metric as deprecated in <code>metadata.yaml</code> by adding \"[DEPRECATED]\" in    its description. Show a warning that the metric will be removed if the    <code>enabled</code> option is set explicitly to <code>true</code> in user settings.</li> <li>Remove the metric.</li> </ol>"},{"location":"docs/k8s/collector/scraping-receivers/#removing-a-default-metric","title":"Removing a default metric","text":"<p>Steps to remove a default metric:</p> <ol> <li>Mark the metric as deprecated in <code>metadata.yaml</code> by adding \"[DEPRECATED]\" in    its description. Show a warning that the metric will be removed if the    <code>enabled</code> option is not explicitly set to <code>false</code> in user settings.</li> <li>Make the metric optional. Show a warning that the metric will be removed if    the <code>enabled</code> option is set to <code>true</code> in user settings.</li> <li>Remove the metric.</li> </ol>"},{"location":"docs/k8s/collector/scraping-receivers/#making-a-default-metric-optional","title":"Making a default metric optional","text":"<p>Steps to turn a metric from default to optional:</p> <ol> <li>Add a warning that the metric will be turned into optional if <code>enabled</code> field    is not set explicitly to any value in user settings. Warning example:    \"WARNING: Metric <code>foo.bar</code> will be disabled by default in v0.65.0. If you    want to keep it, please enable it explicitly in the receiver settings.\"</li> <li>Remove the warning and update <code>metadata.yaml</code> to make the metric optional.</li> </ol>"},{"location":"docs/k8s/collector/scraping-receivers/#adding-a-new-default-metric-or-turning-an-existing-optional-metric-into-default","title":"Adding a new default metric or turning an existing optional metric into default","text":"<p>Adding a new default metric is a breaking change for a scraping receiver because it introduces an unexpected output for users and additional load on metric backends. Steps to apply such a change:</p> <ol> <li>If the metric doesn't exist yet, add one as an optional metric. Add a warning    that the metric will be turned into default if the <code>enabled</code> option is not    set explicitly to any value in user settings. A warning example: \"WARNING:    Metric <code>foo.bar</code> will be enabled by default in v0.65.0. If you don't want the    metric to be emitted, please disable it in the receiver settings.\"</li> <li>Remove the warning and update <code>metadata.yaml</code> to make the metric default.</li> </ol>"},{"location":"docs/k8s/collector/scraping-receivers/#other-changes","title":"Other changes","text":"<p>Other breaking changes SHOULD follow similar strategies inspecting presence of <code>enabled</code> field in user settings. For example, if a metric has to be renamed for any reason, the guidelines for \"Removing an optional metric\" and \"Adding a new default metric\" SHOULD be followed simultaneously.</p> <p>Breaking changes that cannot be done through enabling/disabling metrics (e.g. removing or adding an extra attribute) SHOULD be applied using a feature gate with the following steps:</p> <ol> <li>Add a feature gate that is disabled by default. Enabling the feature gate    changes the metrics behavior in a desired way. For example, if several    metrics emitted by host metrics receiver need to be updated to have an    additional <code>direction</code> attribute, the following feature gate can be used:    <code>receiver.hostmetricsreceiver.emitMetricsWithDirectionAttribute</code>. Show user a    warning if the feature gate is not enabled explicitly, for example:    \"[WARNING] Metrics <code>system.network.packets</code> and <code>system.network.errors</code> will    be changed in v0.65.0 to emit an additional <code>direction</code> attribute, enable a    feature gate <code>receiver.hostmetricsreceiver.emitMetricsWithDirectionAttribute</code>    to apply and test the upcoming changes earlier\".</li> <li>Enable the feature gate by default and update the warning thrown if user    disables the feature gate explicitly.</li> <li>Remove the feature gate along with the old behavior.</li> </ol>"},{"location":"docs/k8s/collector/security-best-practices/","title":"\u5b89\u5168","text":"<p>The OpenTelemetry Collector defaults to operating in a secure manner, but is configuration driven. This document captures important security aspects and considerations for the Collector. This document is intended for both end-users and component developers. It assumes at least a basic understanding of the Collector architecture and functionality.</p> <p>Note: Please review the configuration documentation prior to this security document.</p>"},{"location":"docs/k8s/collector/security-best-practices/#tldr","title":"TL;DR","text":""},{"location":"docs/k8s/collector/security-best-practices/#end-users","title":"End-users","text":"<ul> <li>Configuration</li> <li>SHOULD only enable the minimum required components</li> <li>SHOULD ensure sensitive configuration information is stored securely</li> <li>Permissions</li> <li>SHOULD not run Collector as root/admin user</li> <li>MAY require privileged access for some components</li> <li>Receivers/Exporters</li> <li>SHOULD use encryption and authentication</li> <li>SHOULD limit exposure of servers to authorized users</li> <li>MAY pose a security risk if configuration parameters are modified improperly</li> <li>Processors</li> <li>SHOULD configure obfuscation/scrubbing of sensitive metadata</li> <li>SHOULD configure recommended processors</li> <li>Extensions</li> <li>SHOULD NOT expose sensitive health or telemetry data</li> </ul> <p>For more information about securing the OpenTelemetry Collector, see this blog post.</p>"},{"location":"docs/k8s/collector/security-best-practices/#component-developers","title":"Component Developers","text":"<ul> <li>Configuration</li> <li>MUST come from the central configuration file</li> <li>SHOULD use configuration helpers</li> <li>Permissions</li> <li>SHOULD minimize privileged access</li> <li>MUST document what required privileged access and why</li> <li>Receivers/Exporters</li> <li>MUST default to encrypted connections</li> <li>SHOULD leverage helper functions</li> <li>Extensions</li> <li>SHOULD NOT expose sensitive health or telemetry data by default</li> </ul>"},{"location":"docs/k8s/collector/security-best-practices/#configuration","title":"Configuration","text":"<p>The Collector binary does not contain an embedded or default configuration and MUST NOT start without a configuration file being specified. The configuration file passed to the Collector MUST be validated prior to be loaded. If an invalid configuration is detected, the Collector MUST fail to start as a protective mechanism.</p> <p>Note: Issue #886 proposes adding a default configuration to the binary.</p> <p>The configuration drives the Collector's behavior and care should be taken to ensure the configuration only enables the minimum set of capabilities and as such exposes the minimum set of required ports. In addition, any incoming or outgoing communication SHOULD leverage TLS and authentication.</p> <p>The Collector keeps the configuration in memory, but where the configuration is loaded from at start time depends on the packaging used. For example, in Kubernetes secrets and configmaps CAN be leveraged. In comparison, the Docker image embeds the configuration in the container where is it not stored in an encrypted manner by default.</p> <p>The configuration MAY contain sensitive information including:</p> <ul> <li>Authentication information such as API tokens</li> <li>TLS certificates including private keys</li> </ul> <p>Sensitive information SHOULD be stored securely such as on an encrypted filesystem or secret store. Environment variables CAN be used to handle sensitive and non-sensitive data as the Collector MUST support environment variable expansion.</p> <p>For more information on environment variable expansion, see this documentation.</p> <p>When defining Go structs for configuration data that may contain sensitive information, use the <code>configopaque</code> package to define fields with the <code>configopaque.String</code> type. This ensures that the data is masked when serialized to prevent accidental exposure.</p> <p>For more information, see the configopaque documentation.</p> <p>Component developers MUST get configuration information from the Collector's configuration file. Component developers SHOULD leverage configuration helper functions.</p> <p>More information about configuration is provided in the following sections.</p>"},{"location":"docs/k8s/collector/security-best-practices/#permissions","title":"Permissions","text":"<p>The Collector supports running as a custom user and SHOULD NOT be run as a root/admin user. For the majority of use-cases, the Collector SHOULD NOT require privileged access to function. Some components MAY require privileged access and care should be taken before enabling these components. Collector components MAY require external permissions including network access or RBAC.</p> <p>Component developers SHOULD minimize privileged access requirements and MUST document what requires privileged access and why.</p> <p>More information about permissions is provided in the following sections.</p>"},{"location":"docs/k8s/collector/security-best-practices/#receivers-and-exporters","title":"Receivers and Exporters","text":"<p>Receivers and Exporters can be either push or pull-based. In either case, the connection established SHOULD be over a secure and authenticated channel. Unused receivers and exporters SHOULD be disabled to minimize the attack vector of the Collector.</p> <p>Receivers and Exporters MAY expose buffer, queue, payload, and/or worker settings via configuration parameters. If these settings are available, end-users should proceed with caution before modifying the default values. Improperly setting these values may expose the Collector to additional attack vectors including resource exhaustion.</p> <p>It is possible that a receiver MAY require the Collector run in a privileged mode in order to operate, which could be a security concern, but today this is not the case.</p> <p>Component developers MUST default to encrypted connections (via the <code>insecure: false</code> configuration setting) and SHOULD leverage gRPC and http helper functions.</p>"},{"location":"docs/k8s/collector/security-best-practices/#safeguards-against-denial-of-service-attacks","title":"Safeguards against denial of service attacks","text":"<p>Users SHOULD bind receivers' servers to addresses that limit connections to authorized users. For example, if the OTLP receiver OTLP/gRPC server only has local clients, the <code>endpoint</code> setting SHOULD be bound to <code>localhost</code>:</p> <pre><code>receivers:\notlp:\nprotocols:\ngrpc:\nendpoint: localhost:4317\n</code></pre> <p>Generally, <code>localhost</code>-like addresses should be preferred over the 0.0.0.0 address. For more information, see CWE-1327.</p>"},{"location":"docs/k8s/collector/security-best-practices/#processors","title":"Processors","text":"<p>Processors sit between receivers and exporters. They are responsible for processing the data in some way. From a security perspective, they are useful in a couple ways.</p>"},{"location":"docs/k8s/collector/security-best-practices/#scrubbing-sensitive-data","title":"Scrubbing sensitive data","text":"<p>It is common for a Collector to be used to scrub sensitive data before exporting it to a backend. This is especially important when sending the data to a third-party backend. The Collector SHOULD be configured to obfuscate or scrub sensitive data before exporting.</p> <p>Note: Issue #2466 proposes adding default obfuscation or scrubbing of known sensitive metadata.</p>"},{"location":"docs/k8s/collector/security-best-practices/#safeguards-around-resource-utilization","title":"Safeguards around resource utilization","text":"<p>In addition, processors offer safeguards around resource utilization. The <code>batch</code> and especially <code>memory_limiter</code> processor help ensure that the Collector is resource efficient and does not out of memory when overloaded. At least these two processors SHOULD be enabled on every defined pipeline.</p> <p>For more information on recommended processors and order, see this documentation.</p>"},{"location":"docs/k8s/collector/security-best-practices/#extensions","title":"Extensions","text":"<p>While receivers, processors, and exporters handle telemetry data directly, extensions typical serve different needs.</p>"},{"location":"docs/k8s/collector/security-best-practices/#health-and-telemetry","title":"Health and Telemetry","text":"<p>The initial extensions provided health check information, Collector metrics and traces, and the ability to generate and collect profiling data. When enabled with their default settings, all of these extensions except the health check extension are only accessibly locally to the Collector. Care should be taken when configuring these extensions for remote access as sensitive information may be exposed as a result.</p> <p>Component developers SHOULD NOT expose health or telemetry data outside the Collector by default.</p>"},{"location":"docs/k8s/collector/security-best-practices/#forwarding","title":"Forwarding","text":"<p>A forwarding extension is typically used when some telemetry data not natively supported by the Collector needs to be collected. For example, the <code>http_forwarder</code> extension can receive and forward HTTP payloads. Forwarding extensions are similar to receivers and exporters so the same security considerations apply.</p>"},{"location":"docs/k8s/collector/security-best-practices/#observers","title":"Observers","text":"<p>An observer is capable of performing service discovery of endpoints. Other components of the collector such as receivers MAY subscribe to these extensions to be notified of endpoints coming or going. Observers MAY require certain permissions in order to perform service discovery. For example, the <code>k8s_observer</code> requires certain RBAC permissions in Kubernetes, while the <code>host_observer</code> requires the Collector to run in privileged mode.</p>"},{"location":"docs/k8s/collector/security-best-practices/#subprocesses","title":"Subprocesses","text":"<p>Extensions may also be used to run subprocesses. This can be useful when collection mechanisms that cannot natively be run by the Collector (e.g. FluentBit). Subprocesses expose a completely separate attack vector that would depend on the subprocess itself. In general, care should be taken before running any subprocesses alongside the Collector.</p>"},{"location":"docs/k8s/collector/service-extensions/","title":"\u6536\u96c6\u5668:\u6269\u5c55","text":"<p>Besides the pipeline elements (receivers, processors, and exporters) the Collector uses various service extensions (e.g.: healthcheck, z-pages, etc). This document describes the \u201cextensions\u201d design and how they are implemented.</p>"},{"location":"docs/k8s/collector/service-extensions/#configuration-and-interface","title":"Configuration and Interface","text":"<p>The configuration follows the same pattern used for pipelines: a base configuration type and the creation of factories to instantiate the extension objects.</p> <p>In order to support generic service extensions an interface is defined so the service can interact uniformly with these. At minimum service extensions need to implement the interface that covers Start and Shutdown.</p> <p>In addition to this base interface there is support to notify extensions when pipelines are \u201cready\u201d and when they are about to be stopped, i.e.: \u201cnot ready\u201d to receive data. These are a necessary addition to allow implementing extensions that indicate to LBs and external systems if the service instance is ready or not to receive data (e.g.: a k8s readiness probe). These state changes are under the control of the service server hosting the extensions.</p> <p>There are more complex scenarios in which there can be notifications of state changes from the extensions to their host. These more complex cases are not supported at this moment, but this design doesn\u2019t prevent such extensions in the future1.</p>"},{"location":"docs/k8s/collector/service-extensions/#collector-state-and-extensions","title":"Collector State and Extensions","text":"<p>The diagram below shows the basic state transitions of the OpenTelemetry Collector and how it will interact with the service extensions.</p> <p></p>"},{"location":"docs/k8s/collector/service-extensions/#configuration","title":"Configuration","text":"<p>The config package will be extended to load the service extensions when the configuration is loaded. The settings for service extensions will live in the same configuration file as the pipeline elements. Below is an example of how these sections would look like in the configuration file:</p> <pre><code># Example of the extensions available with the core Collector. The list below\n# includes all configurable options and their respective default value.\nextensions:\nhealth_check:\nport: 13133\npprof:\nendpoint: 'localhost:1777'\nblock_profile_fraction: 0\nmutex_profile_fraction: 0\nzpages:\nendpoint: 'localhost:55679'\n# The service lists extensions not directly related to data pipelines, but used\n# by the service.\nservice:\n# extensions lists the extensions added to the service. They are started\n# in the order presented below and stopped in the reverse order.\nextensions: [health_check, pprof, zpages]\n</code></pre> <p>The configuration base type does not share any common fields.</p> <p>The configuration, analogous to pipelines, allows to have multiple extensions of the same type. Implementers of extensions need to take care to return error if it can only execute a single instance. (Note: the configuration uses composite key names in the form of <code>type[/name]</code> as defined in this this document).</p> <p>The factory follows the same pattern established for pipeline configuration:</p> <pre><code>// Factory is a factory interface for extensions to the service.\ntype Factory interface {\n// Type gets the type of the extension created by this factory.\nType() string\n// CreateDefaultConfig creates the default configuration for the extension.\nCreateDefaultConfig() config.Extension\n// CreateExtension creates a service extension based on the given config.\nCreateExtension(logger *zap.Logger, cfg config.Extension) (component.Extension, error)\n}\n</code></pre>"},{"location":"docs/k8s/collector/service-extensions/#extension-interface","title":"Extension Interface","text":"<p>The interface defined below is the minimum required for extensions in use on the service:</p> <pre><code>// ServiceExtension is the interface for objects hosted by the OpenTelemetry Collector that\n// don't participate directly on data pipelines but provide some functionality\n// to the service, examples: health check endpoint, z-pages, etc.\ntype ServiceExtension interface {\n// Start the ServiceExtension object hosted by the given host. At this point in the\n// process life-cycle the receivers are not started and the host did not\n// receive any data yet.\nStart(host Host) error\n// Shutdown the ServiceExtension instance. This happens after the pipelines were\n// shutdown.\nShutdown() error\n}\n// PipelineWatcher is an extra interface for ServiceExtension hosted by the OpenTelemetry\n// Collector that is to be implemented by extensions interested in changes to pipeline\n// states. Typically this will be used by extensions that change their behavior if data is\n// being ingested or not, e.g.: a k8s readiness probe.\ntype PipelineWatcher interface {\n// Ready notifies the ServiceExtension that all pipelines were built and the\n// receivers were started, i.e.: the service is ready to receive data\n// (notice that it may already have received data when this method is called).\nReady() error\n// NotReady notifies the ServiceExtension that all receivers are about to be stopped,\n// i.e.: pipeline receivers will not accept new data.\n// This is sent before receivers are stopped, so the ServiceExtension can take any\n// appropriate action before that happens.\nNotReady() error\n}\n// Host represents the entity where the extension is being hosted.\n// It is used to allow communication between the extension and its host.\ntype Host interface {\n// ReportFatalError is used to report to the host that the extension\n// encountered a fatal error (i.e.: an error that the instance can't recover\n// from) after its start function had already returned.\nReportFatalError(err error)\n}\n</code></pre>"},{"location":"docs/k8s/collector/service-extensions/#notes","title":"Notes","text":"<ol> <li> <p>This can be done by adding specific interfaces to extension types that support those and having the service checking which of the extension instances support each interface.\u00a0\u21a9</p> </li> </ol>"},{"location":"docs/k8s/collector/standard-warnings/","title":"\u6807\u51c6\u7684\u8b66\u544a","text":"<p>Some components have scenarios that could cause issues. Some components require the collector be interacted with in a specific way in order to ensure the component works as intended. This document describes common warnings that may affect a component in the collector.</p> <p>Visit a component's README to see if it is affected by any of these standard warnings.</p>"},{"location":"docs/k8s/collector/standard-warnings/#unsound-transformations","title":"Unsound Transformations","text":"<p>Incorrect usage of the component may lead to telemetry data that is unsound i.e. not spec-compliant/meaningless. This would most likely be caused by converting metric data types or creating new metrics from existing metrics.</p>"},{"location":"docs/k8s/collector/standard-warnings/#statefulness","title":"Statefulness","text":"<p>The component keeps state related to telemetry data and therefore needs all data from a producer to be sent to the same Collector instance to ensure a correct behavior. Examples of scenarios that require state would be computing/exporting delta metrics, tail-based sampling and grouping telemetry.</p>"},{"location":"docs/k8s/collector/standard-warnings/#identity-conflict","title":"Identity Conflict","text":"<p>The component may change the identity of a metric or the identity of a timeseries. This could be done by modifying the metric/timeseries's name, attributes, or instrumentation scope. Modifying a metric/timeseries's identity could result in a metric/timeseries identity conflict, which caused by two metrics/timeseries sharing the same name, attributes, and instrumentation scope.</p>"},{"location":"docs/k8s/collector/standard-warnings/#orphaned-telemetry","title":"Orphaned Telemetry","text":"<p>The component modifies the incoming telemetry in such a way that a span becomes orphaned, that is, it contains a <code>trace_id</code> or <code>parent_span_id</code> that does not exist. This may occur because the component can modify <code>span_id</code>, <code>trace_id</code>, or <code>parent_span_id</code> or because the component can delete telemetry.</p>"},{"location":"docs/k8s/collector/troubleshooting/","title":"\u6545\u969c\u6392\u9664","text":""},{"location":"docs/k8s/collector/troubleshooting/#_2","title":"\u53ef\u89c2\u5bdf\u6027","text":"<p>\u6536\u96c6\u5668\u63d0\u4f9b\u591a\u79cd\u65b9\u6cd5\u6765\u6d4b\u91cf\u6536\u96c6\u5668\u7684\u8fd0\u884c\u72b6\u51b5\u4ee5\u53ca\u8c03\u67e5\u95ee\u9898\u3002</p>"},{"location":"docs/k8s/collector/troubleshooting/#_3","title":"\u65e5\u5fd7","text":"<p>\u65e5\u5fd7\u53ef\u4ee5\u5e2e\u52a9\u8bc6\u522b\u95ee\u9898\u3002\u603b\u662f\u4ece\u68c0\u67e5\u65e5\u5fd7\u8f93\u51fa\u548c\u5bfb\u627e\u6f5c\u5728\u95ee\u9898\u5f00\u59cb\u3002\u5197\u957f\u7ea7\u522b\u9ed8\u8ba4\u4e3a \u201cINFO\u201d\uff0c\u53ef\u4ee5\u8c03\u6574\u3002</p>"},{"location":"docs/k8s/collector/troubleshooting/#036","title":"0.36 \u53ca\u4ee5\u4e0a\u7248\u672c:","text":"<p>\u5728\u914d\u7f6e\u201cservice::telemetry::logs\u201d\u4e2d\u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\u3002</p> <pre><code>service:\ntelemetry:\nlogs:\nlevel: 'debug'\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#version-035-and-below","title":"Version 0.35 and below:","text":"<p>Pass <code>--log-level</code> flag to the <code>otelcol</code> process. See <code>--help</code> for more details.</p> <pre><code>$ otelcol --log-level DEBUG\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#metrics","title":"Metrics","text":"<p>Prometheus metrics are exposed locally on port <code>8888</code> and path <code>/metrics</code>. For containerized environments it may be desirable to expose this port on a public interface instead of just locally.</p>"},{"location":"docs/k8s/collector/troubleshooting/#version-0430-and-above","title":"Version 0.43.0 and above:","text":"<p>Set the address in the config <code>service::telemetry::metrics</code></p> <pre><code>service:\ntelemetry:\nmetrics:\naddress: ':8888'\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#version-0420-and-below","title":"Version 0.42.0 and below:","text":"<p>Pass <code>--metrics-addr &lt;ADDR&gt;</code> flag to the <code>otelcol</code> process. See <code>--help</code> for more details.</p> <pre><code>$ otelcol --metrics-addr 0.0.0.0:8888\n</code></pre> <p>A grafana dashboard for these metrics can be found here.</p> <p>Also note that a Collector can be configured to scrape its own metrics and send it through configured pipelines. For example:</p> <pre><code>receivers:\nprometheus:\nconfig:\nscrape_configs:\n- job_name: 'otelcol'\nscrape_interval: 10s\nstatic_configs:\n- targets: ['0.0.0.0:8888']\nmetric_relabel_configs:\n- source_labels: [__name__]\nregex: '.*grpc_io.*'\naction: drop\nexporters:\nlogging:\nservice:\npipelines:\nmetrics:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#zpages","title":"zPages","text":"<p>The zpages extension, which if enabled is exposed locally on port <code>55679</code>, can be used to check receivers and exporters trace operations via <code>/debug/tracez</code>. <code>zpages</code> may contain error logs that the Collector does not emit.</p> <p>For containerized environments it may be desirable to expose this port on a public interface instead of just locally. This can be configured via the extensions configuration section. For example:</p> <pre><code>extensions:\nzpages:\nendpoint: 0.0.0.0:55679\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#local-exporters","title":"Local exporters","text":"<p>Local exporters can be configured to inspect the data being processed by the Collector.</p> <p>For live troubleshooting purposes consider leveraging the <code>logging</code> exporter, which can be used to confirm that data is being received, processed and exported by the Collector.</p> <pre><code>receivers:\nzipkin:\nexporters:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [zipkin]\nprocessors: []\nexporters: [logging]\n</code></pre> <p>Get a Zipkin payload to test. For example create a file called <code>trace.json</code> that contains:</p> <pre><code>[\n{\n\"traceId\": \"5982fe77008310cc80f1da5e10147519\",\n\"parentId\": \"90394f6bcffb5d13\",\n\"id\": \"67fae42571535f60\",\n\"kind\": \"SERVER\",\n\"name\": \"/m/n/2.6.1\",\n\"timestamp\": 1516781775726000,\n\"duration\": 26000,\n\"localEndpoint\": {\n\"serviceName\": \"api\"\n},\n\"remoteEndpoint\": {\n\"serviceName\": \"apip\"\n},\n\"tags\": {\n\"data.http_response_code\": \"201\"\n}\n}\n]\n</code></pre> <p>With the Collector running, send this payload to the Collector. For example:</p> <pre><code>$ curl -X POST localhost:9411/api/v2/spans -H'Content-Type: application/json' -d @trace.json\n</code></pre> <p>You should see a log entry like the following from the Collector:</p> <pre><code>2020-11-11T04:12:33.089Z    INFO    loggingexporter/logging_exporter.go:296 TraceExporter   {\"#spans\": 1}\n</code></pre> <p>You can also configure the <code>logging</code> exporter so the entire payload is printed:</p> <pre><code>exporters:\nlogging:\nverbosity: detailed\n</code></pre> <p>With the modified configuration if you re-run the test above the log output should look like:</p> <pre><code>2020-11-11T04:08:17.344Z    DEBUG   loggingexporter/logging_exporter.go:353 ResourceSpans #0\nResource labels:\n     -&gt; service.name: Str(api)\nScopeSpans #0\nSpan #0\n    Trace ID       : 5982fe77008310cc80f1da5e10147519\n    Parent ID      : 90394f6bcffb5d13\n    ID             : 67fae42571535f60\n    Name           : /m/n/2.6.1\n    Kind           : SPAN_KIND_SERVER\n    Start time     : 2018-01-24 08:16:15.726 +0000 UTC\n    End time       : 2018-01-24 08:16:15.752 +0000 UTC\nAttributes:\n     -&gt; data.http_response_code: Str(201)\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#health-check","title":"Health Check","text":"<p>The health_check extension, which by default is available on all interfaces on port <code>13133</code>, can be used to ensure the Collector is functioning properly.</p> <pre><code>extensions:\nhealth_check:\nservice:\nextensions: [health_check]\n</code></pre> <p>It returns a response like the following:</p> <pre><code>{\n\"status\": \"Server available\",\n\"upSince\": \"2020-11-11T04:12:31.6847174Z\",\n\"uptime\": \"49.0132518s\"\n}\n</code></pre>"},{"location":"docs/k8s/collector/troubleshooting/#pprof","title":"pprof","text":"<p>The pprof extension, which by default is available locally on port <code>1777</code>, allows you to profile the Collector as it runs. This is an advanced use-case that should not be needed in most circumstances.</p>"},{"location":"docs/k8s/collector/troubleshooting/#common-issues","title":"Common Issues","text":"<p>To see logs for the Collector:</p> <p>On a Linux systemd system, logs can be found using <code>journalctl</code>: <code>journalctl | grep otelcol</code></p> <p>or to find only errors: <code>journalctl | grep otelcol | grep Error</code></p>"},{"location":"docs/k8s/collector/troubleshooting/#collector-exitrestart","title":"Collector exit/restart","text":"<p>The Collector may exit/restart because:</p> <ul> <li>Memory pressure due to missing or misconfigured   memory_limiter   processor.</li> <li>Improperly sized   for load.</li> <li>Improperly configured (for example, a queue size configured higher than   available memory).</li> <li>Infrastructure resource limits (for example Kubernetes).</li> </ul>"},{"location":"docs/k8s/collector/troubleshooting/#data-being-dropped","title":"Data being dropped","text":"<p>Data may be dropped for a variety of reasons, but most commonly because of an:</p> <ul> <li>Improperly sized Collector   resulting in Collector being unable to process and export the data as fast as   it is received.</li> <li>Exporter destination unavailable or accepting the data too slowly.</li> </ul> <p>To mitigate drops, it is highly recommended to configure the batch processor. In addition, it may be necessary to configure the queued retry options on enabled exporters.</p>"},{"location":"docs/k8s/collector/troubleshooting/#receiving-data-not-working","title":"Receiving data not working","text":"<p>If you are unable to receive data then this is likely because either:</p> <ul> <li>There is a network configuration issue</li> <li>The receiver configuration is incorrect</li> <li>The receiver is defined in the <code>receivers</code> section, but not enabled in any   <code>pipelines</code></li> <li>The client configuration is incorrect</li> </ul> <p>Check the Collector logs as well as <code>zpages</code> for potential issues.</p>"},{"location":"docs/k8s/collector/troubleshooting/#processing-data-not-working","title":"Processing data not working","text":"<p>Most processing issues are a result of either a misunderstanding of how the processor works or a misconfiguration of the processor.</p> <p>Examples of misunderstanding include:</p> <ul> <li>The attributes processors only work for \"tags\" on spans. Span name is handled   by the span processor.</li> <li>Processors for trace data (except tail sampling) work on individual spans.</li> </ul>"},{"location":"docs/k8s/collector/troubleshooting/#exporting-data-not-working","title":"Exporting data not working","text":"<p>If you are unable to export to a destination then this is likely because either:</p> <ul> <li>There is a network configuration issue</li> <li>The exporter configuration is incorrect</li> <li>The destination is unavailable</li> </ul> <p>Check the collector logs as well as <code>zpages</code> for potential issues.</p> <p>More often than not, exporting data does not work because of a network configuration issue. This could be due to a firewall, DNS, or proxy issue. Note that the Collector does have proxy support.</p>"},{"location":"docs/k8s/collector/troubleshooting/#startup-failing-in-windows-docker-containers","title":"Startup failing in Windows Docker containers","text":"<p>The process may fail to start in a Windows Docker container with the following error: <code>The service process could not connect to the service controller</code>. In this case the <code>NO_WINDOWS_SERVICE=1</code> environment variable should be set to force the collector to be started as if it were running in an interactive terminal, without attempting to run as a Windows service.</p>"},{"location":"docs/k8s/collector/vision/","title":"\u91c7\u96c6\u5668\u7684\u957f\u671f\u613f\u666f","text":"<p>\u4ee5\u4e0b\u662f\u5b9a\u4e49\u6211\u4eec\u5bf9 OpenTelemetry Collector \u7684\u957f\u671f\u613f\u666f\u7684\u9ad8\u7ea7\u9879\u76ee\uff0c\u6211\u4eec\u6e34\u671b\u5b9e\u73b0\u7684\u76ee \u6807\u3002\u5f53\u6211\u4eec\u8bbe\u8ba1\u65b0\u529f\u80fd\u548c\u5bf9\u6536\u96c6\u5668\u8fdb\u884c\u66f4\u6539\u65f6\uff0c\u8fd9\u4e2a\u613f\u666f\u662f\u6211\u4eec\u7684\u65e5\u5e38\u6307\u5bfc\u3002</p> <p>\u8fd9\u662f\u4e00\u4efd\u6d3b\u7684\u6587\u4ef6\uff0c\u9884\u8ba1\u5c06\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u6f14\u53d8\u3002</p>"},{"location":"docs/k8s/collector/vision/#_2","title":"\u6027\u80fd","text":"<p>\u5728\u5404\u79cd\u8d1f\u8f7d\u4e0b\u90fd\u5177\u6709\u5f88\u9ad8\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002\u5728\u6781\u7aef\u8d1f\u8f7d\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5177\u6709\u53ef\u9884\u6d4b\u7684\u4f4e\u8d44\u6e90\u6d88 \u8017\u3002</p>"},{"location":"docs/k8s/collector/vision/#_3","title":"\u53ef\u89c2\u6d4b\u7684","text":"<p>\u4ee5\u4e00\u79cd\u6e05\u6670\u7684\u65b9\u5f0f\u516c\u5f00\u81ea\u5df1\u7684\u64cd\u4f5c\u6307\u6807\u3002\u6210\u4e3a\u53ef\u89c2\u5bdf\u670d\u52a1\u7684\u5178\u8303\u3002\u5141\u8bb8\u914d\u7f6e\u53ef\u89c2\u5bdf\u6027\u7ea7\u522b( \u6216\u591a\u6216\u5c11\u7684\u5ea6\u91cf\u3001\u8ddf\u8e2a\u3001\u65e5\u5fd7\u7b49\u62a5\u544a)\u3002\u53c2\u89c1\u66f4\u591a\u7ec6\u8282\u3002</p>"},{"location":"docs/k8s/collector/vision/#_4","title":"\u5177\u6709\u6570\u636e","text":"<p>\u652f\u6301\u8ddf\u8e2a\u3001\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u5176\u4ed6\u76f8\u5173\u6570\u636e\u7c7b\u578b\u3002</p>"},{"location":"docs/k8s/collector/vision/#_5","title":"\u53ef\u7528\u7684\u5f00\u7bb1\u5373\u7528","text":"<p>\u5408\u7406\u7684\u9ed8\u8ba4\u914d\u7f6e\uff0c\u652f\u6301\u6d41\u884c\u7684\u534f\u8bae\uff0c\u5f00\u7bb1\u5373\u7528\u7684\u8fd0\u884c\u548c\u6536\u96c6\u3002</p>"},{"location":"docs/k8s/collector/vision/#_6","title":"\u53ef\u6269\u5c55\u7684","text":"<p>\u53ef\u6269\u5c55\u548c\u81ea\u5b9a\u4e49\uff0c\u65e0\u9700\u89e6\u53ca\u6838\u5fc3\u4ee3\u7801\u3002\u53ef\u4ee5\u57fa\u4e8e\u6838\u5fc3\u521b\u5efa\u81ea\u5b9a\u4e49\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u81ea\u5df1\u7684\u7ec4\u4ef6\u8fdb \u884c\u6269\u5c55\u3002\u6b22\u8fce\u7b2c\u4e09\u65b9\u8d21\u732e\u653f\u7b56\u3002</p>"},{"location":"docs/k8s/collector/vision/#_7","title":"\u7edf\u4e00\u7684\u4ee3\u7801\u5e93","text":"<p>\u4e00\u4e2a\u7528\u4e8e\u5b88\u62a4\u8fdb\u7a0b(Agent)\u548c\u72ec\u7acb\u670d\u52a1(Collector)\u7684\u4ee3\u7801\u5e93\u3002</p> <p>\u6709\u5173\u6211\u4eec\u8ba1\u5212\u5982\u4f55\u5b9e\u73b0\u8fd9\u4e00\u613f\u666f\u7684\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2\u9605\u8def\u7ebf\u56fe\u3002</p>"},{"location":"docs/k8s/operator/","title":"Index","text":""},{"location":"docs/k8s/operator/#opentelemetry-operator-for-kubernetes","title":"OpenTelemetry Operator for Kubernetes","text":"<p>OpenTelemetry Operator \u662fKubernetes Operator\u7684 \u4e00\u4e2a\u5b9e\u73b0\u3002</p> <p>Operator \u7ba1\u7406:</p> <ul> <li>OpenTelemetry Collector</li> <li>\u4f7f\u7528 OpenTelemetry \u5de5\u5177\u5e93\u81ea\u52a8\u68c0\u6d4b\u5de5\u4f5c\u8d1f\u8f7d</li> </ul>"},{"location":"docs/k8s/operator/#_1","title":"\u6587\u6863","text":"<ul> <li>API docs</li> </ul>"},{"location":"docs/k8s/operator/#helm-charts","title":"Helm Charts","text":"<p>\u60a8\u53ef\u4ee5\u901a \u8fc7Helm Chart\u4ece opentelemetry-helm-charts \u5b58\u50a8\u5e93\u5b89\u88c5 Opentelemetry Operator\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u8bbf \u95ee\u8fd9\u91cc\u3002</p>"},{"location":"docs/k8s/operator/#_2","title":"\u5165\u95e8","text":"<p>\u8981\u5728\u73b0\u6709\u96c6\u7fa4\u4e2d\u5b89\u88c5\u64cd\u4f5c\u5668\uff0c\u8bf7\u786e\u4fdd\u5b89\u88c5 \u4e86<code>cert-manager</code>\u5e76\u8fd0\u884c:</p> <pre><code>kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml\n</code></pre> <p>\u4e00\u65e6<code>opentelemetry-operator</code>\u90e8\u7f72\u5c31\u7eea\u5c31\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a OpenTelemetry Collector (otelcol)\u5b9e\u4f8b\uff0c\u5982\u4e0b\u6240\u793a:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: simplest\nspec:\nconfig: |\nreceivers:\notlp:\nprotocols:\ngrpc:\nhttp:\nprocessors:\nmemory_limiter:\ncheck_interval: 1s\nlimit_percentage: 75\nspike_limit_percentage: 15\nbatch:\nsend_batch_size: 10000\ntimeout: 10s\nexporters:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [otlp]\nprocessors: []\nexporters: [logging]\nEOF\n</code></pre> <p>Warning</p> <p>\u5728 OpenTelemetry Collector \u683c\u5f0f\u7a33\u5b9a\u4e4b\u524d\uff0c\u53ef\u80fd\u9700\u8981\u5728\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\u8fdb\u884c\u66f4\u6539\uff0c \u4ee5\u4fdd\u6301\u4e0e\u6240\u5f15\u7528\u7684 OpenTelemetry Collector \u6620\u50cf\u7684\u6700\u65b0\u7248\u672c\u517c\u5bb9\u3002</p> <p>\u8fd9\u5c06\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a<code>simplest</code>\u7684 OpenTelemetry Collector \u5b9e\u4f8b\uff0c\u66b4\u9732\u4e00 \u4e2a<code>jaeger-grpc</code>\u7aef\u53e3\uff0c\u4ee5\u4ece\u63d2\u88c5\u5316\u7684\u5e94\u7528\u7a0b\u5e8f\u4e2d\u6d88\u8d39 <code>span</code> \uff0c\u5e76\u901a\u8fc7<code>logging</code>\u5bfc\u51fa\u8fd9\u4e9b <code>span</code>\uff0c\u8fd9\u5c06<code>span</code>\u5199\u5165\u63a5\u6536<code>span</code>\u7684 OpenTelemetry Collector \u5b9e\u4f8b\u7684\u63a7\u5236\u53f0 (<code>stdout</code>)\u3002</p> <p><code>config</code>\u8282\u70b9\u4fdd\u5b58\u5e94\u8be5\u6309\u539f\u6837\u4f20\u9012\u7ed9\u5e95\u5c42 OpenTelemetry Collector \u5b9e\u4f8b\u7684<code>YAML</code>\u3002\u8bf7\u53c2 \u9605OpenTelemetry Collector\u6587 \u6863\u4ee5\u83b7\u53d6\u53ef\u80fd\u6761\u76ee\u7684\u53c2\u8003\u3002</p> <p>\u6b64\u65f6\uff0cOperator \u5e76\u4e0d\u9a8c\u8bc1\u914d\u7f6e\u6587\u4ef6\u7684\u5185\u5bb9:\u5982\u679c\u914d\u7f6e\u65e0\u6548\uff0c\u5219\u4ecd\u7136\u4f1a\u521b\u5efa\u5b9e\u4f8b\uff0c\u4f46\u662f\u5e95\u5c42\u7684 OpenTelemetry Collector \u53ef\u80fd\u4f1a\u5d29\u6e83\u3002</p> <p>\u64cd\u4f5c\u5458\u68c0\u67e5\u914d\u7f6e\u6587\u4ef6\u4ee5\u53d1\u73b0\u5df2\u914d\u7f6e\u7684\u63a5\u6536\u5668\u53ca\u5176\u7aef\u53e3\u3002\u5982\u679c\u5b83\u627e\u5230\u5177\u6709\u7aef\u53e3\u7684\u63a5\u6536\u5668\uff0c\u5b83\u5c06 \u521b\u5efa\u4e00\u5bf9 kubernetes \u670d\u52a1\uff0c\u5176\u4e2d\u4e00\u4e2a\u662f\u65e0\u5934\u7684\uff0c\u5728\u96c6\u7fa4\u4e2d\u516c\u5f00\u8fd9\u4e9b\u7aef\u53e3\u3002\u65e0\u5934\u670d\u52a1\u5305\u542b\u4e00 \u4e2a<code>service.beta.openshift.io/serving-cert-secret-name</code>\u6ce8\u91ca\uff0c\u5b83\u5c06\u5bfc\u81f4 OpenShift \u521b\u5efa\u4e00\u4e2a\u5305\u542b\u8bc1\u4e66\u548c\u5bc6\u94a5\u7684\u79d8\u5bc6\u3002\u8fd9\u4e2a\u79d8\u5bc6\u53ef\u4ee5\u4f5c\u4e3a\u5377\u3001\u8bc1\u4e66\u548c\u5bc6\u94a5\u6302\u8f7d\u5728\u8fd9\u4e9b\u63a5\u6536\u8005\u7684 TLS \u914d\u7f6e\u4e2d\u3002</p>"},{"location":"docs/k8s/operator/#_3","title":"\u66f4\u65b0","text":"<p>\u5982\u4e0a\u6240\u8ff0\uff0cOpenTelemetry Collector \u683c\u5f0f\u6b63\u5728\u7ee7\u7eed\u53d1\u5c55\u3002\u4f46\u662f\uff0c\u5c3d\u6700\u5927\u52aa\u529b\u5c1d\u8bd5\u5347\u7ea7\u6240\u6709 \u6258\u7ba1\u7684<code>OpenTelemetryCollector</code>\u8d44\u6e90\u3002</p> <p>\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u5e0c\u671b\u9632\u6b62\u64cd\u4f5c\u7b26\u5347\u7ea7\u67d0\u4e9b<code>OpenTelemetryCollector</code>\u8d44\u6e90\u3002\u4f8b\u5982\uff0c\u5f53\u4e00 \u4e2a\u8d44\u6e90\u88ab\u914d\u7f6e\u4e3a\u81ea\u5b9a\u4e49\u7684<code>.Spec.Image</code>\u65f6\uff0c\u6700\u7ec8\u7528\u6237\u53ef\u80fd\u5e0c\u671b\u81ea\u5df1\u7ba1\u7406\u914d\u7f6e\uff0c\u800c\u4e0d\u662f\u8ba9\u64cd \u4f5c\u5458\u5347\u7ea7\u5b83\u3002\u8fd9\u53ef\u4ee5\u901a\u8fc7\u516c\u5f00\u7684\u5c5e\u6027<code>.Spec.UpgradeStrategy</code>\u5728\u8d44\u6e90\u7684\u57fa\u7840\u4e0a\u914d\u7f6e\u3002</p> <p>\u901a\u8fc7\u5c06\u8d44\u6e90\u7684<code>.Spec.UpgradeStrategy</code>\u914d\u7f6e\u4e3a none\uff0c\u64cd\u4f5c\u7b26\u5c06\u5728\u5347\u7ea7\u4f8b\u7a0b\u4e2d\u8df3\u8fc7\u7ed9\u5b9a\u7684\u5b9e \u4f8b\u3002</p> <p><code>.Spec.UpgradeStrategy</code>\u7684\u9ed8\u8ba4\u503c\u548c\u552f\u4e00\u53ef\u63a5\u53d7\u7684\u503c\u662f<code>automatic</code>\u3002</p>"},{"location":"docs/k8s/operator/#_4","title":"\u90e8\u7f72\u6a21\u5f0f","text":"<p><code>OpenTelemetryCollector</code>\u7684<code>CustomResource</code>\u66b4\u9732\u4e86\u4e00\u4e2a\u540d\u4e3a<code>.Spec.Mode</code>\u7684\u5c5e\u6027\uff0c\u8be5\u5c5e \u6027\u53ef\u7528\u4e8e\u6307\u5b9a\u6536\u96c6\u5668\u662f\u5426\u5e94\u8be5\u4f5c\u4e3a<code>DaemonSet</code>, <code>Sidecar</code>, or <code>Deployment</code>(\u9ed8\u8ba4)\u8fd0\u884c \u3002\u8bf7\u770b\u8fd9\u4e2a\u4f8b\u5b50\u4f5c\u4e3a\u53c2\u8003\u3002</p>"},{"location":"docs/k8s/operator/#_5","title":"\u9644\u63a5\u76d2\u6ce8\u5165","text":"<p>\u901a\u8fc7\u5c06 Pod \u6ce8\u91ca<code>sidecar.opentelemetry.io/inject</code>\u8bbe\u7f6e\u4e3a<code>\"true\"</code>\uff0c\u6216\u8005\u8bbe\u7f6e\u4e3a\u5177\u4f53 \u7684<code>OpenTelemetryCollector</code>\u7684\u540d\u79f0\uff0c\u53ef\u4ee5\u5c06\u5e26\u6709 OpenTelemetryCollector \u7684 sidecar \u6ce8\u5165\u5230\u57fa\u4e8e Pod \u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\uff0c\u5982\u4e0b\u6240\u793a:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: sidecar-for-my-app\nspec:\nmode: sidecar\nconfig: |\nreceivers:\njaeger:\nprotocols:\nthrift_compact:\nprocessors:\nexporters:\nlogging:\nservice:\npipelines:\ntraces:\nreceivers: [jaeger]\nprocessors: []\nexporters: [logging]\nEOF\nkubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\nname: myapp\nannotations:\nsidecar.opentelemetry.io/inject: \"true\"\nspec:\ncontainers:\n- name: myapp\nimage: jaegertracing/vertx-create-span:operator-e2e-tests\nports:\n- containerPort: 8080\nprotocol: TCP\nEOF\n</code></pre> <p>\u5f53\u5728\u540c\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u4e2d\u6709\u591a\u4e2a\u6a21\u5f0f\u8bbe\u7f6e\u4e3a<code>Sidecar</code>\u7684<code>OpenTelemetryCollector</code> \u8d44\u6e90\u65f6\uff0c \u5e94\u8be5\u4f7f\u7528\u4e00\u4e2a\u5177\u4f53\u7684\u540d\u79f0\u3002\u5f53\u540c\u4e00\u4e2a\u547d\u540d\u7a7a\u95f4\u4e2d\u53ea\u6709\u4e00\u4e2a<code>Sidecar</code>\u5b9e\u4f8b\u65f6\uff0c\u5f53\u6ce8\u91ca\u88ab\u8bbe\u7f6e \u4e3a<code>\"true\"</code>\u65f6\u4f7f\u7528\u8be5\u5b9e\u4f8b\u3002</p> <p>\u6ce8\u91ca\u503c\u53ef\u4ee5\u6765\u81ea\u540d\u79f0\u7a7a\u95f4\uff0c\u4e5f\u53ef\u4ee5\u6765\u81ea pod\u3002\u6700\u5177\u4f53\u7684\u6ce8\u91ca\u80dc\u51fa\uff0c\u987a\u5e8f\u5982\u4e0b:</p> <ul> <li>pod \u6ce8\u91ca\u88ab\u8bbe\u7f6e\u4e3a\u5177\u4f53\u7684\u5b9e\u4f8b\u540d\u6216<code>\"false\"</code>\u65f6\u4f7f\u7528\u3002</li> <li>\u5f53 pod \u6ce8\u91ca\u4e0d\u5b58\u5728\u6216\u8bbe\u7f6e\u4e3a<code>\"true\"</code>\uff0c\u800c\u547d\u540d\u7a7a\u95f4\u8bbe\u7f6e\u4e3a\u5177\u4f53\u5b9e\u4f8b\u6216\u8bbe\u7f6e\u4e3a<code>\"false\"</code>\u65f6   \uff0c\u4f7f\u7528\u547d\u540d\u7a7a\u95f4\u6ce8\u91ca\u3002</li> </ul> <p>\u6ce8\u91ca\u7684\u53ef\u80fd\u503c\u53ef\u4ee5\u662f:</p> <ul> <li>\"true\" - inject <code>OpenTelemetryCollector</code> resource from the namespace.</li> <li>\"sidecar-for-my-app\" - name of <code>OpenTelemetryCollector</code> CR instance in the   current namespace.</li> <li>\"my-other-namespace/my-instrumentation\" - name and namespace of   <code>OpenTelemetryCollector</code> CR instance in another namespace.</li> <li>\"false\" - do not inject</li> </ul> <p>\u5f53\u4f7f\u7528\u57fa\u4e8e pod \u7684\u5de5\u4f5c\u8d1f\u8f7d\u65f6\uff0c\u4f8b\u5982<code>Deployment</code> \u6216 <code>Statefulset</code>\uff0c\u8bf7\u786e\u4fdd\u5c06\u6ce8\u91ca\u6dfb\u52a0 \u5230<code>PodTemplate</code>\u90e8\u5206\u3002\u5982:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: my-app\nlabels:\napp: my-app\nannotations:\nsidecar.opentelemetry.io/inject: \"true\" # WRONG\nspec:\nselector:\nmatchLabels:\napp: my-app\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: my-app\nannotations:\nsidecar.opentelemetry.io/inject: \"true\" # CORRECT\nspec:\ncontainers:\n- name: myapp\nimage: jaegertracing/vertx-create-span:operator-e2e-tests\nports:\n- containerPort: 8080\nprotocol: TCP\nEOF\n</code></pre> <p>\u5f53\u4f7f\u7528 sidecar \u6a21\u5f0f\u65f6\uff0cOpenTelemetry \u6536\u96c6\u5668\u5bb9\u5668\u5c06\u4f7f\u7528 Kubernetes \u8d44\u6e90\u5c5e\u6027\u8bbe\u7f6e\u73af \u5883\u53d8\u91cf<code>OTEL_RESOURCE_ATTRIBUTES</code>\uff0c\u51c6\u5907 \u7531resourcedetection\u5904 \u7406\u5668\u4f7f\u7528\u3002</p>"},{"location":"docs/k8s/operator/#opentelemetry","title":"OpenTelemetry \u81ea\u52a8\u63d2\u88c5\u6ce8\u5165","text":"<p>\u64cd\u4f5c\u5458\u53ef\u4ee5\u6ce8\u5165\u548c\u914d\u7f6e OpenTelemetry \u81ea\u52a8\u4eea\u5668\u5e93\u3002\u76ee\u524d\u652f\u6301 Apache HTTPD, DotNet, Go, Java, NodeJS \u548c Python\u3002</p> <p>\u8981\u4f7f\u7528\u81ea\u52a8\u68c0\u6d4b\uff0c\u8bf7\u914d\u7f6e\u4e00\u4e2a\u5e26\u6709 SDK \u548c\u68c0\u6d4b\u914d\u7f6e\u7684 <code>Instrumentation</code> \u8d44\u6e90\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: my-instrumentation\nspec:\nexporter:\nendpoint: http://otel-collector:4317\npropagators:\n- tracecontext\n- baggage\n- b3\nsampler:\ntype: parentbased_traceidratio\nargument: \"0.25\"\npython:\nenv:\n# Required if endpoint is set to 4317.\n# Python autoinstrumentation uses http/proto by default\n# so data must be sent to 4318 instead of 4317.\n- name: OTEL_EXPORTER_OTLP_ENDPOINT\nvalue: http://otel-collector:4318\ndotnet:\nenv:\n# Required if endpoint is set to 4317.\n# Dotnet autoinstrumentation uses http/proto by default\n# See https://github.com/open-telemetry/opentelemetry-dotnet-instrumentation/blob/888e2cd216c77d12e56b54ee91dafbc4e7452a52/docs/config.md#otlp\n- name: OTEL_EXPORTER_OTLP_ENDPOINT\nvalue: http://otel-collector:4318\nEOF\n</code></pre> <p><code>propagators</code>\u7684\u503c\u88ab\u6dfb\u52a0\u5230<code>OTEL_PROPAGATORS</code>\u73af\u5883\u53d8\u91cf\u4e2d\u3002 <code>\u4f20\u64ad\u5668</code>\u7684\u6709\u6548\u503c \u7531OpenTelemetry Specification for OTEL_PROPAGATORS\u5b9a \u4e49\u3002</p> <p><code>sampler.type</code>\u7684\u503c\u88ab\u6dfb\u52a0\u5230<code>OTEL_TRACES_SAMPLER</code>\u73af\u5883\u53d8\u91cf\u4e2d\u3002 <code>sampler.type</code>\u7684\u6709\u6548 \u503c \u7531OTEL_TRACES_SAMPLER \u7684 OpenTelemetry \u89c4\u8303\u5b9a \u4e49\u3002 <code>sampler.argument</code>\u7684\u503c\u88ab\u6dfb\u52a0\u5230<code>OTEL_TRACES_SAMPLER_ARG</code>\u73af\u5883\u53d8\u91cf\u4e2d\u3002 <code>sampler.argument</code>\u7684\u6709\u6548\u503c\u5c06\u53d6\u51b3\u4e8e\u6240\u9009\u62e9\u7684\u91c7\u6837\u5668\u3002\u8bf7\u53c2 \u9605OTEL_TRACES_SAMPLER_ARG \u7684 OpenTelemetry \u89c4\u8303\u4e86 \u89e3\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002</p> <p>\u4ee5\u4e0a CR \u53ef\u4ee5\u901a\u8fc7 <code>kubectl get otelinst</code> \u67e5\u8be2\u3002</p> <p>\u7136\u540e\u5411 pod \u6dfb\u52a0\u6ce8\u91ca\u4ee5\u542f\u7528\u6ce8\u5165\u3002\u53ef\u4ee5\u5c06\u6ce8\u91ca\u6dfb\u52a0\u5230\u540d\u79f0\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u4fbf\u8be5\u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u6240 \u6709 pod \u90fd\u5c06\u83b7\u5f97\u68c0\u6d4b\uff0c\u6216\u8005\u5c06\u6ce8\u91ca\u6dfb\u52a0\u5230\u5355\u72ec\u7684 PodSpec \u5bf9\u8c61\u4e2d\uff0c\u8fd9\u4e9b\u5bf9\u8c61\u53ef\u4ee5\u4f5c\u4e3a Deployment\u3001Statefulset \u548c\u5176\u4ed6\u8d44\u6e90\u7684\u4e00\u90e8\u5206\u4f7f\u7528\u3002</p> <p>Java:</p> <pre><code>instrumentation.opentelemetry.io/inject-java: \"true\"\n</code></pre> <p>NodeJS:</p> <pre><code>instrumentation.opentelemetry.io/inject-nodejs: \"true\"\n</code></pre> <p>Python:</p> <pre><code>instrumentation.opentelemetry.io/inject-python: \"true\"\n</code></pre> <p>DotNet:</p> <pre><code>instrumentation.opentelemetry.io/inject-dotnet: \"true\"\n</code></pre> <p>Go:</p> <p>Go auto-instrumentation \u8fd8\u652f\u6301\u7528\u4e8e\u8bbe \u7f6eOTEL_GO_AUTO_TARGET_EXE env var\u7684 \u6ce8\u91ca\u3002\u8fd9\u4e2a env \u53d8\u91cf\u4e5f\u53ef\u4ee5\u901a\u8fc7 Instrumentation \u8d44\u6e90\u8bbe\u7f6e\uff0c\u6ce8\u91ca\u4f18\u5148\u3002\u7531\u4e8e Go \u81ea\u52a8\u68c0 \u6d4b\u9700\u8981\u8bbe\u7f6e<code>OTEL_GO_AUTO_TARGET_EXE</code>\uff0c\u56e0\u6b64\u60a8\u5fc5\u987b\u901a\u8fc7\u6ce8\u91ca\u6216 Instrumentation \u8d44\u6e90\u63d0 \u4f9b\u6709\u6548\u7684\u53ef\u6267\u884c\u8def\u5f84\u3002\u8bbe\u7f6e\u6b64\u503c\u5931\u8d25\u5c06\u5bfc\u81f4\u4eea\u5668\u6ce8\u5165\u4e2d\u6b62\uff0c\u4f7f\u539f\u59cb pod \u4fdd\u6301\u4e0d\u53d8\u3002</p> <pre><code>instrumentation.opentelemetry.io/inject-go: \"true\"\ninstrumentation.opentelemetry.io/otel-go-auto-target-exe: \"/path/to/container/executable\"\n</code></pre> <p>Apache HTTPD:</p> <pre><code>instrumentation.opentelemetry.io/inject-apache-httpd: \"true\"\n</code></pre> <p>OpenTelemetry SDK \u73af\u5883\u53d8\u91cf:</p> <pre><code>instrumentation.opentelemetry.io/inject-sdk: \"true\"\n</code></pre> <p>\u6ce8\u91ca\u7684\u53ef\u80fd\u503c\u53ef\u4ee5\u662f</p> <ul> <li><code>\"true\"</code> - \u4ece\u547d\u540d\u7a7a\u95f4\u6ce8\u5165\u548c <code>Instrumentation</code> \u8d44\u6e90\u3002</li> <li><code>\"my-instrumentation\"</code> - \u5f53\u524d\u547d\u540d\u7a7a\u95f4\u4e2d<code>Instrumentation</code>CR \u5b9e\u4f8b\u7684\u540d\u79f0\u3002</li> <li><code>\"my-other-namespace/my-instrumentation\"</code> - <code>Instrumentation</code> CR \u5b9e\u4f8b\u5728\u53e6\u4e00\u4e2a   \u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u540d\u79f0\u548c\u540d\u79f0\u7a7a\u95f4\u3002</li> <li><code>\"false\"</code> - \u4e0d\u8981\u6ce8\u5c04</li> </ul>"},{"location":"docs/k8s/operator/#pod","title":"\u591a\u5bb9\u5668 Pod","text":"<p>\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u5176\u4ed6\u5185\u5bb9\uff0c\u5219\u5728 pod \u89c4\u8303\u4e2d\u53ef\u7528\u7684\u7b2c\u4e00\u4e2a\u5bb9\u5668\u4e0a\u6267\u884c\u68c0\u6d4b\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b(\u4f8b \u5982\u5728\u6ce8\u5165 Istio sidecar \u7684\u60c5\u51b5\u4e0b)\uff0c\u6709\u5fc5\u8981\u6307\u5b9a\u5fc5\u987b\u5728\u54ea\u4e2a\u5bb9\u5668\u4e0a\u6267\u884c\u6b64\u6ce8\u5165\u3002</p> <p>\u4e3a\u6b64\uff0c\u6709\u53ef\u80fd\u5bf9\u5c06\u8fdb\u884c\u6ce8\u5c04\u7684\u540a\u8231\u8fdb\u884c\u5fae\u8c03\u3002</p> <p>\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4f7f\u7528<code>instrumentation.opentelemetry.io/container-names</code>\u6ce8\u91ca\uff0c\u6211\u4eec\u5c06\u4e3a \u5176\u6307\u5b9a\u4e00\u4e2a\u6216\u591a\u4e2a\u5fc5\u987b\u8fdb\u884c\u6ce8\u5165\u7684 pod \u540d\u79f0(<code>.spec.containers.name</code>):</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: my-deployment-with-multiple-containers\nspec:\nselector:\nmatchLabels:\napp: my-pod-with-multiple-containers\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: my-pod-with-multiple-containers\nannotations:\ninstrumentation.opentelemetry.io/inject-java: 'true'\ninstrumentation.opentelemetry.io/container-names: 'myapp,myapp2'\nspec:\ncontainers:\n- name: myapp\nimage: myImage1\n- name: myapp2\nimage: myImage2\n- name: myapp3\nimage: myImage3\n</code></pre> <p>\u5728\u4e0a\u8ff0\u60c5\u51b5\u4e0b\uff0c<code>myapp</code>\u548c<code>myapp2</code>\u5bb9\u5668\u5c06\u88ab\u68c0\u6d4b\uff0c<code>myapp3</code>\u4e0d\u4f1a\u3002</p> <p>Note</p> <p>Go \u7684\u81ea\u52a8\u68c0\u6d4b \u4e0d \u652f\u6301\u591a\u5bb9\u5668 pod\u3002\u5f53\u6ce8\u5165 Go \u81ea\u52a8\u68c0\u6d4b\u65f6\uff0c\u7b2c\u4e00\u4e2a pod \u5e94\u8be5\u662f\u4f60\u60f3\u8981\u68c0\u6d4b\u7684\u552f\u4e00 pod\u3002</p>"},{"location":"docs/k8s/operator/#_6","title":"\u4f7f\u7528\u5b9a\u5236\u7684\u6216\u4f9b\u5e94\u5546\u7684\u5de5\u5177","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u64cd\u4f5c\u7b26\u4f7f\u7528\u4e0a\u6e38\u81ea\u52a8\u63d2\u88c5\u5e93\u3002\u53ef\u4ee5\u901a\u8fc7\u8986\u76d6 CR \u4e2d\u7684\u6620\u50cf\u5b57\u6bb5\u6765\u914d\u7f6e\u81ea\u5b9a\u4e49\u81ea \u52a8\u68c0\u6d4b\u3002</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: my-instrumentation\nspec:\njava:\nimage: your-customized-auto-instrumentation-image:java\nnodejs:\nimage: your-customized-auto-instrumentation-image:nodejs\npython:\nimage: your-customized-auto-instrumentation-image:python\ndotnet:\nimage: your-customized-auto-instrumentation-image:dotnet\ngo:\nimage: your-customized-auto-instrumentation-image:go\napacheHttpd:\nimage: your-customized-auto-instrumentation-image:apache-httpd\n</code></pre> <p>\u81ea\u52a8\u68c0\u6d4b\u7684 Dockerfiles \u53ef\u4ee5\u5728autoinstrumentation \u76ee\u5f55\u4e2d \u627e\u5230\u3002\u6309\u7167 Dockerfiles \u4e2d\u7684\u8bf4\u660e\u6765\u6784\u5efa\u81ea\u5b9a\u4e49\u5bb9\u5668\u6620\u50cf\u3002</p>"},{"location":"docs/k8s/operator/#apache-httpd","title":"\u4f7f\u7528 Apache HTTPD \u81ea\u52a8\u68c0\u6d4b","text":"<p>\u5bf9\u4e8e<code>Apache HTTPD</code> \u81ea\u52a8\u68c0\u6d4b\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u68c0\u6d4b\u5047\u8bbe HTTPD \u7248\u672c 2.4 \u548c HTTPD \u914d\u7f6e\u76ee \u5f55<code>/usr/local/apache2/conf</code>\u4e3a\u5b83\u5728\u5b98\u65b9\u7684 <code>Apache HTTPD</code> \u955c\u50cf\u4e2d(\u53c2\u89c1 docker.io/httpd:latest)\u3002\u5982\u679c\u60a8\u9700\u8981\u4f7f\u7528 2.2 \u7248\u672c\uff0c\u6216\u8005\u60a8\u7684 HTTPD \u914d\u7f6e\u76ee\u5f55\u4e0d\u540c\uff0c \u6216\u8005\u60a8\u9700\u8981\u8c03\u6574\u4ee3\u7406\u5c5e\u6027\uff0c\u8bf7\u6839\u636e\u4ee5\u4e0b\u793a\u4f8b\u81ea\u5b9a\u4e49\u5de5\u5177\u89c4\u8303:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: my-instrumentation\napache:\nimage: your-customized-auto-instrumentation-image:apache-httpd\nversion: 2.2\nconfigPath: /your-custom-config-path\nattrs:\n- name: ApacheModuleOtelMaxQueueSize\nvalue: '4096'\n- name: ...\nvalue: ...\n</code></pre> <p>\u6240\u6709\u53ef\u7528\u5c5e\u6027\u7684\u5217\u8868\u53ef\u4ee5 \u5728otel-webserver-module\u627e \u5230</p>"},{"location":"docs/k8s/operator/#opentelemetry-sdk","title":"\u53ea\u6ce8\u5165 OpenTelemetry SDK \u73af\u5883\u53d8\u91cf","text":"<p>\u4f60\u53ef\u4ee5\u4e3a\u76ee\u524d\u4e0d\u80fd\u81ea\u52a8\u68c0\u6d4b\u7684\u5e94\u7528\u914d\u7f6e OpenTelemetry SDK\uff0c\u901a\u8fc7\u4f7f\u7528<code>inject-sdk</code>\u4ee3\u66ff( \u4f8b\u5982)<code>inject-python</code>\u6216<code>inject-java</code>\u3002\u8fd9\u5c06\u6ce8\u5165\u73af\u5883\u53d8\u91cf\uff0c \u5982<code>OTEL_RESOURCE_ATTRIBUTES</code>\uff0c <code>OTEL_TRACES_SAMPLER</code>\u548c<code>OTEL_EXPORTER_OTLP_ENDPOINT</code>\uff0c\u60a8\u53ef\u4ee5 \u5728<code>Instrumentation</code>\u4e2d\u914d\u7f6e\uff0c\u4f46\u5b9e\u9645\u4e0a\u4e0d\u4f1a\u63d0\u4f9b SDK\u3002</p> <pre><code>instrumentation.opentelemetry.io/inject-sdk: \"true\"\n</code></pre>"},{"location":"docs/k8s/operator/#_7","title":"\u63a7\u5236\u63d2\u88c5\u529f\u80fd","text":"<p>\u64cd\u4f5c\u7b26\u5141\u8bb8\u901a\u8fc7\u7279\u5f81\u95e8\u6307\u5b9a Instrumentation \u8d44\u6e90\u53ef\u4ee5\u68c0\u6d4b\u7684\u8bed\u8a00\u3002\u8fd9\u4e9b\u7279\u5f81\u95e8\u5fc5\u987b\u901a\u8fc7 <code>--feature-gates</code>\u6807\u5fd7\u4f20\u9012\u7ed9\u64cd\u4f5c\u7b26\u3002\u8be5\u6807\u5fd7\u5141\u8bb8\u4ee5\u9017\u53f7\u5206\u9694\u7684\u7279\u5f81\u95e8\u6807\u8bc6\u7b26\u5217\u8868\u3002\u5728\u95e8 \u7684\u524d\u9762\u52a0\u4e0a'-'\u6765\u7981\u7528\u5bf9\u76f8\u5e94\u8bed\u8a00\u7684\u652f\u6301\u3002\u7528'+'\u4f5c\u4e3a\u95e8\u7684\u524d\u7f00\u6216\u4e0d\u52a0\u524d\u7f00\u5c06\u542f\u7528\u5bf9\u76f8\u5e94\u8bed\u8a00 \u7684\u652f\u6301\u3002\u5982\u679c\u4e00\u79cd\u8bed\u8a00\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u662f\u542f\u7528\u7684\uff0c\u5b83\u7684\u95e8\u53ea\u9700\u8981\u5728\u7981\u7528\u95e8\u65f6\u63d0\u4f9b\u3002</p> Language Gate Default Value Java <code>operator.autoinstrumentation.java</code> enabled NodeJS <code>operator.autoinstrumentation.nodejs</code> enabled Python <code>operator.autoinstrumentation.python</code> enabled DotNet <code>operator.autoinstrumentation.dotnet</code> enabled ApacheHttpD <code>operator.autoinstrumentation.apache-httpd</code> enabled Go <code>operator.autoinstrumentation.go</code> disabled <p>\u59cb\u7ec8\u652f\u6301\u672a\u5728\u8868\u4e2d\u6307\u5b9a\u7684\u8bed\u8a00\uff0c\u5e76\u4e14\u4e0d\u80fd\u7981\u7528\u3002</p>"},{"location":"docs/k8s/operator/#_8","title":"\u76ee\u6807\u5206\u914d\u7a0b\u5e8f","text":"<p>OpenTelemetry Operator \u5e26\u6709\u4e00\u4e2a\u53ef\u9009\u7ec4\u4ef6\uff0c\u5373\u76ee\u6807\u5206\u914d\u5668(Target Allocator, TA)\u3002\u5f53 \u521b\u5efa OpenTelemetryCollector \u81ea\u5b9a\u4e49\u8d44\u6e90(CR)\u5e76\u5c06 TA \u8bbe\u7f6e\u4e3a\u542f\u7528\u65f6\uff0c\u64cd\u4f5c\u5458\u5c06\u521b\u5efa\u4e00\u4e2a \u65b0\u7684\u90e8\u7f72\u548c\u670d\u52a1\uff0c\u4ee5\u4f5c\u4e3a\u8be5 CR \u7684\u4e00\u90e8\u5206\u4e3a\u6bcf\u4e2a Collector pod \u63d0\u4f9b\u7279\u5b9a \u7684<code>http_sd_config</code>\u6307\u4ee4\u3002\u5b83\u8fd8\u5c06\u66f4\u6539 CR \u4e2d\u7684 Prometheus \u63a5\u6536\u5668\u914d\u7f6e\uff0c\u4ee5\u4fbf\u5b83\u4f7f\u7528\u6765\u81ea TA \u7684http_sd_config\u3002\u4e0b \u9762\u7684\u4f8b\u5b50\u5c55\u793a\u4e86\u5982\u4f55\u5f00\u59cb\u4f7f\u7528 Target Allocator:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: collector-with-ta\nspec:\nmode: statefulset\ntargetAllocator:\nenabled: true\nconfig: |\nreceivers:\nprometheus:\nconfig:\nscrape_configs:\n- job_name: 'otel-collector'\nscrape_interval: 10s\nstatic_configs:\n- targets: [ '0.0.0.0:8888' ]\nmetric_relabel_configs:\n- action: labeldrop\nregex: (id|name)\nreplacement: $$1\n- action: labelmap\nregex: label_(.+)\nreplacement: $$1 \nexporters:\nlogging:\nservice:\npipelines:\nmetrics:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre> <p>\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u66ff\u6362\u952e\u4e2d<code>$$</code>\u7684\u7528\u6cd5\u662f\u57fa\u4e8e Prometheus \u63a5\u6536 \u5668README\u6587 \u6863\u4e2d\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u8be5\u6587\u6863\u8bf4\u660e: <code>Note: \u7531\u4e8e\u6536\u96c6\u5668\u914d\u7f6e\u652f\u6301\u73af\u5883\u53d8\u91cf\u66ff\u6362\uff0c\u666e\u7f57\u7c73\u4fee\u65af\u914d\u7f6e\u4e2d\u7684$\u5b57\u7b26\u88ab\u89e3\u91ca\u4e3a\u73af\u5883\u53d8\u91cf\u3002\u5982\u679c\u60f3\u5728prometheus\u914d\u7f6e\u4e2d\u4f7f\u7528$\u5b57\u7b26\uff0c\u5fc5\u987b\u4f7f\u7528$$\u8f6c\u4e49\u3002</code></p> <p>\u5728\u5e55\u540e\uff0cOpenTelemetry \u64cd\u4f5c\u7b26\u5c06\u5728\u5bf9\u8d26\u540e\u5c06 Collector \u7684\u914d\u7f6e\u8f6c\u6362\u4e3a\u4ee5\u4e0b\u5185\u5bb9:</p> <pre><code>receivers:\nprometheus:\nconfig:\nscrape_configs:\n- job_name: otel-collector\nscrape_interval: 10s\nhttp_sd_configs:\n- url: http://collector-with-ta-targetallocator:80/jobs/otel-collector/targets?collector_id=$POD_NAME\nmetric_relabel_configs:\n- action: labeldrop\nregex: (id|name)\nreplacement: $$1\n- action: labelmap\nregex: label_(.+)\nreplacement: $$1\nexporters:\nlogging:\nservice:\npipelines:\nmetrics:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre> <p>\u6ce8\u610f Operator \u5982\u4f55\u4ece' <code>scrape_configs</code> '\u90e8\u5206\u5220\u9664\u4efb\u4f55\u73b0\u6709\u7684\u670d\u52a1\u53d1\u73b0\u914d\u7f6e(\u4f8b\u5982\uff0c' <code>static_configs</code> '\uff0c ' <code>file_sd_configs</code> '\u7b49)\uff0c\u5e76\u6dfb\u52a0\u4e00\u4e2a' <code>http_sd_configs</code> '\u914d \u7f6e\uff0c\u6307\u5411\u5b83\u6240\u63d0\u4f9b\u7684 Target Allocator \u5b9e\u4f8b\u3002</p> <p>OpenTelemetry \u64cd\u4f5c\u7b26\u8fd8\u5c06\u5728\u5bf9\u8d26\u540e\u5c06\u76ee\u6807\u5206\u914d\u5668\u7684 promethueus \u914d\u7f6e\u8f6c\u6362\u4e3a\u4ee5\u4e0b\u5185\u5bb9:</p> <pre><code>config:\nscrape_configs:\n- job_name: otel-collector\nscrape_interval: 10s\nstatic_configs:\n- targets: ['0.0.0.0:8888']\nmetric_relabel_configs:\n- action: labeldrop\nregex: (id|name)\nreplacement: $1\n- action: labelmap\nregex: label_(.+)\nreplacement: $1\n</code></pre> <p>\u6ce8\u610f\uff0c\u5728\u672c\u4f8b\u4e2d\uff0cOperator \u5c06\u66ff\u6362\u952e\u4e2d\u7684<code>$$</code>\u66ff\u6362\u4e3a\u5355\u4e2a<code>$</code>\u3002\u8fd9\u662f\u56e0\u4e3a\u6536\u96c6\u5668\u652f\u6301\u73af\u5883\u53d8 \u91cf\u66ff\u6362\uff0c\u800c TA(\u76ee\u6807\u5206\u914d\u5668)\u4e0d\u652f\u6301\u3002\u56e0\u6b64\uff0c\u4e3a\u4e86\u786e\u4fdd\u517c\u5bb9\u6027\uff0cTA \u914d\u7f6e\u5e94\u8be5\u53ea\u5305\u542b\u4e00\u4e2a<code>$</code> \u7b26\u53f7\u3002</p> <p>\u66f4\u591a\u5173\u4e8e TargetAllocator \u7684\u4fe1\u606f\u53ef\u4ee5\u5728\u8fd9\u91cc\u627e\u5230.</p>"},{"location":"docs/k8s/operator/#_9","title":"\u76ee\u6807\u5206\u914d\u5668\u914d\u7f6e\u91cd\u5199","text":"<p>Prometheus \u63a5\u6536\u5668\u73b0\u5728\u663e\u5f0f\u652f\u6301\u4ece\u76ee\u6807\u5206\u914d\u5668\u83b7\u53d6\u6293\u53d6\u76ee\u6807\u3002\u56e0\u6b64\uff0c\u73b0\u5728\u53ef\u4ee5\u8ba9 Operator \u81ea\u52a8\u6dfb\u52a0\u5fc5\u8981\u7684\u76ee\u6807\u5206\u914d\u5668\u914d\u7f6e\u3002\u6b64\u529f\u80fd\u76ee\u524d\u9700\u8981\u542f \u7528<code>operator.collector.rewritetargetallocator</code>\u529f\u80fd\u6807\u5fd7\u3002\u542f\u7528\u8be5\u6807\u5fd7\u540e\uff0c\u4e0a\u4e00\u8282\u4e2d\u7684 \u914d\u7f6e\u5c06\u5448\u73b0\u4e3a:</p> <pre><code>receivers:\nprometheus:\nconfig:\nglobal:\nscrape_interval: 1m\nscrape_timeout: 10s\nevaluation_interval: 1m\ntarget_allocator:\nendpoint: http://collector-with-ta-targetallocator:80\ninterval: 30s\ncollector_id: $POD_NAME\nexporters:\nlogging:\nservice:\npipelines:\nmetrics:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre> <p>\u8fd9\u8fd8\u5141\u8bb8\u4f7f\u7528\u666e\u7f57\u7c73\u4fee\u65af\u64cd\u4f5c\u7b26 crd \u8fdb\u884c\u66f4\u76f4\u63a5\u7684\u76ee\u6807\u53d1\u73b0\u6536\u96c6\u5668\u914d\u7f6e\u3002\u4e0b\u9762\u662f\u4e00\u4e2a\u6700\u5c0f \u7684\u4f8b\u5b50:</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\nname: collector-with-ta-prometheus-cr\nspec:\nmode: statefulset\ntargetAllocator:\nenabled: true\nserviceAccount: everything-prometheus-operator-needs\nprometheusCR:\nenabled: true\nconfig: |\nreceivers:\nprometheus:\nexporters:\nlogging:\nservice:\npipelines:\nmetrics:\nreceivers: [prometheus]\nprocessors: []\nexporters: [logging]\n</code></pre>"},{"location":"docs/k8s/operator/#_10","title":"\u517c\u5bb9\u6027\u77e9\u9635","text":""},{"location":"docs/k8s/operator/#opentelemetry-opentelemetry","title":"OpenTelemetry \u64cd\u4f5c\u7b26\u4e0e OpenTelemetry \u6536\u96c6\u5668","text":"<p>OpenTelemetry Operator \u9075\u5faa\u4e0e\u64cd\u4f5c\u6570(OpenTelemetry Collector)\u76f8\u540c\u7684\u7248\u672c\u63a7\u5236\uff0c\u76f4\u5230 \u7248\u672c\u7684\u6b21\u8981\u90e8\u5206\u3002\u4f8b\u5982\uff0cOpenTelemetry Operator v0.18.1 \u8ddf\u8e2a OpenTelemetry Collector 0.18.0\u3002\u7248\u672c\u7684\u8865\u4e01\u90e8\u5206\u8868\u793a\u64cd\u4f5c\u7b26\u672c\u8eab\u7684\u8865\u4e01\u7ea7\u522b\uff0c\u800c\u4e0d\u662f OpenTelemetry Collector \u7684\u8865\u4e01\u7ea7\u522b\u3002\u6bcf\u5f53 OpenTelemetry Collector \u7684\u65b0\u8865\u4e01\u7248\u672c\u53d1\u5e03\u65f6\uff0c\u6211\u4eec\u5c06\u53d1 \u5e03\u64cd\u4f5c\u7b26\u7684\u65b0\u8865\u4e01\u7248\u672c\u3002</p> <p>By default, the OpenTelemetry Operator ensures consistent versioning between itself and the managed <code>OpenTelemetryCollector</code> resources. That is, if the OpenTelemetry Operator is based on version <code>0.40.0</code>, it will create resources with an underlying OpenTelemetry Collector at version <code>0.40.0</code>.</p> <p>When a custom <code>Spec.Image</code> is used with an <code>OpenTelemetryCollector</code> resource, the OpenTelemetry Operator will not manage this versioning and upgrading. In this scenario, it is best practice that the OpenTelemetry Operator version should match the underlying core version. Given a <code>OpenTelemetryCollector</code> resource with a <code>Spec.Image</code> configured to a custom image based on underlying OpenTelemetry Collector at version <code>0.40.0</code>, it is recommended that the OpenTelemetry Operator is kept at version <code>0.40.0</code>.</p>"},{"location":"docs/k8s/operator/#opentelemetry-operator-vs-kubernetes-vs-cert-manager","title":"OpenTelemetry Operator vs. Kubernetes vs. Cert Manager","text":"<p>\u6211\u4eec\u52aa\u529b\u4e0e\u5c3d\u53ef\u80fd\u5e7f\u6cdb\u7684 Kubernetes \u7248\u672c\u517c\u5bb9\uff0c\u4f46\u662f Kubernetes \u672c\u8eab\u7684\u4e00\u4e9b\u66f4\u6539\u9700\u8981\u6211 \u4eec\u6253\u7834\u4e0e\u65e7 Kubernetes \u7248\u672c\u7684\u517c\u5bb9\u6027\uff0c\u53ef\u80fd\u662f\u56e0\u4e3a\u4ee3\u7801\u4e0d\u517c\u5bb9\uff0c\u6216\u8005\u4ee5\u53ef\u7ef4\u62a4\u6027\u7684\u540d\u4e49\u3002 \u6bcf\u4e2a\u5df2\u53d1\u5e03\u7684\u64cd\u4f5c\u7b26\u90fd\u5c06\u652f\u6301\u7279\u5b9a\u8303\u56f4\u7684 Kubernetes \u7248\u672c\uff0c\u6700\u665a\u5728\u53d1\u5e03\u671f\u95f4\u786e\u5b9a\u3002</p> <p>\u6211\u4eec\u4f7f\u7528<code>cert-manager</code>\u6765\u5b9e\u73b0\u8fd9\u4e2a\u64cd\u4f5c\u7b26\u7684\u4e00\u4e9b\u7279\u6027\uff0c\u7b2c\u4e09\u5217\u663e\u793a\u4e86\u5df2\u77e5\u4e0e\u8fd9\u4e2a\u64cd\u4f5c\u7b26\u7684 \u7248\u672c\u4e00\u8d77\u5de5\u4f5c\u7684<code>cert-manager</code>\u7684\u7248\u672c\u3002</p> <p>OpenTelemetry \u64cd\u4f5c\u7b26 \u53ef\u80fd \u5728\u7ed9\u5b9a\u8303\u56f4\u4e4b\u5916\u7684\u7248\u672c\u4e0a\u5de5\u4f5c\uff0c\u4f46\u5f53\u6253\u5f00\u65b0\u95ee\u9898\u65f6\uff0c\u8bf7\u786e\u4fdd \u5728\u53d7\u652f\u6301\u7684\u7248\u672c\u4e0a\u6d4b\u8bd5\u60a8\u7684\u573a\u666f\u3002</p> OpenTelemetry Operator Kubernetes Cert-Manager v0.79.0 v1.19 to v1.27 v1 v0.78.0 v1.19 to v1.27 v1 v0.77.0 v1.19 to v1.26 v1 v0.76.1 v1.19 to v1.26 v1 v0.75.0 v1.19 to v1.26 v1 v0.74.0 v1.19 to v1.26 v1 v0.73.0 v1.19 to v1.26 v1 v0.72.0 v1.19 to v1.26 v1 v0.71.0 v1.19 to v1.25 v1 v0.70.0 v1.19 to v1.25 v1 v0.69.0 v1.19 to v1.25 v1 v0.68.0 v1.19 to v1.25 v1 v0.67.0 v1.19 to v1.25 v1 v0.66.0 v1.19 to v1.25 v1 v0.64.1 v1.19 to v1.25 v1 v0.63.1 v1.19 to v1.25 v1 v0.62.1 v1.19 to v1.25 v1 v0.61.0 v1.19 to v1.25 v1 v0.60.0 v1.19 to v1.25 v1 v0.59.0 v1.19 to v1.24 v1 v0.58.0 v1.19 to v1.24 v1 v0.57.2 v1.19 to v1.24 v1 v0.56.0 v1.19 to v1.24 v1"},{"location":"docs/k8s/operator/#_11","title":"\u8d21\u732e\u4e0e\u5f00\u53d1","text":"<p>Please see CONTRIBUTING.md.</p> <p>In addition to the core responsibilities the operator project requires approvers and maintainers to be responsible for releasing the project. See RELEASE.md for more information and release schedule.</p> <p>Approvers (@open-telemetry/operator-approvers):</p> <ul> <li>Benedikt Bongartz, Red Hat</li> <li>Tyler Helmuth, Honeycomb</li> <li>Yuri Oliveira Sa, Red Hat</li> </ul> <p>Emeritus Approvers:</p> <ul> <li>Anthony Mirabella, AWS</li> <li>Dmitrii Anoshin, Splunk</li> <li>Jay Camp, Splunk</li> <li>James Bebbington, Google</li> <li>Owais Lone, Splunk</li> <li>Pablo Baeyens, DataDog</li> </ul> <p>Target Allocator Maintainers (@open-telemetry/operator-ta-maintainers):</p> <ul> <li>Anthony Mirabella, AWS</li> <li>Kristina Pathak, Lightstep</li> <li>Sebastian Poxhofer</li> </ul> <p>Maintainers (@open-telemetry/operator-maintainers):</p> <ul> <li>Jacob Aronoff, Lightstep</li> <li>Pavol Loffay, Red Hat</li> <li>Vineeth Pothulapati, Timescale</li> </ul> <p>Emeritus Maintainers</p> <ul> <li>Alex Boten, Lightstep</li> <li>Bogdan Drutu, Splunk</li> <li>Juraci Paix\u00e3o Kr\u00f6hling, Grafana Labs</li> <li>Tigran Najaryan, Splunk</li> </ul> <p>Learn more about roles in the community repository.</p> <p>Thanks to all the people who already contributed!</p> <p></p>"},{"location":"docs/k8s/operator/#license","title":"License","text":"<p>Apache 2.0 License.</p>"},{"location":"docs/k8s/operator/automatic/","title":"\u6ce8\u5165\u81ea\u52a8\u63d2\u88c5","text":"<p>\u5f00\u653e\u9065\u6d4b\u64cd\u4f5c\u5668\u652f\u6301\u4e3a.NET, Java, Nodejs \u548c Python \u670d\u52a1\u6ce8\u5165\u548c\u914d\u7f6e\u81ea\u52a8\u4eea\u5668\u5e93\u3002</p>"},{"location":"docs/k8s/operator/automatic/#_1","title":"\u5b89\u88c5","text":"<p>\u9996\u5148\uff0c \u5c06OpenTelemetry Operator\u5b89 \u88c5\u5230\u96c6\u7fa4\u4e2d\u3002</p> <p>\u4f60\u53ef\u4ee5\u901a \u8fc7\u64cd\u4f5c\u5458\u91ca\u653e\u6e05\u5355\uff0c \u64cd\u4f5c\u5458\u638c\u8235\u56fe\uff0c \u6216\u4e0e\u64cd\u4f5c\u5458\u4e2d\u5fc3\u3002</p> <p>\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u60a8\u9700\u8981\u5b89 \u88c5cert-manager\u3002\u5982\u679c\u4f7f\u7528 helm \u56fe\u8868 \uff0c\u5219\u53ef\u4ee5\u9009\u62e9\u751f\u6210\u81ea\u7b7e\u540d\u8bc1\u4e66\u3002</p>"},{"location":"docs/k8s/operator/automatic/#_2","title":"\u521b\u5efa\u6536\u96c6\u5668(\u53ef\u9009)","text":"<p>\u5c06\u9065\u6d4b\u6570\u636e\u4ece\u5bb9\u5668\u53d1\u9001\u5230\u6536\u96c6\u5668\u800c\u4e0d\u662f\u76f4\u63a5\u53d1\u9001\u5230\u540e\u7aef\u662f\u6700 \u4f73\u5b9e\u8df5\u3002</p> <p>Collector \u6709\u52a9\u4e8e\u7b80\u5316\u79d8\u5bc6\u7ba1\u7406\uff0c\u4ece\u5e94\u7528\u7a0b\u5e8f\u4e2d\u89e3\u8026\u6570\u636e\u5bfc\u51fa\u95ee\u9898( \u4f8b\u5982\u9700\u8981\u91cd\u8bd5)\uff0c\u5e76\u5141 \u8bb8\u60a8\u5411\u9065\u6d4b\u6dfb\u52a0\u989d\u5916\u7684\u6570\u636e\uff0c\u4f8b\u5982\u4f7f\u7528k8sattributesprocessor\u7ec4\u4ef6\u3002</p> <p>Note</p> <p>\u5982\u679c\u60a8\u9009\u62e9\u4e0d\u4f7f\u7528\u6536\u96c6\u5668\uff0c\u5219\u53ef\u4ee5\u8df3\u5230\u4e0b\u4e00\u8282\u3002</p> <p>Operator \u4e3a Collector \u63d0\u4f9b\u4e00\u4e2a\u81ea\u5b9a\u4e49\u8d44\u6e90\u5b9a\u4e49(CRD)\uff0c\u7528\u4e8e\u521b\u5efa\u4e00\u4e2a\u7531 Operator \u7ba1\u7406\u7684 Collector \u5b9e\u4f8b\u3002</p> <p>\u4e0b\u9762\u7684\u793a\u4f8b\u5c06 Collector \u90e8\u7f72\u4e3a<code>Deployment</code>(\u9ed8\u8ba4\u8bbe\u7f6e)\uff0c\u4f46\u4e5f\u53ef\u4ee5\u4f7f\u7528\u5176 \u4ed6\u90e8\u7f72\u6a21\u5f0f\u3002</p> <p>\u5f53\u4f7f\u7528<code>Deployment</code>\u6a21\u5f0f\u65f6\uff0cOperator \u8fd8\u5c06\u521b\u5efa\u4e00\u4e2a\u53ef\u7528\u4e8e\u4e0e Collector \u4ea4\u4e92\u7684\u670d\u52a1\u3002\u670d \u52a1\u7684\u540d\u79f0\u662f\u9644\u52a0\u5728<code>-collector</code>\u524d\u9762\u7684<code>OpenTelemetryCollector</code>\u8d44\u6e90\u7684\u540d\u79f0\u3002\u5728\u6211\u4eec\u7684\u4f8b \u5b50\u4e2d\uff0c\u8fd9\u5c06\u662f<code>demo-collector</code>\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: OpenTelemetryCollector\nmetadata:\n  name: demo\nspec:\n  config: |\n    receivers:\n      otlp:\n        protocols:\n          grpc:\n          http:\n    processors:\n      memory_limiter:\n        check_interval: 1s\n        limit_percentage: 75\n        spike_limit_percentage: 15\n      batch:\n        send_batch_size: 10000\n        timeout: 10s\n    exporters:\n      logging:\n    service:\n      pipelines:\n        traces:\n          receivers: [otlp]\n          processors: [memory_limiter, batch]\n          exporters: [logging]\n        metrics:\n          receivers: [otlp]\n          processors: [memory_limiter, batch]\n          exporters: [logging]\n        logs:\n          receivers: [otlp]\n          processors: [memory_limiter, batch]\n          exporters: [logging]\nEOF\n</code></pre> <p>\u4e0a\u9762\u7684\u547d\u4ee4\u4f1a\u6267\u884c Collector \u7684\u90e8\u7f72\uff0c\u60a8\u53ef\u4ee5\u5c06\u5176\u7528\u4f5c Pod \u4e2d\u81ea\u52a8\u68c0\u6d4b\u7684\u7aef\u70b9\u3002</p>"},{"location":"docs/k8s/operator/automatic/#instrumentation","title":"\u914d\u7f6e\u81ea\u52a8 Instrumentation","text":"<p>\u4e3a\u4e86\u80fd\u591f\u7ba1\u7406\u81ea\u52a8 Instrumentation\uff0c\u64cd\u4f5c\u4eba\u5458\u9700\u8981\u8fdb\u884c\u914d\u7f6e\uff0c\u4ee5\u4e86\u89e3\u8981\u5bf9\u54ea\u4e9b Pod \u8fdb\u884c Instrumentation \u4ee5\u53ca\u5bf9\u8fd9\u4e9b Pod \u4f7f\u7528\u54ea\u79cd\u81ea\u52a8 Instrumentation\u3002\u8fd9\u662f\u901a \u8fc7Instrumentation CRD\u5b8c\u6210\u7684\u3002</p> <p>\u6b63\u786e\u521b\u5efa Instrumentation \u8d44\u6e90\u5bf9\u4e8e\u4f7f\u81ea\u52a8\u68c0\u6d4b\u5de5\u4f5c\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u8981\u4f7f\u81ea\u52a8\u68c0\u6d4b \u6b63\u5e38\u5de5\u4f5c\uff0c\u9700\u8981\u786e\u4fdd\u6240\u6709\u7aef\u70b9\u548c\u73af\u5883\u53d8\u91cf\u90fd\u6b63\u786e\u3002</p>"},{"location":"docs/k8s/operator/automatic/#net","title":".NET","text":"<p>\u4e0b\u9762\u7684\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a\u57fa\u672c\u7684 Instrumentation \u8d44\u6e90\uff0c\u8be5\u8d44\u6e90\u662f\u4e13\u95e8\u4e3a Instrumentation .NET \u670d\u52a1\u914d\u7f6e\u7684\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: demo-instrumentation\nspec:\n  exporter:\n    endpoint: http://demo-collector:4318\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"1\"\nEOF\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cauto-instruments .NET \u670d\u52a1\u4f7f\u7528<code>otlp</code>\u548c<code>http/protobuf</code>\u534f\u8bae\u3002\u8fd9\u610f\u5473\u7740 \u914d\u7f6e\u7684\u7aef\u70b9\u5fc5\u987b\u80fd\u591f\u901a\u8fc7<code>http/protobuf</code>\u63a5\u6536 OTLP\u3002\u56e0\u6b64\uff0c\u8be5\u793a\u4f8b\u4f7f \u7528<code>http://demo-collector:4318</code>\uff0c\u5b83\u5c06\u8fde\u63a5\u5230\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6536\u96c6\u5668\u7684 otlreceiver \u7684<code>http</code>\u7aef\u53e3\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c.NET \u81ea\u52a8\u68c0\u6d4b\u9644\u5e26\u4e86\u8bb8\u591a\u68c0\u6d4b\u5e93\u3002\u8fd9\u4f7f\u68c0\u6d4b\u53d8\u5f97\u5bb9\u6613\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8fc7\u591a\u6216 \u4e0d\u9700\u8981\u7684\u6570\u636e\u3002\u5982\u679c\u6709\u4efb\u4f55\u4f60\u4e0d\u60f3\u4f7f\u7528\u7684\u5e93\uff0c\u4f60\u53ef\u4ee5\u8bbe \u7f6e<code>OTEL_DOTNET_AUTO_[SIGNAL]_[NAME]_INSTRUMENTATION_ENABLED=false</code> \uff0c\u5176 \u4e2d<code>[SIGNAL]</code>\u662f\u4fe1\u53f7\u7684\u7c7b\u578b\uff0c<code>[NAME]</code> \u662f\u533a\u5206\u5927\u5c0f\u5199\u7684\u5e93\u540d\u3002</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: demo-instrumentation\nspec:\nexporter:\nendpoint: http://demo-collector:4318\npropagators:\n- tracecontext\n- baggage\nsampler:\ntype: parentbased_traceidratio\nargument: '1'\ndotnet:\nenv:\n- name: OTEL_DOTNET_AUTO_TRACES_GRPCNETCLIENT_INSTRUMENTATION_ENABLED\nvalue: false\n- name: OTEL_DOTNET_AUTO_METRICS_PROCESS_INSTRUMENTATION_ENABLED\nvalue: false\n</code></pre> <p>\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2 \u9605.NET Auto Instrumentation \u6587\u6863.</p>"},{"location":"docs/k8s/operator/automatic/#java","title":"Java","text":"<p>\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u4e00\u4e2a\u57fa\u672c\u7684 Instrumentation \u8d44\u6e90\uff0c\u8be5\u8d44\u6e90\u88ab\u914d\u7f6e\u4e3a\u68c0\u6d4b Java \u670d\u52a1\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: demo-instrumentation\nspec:\n  exporter:\n    endpoint: http://demo-collector:4317\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"1\"\nEOF\n</code></pre> <p>By default, the Instrumentation resource that auto-instruments Java services uses <code>otlp</code> with the <code>grpc</code> protocol. This means that the configured endpoint must be able to receive OTLP over <code>grpc</code>. Therefore, the example uses <code>http://demo-collector:4317</code>, which connects to the <code>grpc</code> port of the otlpreceiver of the Collector created in the previous step.</p> <p>By default, the Java auto-instrumentation ships with many instrumentation libraries. This makes instrumentation easy, but could result in too much or unwanted data. If there are any libraries you do not want to use you can set the <code>OTEL_INSTRUMENTATION_[NAME]_ENABLED=false</code> where <code>[NAME]</code> is the name of the library. If you know exactly which libraries you want to use, you can disable the default libraries by setting <code>OTEL_INSTRUMENTATION_COMMON_DEFAULT_ENABLED=false</code> and then use <code>OTEL_INSTRUMENTATION_[NAME]_ENABLED=true</code> where <code>[NAME]</code> is the name of the library. For more details, see Suppressing specific auto-instrumentation.</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: demo-instrumentation\nspec:\nexporter:\nendpoint: http://demo-collector:4317\npropagators:\n- tracecontext\n- baggage\nsampler:\ntype: parentbased_traceidratio\nargument: '1'\njava:\nenv:\n- name: OTEL_INSTRUMENTATION_KAFKA_ENABLED\nvalue: false\n- name: OTEL_INSTRUMENTATION_REDISCALA_ENABLED\nvalue: false\n</code></pre> <p>\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2 \u89c1Java \u4ee3\u7406\u914d\u7f6e.</p>"},{"location":"docs/k8s/operator/automatic/#nodejs","title":"Node.js","text":"<p>\u4e0b\u9762\u7684\u547d\u4ee4\u521b\u5efa\u4e86\u4e00\u4e2a\u57fa\u672c\u7684\u63d2\u88c5\u8d44\u6e90\uff0c\u8be5\u8d44\u6e90\u914d\u7f6e\u7528\u4e8e\u68c0\u6d4b<code>Node.js</code>\u670d\u52a1\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: demo-instrumentation\nspec:\n  exporter:\n    endpoint: http://demo-collector:4317\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"1\"\nEOF\n</code></pre> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u81ea\u52a8\u68c0\u6d4b Node.js \u670d\u52a1\u7684<code>Instrumentation</code>\u8d44\u6e90\u4f7f\u7528 <code>otlp</code> \u548c <code>grpc</code> \u534f \u8bae\u3002\u8fd9\u610f\u5473\u7740\u914d\u7f6e\u7684\u7aef\u70b9\u5fc5\u987b\u80fd\u591f\u901a\u8fc7 <code>grpc</code> \u63a5\u6536 <code>otlp</code> \u3002\u56e0\u6b64\uff0c\u672c\u4f8b\u4f7f\u7528 <code>http://demo-collector:4317</code>\uff0c\u5b83\u8fde\u63a5\u5230\u5728\u4e0a\u4e00\u6b65\u4e2d\u521b\u5efa\u7684\u6536\u96c6\u5668\u7684<code>otlreceiver</code>\u7684 <code>grpc</code> \u7aef\u53e3\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cNode.js \u81ea\u52a8\u68c0\u6d4b\u9644\u5e26\u4e86\u8bb8\u591a\u68c0\u6d4b\u5e93\u3002\u76ee\u524d\uff0c\u8fd8\u6ca1\u6709\u529e\u6cd5\u9009\u62e9\u53ea\u52a0\u5165\u7279\u5b9a\u7684 \u8f6f\u4ef6\u5305\u6216\u7981\u7528\u7279\u5b9a\u7684\u8f6f\u4ef6\u5305\u3002\u5982\u679c\u60a8\u4e0d\u60f3\u4f7f\u7528\u9ed8\u8ba4\u6620\u50cf\u5305\u542b\u7684\u5305\uff0c\u90a3\u4e48\u60a8\u5fc5\u987b\u63d0\u4f9b\u81ea\u5df1\u7684\u6620 \u50cf\uff0c\u8be5\u6620\u50cf\u53ea\u5305\u542b\u60a8\u60f3\u8981\u7684\u5305\uff0c\u6216\u8005\u4f7f\u7528\u624b\u52a8\u68c0\u6d4b\u3002</p> <p>\u66f4\u591a\u7ec6\u8282\u8bf7\u53c2\u89c1Node.js auto-instrumentation.</p>"},{"location":"docs/k8s/operator/automatic/#python","title":"Python","text":"<p>\u4e0b\u9762\u7684\u547d\u4ee4\u5c06\u521b\u5efa\u4e00\u4e2a\u57fa\u672c\u7684 Instrumentation \u8d44\u6e90\uff0c\u8be5\u8d44\u6e90\u662f\u4e13\u95e8\u4e3a Instrumentation Python \u670d\u52a1\u914d\u7f6e\u7684\u3002</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\n  name: demo-instrumentation\nspec:\n  exporter:\n    endpoint: http://demo-collector:4318\n  propagators:\n    - tracecontext\n    - baggage\n  sampler:\n    type: parentbased_traceidratio\n    argument: \"1\"\nEOF\n</code></pre> <p>By default, the Instrumentation resource that auto-instruments python services uses <code>otlp</code> with the <code>http/protobuf</code> protocol. This means that the configured endpoint must be able to receive OTLP over <code>http/protobuf</code>. Therefore, the example uses <code>http://demo-collector:4318</code>, which will connect to the <code>http</code> port of the otlpreceiver of the Collector created in the previous step.</p> <p>As of operator v0.67.0, the Instrumentation resource automatically sets <code>OTEL_EXPORTER_OTLP_TRACES_PROTOCOL</code> and <code>OTEL_EXPORTER_OTLP_METRICS_PROTOCOL</code> to <code>http/protobuf</code> for Python services. If you use an older version of the Operator you MUST set these env variables to <code>http/protobuf</code>, or python auto-instrumentation will not work.</p> <p>By default the Python auto-instrumentation will detect the packages in your Python service and instrument anything it can. This makes instrumentation easy, but can result in too much or unwanted data. If there are any packages you do not want to instrument, you can set the <code>OTEL_PYTHON_DISABLED_INSTRUMENTATIONS</code> environment variable</p> <pre><code>apiVersion: opentelemetry.io/v1alpha1\nkind: Instrumentation\nmetadata:\nname: demo-instrumentation\nspec:\nexporter:\nendpoint: http://demo-collector:4318\npropagators:\n- tracecontext\n- baggage\nsampler:\ntype: parentbased_traceidratio\nargument: '1'\npython:\nenv:\n- name: OTEL_PYTHON_DISABLED_INSTRUMENTATIONS\nvalue:\n&lt;comma-separated list of package names to exclude from\ninstrumentation&gt;\n</code></pre> <p>\u6709\u5173\u66f4\u591a\u7ec6\u8282\uff0c\u8bf7\u53c2\u9605 Python Agent Configuration \u6587\u6863</p> <p>\u65e2\u7136\u5df2\u7ecf\u521b\u5efa\u4e86 Instrumentation \u5bf9\u8c61\uff0c\u90a3\u4e48\u96c6\u7fa4\u5c31\u80fd\u591f\u81ea\u52a8\u68c0\u6d4b\u670d\u52a1\u5e76\u5c06\u6570\u636e\u53d1\u9001\u5230\u7aef \u70b9\u3002\u7136\u800c\uff0cOpenTelemetry Operator \u7684\u81ea\u52a8\u68c0\u6d4b\u9075\u5faa\u4e00\u4e2a\u53ef\u9009\u62e9\u7684\u6a21\u578b\u3002\u4e3a\u4e86\u6fc0\u6d3b\u81ea\u52a8\u68c0 \u6d4b\uff0c\u60a8\u9700\u8981\u5728\u90e8\u7f72\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u6ce8\u91ca\u3002</p>"},{"location":"docs/k8s/operator/automatic/#_3","title":"\u5411\u73b0\u6709\u90e8\u7f72\u6dfb\u52a0\u6ce8\u91ca","text":"<p>\u6700\u540e\u4e00\u6b65\u662f\u9009\u62e9\u81ea\u52a8\u68c0\u6d4b\u670d\u52a1\u3002\u8fd9\u662f\u901a\u8fc7\u66f4\u65b0\u4f60\u7684\u670d\u52a1 \u7684<code>spec.template.metadata.annotations</code>\u6765\u5305\u542b\u4e00\u4e2a\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u6ce8\u91ca\u6765\u5b9e\u73b0\u7684:</p> <ul> <li>.NET: <code>instrumentation.opentelemetry.io/inject-dotnet: \"true\"</code></li> <li>Java: <code>instrumentation.opentelemetry.io/inject-java: \"true\"</code></li> <li>Node.js: <code>instrumentation.opentelemetry.io/inject-nodejs: \"true\"</code></li> <li>Python: <code>instrumentation.opentelemetry.io/inject-python: \"true\"</code></li> </ul> <p>\u6ce8\u91ca\u7684\u53ef\u80fd\u503c\u53ef\u4ee5\u662f</p> <ul> <li><code>\"true\"</code> - \u4ee5\u5f53\u524d\u547d\u540d\u7a7a\u95f4\u7684\u9ed8\u8ba4\u540d\u79f0\u6ce8\u5165<code>Instrumentation</code>\u8d44\u6e90\u3002</li> <li><code>\"my-instrumentation\"</code> - \u5728\u5f53\u524d\u547d\u540d\u7a7a\u95f4\u4e2d\u6ce8\u5165\u540d\u4e3a<code>\"my-instrumentation\"</code>\u7684   <code>Instrumentation</code> CR \u5b9e\u4f8b\u3002</li> <li><code>\"my-other-namespace/my-instrumentation\"</code> - \u4ece\u53e6\u4e00\u4e2a\u547d\u540d\u7a7a   \u95f4<code>\"my-other-namespace\"</code>\u6ce8\u5165\u540d\u4e3a<code>\"my-instrumentation\"</code>\u7684 <code>Instrumentation</code> CR   \u5b9e\u4f8b.</li> <li><code>\"false\"</code> - \u4e0d\u8981\u6ce8\u5c04</li> </ul> <p>\u6216\u8005\uff0c\u53ef\u4ee5\u5c06\u6ce8\u91ca\u6dfb\u52a0\u5230\u540d\u79f0\u7a7a\u95f4\u4e2d\uff0c\u8fd9\u5c06\u5bfc\u81f4\u8be5\u540d\u79f0\u7a7a\u95f4\u4e2d\u7684\u6240\u6709\u670d\u52a1\u9009\u62e9\u52a0\u5165\u81ea\u52a8\u68c0\u6d4b \u3002\u8bf7\u53c2\u9605\u64cd\u4f5c\u5458\u81ea\u52a8\u68c0\u6d4b\u6587\u6863\u4e86\u89e3\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"docs/k8s/operator/apis/","title":"API Reference","text":"<p>Packages:</p> <ul> <li>API Reference</li> <li>opentelemetry.io/v1alpha1</li> </ul>"},{"location":"docs/k8s/operator/apis/#opentelemetryiov1alpha1","title":"opentelemetry.io/v1alpha1","text":"<p>Resource Types:</p> <ul> <li> <p>Instrumentation</p> </li> <li> <p>OpenTelemetryCollector</p> </li> </ul>"},{"location":"docs/k8s/operator/apis/Instrumentation/","title":"Instrumentation","text":""},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentation","title":"Instrumentation","text":"<p>\u21a9 Parent</p> <p>Instrumentation is the spec for OpenTelemetry instrumentation.</p> Name Type Description Required apiVersion string opentelemetry.io/v1alpha1 true kind string Instrumentation true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            InstrumentationSpec defines the desired state of OpenTelemetry SDK and instrumentation. false status object            InstrumentationStatus defines status of the instrumentation. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspec","title":"Instrumentation.spec","text":"<p>\u21a9 Parent</p> <p>InstrumentationSpec defines the desired state of OpenTelemetry SDK and instrumentation.</p> Name Type Description Required apacheHttpd object            Apache defines configuration for Apache HTTPD auto-instrumentation. false dotnet object            DotNet defines configuration for DotNet auto-instrumentation. false env []object            Env defines common env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false exporter object            Exporter defines exporter configuration. false go object            Go defines configuration for Go auto-instrumentation. When using Go auto-instrumenetation you must provide a value for the OTEL_GO_AUTO_TARGET_EXE env var via the Instrumentation env vars or via the instrumentation.opentelemetry.io/otel-go-auto-target-exe pod annotation. Failure to set this value causes instrumentation injection to abort, leaving the original pod unchanged. false java object            Java defines configuration for java auto-instrumentation. false nodejs object            NodeJS defines configuration for nodejs auto-instrumentation. false propagators []enum            Propagators defines inter-process context propagation configuration. Values in this list will be set in the OTEL_PROPAGATORS env var. Enum=tracecontext;baggage;b3;b3multi;jaeger;xray;ottrace;none false python object            Python defines configuration for python auto-instrumentation. false resource object            Resource defines the configuration for the resource attributes, as defined by the OpenTelemetry specification. false sampler object            Sampler defines sampling configuration. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpd","title":"Instrumentation.spec.apacheHttpd","text":"<p>\u21a9 Parent</p> <p>Apache defines configuration for Apache HTTPD auto-instrumentation.</p> Name Type Description Required attrs []object            Attrs defines Apache HTTPD agent specific attributes. The precedence is: `agent default attributes` &gt; `instrument spec attributes` . Attributes are documented at https://github.com/open-telemetry/opentelemetry-cpp-contrib/tree/main/instrumentation/otel-webserver-module false configPath string            Location of Apache HTTPD server configuration. Needed only if different from default \"/usr/local/apache2/conf\" false env []object            Env defines Apache HTTPD specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with Apache SDK and auto-instrumentation. false resourceRequirements object            Resources describes the compute resource requirements. false version string            Apache HTTPD server version. One of 2.4 or 2.2. Default is 2.4 false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindex","title":"Instrumentation.spec.apacheHttpd.attrs[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindexvaluefrom","title":"Instrumentation.spec.apacheHttpd.attrs[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.apacheHttpd.attrs[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindexvaluefromfieldref","title":"Instrumentation.spec.apacheHttpd.attrs[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindexvaluefromresourcefieldref","title":"Instrumentation.spec.apacheHttpd.attrs[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdattrsindexvaluefromsecretkeyref","title":"Instrumentation.spec.apacheHttpd.attrs[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindex","title":"Instrumentation.spec.apacheHttpd.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindexvaluefrom","title":"Instrumentation.spec.apacheHttpd.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.apacheHttpd.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindexvaluefromfieldref","title":"Instrumentation.spec.apacheHttpd.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.apacheHttpd.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.apacheHttpd.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdresourcerequirements","title":"Instrumentation.spec.apacheHttpd.resourceRequirements","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecapachehttpdresourcerequirementsclaimsindex","title":"Instrumentation.spec.apacheHttpd.resourceRequirements.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnet","title":"Instrumentation.spec.dotnet","text":"<p>\u21a9 Parent</p> <p>DotNet defines configuration for DotNet auto-instrumentation.</p> Name Type Description Required env []object            Env defines DotNet specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with DotNet SDK and auto-instrumentation. false resourceRequirements object            Resources describes the compute resource requirements. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindex","title":"Instrumentation.spec.dotnet.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindexvaluefrom","title":"Instrumentation.spec.dotnet.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.dotnet.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindexvaluefromfieldref","title":"Instrumentation.spec.dotnet.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.dotnet.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.dotnet.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetresourcerequirements","title":"Instrumentation.spec.dotnet.resourceRequirements","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecdotnetresourcerequirementsclaimsindex","title":"Instrumentation.spec.dotnet.resourceRequirements.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindex","title":"Instrumentation.spec.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindexvaluefrom","title":"Instrumentation.spec.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindexvaluefromfieldref","title":"Instrumentation.spec.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecexporter","title":"Instrumentation.spec.exporter","text":"<p>\u21a9 Parent</p> <p>Exporter defines exporter configuration.</p> Name Type Description Required endpoint string            Endpoint is address of the collector with OTLP endpoint. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgo","title":"Instrumentation.spec.go","text":"<p>\u21a9 Parent</p> <p>Go defines configuration for Go auto-instrumentation. When using Go auto-instrumenetation you must provide a value for the OTEL_GO_AUTO_TARGET_EXE env var via the Instrumentation env vars or via the instrumentation.opentelemetry.io/otel-go-auto-target-exe pod annotation. Failure to set this value causes instrumentation injection to abort, leaving the original pod unchanged.</p> Name Type Description Required env []object            Env defines Go specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with Go SDK and auto-instrumentation. false resourceRequirements object            Resources describes the compute resource requirements. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindex","title":"Instrumentation.spec.go.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindexvaluefrom","title":"Instrumentation.spec.go.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.go.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindexvaluefromfieldref","title":"Instrumentation.spec.go.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.go.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.go.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoresourcerequirements","title":"Instrumentation.spec.go.resourceRequirements","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecgoresourcerequirementsclaimsindex","title":"Instrumentation.spec.go.resourceRequirements.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjava","title":"Instrumentation.spec.java","text":"<p>\u21a9 Parent</p> <p>Java defines configuration for java auto-instrumentation.</p> Name Type Description Required env []object            Env defines java specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with javaagent auto-instrumentation JAR. false resources object            Resources describes the compute resource requirements. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindex","title":"Instrumentation.spec.java.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindexvaluefrom","title":"Instrumentation.spec.java.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.java.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindexvaluefromfieldref","title":"Instrumentation.spec.java.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.java.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.java.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaresources","title":"Instrumentation.spec.java.resources","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecjavaresourcesclaimsindex","title":"Instrumentation.spec.java.resources.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejs","title":"Instrumentation.spec.nodejs","text":"<p>\u21a9 Parent</p> <p>NodeJS defines configuration for nodejs auto-instrumentation.</p> Name Type Description Required env []object            Env defines nodejs specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with NodeJS SDK and auto-instrumentation. false resourceRequirements object            Resources describes the compute resource requirements. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindex","title":"Instrumentation.spec.nodejs.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindexvaluefrom","title":"Instrumentation.spec.nodejs.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.nodejs.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindexvaluefromfieldref","title":"Instrumentation.spec.nodejs.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.nodejs.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.nodejs.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsresourcerequirements","title":"Instrumentation.spec.nodejs.resourceRequirements","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecnodejsresourcerequirementsclaimsindex","title":"Instrumentation.spec.nodejs.resourceRequirements.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpython","title":"Instrumentation.spec.python","text":"<p>\u21a9 Parent</p> <p>Python defines configuration for python auto-instrumentation.</p> Name Type Description Required env []object            Env defines python specific env vars. There are four layers for env vars' definitions and the precedence order is: `original container env vars` &gt; `language specific env vars` &gt; `common env vars` &gt; `instrument spec configs' vars`. If the former var had been defined, then the other vars would be ignored. false image string            Image is a container image with Python SDK and auto-instrumentation. false resourceRequirements object            Resources describes the compute resource requirements. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindex","title":"Instrumentation.spec.python.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindexvaluefrom","title":"Instrumentation.spec.python.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindexvaluefromconfigmapkeyref","title":"Instrumentation.spec.python.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindexvaluefromfieldref","title":"Instrumentation.spec.python.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindexvaluefromresourcefieldref","title":"Instrumentation.spec.python.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonenvindexvaluefromsecretkeyref","title":"Instrumentation.spec.python.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonresourcerequirements","title":"Instrumentation.spec.python.resourceRequirements","text":"<p>\u21a9 Parent</p> <p>Resources describes the compute resource requirements.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecpythonresourcerequirementsclaimsindex","title":"Instrumentation.spec.python.resourceRequirements.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecresource","title":"Instrumentation.spec.resource","text":"<p>\u21a9 Parent</p> <p>Resource defines the configuration for the resource attributes, as defined by the OpenTelemetry specification.</p> Name Type Description Required addK8sUIDAttributes boolean            AddK8sUIDAttributes defines whether K8s UID attributes should be collected (e.g. k8s.deployment.uid). false resourceAttributes map[string]string            Attributes defines attributes that are added to the resource. For example environment: dev false"},{"location":"docs/k8s/operator/apis/Instrumentation/#instrumentationspecsampler","title":"Instrumentation.spec.sampler","text":"<p>\u21a9 Parent</p> <p>Sampler defines sampling configuration.</p> Name Type Description Required argument string            Argument defines sampler argument. The value depends on the sampler type. For instance for parentbased_traceidratio sampler type it is a number in range [0..1] e.g. 0.25. The value will be set in the OTEL_TRACES_SAMPLER_ARG env var. false type enum            Type defines sampler type. The value will be set in the OTEL_TRACES_SAMPLER env var. The value can be for instance parentbased_always_on, parentbased_always_off, parentbased_traceidratio... Enum: always_on, always_off, traceidratio, parentbased_always_on, parentbased_always_off, parentbased_traceidratio, jaeger_remote, xray false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/","title":"OpenTelemetryCollector","text":""},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollector","title":"OpenTelemetryCollector","text":"<p>\u21a9 Parent</p> <p>OpenTelemetryCollector is the Schema for the opentelemetrycollectors API.</p> Name Type Description Required apiVersion string opentelemetry.io/v1alpha1 true kind string OpenTelemetryCollector true metadata object Refer to the Kubernetes API documentation for the fields of the `metadata` field. true spec object            OpenTelemetryCollectorSpec defines the desired state of OpenTelemetryCollector. false status object            OpenTelemetryCollectorStatus defines the observed state of OpenTelemetryCollector. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspec","title":"OpenTelemetryCollector.spec","text":"<p>\u21a9 Parent</p> <p>OpenTelemetryCollectorSpec defines the desired state of OpenTelemetryCollector.</p> Name Type Description Required affinity object            If specified, indicates the pod's scheduling constraints false args map[string]string            Args is the set of arguments to pass to the OpenTelemetry Collector binary false autoscaler object            Autoscaler specifies the pod autoscaling configuration to use for the OpenTelemetryCollector workload. false config string            Config is the raw JSON to be used as the collector's configuration. Refer to the OpenTelemetry Collector documentation for details. false env []object            ENV vars to set on the OpenTelemetry Collector's Pods. These can then in certain cases be consumed in the config file for the Collector. false envFrom []object            List of sources to populate environment variables on the OpenTelemetry Collector's Pods. These can then in certain cases be consumed in the config file for the Collector. false hostNetwork boolean            HostNetwork indicates if the pod should run in the host networking namespace. false image string            Image indicates the container image to use for the OpenTelemetry Collector. false imagePullPolicy string            ImagePullPolicy indicates the pull policy to be used for retrieving the container image (Always, Never, IfNotPresent) false ingress object            Ingress is used to specify how OpenTelemetry Collector is exposed. This functionality is only available if one of the valid modes is set. Valid modes are: deployment, daemonset and statefulset. false lifecycle object            Actions that the management system should take in response to container lifecycle events. Cannot be updated. false livenessProbe object            Liveness config for the OpenTelemetry Collector except the probe handler which is auto generated from the health extension of the collector. It is only effective when healthcheckextension is configured in the OpenTelemetry Collector pipeline. false maxReplicas integer            MaxReplicas sets an upper bound to the autoscaling feature. If MaxReplicas is set autoscaling is enabled. Deprecated: use \"OpenTelemetryCollector.Spec.Autoscaler.MaxReplicas\" instead. Format: int32 false minReplicas integer            MinReplicas sets a lower bound to the autoscaling feature.  Set this if you are using autoscaling. It must be at least 1 Deprecated: use \"OpenTelemetryCollector.Spec.Autoscaler.MinReplicas\" instead. Format: int32 false mode enum            Mode represents how the collector should be deployed (deployment, daemonset, statefulset or sidecar) Enum: daemonset, deployment, sidecar, statefulset false nodeSelector map[string]string            NodeSelector to schedule OpenTelemetry Collector pods. This is only relevant to daemonset, statefulset, and deployment mode false podAnnotations map[string]string            PodAnnotations is the set of annotations that will be attached to Collector and Target Allocator pods. false podSecurityContext object            PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext.  Field values of container.securityContext take precedence over field values of PodSecurityContext. false ports []object            Ports allows a set of ports to be exposed by the underlying v1.Service. By default, the operator will attempt to infer the required ports by parsing the .Spec.Config property but this property can be used to open additional ports that can't be inferred by the operator, like for custom receivers. false priorityClassName string            If specified, indicates the pod's priority. If not specified, the pod priority will be default or zero if there is no default. false replicas integer            Replicas is the number of pod instances for the underlying OpenTelemetry Collector. Set this if your are not using autoscaling Format: int32 false resources object            Resources to set on the OpenTelemetry Collector pods. false securityContext object            SecurityContext will be set as the container security context. false serviceAccount string            ServiceAccount indicates the name of an existing service account to use with this instance. When set, the operator will not automatically create a ServiceAccount for the collector. false targetAllocator object            TargetAllocator indicates a value which determines whether to spawn a target allocation resource or not. false terminationGracePeriodSeconds integer            Duration in seconds the pod needs to terminate gracefully upon probe failure. Format: int64 false tolerations []object            Toleration to schedule OpenTelemetry Collector pods. This is only relevant to daemonset, statefulset, and deployment mode false upgradeStrategy enum            UpgradeStrategy represents how the operator will handle upgrades to the CR when a newer version of the operator is deployed Enum: automatic, none false volumeClaimTemplates []object            VolumeClaimTemplates will provide stable storage using PersistentVolumes. Only available when the mode=statefulset. false volumeMounts []object            VolumeMounts represents the mount points to use in the underlying collector deployment(s) false volumes []object            Volumes represents which volumes to use in the underlying collector deployment(s). false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinity","title":"OpenTelemetryCollector.spec.affinity","text":"<p>\u21a9 Parent</p> <p>If specified, indicates the pod's scheduling constraints</p> Name Type Description Required nodeAffinity object            Describes node affinity scheduling rules for the pod. false podAffinity object            Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). false podAntiAffinity object            Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinity","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity","text":"<p>\u21a9 Parent</p> <p>Describes node affinity scheduling rules for the pod.</p> Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution []object            The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding matchExpressions; the node(s) with the highest sum are the most preferred. false requiredDuringSchedulingIgnoredDuringExecution object            If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinitypreferredduringschedulingignoredduringexecutionindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[index]","text":"<p>\u21a9 Parent</p> <p>An empty preferred scheduling term matches all objects with implicit weight 0 (i.e. it's a no-op). A null preferred scheduling term matches no objects (i.e. is also a no-op).</p> Name Type Description Required preference object            A node selector term, associated with the corresponding weight. true weight integer            Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100. Format: int32 true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinitypreferredduringschedulingignoredduringexecutionindexpreference","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].preference","text":"<p>\u21a9 Parent</p> <p>A node selector term, associated with the corresponding weight.</p> Name Type Description Required matchExpressions []object            A list of node selector requirements by node's labels. false matchFields []object            A list of node selector requirements by node's fields. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinitypreferredduringschedulingignoredduringexecutionindexpreferencematchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].preference.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            The label key that the selector applies to. true operator string            Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. true values []string            An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinitypreferredduringschedulingignoredduringexecutionindexpreferencematchfieldsindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].preference.matchFields[index]","text":"<p>\u21a9 Parent</p> <p>A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            The label key that the selector applies to. true operator string            Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. true values []string            An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinityrequiredduringschedulingignoredduringexecution","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution","text":"<p>\u21a9 Parent</p> <p>If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to an update), the system may or may not try to eventually evict the pod from its node.</p> Name Type Description Required nodeSelectorTerms []object            Required. A list of node selector terms. The terms are ORed. true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinityrequiredduringschedulingignoredduringexecutionnodeselectortermsindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[index]","text":"<p>\u21a9 Parent</p> <p>A null or empty node selector term matches no objects. The requirements of them are ANDed. The TopologySelectorTerm type implements a subset of the NodeSelectorTerm.</p> Name Type Description Required matchExpressions []object            A list of node selector requirements by node's labels. false matchFields []object            A list of node selector requirements by node's fields. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinityrequiredduringschedulingignoredduringexecutionnodeselectortermsindexmatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[index].matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            The label key that the selector applies to. true operator string            Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. true values []string            An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitynodeaffinityrequiredduringschedulingignoredduringexecutionnodeselectortermsindexmatchfieldsindex","title":"OpenTelemetryCollector.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[index].matchFields[index]","text":"<p>\u21a9 Parent</p> <p>A node selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            The label key that the selector applies to. true operator string            Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist. Gt, and Lt. true values []string            An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinity","title":"OpenTelemetryCollector.spec.affinity.podAffinity","text":"<p>\u21a9 Parent</p> <p>Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)).</p> Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution []object            The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. false requiredDuringSchedulingIgnoredDuringExecution []object            If the affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index]","text":"<p>\u21a9 Parent</p> <p>The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)</p> Name Type Description Required podAffinityTerm object            Required. A pod affinity term, associated with the corresponding weight. true weight integer            weight associated with matching the corresponding podAffinityTerm, in the range 1-100. Format: int32 true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinityterm","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm","text":"<p>\u21a9 Parent</p> <p>Required. A pod affinity term, associated with the corresponding weight.</p> Name Type Description Required topologyKey string            This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. true labelSelector object            A label query over a set of resources, in this case pods. false namespaceSelector object            A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces. false namespaces []string            namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \"this pod's namespace\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermlabelselector","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.labelSelector","text":"<p>\u21a9 Parent</p> <p>A label query over a set of resources, in this case pods.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermlabelselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.labelSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermnamespaceselector","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.namespaceSelector","text":"<p>\u21a9 Parent</p> <p>A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermnamespaceselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.namespaceSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinityrequiredduringschedulingignoredduringexecutionindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[index]","text":"<p>\u21a9 Parent</p> <p>Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key  matches that of any node on which a pod of the set of pods is running Name Type Description Required topologyKey string            This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. true labelSelector object            A label query over a set of resources, in this case pods. false namespaceSelector object            A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces. false namespaces []string            namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \"this pod's namespace\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinityrequiredduringschedulingignoredduringexecutionindexlabelselector","title":"OpenTelemetryCollector.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].labelSelector","text":"<p>\u21a9 Parent</p> <p>A label query over a set of resources, in this case pods.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinityrequiredduringschedulingignoredduringexecutionindexlabelselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].labelSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinityrequiredduringschedulingignoredduringexecutionindexnamespaceselector","title":"OpenTelemetryCollector.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].namespaceSelector","text":"<p>\u21a9 Parent</p> <p>A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodaffinityrequiredduringschedulingignoredduringexecutionindexnamespaceselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].namespaceSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinity","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity","text":"<p>\u21a9 Parent</p> <p>Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)).</p> Name Type Description Required preferredDuringSchedulingIgnoredDuringExecution []object            The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified by this field, but it may choose a node that violates one or more of the expressions. The node that is most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.), compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most preferred. false requiredDuringSchedulingIgnoredDuringExecution []object            If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be met at some point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index]","text":"<p>\u21a9 Parent</p> <p>The weights of all of the matched WeightedPodAffinityTerm fields are added per-node to find the most preferred node(s)</p> Name Type Description Required podAffinityTerm object            Required. A pod affinity term, associated with the corresponding weight. true weight integer            weight associated with matching the corresponding podAffinityTerm, in the range 1-100. Format: int32 true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinityterm","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm","text":"<p>\u21a9 Parent</p> <p>Required. A pod affinity term, associated with the corresponding weight.</p> Name Type Description Required topologyKey string            This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. true labelSelector object            A label query over a set of resources, in this case pods. false namespaceSelector object            A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces. false namespaces []string            namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \"this pod's namespace\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermlabelselector","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.labelSelector","text":"<p>\u21a9 Parent</p> <p>A label query over a set of resources, in this case pods.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermlabelselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.labelSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermnamespaceselector","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.namespaceSelector","text":"<p>\u21a9 Parent</p> <p>A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinitypreferredduringschedulingignoredduringexecutionindexpodaffinitytermnamespaceselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution[index].podAffinityTerm.namespaceSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinityrequiredduringschedulingignoredduringexecutionindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[index]","text":"<p>\u21a9 Parent</p> <p>Defines a set of pods (namely those matching the labelSelector relative to the given namespace(s)) that this pod should be co-located (affinity) or not co-located (anti-affinity) with, where co-located is defined as running on a node whose value of the label with key  matches that of any node on which a pod of the set of pods is running Name Type Description Required topologyKey string            This pod should be co-located (affinity) or not co-located (anti-affinity) with the pods matching the labelSelector in the specified namespaces, where co-located is defined as running on a node whose value of the label with key topologyKey matches that of any node on which any of the selected pods is running. Empty topologyKey is not allowed. true labelSelector object            A label query over a set of resources, in this case pods. false namespaceSelector object            A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces. false namespaces []string            namespaces specifies a static list of namespace names that the term applies to. The term is applied to the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or empty namespaces list and null namespaceSelector means \"this pod's namespace\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinityrequiredduringschedulingignoredduringexecutionindexlabelselector","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].labelSelector","text":"<p>\u21a9 Parent</p> <p>A label query over a set of resources, in this case pods.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinityrequiredduringschedulingignoredduringexecutionindexlabelselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].labelSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinityrequiredduringschedulingignoredduringexecutionindexnamespaceselector","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].namespaceSelector","text":"<p>\u21a9 Parent</p> <p>A label query over the set of namespaces that the term applies to. The term is applied to the union of the namespaces selected by this field and the ones listed in the namespaces field. null selector and null or empty namespaces list means \"this pod's namespace\". An empty selector ({}) matches all namespaces.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecaffinitypodantiaffinityrequiredduringschedulingignoredduringexecutionindexnamespaceselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.affinity.podAntiAffinity.requiredDuringSchedulingIgnoredDuringExecution[index].namespaceSelector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscaler","title":"OpenTelemetryCollector.spec.autoscaler","text":"<p>\u21a9 Parent</p> <p>Autoscaler specifies the pod autoscaling configuration to use for the OpenTelemetryCollector workload.</p> Name Type Description Required behavior object            HorizontalPodAutoscalerBehavior configures the scaling behavior of the target in both Up and Down directions (scaleUp and scaleDown fields respectively). false maxReplicas integer            MaxReplicas sets an upper bound to the autoscaling feature. If MaxReplicas is set autoscaling is enabled. Format: int32 false metrics []object            Metrics is meant to provide a customizable way to configure HPA metrics. currently the only supported custom metrics is type=Pod. Use TargetCPUUtilization or TargetMemoryUtilization instead if scaling on these common resource metrics. false minReplicas integer            MinReplicas sets a lower bound to the autoscaling feature.  Set this if your are using autoscaling. It must be at least 1 Format: int32 false targetCPUUtilization integer            TargetCPUUtilization sets the target average CPU used across all replicas. If average CPU exceeds this value, the HPA will scale up. Defaults to 90 percent. Format: int32 false targetMemoryUtilization integer            TargetMemoryUtilization sets the target average memory utilization across all replicas Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalerbehavior","title":"OpenTelemetryCollector.spec.autoscaler.behavior","text":"<p>\u21a9 Parent</p> <p>HorizontalPodAutoscalerBehavior configures the scaling behavior of the target in both Up and Down directions (scaleUp and scaleDown fields respectively).</p> Name Type Description Required scaleDown object            scaleDown is scaling policy for scaling Down. If not set, the default value is to allow to scale down to minReplicas pods, with a 300 second stabilization window (i.e., the highest recommendation for the last 300sec is used). false scaleUp object            scaleUp is scaling policy for scaling Up. If not set, the default value is the higher of: * increase no more than 4 pods per 60 seconds * double the number of pods per 60 seconds No stabilization is used. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalerbehaviorscaledown","title":"OpenTelemetryCollector.spec.autoscaler.behavior.scaleDown","text":"<p>\u21a9 Parent</p> <p>scaleDown is scaling policy for scaling Down. If not set, the default value is to allow to scale down to minReplicas pods, with a 300 second stabilization window (i.e., the highest recommendation for the last 300sec is used).</p> Name Type Description Required policies []object            policies is a list of potential scaling polices which can be used during scaling. At least one policy must be specified, otherwise the HPAScalingRules will be discarded as invalid false selectPolicy string            selectPolicy is used to specify which policy should be used. If not set, the default value Max is used. false stabilizationWindowSeconds integer            stabilizationWindowSeconds is the number of seconds for which past recommendations should be considered while scaling up or scaling down. StabilizationWindowSeconds must be greater than or equal to zero and less than or equal to 3600 (one hour). If not set, use the default values: - For scale up: 0 (i.e. no stabilization is done). - For scale down: 300 (i.e. the stabilization window is 300 seconds long). Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalerbehaviorscaledownpoliciesindex","title":"OpenTelemetryCollector.spec.autoscaler.behavior.scaleDown.policies[index]","text":"<p>\u21a9 Parent</p> <p>HPAScalingPolicy is a single policy which must hold true for a specified past interval.</p> Name Type Description Required periodSeconds integer            periodSeconds specifies the window of time for which the policy should hold true. PeriodSeconds must be greater than zero and less than or equal to 1800 (30 min). Format: int32 true type string            type is used to specify the scaling policy. true value integer            value contains the amount of change which is permitted by the policy. It must be greater than zero Format: int32 true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalerbehaviorscaleup","title":"OpenTelemetryCollector.spec.autoscaler.behavior.scaleUp","text":"<p>\u21a9 Parent</p> <p>scaleUp is scaling policy for scaling Up. If not set, the default value is the higher of: _ increase no more than 4 pods per 60 seconds _ double the number of pods per 60 seconds No stabilization is used.</p> Name Type Description Required policies []object            policies is a list of potential scaling polices which can be used during scaling. At least one policy must be specified, otherwise the HPAScalingRules will be discarded as invalid false selectPolicy string            selectPolicy is used to specify which policy should be used. If not set, the default value Max is used. false stabilizationWindowSeconds integer            stabilizationWindowSeconds is the number of seconds for which past recommendations should be considered while scaling up or scaling down. StabilizationWindowSeconds must be greater than or equal to zero and less than or equal to 3600 (one hour). If not set, use the default values: - For scale up: 0 (i.e. no stabilization is done). - For scale down: 300 (i.e. the stabilization window is 300 seconds long). Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalerbehaviorscaleuppoliciesindex","title":"OpenTelemetryCollector.spec.autoscaler.behavior.scaleUp.policies[index]","text":"<p>\u21a9 Parent</p> <p>HPAScalingPolicy is a single policy which must hold true for a specified past interval.</p> Name Type Description Required periodSeconds integer            periodSeconds specifies the window of time for which the policy should hold true. PeriodSeconds must be greater than zero and less than or equal to 1800 (30 min). Format: int32 true type string            type is used to specify the scaling policy. true value integer            value contains the amount of change which is permitted by the policy. It must be greater than zero Format: int32 true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindex","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index]","text":"<p>\u21a9 Parent</p> <p>MetricSpec defines a subset of metrics to be defined for the HPA's metric array more metric type can be supported as needed. See https://pkg.go.dev/k8s.io/api/autoscaling/v2#MetricSpec for reference.</p> Name Type Description Required type string            MetricSourceType indicates the type of metric. true pods object            PodsMetricSource indicates how to scale on a metric describing each pod in the current scale target (for example, transactions-processed-per-second). The values will be averaged together before being compared to the target value. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindexpods","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index].pods","text":"<p>\u21a9 Parent</p> <p>PodsMetricSource indicates how to scale on a metric describing each pod in the current scale target (for example, transactions-processed-per-second). The values will be averaged together before being compared to the target value.</p> Name Type Description Required metric object            metric identifies the target metric by name and selector true target object            target specifies the target value for the given metric true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindexpodsmetric","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index].pods.metric","text":"<p>\u21a9 Parent</p> <p>metric identifies the target metric by name and selector</p> Name Type Description Required name string            name is the name of the given metric true selector object            selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindexpodsmetricselector","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index].pods.metric.selector","text":"<p>\u21a9 Parent</p> <p>selector is the string-encoded form of a standard kubernetes label selector for the given metric When set, it is passed as an additional parameter to the metrics server for more specific metrics scoping. When unset, just the metricName will be used to gather metrics.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindexpodsmetricselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index].pods.metric.selector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecautoscalermetricsindexpodstarget","title":"OpenTelemetryCollector.spec.autoscaler.metrics[index].pods.target","text":"<p>\u21a9 Parent</p> <p>target specifies the target value for the given metric</p> Name Type Description Required type string            type represents whether the metric type is Utilization, Value, or AverageValue true averageUtilization integer            averageUtilization is the target value of the average of the resource metric across all relevant pods, represented as a percentage of the requested value of the resource for the pods. Currently only valid for Resource metric source type Format: int32 false averageValue int or string            averageValue is the target value of the average of the metric across all relevant pods (as a quantity) false value int or string            value is the target value of the metric (as a quantity). false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindex","title":"OpenTelemetryCollector.spec.env[index]","text":"<p>\u21a9 Parent</p> <p>EnvVar represents an environment variable present in a Container.</p> Name Type Description Required name string            Name of the environment variable. Must be a C_IDENTIFIER. true value string            Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. \"$$(VAR_NAME)\" will produce the string literal \"$(VAR_NAME)\". Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to \"\". false valueFrom object            Source for the environment variable's value. Cannot be used if value is not empty. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindexvaluefrom","title":"OpenTelemetryCollector.spec.env[index].valueFrom","text":"<p>\u21a9 Parent</p> <p>Source for the environment variable's value. Cannot be used if value is not empty.</p> Name Type Description Required configMapKeyRef object            Selects a key of a ConfigMap. false fieldRef object            Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['']`, `metadata.annotations['']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef object            Selects a key of a secret in the pod's namespace false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindexvaluefromconfigmapkeyref","title":"OpenTelemetryCollector.spec.env[index].valueFrom.configMapKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a ConfigMap.</p> Name Type Description Required key string            The key to select. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap or its key must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindexvaluefromfieldref","title":"OpenTelemetryCollector.spec.env[index].valueFrom.fieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a field of the pod: supports metadata.name, metadata.namespace, <code>metadata.labels['&lt;KEY&gt;']</code>, <code>metadata.annotations['&lt;KEY&gt;']</code>, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindexvaluefromresourcefieldref","title":"OpenTelemetryCollector.spec.env[index].valueFrom.resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvindexvaluefromsecretkeyref","title":"OpenTelemetryCollector.spec.env[index].valueFrom.secretKeyRef","text":"<p>\u21a9 Parent</p> <p>Selects a key of a secret in the pod's namespace</p> Name Type Description Required key string            The key of the secret to select from.  Must be a valid secret key. true name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvfromindex","title":"OpenTelemetryCollector.spec.envFrom[index]","text":"<p>\u21a9 Parent</p> <p>EnvFromSource represents the source of a set of ConfigMaps</p> Name Type Description Required configMapRef object            The ConfigMap to select from false prefix string            An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER. false secretRef object            The Secret to select from false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvfromindexconfigmapref","title":"OpenTelemetryCollector.spec.envFrom[index].configMapRef","text":"<p>\u21a9 Parent</p> <p>The ConfigMap to select from</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the ConfigMap must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecenvfromindexsecretref","title":"OpenTelemetryCollector.spec.envFrom[index].secretRef","text":"<p>\u21a9 Parent</p> <p>The Secret to select from</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            Specify whether the Secret must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecingress","title":"OpenTelemetryCollector.spec.ingress","text":"<p>\u21a9 Parent</p> <p>Ingress is used to specify how OpenTelemetry Collector is exposed. This functionality is only available if one of the valid modes is set. Valid modes are: deployment, daemonset and statefulset.</p> Name Type Description Required annotations map[string]string            Annotations to add to ingress. e.g. 'cert-manager.io/cluster-issuer: \"letsencrypt\"' false hostname string            Hostname by which the ingress proxy can be reached. false ingressClassName string            IngressClassName is the name of an IngressClass cluster resource. Ingress controller implementations use this field to know whether they should be serving this Ingress resource. false route object            Route is an OpenShift specific section that is only considered when type \"route\" is used. false tls []object            TLS configuration. false type enum            Type default value is: \"\" Supported types are: ingress Enum: ingress, route false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecingressroute","title":"OpenTelemetryCollector.spec.ingress.route","text":"<p>\u21a9 Parent</p> <p>Route is an OpenShift specific section that is only considered when type \"route\" is used.</p> Name Type Description Required termination enum            Termination indicates termination type. By default \"edge\" is used. Enum: insecure, edge, passthrough, reencrypt false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecingresstlsindex","title":"OpenTelemetryCollector.spec.ingress.tls[index]","text":"<p>\u21a9 Parent</p> <p>IngressTLS describes the transport layer security associated with an ingress.</p> Name Type Description Required hosts []string            hosts is a list of hosts included in the TLS certificate. The values in this list must match the name/s used in the tlsSecret. Defaults to the wildcard host setting for the loadbalancer controller fulfilling this Ingress, if left unspecified. false secretName string            secretName is the name of the secret used to terminate TLS traffic on port 443. Field is left optional to allow TLS routing based on SNI hostname alone. If the SNI host in a listener conflicts with the \"Host\" header field used by an IngressRule, the SNI host is used for termination and value of the \"Host\" header is used for routing. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycle","title":"OpenTelemetryCollector.spec.lifecycle","text":"<p>\u21a9 Parent</p> <p>Actions that the management system should take in response to container lifecycle events. Cannot be updated.</p> Name Type Description Required postStart object            PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks false preStop object            PreStop is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The Pod's termination grace period countdown begins before the PreStop hook is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period (unless delayed by finalizers). Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecyclepoststart","title":"OpenTelemetryCollector.spec.lifecycle.postStart","text":"<p>\u21a9 Parent</p> <p>PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks</p> Name Type Description Required exec object            Exec specifies the action to take. false httpGet object            HTTPGet specifies the http request to perform. false tcpSocket object            Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility. There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is specified. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecyclepoststartexec","title":"OpenTelemetryCollector.spec.lifecycle.postStart.exec","text":"<p>\u21a9 Parent</p> <p>Exec specifies the action to take.</p> Name Type Description Required command []string            Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecyclepoststarthttpget","title":"OpenTelemetryCollector.spec.lifecycle.postStart.httpGet","text":"<p>\u21a9 Parent</p> <p>HTTPGet specifies the http request to perform.</p> Name Type Description Required port int or string            Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. true host string            Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. false httpHeaders []object            Custom headers to set in the request. HTTP allows repeated headers. false path string            Path to access on the HTTP server. false scheme string            Scheme to use for connecting to the host. Defaults to HTTP. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecyclepoststarthttpgethttpheadersindex","title":"OpenTelemetryCollector.spec.lifecycle.postStart.httpGet.httpHeaders[index]","text":"<p>\u21a9 Parent</p> <p>HTTPHeader describes a custom header to be used in HTTP probes</p> Name Type Description Required name string            The header field name. This will be canonicalized upon output, so case-variant names will be understood as the same header. true value string            The header field value true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecyclepoststarttcpsocket","title":"OpenTelemetryCollector.spec.lifecycle.postStart.tcpSocket","text":"<p>\u21a9 Parent</p> <p>Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility. There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is specified.</p> Name Type Description Required port int or string            Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. true host string            Optional: Host name to connect to, defaults to the pod IP. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycleprestop","title":"OpenTelemetryCollector.spec.lifecycle.preStop","text":"<p>\u21a9 Parent</p> <p>PreStop is called immediately before a container is terminated due to an API request or management event such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called if the container crashes or exits. The Pod's termination grace period countdown begins before the PreStop hook is executed. Regardless of the outcome of the handler, the container will eventually terminate within the Pod's termination grace period (unless delayed by finalizers). Other management of the container blocks until the hook completes or until the termination grace period is reached. More info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks</p> Name Type Description Required exec object            Exec specifies the action to take. false httpGet object            HTTPGet specifies the http request to perform. false tcpSocket object            Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility. There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is specified. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycleprestopexec","title":"OpenTelemetryCollector.spec.lifecycle.preStop.exec","text":"<p>\u21a9 Parent</p> <p>Exec specifies the action to take.</p> Name Type Description Required command []string            Command is the command line to execute inside the container, the working directory for the command  is root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycleprestophttpget","title":"OpenTelemetryCollector.spec.lifecycle.preStop.httpGet","text":"<p>\u21a9 Parent</p> <p>HTTPGet specifies the http request to perform.</p> Name Type Description Required port int or string            Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. true host string            Host name to connect to, defaults to the pod IP. You probably want to set \"Host\" in httpHeaders instead. false httpHeaders []object            Custom headers to set in the request. HTTP allows repeated headers. false path string            Path to access on the HTTP server. false scheme string            Scheme to use for connecting to the host. Defaults to HTTP. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycleprestophttpgethttpheadersindex","title":"OpenTelemetryCollector.spec.lifecycle.preStop.httpGet.httpHeaders[index]","text":"<p>\u21a9 Parent</p> <p>HTTPHeader describes a custom header to be used in HTTP probes</p> Name Type Description Required name string            The header field name. This will be canonicalized upon output, so case-variant names will be understood as the same header. true value string            The header field value true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclifecycleprestoptcpsocket","title":"OpenTelemetryCollector.spec.lifecycle.preStop.tcpSocket","text":"<p>\u21a9 Parent</p> <p>Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility. There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is specified.</p> Name Type Description Required port int or string            Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. true host string            Optional: Host name to connect to, defaults to the pod IP. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspeclivenessprobe","title":"OpenTelemetryCollector.spec.livenessProbe","text":"<p>\u21a9 Parent</p> <p>Liveness config for the OpenTelemetry Collector except the probe handler which is auto generated from the health extension of the collector. It is only effective when healthcheckextension is configured in the OpenTelemetry Collector pipeline.</p> Name Type Description Required failureThreshold integer            Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. Format: int32 false initialDelaySeconds integer            Number of seconds after the container has started before liveness probes are initiated. Defaults to 0 seconds. Minimum value is 0. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes Format: int32 false periodSeconds integer            How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. Format: int32 false successThreshold integer            Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1. Format: int32 false terminationGracePeriodSeconds integer            Optional duration in seconds the pod needs to terminate gracefully upon probe failure. The grace period is the duration in seconds after the processes running in the pod are sent a termination signal and the time when the processes are forcibly halted with a kill signal. Set this value longer than the expected cleanup time for your process. If this value is nil, the pod's terminationGracePeriodSeconds will be used. Otherwise, this value overrides the value provided by the pod spec. Value must be non-negative integer. The value zero indicates stop immediately via the kill signal (no opportunity to shut down). This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate. Minimum value is 1. spec.terminationGracePeriodSeconds is used if unset. Format: int64 false timeoutSeconds integer            Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecpodsecuritycontext","title":"OpenTelemetryCollector.spec.podSecurityContext","text":"<p>\u21a9 Parent</p> <p>PodSecurityContext holds pod-level security attributes and common container settings. Some fields are also present in container.securityContext. Field values of container.securityContext take precedence over field values of PodSecurityContext.</p> Name Type Description Required fsGroup integer            A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet to change the ownership of that volume to be owned by the pod:   1. The owning GID will be the FSGroup 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission bits are OR'd with rw-rw----   If unset, the Kubelet will not modify the ownership and permissions of any volume. Note that this field cannot be set when spec.os.name is windows. Format: int64 false fsGroupChangePolicy string            fsGroupChangePolicy defines behavior of changing ownership and permission of the volume before being exposed inside Pod. This field will only apply to volume types which support fsGroup based ownership(and permissions). It will have no effect on ephemeral volume types such as: secret, configmaps and emptydir. Valid values are \"OnRootMismatch\" and \"Always\". If not specified, \"Always\" is used. Note that this field cannot be set when spec.os.name is windows. false runAsGroup integer            The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. Note that this field cannot be set when spec.os.name is windows. Format: int64 false runAsNonRoot boolean            Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. false runAsUser integer            The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. Note that this field cannot be set when spec.os.name is windows. Format: int64 false seLinuxOptions object            The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container.  May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. Note that this field cannot be set when spec.os.name is windows. false seccompProfile object            The seccomp options to use by the containers in this pod. Note that this field cannot be set when spec.os.name is windows. false supplementalGroups []integer            A list of groups applied to the first process run in each container, in addition to the container's primary GID, the fsGroup (if specified), and group memberships defined in the container image for the uid of the container process. If unspecified, no additional groups are added to any container. Note that group memberships defined in the container image for the uid of the container process are still effective, even if they are not included in this list. Note that this field cannot be set when spec.os.name is windows. false sysctls []object            Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the container runtime) might fail to launch. Note that this field cannot be set when spec.os.name is windows. false windowsOptions object            The Windows specific settings applied to all containers. If unspecified, the options within a container's SecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecpodsecuritycontextselinuxoptions","title":"OpenTelemetryCollector.spec.podSecurityContext.seLinuxOptions","text":"<p>\u21a9 Parent</p> <p>The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in SecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that container. Note that this field cannot be set when spec.os.name is windows.</p> Name Type Description Required level string            Level is SELinux level label that applies to the container. false role string            Role is a SELinux role label that applies to the container. false type string            Type is a SELinux type label that applies to the container. false user string            User is a SELinux user label that applies to the container. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecpodsecuritycontextseccompprofile","title":"OpenTelemetryCollector.spec.podSecurityContext.seccompProfile","text":"<p>\u21a9 Parent</p> <p>The seccomp options to use by the containers in this pod. Note that this field cannot be set when spec.os.name is windows.</p> Name Type Description Required type string            type indicates which kind of seccomp profile will be applied. Valid options are:   Localhost - a profile defined in a file on the node should be used. RuntimeDefault - the container runtime default profile should be used. Unconfined - no profile should be applied. true localhostProfile string            localhostProfile indicates a profile defined in a file on the node should be used. The profile must be preconfigured on the node to work. Must be a descending path, relative to the kubelet's configured seccomp profile location. Must only be set if type is \"Localhost\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecpodsecuritycontextsysctlsindex","title":"OpenTelemetryCollector.spec.podSecurityContext.sysctls[index]","text":"<p>\u21a9 Parent</p> <p>Sysctl defines a kernel parameter to be set</p> Name Type Description Required name string            Name of a property to set true value string            Value of a property to set true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecpodsecuritycontextwindowsoptions","title":"OpenTelemetryCollector.spec.podSecurityContext.windowsOptions","text":"<p>\u21a9 Parent</p> <p>The Windows specific settings applied to all containers. If unspecified, the options within a container's SecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux.</p> Name Type Description Required gmsaCredentialSpec string            GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. false gmsaCredentialSpecName string            GMSACredentialSpecName is the name of the GMSA credential spec to use. false hostProcess boolean            HostProcess determines if a container should be run as a 'Host Process' container. This field is alpha-level and will only be honored by components that enable the WindowsHostProcessContainers feature flag. Setting this field without the feature flag will result in errors when validating the Pod. All of a Pod's containers must have the same effective HostProcess value (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).  In addition, if HostProcess is true then HostNetwork must also be set to true. false runAsUserName string            The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecportsindex","title":"OpenTelemetryCollector.spec.ports[index]","text":"<p>\u21a9 Parent</p> <p>ServicePort contains information on service's port.</p> Name Type Description Required port integer            The port that will be exposed by this service. Format: int32 true appProtocol string            The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed names are reserved for IANA standard service names (as per RFC-6335 and https://www.iana.org/assignments/service-names). Non-standard protocols should use prefixed names such as mycompany.com/my-custom-protocol. false name string            The name of this port within the service. This must be a DNS_LABEL. All ports within a ServiceSpec must have unique names. When considering the endpoints for a Service, this must match the 'name' field in the EndpointPort. Optional if only one ServicePort is defined on this service. false nodePort integer            The port on each node on which this service is exposed when type is NodePort or LoadBalancer.  Usually assigned by the system. If a value is specified, in-range, and not in use it will be used, otherwise the operation will fail.  If not specified, a port will be allocated if this Service requires one.  If this field is specified when creating a Service which does not need it, creation will fail. This field will be wiped when updating a Service to no longer need it (e.g. changing type from NodePort to ClusterIP). More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport Format: int32 false protocol string            The IP protocol for this port. Supports \"TCP\", \"UDP\", and \"SCTP\". Default is TCP. Default: TCP false targetPort int or string            Number or name of the port to access on the pods targeted by the service. Number must be in the range 1 to 65535. Name must be an IANA_SVC_NAME. If this is a string, it will be looked up as a named port in the target Pod's container ports. If this is not specified, the value of the 'port' field is used (an identity map). This field is ignored for services with clusterIP=None, and should be omitted or set equal to the 'port' field. More info: https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecresources","title":"OpenTelemetryCollector.spec.resources","text":"<p>\u21a9 Parent</p> <p>Resources to set on the OpenTelemetry Collector pods.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecresourcesclaimsindex","title":"OpenTelemetryCollector.spec.resources.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecsecuritycontext","title":"OpenTelemetryCollector.spec.securityContext","text":"<p>\u21a9 Parent</p> <p>SecurityContext will be set as the container security context.</p> Name Type Description Required allowPrivilegeEscalation boolean            AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process. This bool directly controls if the no_new_privs flag will be set on the container process. AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN Note that this field cannot be set when spec.os.name is windows. false capabilities object            The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. Note that this field cannot be set when spec.os.name is windows. false privileged boolean            Run container in privileged mode. Processes in privileged containers are essentially equivalent to root on the host. Defaults to false. Note that this field cannot be set when spec.os.name is windows. false procMount string            procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType feature flag to be enabled. Note that this field cannot be set when spec.os.name is windows. false readOnlyRootFilesystem boolean            Whether this container has a read-only root filesystem. Default is false. Note that this field cannot be set when spec.os.name is windows. false runAsGroup integer            The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows. Format: int64 false runAsNonRoot boolean            Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If unset or false, no such validation will be performed. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. false runAsUser integer            The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if unspecified. May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows. Format: int64 false seLinuxOptions object            The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container.  May also be set in PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows. false seccompProfile object            The seccomp options to use by this container. If seccomp options are provided at both the pod &amp; container level, the container options override the pod options. Note that this field cannot be set when spec.os.name is windows. false windowsOptions object            The Windows specific settings applied to all containers. If unspecified, the options from the PodSecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecsecuritycontextcapabilities","title":"OpenTelemetryCollector.spec.securityContext.capabilities","text":"<p>\u21a9 Parent</p> <p>The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted by the container runtime. Note that this field cannot be set when spec.os.name is windows.</p> Name Type Description Required add []string            Added capabilities false drop []string            Removed capabilities false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecsecuritycontextselinuxoptions","title":"OpenTelemetryCollector.spec.securityContext.seLinuxOptions","text":"<p>\u21a9 Parent</p> <p>The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a random SELinux context for each container. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows.</p> Name Type Description Required level string            Level is SELinux level label that applies to the container. false role string            Role is a SELinux role label that applies to the container. false type string            Type is a SELinux type label that applies to the container. false user string            User is a SELinux user label that applies to the container. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecsecuritycontextseccompprofile","title":"OpenTelemetryCollector.spec.securityContext.seccompProfile","text":"<p>\u21a9 Parent</p> <p>The seccomp options to use by this container. If seccomp options are provided at both the pod &amp; container level, the container options override the pod options. Note that this field cannot be set when spec.os.name is windows.</p> Name Type Description Required type string            type indicates which kind of seccomp profile will be applied. Valid options are:   Localhost - a profile defined in a file on the node should be used. RuntimeDefault - the container runtime default profile should be used. Unconfined - no profile should be applied. true localhostProfile string            localhostProfile indicates a profile defined in a file on the node should be used. The profile must be preconfigured on the node to work. Must be a descending path, relative to the kubelet's configured seccomp profile location. Must only be set if type is \"Localhost\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecsecuritycontextwindowsoptions","title":"OpenTelemetryCollector.spec.securityContext.windowsOptions","text":"<p>\u21a9 Parent</p> <p>The Windows specific settings applied to all containers. If unspecified, the options from the PodSecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux.</p> Name Type Description Required gmsaCredentialSpec string            GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa) inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field. false gmsaCredentialSpecName string            GMSACredentialSpecName is the name of the GMSA credential spec to use. false hostProcess boolean            HostProcess determines if a container should be run as a 'Host Process' container. This field is alpha-level and will only be honored by components that enable the WindowsHostProcessContainers feature flag. Setting this field without the feature flag will result in errors when validating the Pod. All of a Pod's containers must have the same effective HostProcess value (it is not allowed to have a mix of HostProcess containers and non-HostProcess containers).  In addition, if HostProcess is true then HostNetwork must also be set to true. false runAsUserName string            The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspectargetallocator","title":"OpenTelemetryCollector.spec.targetAllocator","text":"<p>\u21a9 Parent</p> <p>TargetAllocator indicates a value which determines whether to spawn a target allocation resource or not.</p> Name Type Description Required allocationStrategy enum            AllocationStrategy determines which strategy the target allocator should use for allocation. The current options are least-weighted and consistent-hashing. The default option is least-weighted Enum: least-weighted, consistent-hashing false enabled boolean            Enabled indicates whether to use a target allocation mechanism for Prometheus targets or not. false filterStrategy string            FilterStrategy determines how to filter targets before allocating them among the collectors. The only current option is relabel-config (drops targets based on prom relabel_config). Filtering is disabled by default. false image string            Image indicates the container image to use for the OpenTelemetry TargetAllocator. false prometheusCR object            PrometheusCR defines the configuration for the retrieval of PrometheusOperator CRDs ( servicemonitor.monitoring.coreos.com/v1 and podmonitor.monitoring.coreos.com/v1 )  retrieval. All CR instances which the ServiceAccount has access to will be retrieved. This includes other namespaces. false replicas integer            Replicas is the number of pod instances for the underlying TargetAllocator. This should only be set to a value other than 1 if a strategy that allows for high availability is chosen. Currently, the only allocation strategy that can be run in a high availability mode is consistent-hashing. Format: int32 false resources object            Resources to set on the OpenTelemetryTargetAllocator containers. false serviceAccount string            ServiceAccount indicates the name of an existing service account to use with this instance. When set, the operator will not automatically create a ServiceAccount for the TargetAllocator. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspectargetallocatorprometheuscr","title":"OpenTelemetryCollector.spec.targetAllocator.prometheusCR","text":"<p>\u21a9 Parent</p> <p>PrometheusCR defines the configuration for the retrieval of PrometheusOperator CRDs ( servicemonitor.monitoring.coreos.com/v1 and podmonitor.monitoring.coreos.com/v1 ) retrieval. All CR instances which the ServiceAccount has access to will be retrieved. This includes other namespaces.</p> Name Type Description Required enabled boolean            Enabled indicates whether to use a PrometheusOperator custom resources as targets or not. false podMonitorSelector map[string]string            PodMonitors to be selected for target discovery. This is a map of {key,value} pairs. Each {key,value} in the map is going to exactly match a label in a PodMonitor's meta labels. The requirements are ANDed. false serviceMonitorSelector map[string]string            ServiceMonitors to be selected for target discovery. This is a map of {key,value} pairs. Each {key,value} in the map is going to exactly match a label in a ServiceMonitor's meta labels. The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspectargetallocatorresources","title":"OpenTelemetryCollector.spec.targetAllocator.resources","text":"<p>\u21a9 Parent</p> <p>Resources to set on the OpenTelemetryTargetAllocator containers.</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspectargetallocatorresourcesclaimsindex","title":"OpenTelemetryCollector.spec.targetAllocator.resources.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspectolerationsindex","title":"OpenTelemetryCollector.spec.tolerations[index]","text":"<p>\u21a9 Parent</p> <p>The pod this Toleration is attached to tolerates any taint that matches the triple  using the matching operator . Name Type Description Required effect string            Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed values are NoSchedule, PreferNoSchedule and NoExecute. false key string            Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is empty, operator must be Exists; this combination means to match all values and all keys. false operator string            Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular category. false tolerationSeconds integer            TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute, otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by the system. Format: int64 false value string            Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty, otherwise just a regular string. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindex","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index]","text":"<p>\u21a9 Parent</p> <p>PersistentVolumeClaim is a user's request for and claim to a persistent volume</p> Name Type Description Required apiVersion string            APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources false kind string            Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds false metadata object            Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata false spec object            spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims false status object            status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexmetadata","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].metadata","text":"<p>\u21a9 Parent</p> <p>Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</p> Name Type Description Required annotations map[string]string false finalizers []string false labels map[string]string false name string false namespace string false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspec","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec","text":"<p>\u21a9 Parent</p> <p>spec defines the desired characteristics of a volume requested by a pod author. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims</p> Name Type Description Required accessModes []string            accessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 false dataSource object            dataSource field can be used to specify either: * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot) * An existing PVC (PersistentVolumeClaim) If the provisioner or an external controller can support the specified data source, it will create a new volume based on the contents of the specified data source. When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef, and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified. If the namespace is specified, then dataSourceRef will not be copied to dataSource. false dataSourceRef object            dataSourceRef specifies the object from which to populate the volume with data, if a non-empty volume is desired. This may be any object from a non-empty API group (non core object) or a PersistentVolumeClaim object. When this field is specified, volume binding will only succeed if the type of the specified object matches some installed volume populator or dynamic provisioner. This field will replace the functionality of the dataSource field and as such if both fields are non-empty, they must have the same value. For backwards compatibility, when namespace isn't specified in dataSourceRef, both fields (dataSource and dataSourceRef) will be set to the same value automatically if one of them is empty and the other is non-empty. When namespace is specified in dataSourceRef, dataSource isn't set to the same value and must be empty. There are three important differences between dataSource and dataSourceRef: * While dataSource only allows two specific types of objects, dataSourceRef allows any non-core object, as well as PersistentVolumeClaim objects. * While dataSource ignores disallowed values (dropping them), dataSourceRef preserves all values, and generates an error if a disallowed value is specified. * While dataSource only allows local objects, dataSourceRef allows objects in any namespaces. (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled. (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled. false resources object            resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements that are lower than previous value but must still be higher than capacity recorded in the status field of the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources false selector object            selector is a label query over volumes to consider for binding. false storageClassName string            storageClassName is the name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 false volumeMode string            volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. false volumeName string            volumeName is the binding reference to the PersistentVolume backing this claim. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecdatasource","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.dataSource","text":"<p>\u21a9 Parent</p> <p>dataSource field can be used to specify either: _ An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot) _ An existing PVC (PersistentVolumeClaim) If the provisioner or an external controller can support the specified data source, it will create a new volume based on the contents of the specified data source. When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef, and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified. If the namespace is specified, then dataSourceRef will not be copied to dataSource.</p> Name Type Description Required kind string            Kind is the type of resource being referenced true name string            Name is the name of resource being referenced true apiGroup string            APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecdatasourceref","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.dataSourceRef","text":"<p>\u21a9 Parent</p> <p>dataSourceRef specifies the object from which to populate the volume with data, if a non-empty volume is desired. This may be any object from a non-empty API group (non core object) or a PersistentVolumeClaim object. When this field is specified, volume binding will only succeed if the type of the specified object matches some installed volume populator or dynamic provisioner. This field will replace the functionality of the dataSource field and as such if both fields are non-empty, they must have the same value. For backwards compatibility, when namespace isn't specified in dataSourceRef, both fields (dataSource and dataSourceRef) will be set to the same value automatically if one of them is empty and the other is non-empty. When namespace is specified in dataSourceRef, dataSource isn't set to the same value and must be empty. There are three important differences between dataSource and dataSourceRef: _ While dataSource only allows two specific types of objects, dataSourceRef allows any non-core object, as well as PersistentVolumeClaim objects. _ While dataSource ignores disallowed values (dropping them), dataSourceRef preserves all values, and generates an error if a disallowed value is specified. * While dataSource only allows local objects, dataSourceRef allows objects in any namespaces. (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled. (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.</p> Name Type Description Required kind string            Kind is the type of resource being referenced true name string            Name is the name of resource being referenced true apiGroup string            APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. false namespace string            Namespace is the namespace of resource being referenced Note that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details. (Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecresources","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.resources","text":"<p>\u21a9 Parent</p> <p>resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements that are lower than previous value but must still be higher than capacity recorded in the status field of the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecresourcesclaimsindex","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.resources.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecselector","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.selector","text":"<p>\u21a9 Parent</p> <p>selector is a label query over volumes to consider for binding.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexspecselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].spec.selector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexstatus","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].status","text":"<p>\u21a9 Parent</p> <p>status represents the current information/status of a persistent volume claim. Read-only. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims</p> Name Type Description Required accessModes []string            accessModes contains the actual access modes the volume backing the PVC has. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 false allocatedResources map[string]int or string            allocatedResources is the storage resource within AllocatedResources tracks the capacity allocated to a PVC. It may be larger than the actual capacity when a volume expansion operation is requested. For storage quota, the larger value from allocatedResources and PVC.spec.resources is used. If allocatedResources is not set, PVC.spec.resources alone is used for quota calculation. If a volume expansion capacity request is lowered, allocatedResources is only lowered if there are no expansion operations in progress and if the actual volume capacity is equal or lower than the requested capacity. This is an alpha field and requires enabling RecoverVolumeExpansionFailure feature. false capacity map[string]int or string            capacity represents the actual resources of the underlying volume. false conditions []object            conditions is the current Condition of persistent volume claim. If underlying persistent volume is being resized then the Condition will be set to 'ResizeStarted'. false phase string            phase represents the current phase of PersistentVolumeClaim. false resizeStatus string            resizeStatus stores status of resize operation. ResizeStatus is not set by default but when expansion is complete resizeStatus is set to empty string by resize controller or kubelet. This is an alpha field and requires enabling RecoverVolumeExpansionFailure feature. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumeclaimtemplatesindexstatusconditionsindex","title":"OpenTelemetryCollector.spec.volumeClaimTemplates[index].status.conditions[index]","text":"<p>\u21a9 Parent</p> <p>PersistentVolumeClaimCondition contains details about state of pvc</p> Name Type Description Required status string true type string            PersistentVolumeClaimConditionType is a valid value of PersistentVolumeClaimCondition.Type true lastProbeTime string            lastProbeTime is the time we probed the condition. Format: date-time false lastTransitionTime string            lastTransitionTime is the time the condition transitioned from one status to another. Format: date-time false message string            message is the human-readable message indicating details about last transition. false reason string            reason is a unique, this should be a short, machine understandable string that gives the reason for condition's last transition. If it reports \"ResizeStarted\" that means the underlying persistent volume is being resized. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumemountsindex","title":"OpenTelemetryCollector.spec.volumeMounts[index]","text":"<p>\u21a9 Parent</p> <p>VolumeMount describes a mounting of a Volume within a container.</p> Name Type Description Required mountPath string            Path within the container at which the volume should be mounted.  Must not contain ':'. true name string            This must match the Name of a Volume. true mountPropagation string            mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. false readOnly boolean            Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. false subPath string            Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's root). false subPathExpr string            Expanded path within the volume from which the container's volume should be mounted. Behaves similarly to SubPath but environment variable references $(VAR_NAME) are expanded using the container's environment. Defaults to \"\" (volume's root). SubPathExpr and SubPath are mutually exclusive. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindex","title":"OpenTelemetryCollector.spec.volumes[index]","text":"<p>\u21a9 Parent</p> <p>Volume represents a named volume in a pod that may be accessed by any container in the pod.</p> Name Type Description Required name string            name of the volume. Must be a DNS_LABEL and unique within the pod. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names true awsElasticBlockStore object            awsElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore false azureDisk object            azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod. false azureFile object            azureFile represents an Azure File Service mount on the host and bind mount to the pod. false cephfs object            cephFS represents a Ceph FS mount on the host that shares a pod's lifetime false cinder object            cinder represents a cinder volume attached and mounted on kubelets host machine. More info: https://examples.k8s.io/mysql-cinder-pd/README.md false configMap object            configMap represents a configMap that should populate this volume false csi object            csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature). false downwardAPI object            downwardAPI represents downward API about the pod that should populate this volume false emptyDir object            emptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir false ephemeral object            ephemeral represents a volume that is handled by a cluster storage driver. The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts, and deleted when the pod is removed.   Use this if: a) the volume is only needed while the pod runs, b) features of normal volumes like restoring from snapshot or capacity tracking are needed, c) the storage driver is specified through a storage class, and d) the storage driver supports dynamic volume provisioning through a PersistentVolumeClaim (see EphemeralVolumeSource for more information on the connection between this volume type and PersistentVolumeClaim).   Use PersistentVolumeClaim or one of the vendor-specific APIs for volumes that persist for longer than the lifecycle of an individual pod.   Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to be used that way - see the documentation of the driver for more information.   A pod can use both types of ephemeral volumes and persistent volumes at the same time. false fc object            fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod. false flexVolume object            flexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin. false flocker object            flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running false gcePersistentDisk object            gcePersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk false gitRepo object            gitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container. false glusterfs object            glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/glusterfs/README.md false hostPath object            hostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath --- TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not mount host directories as read/write. false iscsi object            iscsi represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://examples.k8s.io/volumes/iscsi/README.md false nfs object            nfs represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs false persistentVolumeClaim object            persistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims false photonPersistentDisk object            photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine false portworxVolume object            portworxVolume represents a portworx volume attached and mounted on kubelets host machine false projected object            projected items for all in one resources secrets, configmaps, and downward API false quobyte object            quobyte represents a Quobyte mount on the host that shares a pod's lifetime false rbd object            rbd represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/rbd/README.md false scaleIO object            scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes. false secret object            secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret false storageos object            storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes. false vsphereVolume object            vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexawselasticblockstore","title":"OpenTelemetryCollector.spec.volumes[index].awsElasticBlockStore","text":"<p>\u21a9 Parent</p> <p>awsElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore</p> Name Type Description Required volumeID string            volumeID is unique ID of the persistent disk resource in AWS (Amazon EBS volume). More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore true fsType string            fsType is the filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore TODO: how do we prevent errors in the filesystem from compromising the machine false partition integer            partition is the partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). Format: int32 false readOnly boolean            readOnly value true will force the readOnly setting in VolumeMounts. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexazuredisk","title":"OpenTelemetryCollector.spec.volumes[index].azureDisk","text":"<p>\u21a9 Parent</p> <p>azureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.</p> Name Type Description Required diskName string            diskName is the Name of the data disk in the blob storage true diskURI string            diskURI is the URI of data disk in the blob storage true cachingMode string            cachingMode is the Host Caching mode: None, Read Only, Read Write. false fsType string            fsType is Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. false kind string            kind expected values are Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage account  Managed: azure managed data disk (only in managed availability set). defaults to shared false readOnly boolean            readOnly Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexazurefile","title":"OpenTelemetryCollector.spec.volumes[index].azureFile","text":"<p>\u21a9 Parent</p> <p>azureFile represents an Azure File Service mount on the host and bind mount to the pod.</p> Name Type Description Required secretName string            secretName is the  name of secret that contains Azure Storage Account Name and Key true shareName string            shareName is the azure share Name true readOnly boolean            readOnly defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcephfs","title":"OpenTelemetryCollector.spec.volumes[index].cephfs","text":"<p>\u21a9 Parent</p> <p>cephFS represents a Ceph FS mount on the host that shares a pod's lifetime</p> Name Type Description Required monitors []string            monitors is Required: Monitors is a collection of Ceph monitors More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it true path string            path is Optional: Used as the mounted root, rather than the full Ceph tree, default is / false readOnly boolean            readOnly is Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it false secretFile string            secretFile is Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it false secretRef object            secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it false user string            user is optional: User is the rados user name, default is admin More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcephfssecretref","title":"OpenTelemetryCollector.spec.volumes[index].cephfs.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef is Optional: SecretRef is reference to the authentication secret for User, default is empty. More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcinder","title":"OpenTelemetryCollector.spec.volumes[index].cinder","text":"<p>\u21a9 Parent</p> <p>cinder represents a cinder volume attached and mounted on kubelets host machine. More info: https://examples.k8s.io/mysql-cinder-pd/README.md</p> Name Type Description Required volumeID string            volumeID used to identify the volume in cinder. More info: https://examples.k8s.io/mysql-cinder-pd/README.md true fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://examples.k8s.io/mysql-cinder-pd/README.md false readOnly boolean            readOnly defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. More info: https://examples.k8s.io/mysql-cinder-pd/README.md false secretRef object            secretRef is optional: points to a secret object containing parameters used to connect to OpenStack. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcindersecretref","title":"OpenTelemetryCollector.spec.volumes[index].cinder.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef is optional: points to a secret object containing parameters used to connect to OpenStack.</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexconfigmap","title":"OpenTelemetryCollector.spec.volumes[index].configMap","text":"<p>\u21a9 Parent</p> <p>configMap represents a configMap that should populate this volume</p> Name Type Description Required defaultMode integer            defaultMode is optional: mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false items []object            items if unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            optional specify whether the ConfigMap or its keys must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexconfigmapitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].configMap.items[index]","text":"<p>\u21a9 Parent</p> <p>Maps a string key to a path within a volume.</p> Name Type Description Required key string            key is the key to project. true path string            path is the relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. true mode integer            mode is Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcsi","title":"OpenTelemetryCollector.spec.volumes[index].csi","text":"<p>\u21a9 Parent</p> <p>csi (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI drivers (Beta feature).</p> Name Type Description Required driver string            driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct name as registered in the cluster. true fsType string            fsType to mount. Ex. \"ext4\", \"xfs\", \"ntfs\". If not provided, the empty value is passed to the associated CSI driver which will determine the default filesystem to apply. false nodePublishSecretRef object            nodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and  may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed. false readOnly boolean            readOnly specifies a read-only configuration for the volume. Defaults to false (read/write). false volumeAttributes map[string]string            volumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your driver's documentation for supported values. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexcsinodepublishsecretref","title":"OpenTelemetryCollector.spec.volumes[index].csi.nodePublishSecretRef","text":"<p>\u21a9 Parent</p> <p>nodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional, and may be empty if no secret is required. If the secret object contains more than one secret, all secret references are passed.</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexdownwardapi","title":"OpenTelemetryCollector.spec.volumes[index].downwardAPI","text":"<p>\u21a9 Parent</p> <p>downwardAPI represents downward API about the pod that should populate this volume</p> Name Type Description Required defaultMode integer            Optional: mode bits to use on created files by default. Must be a Optional: mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false items []object            Items is a list of downward API volume file false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexdownwardapiitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].downwardAPI.items[index]","text":"<p>\u21a9 Parent</p> <p>DownwardAPIVolumeFile represents information to create the file containing the pod field</p> Name Type Description Required path string            Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' true fieldRef object            Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. false mode integer            Optional: mode bits used to set permissions on this file, must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexdownwardapiitemsindexfieldref","title":"OpenTelemetryCollector.spec.volumes[index].downwardAPI.items[index].fieldRef","text":"<p>\u21a9 Parent</p> <p>Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexdownwardapiitemsindexresourcefieldref","title":"OpenTelemetryCollector.spec.volumes[index].downwardAPI.items[index].resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexemptydir","title":"OpenTelemetryCollector.spec.volumes[index].emptyDir","text":"<p>\u21a9 Parent</p> <p>emptyDir represents a temporary directory that shares a pod's lifetime. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir</p> Name Type Description Required medium string            medium represents what type of storage medium should back this directory. The default is \"\" which means to use the node's default medium. Must be an empty string (default) or Memory. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir false sizeLimit int or string            sizeLimit is the total amount of local storage required for this EmptyDir volume. The size limit is also applicable for memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil which means that the limit is undefined. More info: https://kubernetes.io/docs/concepts/storage/volumes#emptydir false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeral","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral","text":"<p>\u21a9 Parent</p> <p>ephemeral represents a volume that is handled by a cluster storage driver. The volume's lifecycle is tied to the pod that defines it - it will be created before the pod starts, and deleted when the pod is removed. Use this if: a) the volume is only needed while the pod runs, b) features of normal volumes like restoring from snapshot or capacity tracking are needed, c) the storage driver is specified through a storage class, and d) the storage driver supports dynamic volume provisioning through a PersistentVolumeClaim (see EphemeralVolumeSource for more information on the connection between this volume type and PersistentVolumeClaim). Use PersistentVolumeClaim or one of the vendor-specific APIs for volumes that persist for longer than the lifecycle of an individual pod. Use CSI for light-weight local ephemeral volumes if the CSI driver is meant to be used that way - see the documentation of the driver for more information. A pod can use both types of ephemeral volumes and persistent volumes at the same time.</p> Name Type Description Required volumeClaimTemplate object            Will be used to create a stand-alone PVC to provision the volume. The pod in which this EphemeralVolumeSource is embedded will be the owner of the PVC, i.e. the PVC will be deleted together with the pod.  The name of the PVC will be `-` where `` is the name from the `PodSpec.Volumes` array entry. Pod validation will reject the pod if the concatenated name is not valid for a PVC (for example, too long).   An existing PVC with that name that is not owned by the pod will *not* be used for the pod to avoid using an unrelated volume by mistake. Starting the pod is then blocked until the unrelated PVC is removed. If such a pre-created PVC is meant to be used by the pod, the PVC has to updated with an owner reference to the pod once the pod exists. Normally this should not be necessary, but it may be useful when manually reconstructing a broken cluster.   This field is read-only and no changes will be made by Kubernetes to the PVC after it has been created.   Required, must not be nil. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplate","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate","text":"<p>\u21a9 Parent</p> <p>Will be used to create a stand-alone PVC to provision the volume. The pod in which this EphemeralVolumeSource is embedded will be the owner of the PVC, i.e. the PVC will be deleted together with the pod. The name of the PVC will be <code>&lt;pod name&gt;-&lt;volume name&gt;</code> where <code>&lt;volume name&gt;</code> is the name from the <code>PodSpec.Volumes</code> array entry. Pod validation will reject the pod if the concatenated name is not valid for a PVC (for example, too long). An existing PVC with that name that is not owned by the pod will not be used for the pod to avoid using an unrelated volume by mistake. Starting the pod is then blocked until the unrelated PVC is removed. If such a pre-created PVC is meant to be used by the pod, the PVC has to updated with an owner reference to the pod once the pod exists. Normally this should not be necessary, but it may be useful when manually reconstructing a broken cluster. This field is read-only and no changes will be made by Kubernetes to the PVC after it has been created. Required, must not be nil.</p> Name Type Description Required spec object            The specification for the PersistentVolumeClaim. The entire content is copied unchanged into the PVC that gets created from this template. The same fields as in a PersistentVolumeClaim are also valid here. true metadata object            May contain labels and annotations that will be copied into the PVC when creating it. No other fields are allowed and will be rejected during validation. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespec","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec","text":"<p>\u21a9 Parent</p> <p>The specification for the PersistentVolumeClaim. The entire content is copied unchanged into the PVC that gets created from this template. The same fields as in a PersistentVolumeClaim are also valid here.</p> Name Type Description Required accessModes []string            accessModes contains the desired access modes the volume should have. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1 false dataSource object            dataSource field can be used to specify either: * An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot) * An existing PVC (PersistentVolumeClaim) If the provisioner or an external controller can support the specified data source, it will create a new volume based on the contents of the specified data source. When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef, and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified. If the namespace is specified, then dataSourceRef will not be copied to dataSource. false dataSourceRef object            dataSourceRef specifies the object from which to populate the volume with data, if a non-empty volume is desired. This may be any object from a non-empty API group (non core object) or a PersistentVolumeClaim object. When this field is specified, volume binding will only succeed if the type of the specified object matches some installed volume populator or dynamic provisioner. This field will replace the functionality of the dataSource field and as such if both fields are non-empty, they must have the same value. For backwards compatibility, when namespace isn't specified in dataSourceRef, both fields (dataSource and dataSourceRef) will be set to the same value automatically if one of them is empty and the other is non-empty. When namespace is specified in dataSourceRef, dataSource isn't set to the same value and must be empty. There are three important differences between dataSource and dataSourceRef: * While dataSource only allows two specific types of objects, dataSourceRef allows any non-core object, as well as PersistentVolumeClaim objects. * While dataSource ignores disallowed values (dropping them), dataSourceRef preserves all values, and generates an error if a disallowed value is specified. * While dataSource only allows local objects, dataSourceRef allows objects in any namespaces. (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled. (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled. false resources object            resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements that are lower than previous value but must still be higher than capacity recorded in the status field of the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources false selector object            selector is a label query over volumes to consider for binding. false storageClassName string            storageClassName is the name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 false volumeMode string            volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not included in claim spec. false volumeName string            volumeName is the binding reference to the PersistentVolume backing this claim. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecdatasource","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.dataSource","text":"<p>\u21a9 Parent</p> <p>dataSource field can be used to specify either: _ An existing VolumeSnapshot object (snapshot.storage.k8s.io/VolumeSnapshot) _ An existing PVC (PersistentVolumeClaim) If the provisioner or an external controller can support the specified data source, it will create a new volume based on the contents of the specified data source. When the AnyVolumeDataSource feature gate is enabled, dataSource contents will be copied to dataSourceRef, and dataSourceRef contents will be copied to dataSource when dataSourceRef.namespace is not specified. If the namespace is specified, then dataSourceRef will not be copied to dataSource.</p> Name Type Description Required kind string            Kind is the type of resource being referenced true name string            Name is the name of resource being referenced true apiGroup string            APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecdatasourceref","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.dataSourceRef","text":"<p>\u21a9 Parent</p> <p>dataSourceRef specifies the object from which to populate the volume with data, if a non-empty volume is desired. This may be any object from a non-empty API group (non core object) or a PersistentVolumeClaim object. When this field is specified, volume binding will only succeed if the type of the specified object matches some installed volume populator or dynamic provisioner. This field will replace the functionality of the dataSource field and as such if both fields are non-empty, they must have the same value. For backwards compatibility, when namespace isn't specified in dataSourceRef, both fields (dataSource and dataSourceRef) will be set to the same value automatically if one of them is empty and the other is non-empty. When namespace is specified in dataSourceRef, dataSource isn't set to the same value and must be empty. There are three important differences between dataSource and dataSourceRef: _ While dataSource only allows two specific types of objects, dataSourceRef allows any non-core object, as well as PersistentVolumeClaim objects. _ While dataSource ignores disallowed values (dropping them), dataSourceRef preserves all values, and generates an error if a disallowed value is specified. * While dataSource only allows local objects, dataSourceRef allows objects in any namespaces. (Beta) Using this field requires the AnyVolumeDataSource feature gate to be enabled. (Alpha) Using the namespace field of dataSourceRef requires the CrossNamespaceVolumeDataSource feature gate to be enabled.</p> Name Type Description Required kind string            Kind is the type of resource being referenced true name string            Name is the name of resource being referenced true apiGroup string            APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind must be in the core API group. For any other third-party types, APIGroup is required. false namespace string            Namespace is the namespace of resource being referenced Note that when a namespace is specified, a gateway.networking.k8s.io/ReferenceGrant object is required in the referent namespace to allow that namespace's owner to accept the reference. See the ReferenceGrant documentation for details. (Alpha) This field requires the CrossNamespaceVolumeDataSource feature gate to be enabled. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecresources","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.resources","text":"<p>\u21a9 Parent</p> <p>resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure feature is enabled users are allowed to specify resource requirements that are lower than previous value but must still be higher than capacity recorded in the status field of the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources</p> Name Type Description Required claims []object            Claims lists the names of resources, defined in spec.resourceClaims, that are used by this container.   This is an alpha field and requires enabling the DynamicResourceAllocation feature gate.   This field is immutable. It can only be set for containers. false limits map[string]int or string            Limits describes the maximum amount of compute resources allowed. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false requests map[string]int or string            Requests describes the minimum amount of compute resources required. If Requests is omitted for a container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined value. Requests cannot exceed Limits. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecresourcesclaimsindex","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.resources.claims[index]","text":"<p>\u21a9 Parent</p> <p>ResourceClaim references one entry in PodSpec.ResourceClaims.</p> Name Type Description Required name string            Name must match the name of one entry in pod.spec.resourceClaims of the Pod where this field is used. It makes that resource available inside a container. true"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecselector","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.selector","text":"<p>\u21a9 Parent</p> <p>selector is a label query over volumes to consider for binding.</p> Name Type Description Required matchExpressions []object            matchExpressions is a list of label selector requirements. The requirements are ANDed. false matchLabels map[string]string            matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is \"key\", the operator is \"In\", and the values array contains only \"value\". The requirements are ANDed. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatespecselectormatchexpressionsindex","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.spec.selector.matchExpressions[index]","text":"<p>\u21a9 Parent</p> <p>A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.</p> Name Type Description Required key string            key is the label key that the selector applies to. true operator string            operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist. true values []string            values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexephemeralvolumeclaimtemplatemetadata","title":"OpenTelemetryCollector.spec.volumes[index].ephemeral.volumeClaimTemplate.metadata","text":"<p>\u21a9 Parent</p> <p>May contain labels and annotations that will be copied into the PVC when creating it. No other fields are allowed and will be rejected during validation.</p> Name Type Description Required annotations map[string]string false finalizers []string false labels map[string]string false name string false namespace string false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexfc","title":"OpenTelemetryCollector.spec.volumes[index].fc","text":"<p>\u21a9 Parent</p> <p>fc represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to the pod.</p> Name Type Description Required fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. TODO: how do we prevent errors in the filesystem from compromising the machine false lun integer            lun is Optional: FC target lun number Format: int32 false readOnly boolean            readOnly is Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false targetWWNs []string            targetWWNs is Optional: FC target worldwide names (WWNs) false wwids []string            wwids Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must be set, but not both simultaneously. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexflexvolume","title":"OpenTelemetryCollector.spec.volumes[index].flexVolume","text":"<p>\u21a9 Parent</p> <p>flexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin.</p> Name Type Description Required driver string            driver is the name of the driver to use for this volume. true fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". The default filesystem depends on FlexVolume script. false options map[string]string            options is Optional: this field holds extra command options if any. false readOnly boolean            readOnly is Optional: defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false secretRef object            secretRef is Optional: secretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexflexvolumesecretref","title":"OpenTelemetryCollector.spec.volumes[index].flexVolume.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef is Optional: secretRef is reference to the secret object containing sensitive information to pass to the plugin scripts. This may be empty if no secret object is specified. If the secret object contains more than one secret, all secrets are passed to the plugin scripts.</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexflocker","title":"OpenTelemetryCollector.spec.volumes[index].flocker","text":"<p>\u21a9 Parent</p> <p>flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker control service being running</p> Name Type Description Required datasetName string            datasetName is Name of the dataset stored as metadata -&gt; name on the dataset for Flocker should be considered as deprecated false datasetUUID string            datasetUUID is the UUID of the dataset. This is unique identifier of a Flocker dataset false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexgcepersistentdisk","title":"OpenTelemetryCollector.spec.volumes[index].gcePersistentDisk","text":"<p>\u21a9 Parent</p> <p>gcePersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk</p> Name Type Description Required pdName string            pdName is unique name of the PD resource in GCE. Used to identify the disk in GCE. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk true fsType string            fsType is filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk TODO: how do we prevent errors in the filesystem from compromising the machine false partition integer            partition is the partition in the volume that you want to mount. If omitted, the default is to mount by volume name. Examples: For volume /dev/sda1, you specify the partition as \"1\". Similarly, the volume partition for /dev/sda is \"0\" (or you can leave the property empty). More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk Format: int32 false readOnly boolean            readOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexgitrepo","title":"OpenTelemetryCollector.spec.volumes[index].gitRepo","text":"<p>\u21a9 Parent</p> <p>gitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo using git, then mount the EmptyDir into the Pod's container.</p> Name Type Description Required repository string            repository is the URL true directory string            directory is the target directory name. Must not contain or start with '..'.  If '.' is supplied, the volume directory will be the git repository.  Otherwise, if specified, the volume will contain the git repository in the subdirectory with the given name. false revision string            revision is the commit hash for the specified revision. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexglusterfs","title":"OpenTelemetryCollector.spec.volumes[index].glusterfs","text":"<p>\u21a9 Parent</p> <p>glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/glusterfs/README.md</p> Name Type Description Required endpoints string            endpoints is the endpoint name that details Glusterfs topology. More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod true path string            path is the Glusterfs volume path. More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod true readOnly boolean            readOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to false. More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexhostpath","title":"OpenTelemetryCollector.spec.volumes[index].hostPath","text":"<p>\u21a9 Parent</p> <p>hostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath --- TODO(jonesdl) We need to restrict who can use host directory mounts and who can/can not mount host directories as read/write.</p> Name Type Description Required path string            path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath true type string            type for HostPath Volume Defaults to \"\" More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexiscsi","title":"OpenTelemetryCollector.spec.volumes[index].iscsi","text":"<p>\u21a9 Parent</p> <p>iscsi represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to the pod. More info: https://examples.k8s.io/volumes/iscsi/README.md</p> Name Type Description Required iqn string            iqn is the target iSCSI Qualified Name. true lun integer            lun represents iSCSI Target Lun number. Format: int32 true targetPortal string            targetPortal is iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). true chapAuthDiscovery boolean            chapAuthDiscovery defines whether support iSCSI Discovery CHAP authentication false chapAuthSession boolean            chapAuthSession defines whether support iSCSI Session CHAP authentication false fsType string            fsType is the filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi TODO: how do we prevent errors in the filesystem from compromising the machine false initiatorName string            initiatorName is the custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI interface : will be created for the connection. false iscsiInterface string            iscsiInterface is the interface Name that uses an iSCSI transport. Defaults to 'default' (tcp). false portals []string            portals is the iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default (typically TCP ports 860 and 3260). false readOnly boolean            readOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. false secretRef object            secretRef is the CHAP Secret for iSCSI target and initiator authentication false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexiscsisecretref","title":"OpenTelemetryCollector.spec.volumes[index].iscsi.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef is the CHAP Secret for iSCSI target and initiator authentication</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexnfs","title":"OpenTelemetryCollector.spec.volumes[index].nfs","text":"<p>\u21a9 Parent</p> <p>nfs represents an NFS mount on the host that shares a pod's lifetime More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs</p> Name Type Description Required path string            path that is exported by the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs true server string            server is the hostname or IP address of the NFS server. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs true readOnly boolean            readOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More info: https://kubernetes.io/docs/concepts/storage/volumes#nfs false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexpersistentvolumeclaim","title":"OpenTelemetryCollector.spec.volumes[index].persistentVolumeClaim","text":"<p>\u21a9 Parent</p> <p>persistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same namespace. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims</p> Name Type Description Required claimName string            claimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims true readOnly boolean            readOnly Will force the ReadOnly setting in VolumeMounts. Default false. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexphotonpersistentdisk","title":"OpenTelemetryCollector.spec.volumes[index].photonPersistentDisk","text":"<p>\u21a9 Parent</p> <p>photonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host machine</p> Name Type Description Required pdID string            pdID is the ID that identifies Photon Controller persistent disk true fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexportworxvolume","title":"OpenTelemetryCollector.spec.volumes[index].portworxVolume","text":"<p>\u21a9 Parent</p> <p>portworxVolume represents a portworx volume attached and mounted on kubelets host machine</p> Name Type Description Required volumeID string            volumeID uniquely identifies a Portworx volume true fsType string            fSType represents the filesystem type to mount Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\". Implicitly inferred to be \"ext4\" if unspecified. false readOnly boolean            readOnly defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojected","title":"OpenTelemetryCollector.spec.volumes[index].projected","text":"<p>\u21a9 Parent</p> <p>projected items for all in one resources secrets, configmaps, and downward API</p> Name Type Description Required defaultMode integer            defaultMode are the mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false sources []object            sources is the list of volume projections false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindex","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index]","text":"<p>\u21a9 Parent</p> <p>Projection that may be projected along with other supported volume types</p> Name Type Description Required configMap object            configMap information about the configMap data to project false downwardAPI object            downwardAPI information about the downwardAPI data to project false secret object            secret information about the secret data to project false serviceAccountToken object            serviceAccountToken is information about the serviceAccountToken data to project false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexconfigmap","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].configMap","text":"<p>\u21a9 Parent</p> <p>configMap information about the configMap data to project</p> Name Type Description Required items []object            items if unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            optional specify whether the ConfigMap or its keys must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexconfigmapitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].configMap.items[index]","text":"<p>\u21a9 Parent</p> <p>Maps a string key to a path within a volume.</p> Name Type Description Required key string            key is the key to project. true path string            path is the relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. true mode integer            mode is Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexdownwardapi","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].downwardAPI","text":"<p>\u21a9 Parent</p> <p>downwardAPI information about the downwardAPI data to project</p> Name Type Description Required items []object            Items is a list of DownwardAPIVolume file false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexdownwardapiitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].downwardAPI.items[index]","text":"<p>\u21a9 Parent</p> <p>DownwardAPIVolumeFile represents information to create the file containing the pod field</p> Name Type Description Required path string            Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..' true fieldRef object            Required: Selects a field of the pod: only annotations, labels, name and namespace are supported. false mode integer            Optional: mode bits used to set permissions on this file, must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false resourceFieldRef object            Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexdownwardapiitemsindexfieldref","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].downwardAPI.items[index].fieldRef","text":"<p>\u21a9 Parent</p> <p>Required: Selects a field of the pod: only annotations, labels, name and namespace are supported.</p> Name Type Description Required fieldPath string            Path of the field to select in the specified API version. true apiVersion string            Version of the schema the FieldPath is written in terms of, defaults to \"v1\". false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexdownwardapiitemsindexresourcefieldref","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].downwardAPI.items[index].resourceFieldRef","text":"<p>\u21a9 Parent</p> <p>Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, requests.cpu and requests.memory) are currently supported.</p> Name Type Description Required resource string            Required: resource to select true containerName string            Container name: required for volumes, optional for env vars false divisor int or string            Specifies the output format of the exposed resources, defaults to \"1\" false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexsecret","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].secret","text":"<p>\u21a9 Parent</p> <p>secret information about the secret data to project</p> Name Type Description Required items []object            items if unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. false name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false optional boolean            optional field specify whether the Secret or its key must be defined false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexsecretitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].secret.items[index]","text":"<p>\u21a9 Parent</p> <p>Maps a string key to a path within a volume.</p> Name Type Description Required key string            key is the key to project. true path string            path is the relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. true mode integer            mode is Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexprojectedsourcesindexserviceaccounttoken","title":"OpenTelemetryCollector.spec.volumes[index].projected.sources[index].serviceAccountToken","text":"<p>\u21a9 Parent</p> <p>serviceAccountToken is information about the serviceAccountToken data to project</p> Name Type Description Required path string            path is the path relative to the mount point of the file to project the token into. true audience string            audience is the intended audience of the token. A recipient of a token must identify itself with an identifier specified in the audience of the token, and otherwise should reject the token. The audience defaults to the identifier of the apiserver. false expirationSeconds integer            expirationSeconds is the requested duration of validity of the service account token. As the token approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes. Format: int64 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexquobyte","title":"OpenTelemetryCollector.spec.volumes[index].quobyte","text":"<p>\u21a9 Parent</p> <p>quobyte represents a Quobyte mount on the host that shares a pod's lifetime</p> Name Type Description Required registry string            registry represents a single or multiple Quobyte Registry services specified as a string as host:port pair (multiple entries are separated with commas) which acts as the central registry for volumes true volume string            volume is a string that references an already created Quobyte volume by name. true group string            group to map volume access to Default is no group false readOnly boolean            readOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false. false tenant string            tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes, value is set by the plugin false user string            user to map volume access to Defaults to serivceaccount user false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexrbd","title":"OpenTelemetryCollector.spec.volumes[index].rbd","text":"<p>\u21a9 Parent</p> <p>rbd represents a Rados Block Device mount on the host that shares a pod's lifetime. More info: https://examples.k8s.io/volumes/rbd/README.md</p> Name Type Description Required image string            image is the rados image name. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it true monitors []string            monitors is a collection of Ceph monitors. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it true fsType string            fsType is the filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported by the host operating system. Examples: \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd TODO: how do we prevent errors in the filesystem from compromising the machine false keyring string            keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it false pool string            pool is the rados pool name. Default is rbd. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it false readOnly boolean            readOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it false secretRef object            secretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it false user string            user is the rados user name. Default is admin. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexrbdsecretref","title":"OpenTelemetryCollector.spec.volumes[index].rbd.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is nil. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexscaleio","title":"OpenTelemetryCollector.spec.volumes[index].scaleIO","text":"<p>\u21a9 Parent</p> <p>scaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.</p> Name Type Description Required gateway string            gateway is the host address of the ScaleIO API Gateway. true secretRef object            secretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail. true system string            system is the name of the storage system as configured in ScaleIO. true fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Default is \"xfs\". false protectionDomain string            protectionDomain is the name of the ScaleIO Protection Domain for the configured storage. false readOnly boolean            readOnly Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false sslEnabled boolean            sslEnabled Flag enable/disable SSL communication with Gateway, default false false storageMode string            storageMode indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is ThinProvisioned. false storagePool string            storagePool is the ScaleIO Storage Pool associated with the protection domain. false volumeName string            volumeName is the name of a volume already created in the ScaleIO system that is associated with this volume source. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexscaleiosecretref","title":"OpenTelemetryCollector.spec.volumes[index].scaleIO.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef references to the secret for ScaleIO user and other sensitive information. If this is not provided, Login operation will fail.</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexsecret","title":"OpenTelemetryCollector.spec.volumes[index].secret","text":"<p>\u21a9 Parent</p> <p>secret represents a secret that should populate this volume. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret</p> Name Type Description Required defaultMode integer            defaultMode is Optional: mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false items []object            items If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the volume as a file whose name is the key and content is the value. If specified, the listed keys will be projected into the specified paths, and unlisted keys will not be present. If a key is specified which is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be relative and may not contain the '..' path or start with '..'. false optional boolean            optional field specify whether the Secret or its keys must be defined false secretName string            secretName is the name of the secret in the pod's namespace to use. More info: https://kubernetes.io/docs/concepts/storage/volumes#secret false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexsecretitemsindex","title":"OpenTelemetryCollector.spec.volumes[index].secret.items[index]","text":"<p>\u21a9 Parent</p> <p>Maps a string key to a path within a volume.</p> Name Type Description Required key string            key is the key to project. true path string            path is the relative path of the file to map the key to. May not be an absolute path. May not contain the path element '..'. May not start with the string '..'. true mode integer            mode is Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. Format: int32 false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexstorageos","title":"OpenTelemetryCollector.spec.volumes[index].storageos","text":"<p>\u21a9 Parent</p> <p>storageOS represents a StorageOS volume attached and mounted on Kubernetes nodes.</p> Name Type Description Required fsType string            fsType is the filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. false readOnly boolean            readOnly defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts. false secretRef object            secretRef specifies the secret to use for obtaining the StorageOS API credentials.  If not specified, default values will be attempted. false volumeName string            volumeName is the human-readable name of the StorageOS volume.  Volume names are only unique within a namespace. false volumeNamespace string            volumeNamespace specifies the scope of the volume within StorageOS.  If no namespace is specified then the Pod's namespace will be used.  This allows the Kubernetes name scoping to be mirrored within StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to \"default\" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within StorageOS will be created. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexstorageossecretref","title":"OpenTelemetryCollector.spec.volumes[index].storageos.secretRef","text":"<p>\u21a9 Parent</p> <p>secretRef specifies the secret to use for obtaining the StorageOS API credentials. If not specified, default values will be attempted.</p> Name Type Description Required name string            Name of the referent. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names TODO: Add other useful fields. apiVersion, kind, uid? false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorspecvolumesindexvspherevolume","title":"OpenTelemetryCollector.spec.volumes[index].vsphereVolume","text":"<p>\u21a9 Parent</p> <p>vsphereVolume represents a vSphere volume attached and mounted on kubelets host machine</p> Name Type Description Required volumePath string            volumePath is the path that identifies vSphere volume vmdk true fsType string            fsType is filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. \"ext4\", \"xfs\", \"ntfs\". Implicitly inferred to be \"ext4\" if unspecified. false storagePolicyID string            storagePolicyID is the storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName. false storagePolicyName string            storagePolicyName is the storage Policy Based Management (SPBM) profile name. false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorstatus","title":"OpenTelemetryCollector.status","text":"<p>\u21a9 Parent</p> <p>OpenTelemetryCollectorStatus defines the observed state of OpenTelemetryCollector.</p> Name Type Description Required image string            Image indicates the container image to use for the OpenTelemetry Collector. false messages []string            Messages about actions performed by the operator on this resource. Deprecated: use Kubernetes events instead. false replicas integer            Replicas is currently not being set and might be removed in the next version. Deprecated: use \"OpenTelemetryCollector.Status.Scale.Replicas\" instead. Format: int32 false scale object            Scale is the OpenTelemetryCollector's scale subresource status. false version string            Version of the managed OpenTelemetry Collector (operand) false"},{"location":"docs/k8s/operator/apis/OpenTelemetryCollector/#opentelemetrycollectorstatusscale","title":"OpenTelemetryCollector.status.scale","text":"<p>\u21a9 Parent</p> <p>Scale is the OpenTelemetryCollector's scale subresource status.</p> Name Type Description Required replicas integer            The total number non-terminated pods targeted by this OpenTelemetryCollector's deployment or statefulSet. Format: int32 false selector string            The selector used to match the OpenTelemetryCollector's deployment or statefulSet pods. false statusReplicas string            StatusReplicas is the number of pods targeted by this OpenTelemetryCollector's with a Ready Condition / Total number of non-terminated pods targeted by this OpenTelemetryCollector's (their labels match the selector). Deployment, Daemonset, StatefulSet. false"},{"location":"docs/migration/","title":"Migration","text":""},{"location":"docs/migration/#opentracing-and-opencensus","title":"OpenTracing and OpenCensus","text":"<p>OpenTelemetry was created as a merger of OpenTracing and OpenCensus. From the start, OpenTelemetry was considered to be the next major version of both OpenTracing and OpenCensus. Because of that, one of the key goals of the OpenTelemetry project is to provide backward compatibility with both projects and a migration story for existing users.</p> <p>If you come from one of these projects, you can follow the migration guides for both OpenTracing and OpenCensus</p>"},{"location":"docs/migration/#jaeger-client","title":"Jaeger Client","text":"<p>The Jaeger community deprecated their Client Libraries and recommends using the OpenTelemetry APIs, SDKs and instrumentations.</p> <p>The Jaeger backend can receive trace data via the OpenTelemetry Protocol (OTLP) since v1.35. Therefore you can migrate your OpenTelemetry SDKs and collectors from using the Jaeger exporter to the OTLP exporter.</p>"},{"location":"docs/migration/opentracing/","title":"Migrating from OpenTracing","text":"<p>Backward compatibility with OpenTracing has been a priority for the OpenTelemetry project from the start. To ease migration, OpenTelemetry supports the use of both the OpenTelemetry and OpenTracing APIs in the same codebase. This allows OpenTracing instrumentation to be recorded using OpenTelemetry SDKs.</p> <p>To accomplish this, each OpenTelemetry SDK provides an OpenTracing shim, which acts as a bridge between the OpenTracing API and the OpenTelemetry SDK. Note that OpenTracing shims are disabled by default.</p>"},{"location":"docs/migration/opentracing/#language-version-support","title":"Language version support","text":"<p>Before using an OpenTracing shim, check your project's language and runtime component versions, and update if necessary. The minimum language versions of the OpenTracing and OpenTelemetry APIs are listed in the table below.</p> Language OpenTracing API OpenTelemetry API Go 1.13 1.16 Java 7 8 Python 2.7 3.6 Javascript 6 8.5 .NET 1.3 1.4 C++ 11 11 <p>Note that the OpenTelemetry API and SDKs generally have higher language version requirements than their OpenTracing counterparts.</p>"},{"location":"docs/migration/opentracing/#migration-overview","title":"Migration overview","text":"<p>Many codebases are currently instrumented with OpenTracing. These codebases use the OpenTracing API to instrument their application code and/or install OpenTracing plugins to instrument their libraries and frameworks.</p> <p>A general approach to migrating to OpenTelemetry can be summarized as follows:</p> <ol> <li>Install OpenTelemetry SDK(s), and remove the current OpenTracing    implementation -- for example, a Jaeger client.</li> <li>Install the OpenTelemetry instrumentation libraries, and remove the    OpenTracing equivalents.</li> <li>Update your dashboards, alerts, etc., to consume the new OpenTelemetry data.</li> <li>When writing new application code, write all new instrumentation using the    OpenTelemetry API.</li> <li>Progressively re-instrument your application using the OpenTelemetry API.    There is no hard requirement to remove existing OpenTracing API calls from    your application, they will continue to work.</li> </ol> <p>While migrating a sizable application can require significant effort, as suggested above, we recommend that OpenTracing users progressively migrate their application code. This will ease the burden of migration and help avoid breaks in observability.</p> <p>The steps below present a careful, incremental approach to transitioning to OpenTelemetry.</p>"},{"location":"docs/migration/opentracing/#step-1-install-the-opentelemetry-sdk","title":"Step 1: Install the OpenTelemetry SDK","text":"<p>Before changing any instrumentation, ensure that you can switch to the OpenTelemetry SDK without causing any break in the telemetry the application currently emits. Doing this step on its own \u2013 without simultaneously introducing any new instrumentation \u2013 is recommended, as it makes it easier to determine whether there is any kind of break in instrumentation.</p> <ol> <li>Replace the OpenTracing Tracer implementation you are currently using with    the OpenTelemetry SDK. For example, if you are using the Jaeger, remove the    Jaeger client and install the equivalent OpenTelemetry client.</li> <li>Install the OpenTracing Shim. This shim allows the OpenTelemetry SDK to    consume OpenTracing instrumentation.</li> <li>Configure the OpenTelemetry SDK to export data using the same protocol and    format that the OpenTracing client was using. For example, if you were using    an OpenTracing client that exported tracing data in Zipkin format, configure    the OpenTelemetry client to do the same.</li> <li>Alternatively, configure the OpenTelemetry SDK to emit OTLP, and send the    data to a Collector, where you can manage exporting data in multiple formats.</li> </ol> <p>Once you have the OpenTelemetry SDK installed, confirm that you can deploy your application and still receive the same OpenTracing-based telemetry. In other words, confirm that your dashboards, alerts, and other tracing-based analysis tools are still working.</p>"},{"location":"docs/migration/opentracing/#step-2-progressively-replace-instrumentation","title":"Step 2: Progressively replace instrumentation","text":"<p>Once the OpenTelemetry SDK is installed, all new instrumentation can now be written using the OpenTelemetry API. With few exceptions, OpenTelemetry and OpenTracing instrumentation will work together seamlessly (see limits on compatibility below).</p> <p>What about existing instrumentation? There is no hard requirement to migrate existing application code to OpenTelemetry. However, we do recommend migrating from any OpenTracing instrumentation libraries \u2013 libraries used to instrument web frameworks, HTTP clients, database clients, etc. \u2013 to their OpenTelemetry equivalents. This will improve support, as many OpenTracing libraries will be retired and may no longer be updated.</p> <p>It is important to note that when switching to an OpenTelemetry instrumentation library, the data which is produced will change. OpenTelemetry has an improved model for how we instrument software (what we refer to as our \"semantic conventions\"). In many cases, OpenTelemetry produces better, more comprehensive tracing data. However, \"better\" also means \"different.\" This means that existing dashboards, alerts, etc. based on older OpenTracing instrumentation libraries may no longer work when those libraries are replaced.</p> <p>For existing instrumentation, it is recommended to:</p> <ol> <li>Replace one piece of OpenTracing instrumentation with its OpenTelemetry    equivalent.</li> <li>Observe how this changes the telemetry which your application produces.</li> <li>Create new dashboards, alerts, etc which consume this new telemetry. Set up    these dashboards before deploying the new OpenTelemetry library to    production.</li> <li>Optionally, add processing rules to the Collector which converts the new    telemetry back into the old telemetry. The Collector can then be configured    to emit both versions of the same telemetry, creating a data overlap. This    allows new dashboards to populate themselves while you continue to use the    old dashboards.</li> </ol>"},{"location":"docs/migration/opentracing/#limits-on-compatibility","title":"Limits on compatibility","text":"<p>In this section, we describe limits on compatibility other than the language version constraints mentioned earlier.</p>"},{"location":"docs/migration/opentracing/#semantic-conventions","title":"Semantic conventions","text":"<p>As mentioned above, OpenTelemetry has an improved model for instrumenting software. This means that the \"tags\" which are set by OpenTracing instrumentation may be different from the \"attributes\" which are set by OpenTelemetry. In other words, when replacing existing instrumentation, the data OpenTelemetry produces may be different from the data OpenTracing produces.</p> <p>Again, for clarity: When changing instrumentation, be sure to also update any dashboards, alerts, etc. which relied on the old data.</p>"},{"location":"docs/migration/opentracing/#baggage","title":"Baggage","text":"<p>In OpenTracing, baggage is carried with a SpanContext object associated with a Span. In OpenTelemetry, context and propagation are lower-level concepts \u2013 spans, baggage, metrics instruments, and other items are carried within a context object.</p> <p>As a result of this change, baggage which is set using the OpenTracing API is not available to OpenTelemetry Propagators. As a result, mixing the OpenTelemetry and OpenTracing APIs is not recommended when using baggage.</p> <p>Specifically, when baggage is set using the OpenTracing API:</p> <ul> <li>It is not accessible via the OpenTelemetry API.</li> <li>It is not injected by the OpenTelemetry propagators.</li> </ul> <p>If you are using baggage, it is recommended that all baggage-related API calls be switched to OpenTelemetry at the same time. Be sure to check that any critical baggage items are still being propagated before rolling these changes into production.</p>"},{"location":"docs/migration/opentracing/#context-management-in-javascript","title":"Context management in Javascript","text":"<p>In Javascript, the OpenTelemetry API makes use of commonly available context managers, such as <code>async_hooks</code> for Node.js and <code>Zones.js</code> for the browser. These context managers make tracing instrumentation a much less invasive and onerous task, compared to adding a span as a parameter to every method which needs to be traced.</p> <p>However, the OpenTracing API predates the common use of these context managers. OpenTracing code which passes the current active span as a parameter may create problems when mixed with OpenTelemetry code that stores the active span in a context manager. Using both methods within the same trace may create broken or mismatched spans, and is not recommended.</p> <p>Instead of mixing the two APIs in the same trace, we recommend that you migrate complete code paths from OpenTracing to OpenTelemetry as a single unit, so that only one API is used at a time.</p>"},{"location":"docs/migration/opentracing/#specification-and-implementation-details","title":"Specification and implementation details","text":"<p>For details on how each OpenTracing shim works, see the appropriate language-specific documentation. For details on the design of the OpenTracing shim, see OpenTracing Compatibility.</p>"},{"location":"docs/specs/status/","title":"\u89c4\u8303\u72b6\u6001\u6458\u8981","text":"<p>OpenTelemetry \u662f\u5728\u4e00\u4e2a\u4fe1\u53f7\u4e00\u4e2a\u4fe1\u53f7\u7684\u57fa\u7840\u4e0a\u5f00\u53d1\u7684\u3002\u8ddf\u8e2a\u3001\u5ea6\u91cf\u3001\u884c\u674e\u548c\u65e5\u5fd7\u8bb0\u5f55\u90fd\u662f \u4fe1\u53f7\u7684\u4f8b\u5b50\u3002\u4fe1\u53f7\u5efa\u7acb\u5728\u4e0a\u4e0b\u6587\u4f20\u64ad\u4e4b\u4e0a\uff0c\u4e0a\u4e0b\u6587\u4f20\u64ad\u662f\u4e00\u79cd\u8de8\u5206\u5e03\u5f0f\u7cfb\u7edf\u5173\u8054\u6570\u636e\u7684\u5171\u4eab \u673a\u5236\u3002</p> <p>\u6bcf\u4e2a\u4fe1\u53f7\u7531\u56db\u4e2a\u6838\u5fc3\u6210\u5206\u7ec4\u6210:</p> <ul> <li>APIs</li> <li>SDKs</li> <li>OpenTelemetry Protocol (OTLP)</li> <li>Collector</li> </ul> <p>\u4fe1\u53f7\u4e5f\u6709\u8d21\u732e\u7ec4\u4ef6\uff0c\u4e00\u4e2a\u63d2\u4ef6\u548c\u4eea\u5668\u7684\u751f\u6001\u7cfb\u7edf\u3002\u6240\u6709\u68c0\u6d4b\u5de5\u5177\u90fd\u5171\u4eab\u76f8\u540c\u7684\u8bed\u4e49\u7ea6\u5b9a\uff0c\u4ee5 \u786e\u4fdd\u5b83\u4eec\u5728\u89c2\u5bdf\u5e38\u89c1\u64cd\u4f5c(\u5982 HTTP \u8bf7\u6c42)\u65f6\u751f\u6210\u76f8\u540c\u7684\u6570\u636e\u3002</p> <p>\u8981\u4e86\u89e3\u6709\u5173\u4fe1\u53f7\u548c\u7ec4\u4ef6\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 OTel \u89c4\u8303\u6982\u8ff0.</p>"},{"location":"docs/specs/status/#_1","title":"\u7ec4\u4ef6\u751f\u547d\u5468\u671f","text":"<p>\u7ec4\u4ef6\u9075\u5faa\u5f00\u53d1\u751f\u547d\u5468\u671f:\u8349\u7a3f\u3001\u5b9e\u9a8c\u3001\u7a33\u5b9a\u3001\u5f03\u7528\u3001\u5220\u9664\u3002</p> <ul> <li>Draft components are under design, and have not been added to the   specification.</li> <li>Experimental components are released and available for beta testing.</li> <li>Stable components are backwards compatible and covered under long term   support.</li> <li>Deprecated components are stable but may eventually be removed.</li> </ul> <p>\u6709\u5173\u751f\u547d\u5468\u671f\u548c\u957f\u671f\u652f\u6301\u7684\u5b8c\u6574\u5b9a\u4e49\uff0c\u8bf7\u53c2 \u89c1[\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027]\u3002(/docs/specs/otel/versioning-and-stability/).</p>"},{"location":"docs/specs/status/#_2","title":"\u5f53\u524d\u7684\u72b6\u6001","text":"<p>\u4ee5\u4e0b\u662f\u5f53\u524d\u53ef\u7528\u4fe1\u53f7\u7684\u9ad8\u7ea7\u72b6\u6001\u62a5\u544a\u3002\u8bf7\u6ce8\u610f\uff0c\u867d\u7136 OpenTelemetry \u5ba2\u6237\u7aef\u9075\u5faa\u5171\u4eab\u89c4\u8303 \uff0c\u4f46\u5b83\u4eec\u662f\u72ec\u7acb\u5f00\u53d1\u7684\u3002</p> <p>Checking the current status for each client in the README of its github repo is recommended. Client support for specific features can be found in the specification compliance tables.</p> <p>Note that, for each of the following sections, the Collector status is the same as the Protocol status.</p>"},{"location":"docs/specs/status/#tracing","title":"Tracing","text":"<ul> <li>{{% spec_status \"API\" \"otel/trace/api\" \"Status\" %}}</li> <li>{{% spec_status \"SDK\" \"otel/trace/sdk\" \"Status\" %}}</li> <li>{{% spec_status \"Protocol\" \"otlp\" \"Status\" %}}</li> <li>Notes:</li> <li>The tracing specification is now completely stable, and covered by long term     support.</li> <li>The tracing specification is still extensible, but only in a backwards     compatible manner.</li> <li>OpenTelemetry clients are versioned to v1.0 once their tracing     implementation is complete.</li> </ul>"},{"location":"docs/specs/status/#metrics","title":"Metrics","text":"<ul> <li>{{% spec_status \"API\" \"otel/metrics/api\" \"Status\" %}}</li> <li>{{% spec_status \"SDK\" \"otel/metrics/sdk\" \"Status\" %}}</li> <li>{{% spec_status \"Protocol\" \"otlp\" \"Status\" %}}</li> <li>Notes:</li> <li>OpenTelemetry Metrics is currently under active development.</li> <li>The data model is stable and released as part of the OTLP protocol.</li> <li>Experimental support for metric pipelines is available in the Collector.</li> <li>Collector support for Prometheus is under development, in collaboration with     the Prometheus community.</li> </ul>"},{"location":"docs/specs/status/#baggage","title":"Baggage","text":"<ul> <li>{{% spec_status \"API\" \"otel/baggage/api\" \"Status\" %}}</li> <li>SDK: stable</li> <li>Protocol: N/A</li> <li>Notes:</li> <li>OpenTelemetry Baggage is now completely stable.</li> <li>Baggage is not an observability tool, it is a system for attaching arbitrary     keys and values to a transaction, so that downstream services may access     them. As such, there is no OTLP or Collector component to baggage.</li> </ul>"},{"location":"docs/specs/status/#logging","title":"Logging","text":"<ul> <li>{{% spec_status \"Bridge API\" \"otel/logs/bridge-api\" \"Status\" %}}</li> <li>{{% spec_status \"SDK\" \"otel/logs/sdk\" \"Status\" %}}</li> <li>{{% spec_status \"Event API\" \"otel/logs/event-api\" \"Status\" %}}</li> <li>{{% spec_status \"Protocol\" \"otlp\" \"Status\" %}}</li> <li>Notes:</li> <li>The logs data model is released as part of the OpenTelemetry Protocol.</li> <li>Log processing for many data formats has been added to the Collector, thanks     to the donation of Stanza to the OpenTelemetry project.</li> <li>The OpenTelemetry Log Bridge API allows for writing appenders which bridge     logs from existing log frameworks into OpenTelemetry. The Logs Bridge API is     not meant to be called directly by end users. Log appenders are under     development in many languages.</li> <li>The OpenTelemetry Log SDK is the standard implementation of the Log Bridge     API. Applications configure the SDK to indicate how logs are processed and     exported (e.g. using OTLP).</li> <li>The OpenTelemetry Event API allows log records to be emitted which conform     to the event semantic conventions. In contrast to the Log Bridge API,     the Event API is intended to be called by end users. The Event API is under     active development.</li> </ul>"},{"location":"docs/specs/otel/","title":"Index","text":""},{"location":"docs/specs/otel/#_1","title":"\u89c4\u8303","text":""},{"location":"docs/specs/otel/#_2","title":"\u5185\u5bb9","text":"<ul> <li>\u6982\u8ff0</li> <li>\u672f\u8bed</li> <li>OpenTelemetry \u5ba2\u6237\u7aef\u7684\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027</li> <li>\u56fe\u4e66\u9986\u7684\u6307\u5bfc\u65b9\u9488</li> <li>\u5305/\u5e93\u5e03\u5c40</li> <li>\u4e00\u822c\u9519\u8bef\u5904\u7406\u51c6\u5219</li> <li>API \u89c4\u8303</li> <li>\u4e0a\u4e0b\u6587<ul> <li>Propagators</li> </ul> </li> <li>Baggage</li> <li>\u8ffd\u8e2a</li> <li>\u6307\u6807</li> <li>\u65e5\u4e4b\u60e0<ul> <li>Bridge API</li> <li>Event API</li> </ul> </li> <li>SDK \u89c4\u8303</li> <li>\u8ffd\u8e2a</li> <li>\u6307\u6807</li> <li>\u65e5\u5fd7</li> <li>\u8d44\u6e90</li> <li>\u914d\u7f6e</li> <li>Data \u89c4\u8303</li> <li>\u8bed\u4e49\u7ea6\u5b9a</li> <li>\u534f\u8bae<ul> <li>\u6307\u6807</li> <li>\u65e5\u5fd7</li> </ul> </li> <li>\u517c\u5bb9<ul> <li>OpenCensus</li> <li>OpenTracing</li> <li>Prometheus \u548c OpenMetrics</li> <li>\u4ee5\u975e otlp \u65e5\u5fd7\u683c\u5f0f\u8ddf\u8e2a\u4e0a\u4e0b\u6587</li> </ul> </li> </ul>"},{"location":"docs/specs/otel/#_3","title":"\u7b26\u53f7\u7ea6\u5b9a\u548c\u9075\u4ece\u6027","text":"<p>\u89c4\u8303\u4e2d\u7684\u5173\u952e\u5b57\"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"NOT RECOMMENDED\", \"MAY\", and \"OPTIONAL\" \u5e94\u6309\u7167BCP 14 [RFC2119] [RFC8174]\u4e2d\u6240\u63cf\u8ff0\u7684\u89e3\u91ca\uff0c\u5f53\u4e14\u4ec5\u5f53\u5b83\u4eec\u4ee5\u5927 \u5199\u5b57\u6bcd\u51fa\u73b0\u65f6\uff0c\u5982\u4e0b\u6240\u793a\u3002</p> <p>\u5982\u679c\u89c4\u8303\u7684\u5b9e\u73b0\u4e0d\u80fd\u6ee1\u8db3\u89c4\u8303\u4e2d\u5b9a\u4e49\u7684\"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", or \"SHALL NOT\"\u4e2d\u7684\u4e00\u4e2a\u6216\u591a\u4e2a\u8981\u6c42\uff0c\u5219\u8be5\u89c4\u8303\u7684\u5b9e\u73b0\u4e0d\u5408\u89c4\u3002\u76f8\u53cd\uff0c\u5982 \u679c\u89c4\u8303\u7684\u5b9e\u73b0\u6ee1\u8db3\u89c4\u8303\u4e2d\u5b9a\u4e49\u7684\u6240\u6709\"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", and \"SHALL NOT\"\u9700\u6c42\uff0c\u5219\u8be5\u5b9e\u73b0\u662f\u517c\u5bb9\u7684\u3002</p>"},{"location":"docs/specs/otel/#_4","title":"\u9879\u76ee\u547d\u540d","text":"<ul> <li>\u5b98\u65b9\u9879\u76ee\u540d\u79f0\u662f\u201cOpenTelemetry\u201d(\u201cOpen\u201d\u548c\u201cTelemetry\u201d\u4e4b\u95f4\u6ca1\u6709\u7a7a\u683c)\u3002</li> <li>OpenTelemetry \u9879\u76ee\u4f7f\u7528\u7684\u5b98\u65b9\u7f29\u5199\u662f\u201cOTel\u201d\u3002\u907f\u514d\u4f7f\u7528\u201cOT\u201d\uff0c\u4ee5\u907f\u514d\u4e0e\u73b0\u5728\u5df2\u5f03\u7528\u7684   \u201cOpenTracing\u201d\u9879\u76ee\u6df7\u6dc6\u3002</li> <li>\u5b50\u9879\u76ee\u7684\u5b98\u65b9\u540d\u79f0\uff0c\u5982\u8bed\u8a00\u7279\u5b9a\u5b9e\u73b0\uff0c\u9075\u5faa\u201cOpenTelemetry {\u7f16\u7a0b\u8bed\u8a00\uff0c\u8fd0\u884c\u65f6\u6216\u7ec4\u4ef6\u7684   \u540d\u79f0}\u201d\u7684\u6a21\u5f0f\uff0c\u4f8b\u5982\uff0c\u201cOpenTelemetry Python\u201d\uff0c\u201cOpenTelemetry .NET\u201d\u6216   \u201cOpenTelemetry Collector\u201d\u3002</li> </ul>"},{"location":"docs/specs/otel/#_5","title":"\u9879\u76ee\u7b80\u4ecb","text":"<p>\u8bf7\u53c2\u9605\u9879\u76ee\u5b58\u50a8\u5e93\u4e86\u89e3\u4ee5\u4e0b\u4fe1\u606f\uff0c\u4ee5\u53ca\u66f4\u591a:</p> <ul> <li>Change / contribution process</li> <li>Project timeline</li> <li>Versioning the specification</li> <li>License</li> </ul>"},{"location":"docs/specs/otel/document-status/","title":"\u6587\u6863\u72b6\u6001\u7684\u5b9a\u4e49","text":"<p>Specification documents (files) may explicitly define a \"Status\", typically shown immediately after the document title. When present, the \"Status\" applies to the individual document only and not to the entire specification or any other documents. The following table describes what the statuses mean.</p>"},{"location":"docs/specs/otel/document-status/#lifecycle-status","title":"Lifecycle status","text":"<p>The support guarantees and allowed changes are governed by the lifecycle of the document.Lifecycle stages are defined in the Versioning and Stability document.</p> Status Explanation No explicit \"Status\" Equivalent to Experimental. Experimental Breaking changes are allowed. Stable Breaking changes are no longer allowed. See stability guarantees for details. Deprecated Changes are no longer allowed, except for editorial changes."},{"location":"docs/specs/otel/document-status/#feature-freeze","title":"Feature freeze","text":"<p>In addition to the statuses above, documents may be marked as <code>Feature-freeze</code>. These documents are not currently accepting new feature requests, to allow the Technical Committee time to focus on other areas of the specification. Editorial changes are still accepted. Changes that address production issues with existing features are still accepted.</p> <p>Feature freeze is separate from a lifecycle status. The lifecycle represents the support requirements for the document, feature freeze only indicates the current focus of the specification community. The feature freeze label may be applied to a document at any lifecycle stage. By definition, deprecated documents have a feature freeze in place.</p>"},{"location":"docs/specs/otel/document-status/#mixed","title":"Mixed","text":"<p>Some documents have individual sections with different statuses. These documents are marked with the status <code>Mixed</code> at the top, for clarity.</p>"},{"location":"docs/specs/otel/error-handling/","title":"\u9519\u8bef\u5904\u7406","text":"<p>OpenTelemetry generates telemetry data to help users monitor application code. In most cases, the work that the library performs is not essential from the perspective of application business logic. We assume that users would prefer to lose telemetry data rather than have the library significantly change the behavior of the instrumented application.</p> <p>OpenTelemetry may be enabled via platform extensibility mechanisms, or dynamically loaded at runtime. This makes the use of the library non-obvious for end users, and may even be outside of the application developer's control. This makes for some unique requirements with respect to error handling.</p>"},{"location":"docs/specs/otel/error-handling/#basic-error-handling-principles","title":"Basic error handling principles","text":"<p>OpenTelemetry implementations MUST NOT throw unhandled exceptions at run time.</p> <ol> <li>API methods MUST NOT throw unhandled exceptions when used incorrectly by end    users. The API and SDK SHOULD provide safe defaults for missing or invalid    arguments. For instance, a name like <code>empty</code> may be used if the user passes    in <code>null</code> as the span name argument during <code>Span</code> construction.</li> <li>The API or SDK MAY fail fast and cause the application to fail on    initialization, e.g. because of a bad user config or environment, but MUST    NOT cause the application to fail later at run time, e.g. due to dynamic    config settings received from the Collector.</li> <li>The SDK MUST NOT throw unhandled exceptions for errors in their own    operations. For example, an exporter should not throw an exception when it    cannot reach the endpoint to which it sends telemetry data.</li> </ol>"},{"location":"docs/specs/otel/error-handling/#guidance","title":"Guidance","text":"<ol> <li>API methods that accept external callbacks MUST handle all errors.</li> <li>Background tasks (e.g. threads, asynchronous tasks, and spawned processes)    should run in the context of a global error handler to ensure that exceptions    do not affect the end user application.</li> <li>Long-running background tasks should not fail permanently in response to    internal errors. In general, internal exceptions should only affect the    execution context of the request that caused the exception.</li> <li>Internal error handling should follow language-specific conventions. In    general, developers should minimize the scope of error handlers and add    special processing for expected exceptions.</li> <li>Beware external callbacks and overridable interfaces: Expect them to throw.</li> <li>Beware to call any methods that wasn't explicitly provided by API and SDK    users as a callbacks. Method <code>ToString</code> that SDK may decide to call on user    object may be badly implemented and lead to stack overflow. It is common that    the application never calls this method and this bad implementation would    never be caught by an application owner.</li> <li>Whenever API call returns values that is expected to be non-<code>null</code> value - in    case of error in processing logic - SDK MUST return a \"no-op\" or any other    \"default\" object that was (ideally) pre-allocated and readily available.    This way API call sites will not crash on attempts to access methods and    properties of a <code>null</code> objects.</li> </ol>"},{"location":"docs/specs/otel/error-handling/#error-handling-and-performance","title":"Error handling and performance","text":"<p>Error handling and extensive input validation may cause performance degradation, especially on dynamic languages where the input object types are not guaranteed in compile time. Runtime type checks will impact performance and are error prone, exceptions may occur despite the best effort.</p> <p>It is recommended to have a global exception handling logic that will guarantee that exceptions are not leaking to the user code. And make a reasonable trade off of the SDK performance and fullness of type checks that will provide a better on-error behavior and SDK errors troubleshooting.</p>"},{"location":"docs/specs/otel/error-handling/#self-diagnostics","title":"Self-diagnostics","text":"<p>All OpenTelemetry libraries -- the API, SDK, exporters, instrumentations, etc. -- are encouraged to expose self-troubleshooting metrics, spans, and other telemetry that can be easily enabled and filtered out by default.</p> <p>One good example of such telemetry is a <code>Span</code> exporter that indicates how much time exporters spend uploading telemetry. Another example may be a metric exposed by a <code>SpanProcessor</code> that describes the current queue size of telemetry data to be uploaded.</p> <p>Whenever the library suppresses an error that would otherwise have been exposed to the user, the library SHOULD log the error using language-specific conventions. SDKs MAY expose callbacks to allow end users to handle self-diagnostics separately from application code.</p>"},{"location":"docs/specs/otel/error-handling/#configuring-error-handlers","title":"Configuring Error Handlers","text":"<p>SDK implementations MUST allow end users to change the library's default error handling behavior for relevant errors. Application developers may want to run with strict error handling in a staging environment to catch invalid uses of the API, or malformed config. Note that configuring a custom error handler in this way is the only exception to the basic error handling principles outlined above. The mechanism by which end users set or register a custom error handler should follow language-specific conventions.</p>"},{"location":"docs/specs/otel/error-handling/#examples","title":"Examples","text":"<p>These are examples of how end users might register custom error handlers. Examples are for illustration purposes only. OpenTelemetry client authors are free to deviate from these provided that their design matches the requirements outlined above.</p>"},{"location":"docs/specs/otel/error-handling/#go","title":"Go","text":"<pre><code>// The basic Error Handler interface\ntype ErrorHandler interface {\nHandle(err error)\n}\nfunc Handler() ErrorHandler\nfunc SetHandler(handler ErrorHandler)\n</code></pre> <pre><code>// Registering a custom Error Handler\ntype IgnoreExporterErrorsHandler struct{}\nfunc (IgnoreExporterErrorsHandler) Handle(err error) {\nswitch err.(type) {\ncase *SpanExporterError:\ndefault:\nfmt.Println(err)\n}\n}\nfunc main() {\n// Other setup ...\nopentelemetrysdk.SetHandler(IgnoreExporterErrorsHandler{})\n}\n</code></pre>"},{"location":"docs/specs/otel/error-handling/#java","title":"Java","text":"<p>OpenTelemetry Java uses java.util.logging to output and handle all logs, including errors. Custom handlers and filters can be registered both in code and using the Java logging configuration file.</p> <pre><code>## Turn off all error logging\nio.opentelemetry.level = OFF\n</code></pre> <pre><code>// Creating a custom filter which does not log errors that come from the exporter\npublic class IgnoreExportErrorsFilter implements Filter {\npublic boolean isLoggable(LogRecord record) {\nreturn !record.getMessage().contains(\"Exception thrown by the export\");\n}\n}\n</code></pre> <pre><code>## Registering the custom filter on the BatchSpanProcessor\nio.opentelemetry.sdk.trace.export.BatchSpanProcessor = io.opentelemetry.extensions.logging.IgnoreExportErrorsFilter\n</code></pre>"},{"location":"docs/specs/otel/glossary/","title":"\u672f\u8bed\u8868","text":"<p>\u672c\u6587\u6863\u5b9a\u4e49\u4e86\u5728\u672c\u89c4\u8303\u4e2d\u4f7f\u7528\u7684\u4e00\u4e9b\u672f\u8bed\u3002</p> <p>\u5176\u4ed6\u4e00\u4e9b\u57fa\u672c\u672f\u8bed\u5728\u6982\u8ff0\u6587\u6863\u4e2d\u6709\u8bb0\u5f55\u3002</p>"},{"location":"docs/specs/otel/glossary/#_2","title":"\u7528\u6237\u89d2\u8272","text":""},{"location":"docs/specs/otel/glossary/#_3","title":"\u5e94\u7528\u7a0b\u5e8f\u6240\u6709\u8005","text":"<p>\u5e94\u7528\u7a0b\u5e8f\u6216\u670d\u52a1\u7684\u7ef4\u62a4\u8005\uff0c\u8d1f\u8d23\u914d\u7f6e\u548c\u7ba1\u7406 OpenTelemetry SDK \u7684\u751f\u547d\u5468\u671f\u3002</p>"},{"location":"docs/specs/otel/glossary/#_4","title":"\u5e93\u4f5c\u8005","text":"<p>\u5171\u4eab\u5e93\u7684\u7ef4\u62a4\u8005\uff0c\u8bb8\u591a\u5e94\u7528\u7a0b\u5e8f\u90fd\u4f9d\u8d56\u4e8e\u8fd9\u4e2a\u5e93\uff0cOpenTelemetry \u4eea\u5668\u4e5f\u5c06\u5176\u4f5c\u4e3a\u76ee\u6807\u3002</p>"},{"location":"docs/specs/otel/glossary/#_5","title":"\u63d2\u88c5\u4f5c\u8005","text":"<p>OpenTelemetry \u5de5\u5177\u7684\u7ef4\u62a4\u8005\uff0c\u6839\u636e OpenTelemetry API \u7f16\u5199\u3002\u8fd9\u53ef\u80fd\u662f\u5728\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801 \u3001\u5171\u4eab\u5e93\u6216\u5de5\u5177\u5e93\u4e2d\u7f16\u5199\u7684\u5de5\u5177\u3002</p>"},{"location":"docs/specs/otel/glossary/#_6","title":"\u63d2\u4ef6\u4f5c\u8005","text":"<p>OpenTelemetry SDK \u63d2\u4ef6\u7684\u7ef4\u62a4\u8005\uff0c\u9488\u5bf9 OpenTelemetry SDK \u63d2\u4ef6\u63a5\u53e3\u7f16\u5199\u3002</p>"},{"location":"docs/specs/otel/glossary/#_7","title":"\u901a\u7528","text":""},{"location":"docs/specs/otel/glossary/#_8","title":"\u4fe1\u53f7","text":"<p>OpenTelemetry \u662f\u56f4\u7ed5\u4fe1\u53f7\u6216\u9065\u6d4b\u5206\u7c7b\u6784\u5efa\u7684\u3002\u6307\u6807\u3001\u65e5\u5fd7\u3001\u8f68\u8ff9\u548c\u884c\u674e\u90fd\u662f\u4fe1\u53f7\u7684\u4f8b\u5b50\u3002 \u6bcf\u4e2a\u4fe1\u53f7\u4ee3\u8868\u4e00\u4e2a\u8fde\u8d2f\u7684\u3001\u72ec\u7acb\u7684\u529f\u80fd\u96c6\u3002\u6bcf\u4e2a\u4fe1\u53f7\u9075\u5faa\u4e00\u4e2a\u5355\u72ec\u7684\u751f\u547d\u5468\u671f\uff0c\u5b9a\u4e49\u5176\u5f53\u524d \u7684\u7a33\u5b9a\u6c34\u5e73\u3002</p>"},{"location":"docs/specs/otel/glossary/#_9","title":"\u5305","text":"<p>\u5728\u672c\u89c4\u8303\u4e2d\uff0c\u672f\u8bed \u5305 \u63cf\u8ff0\u4e86\u4e00\u7ec4\u4ee3\u8868\u5355\u4e2a\u4f9d\u8d56\u7684\u4ee3\u7801\uff0c\u8fd9\u4e9b\u4f9d\u8d56\u53ef\u4ee5\u72ec\u7acb\u4e8e\u5176\u4ed6\u5305\u5bfc \u5165\u5230\u7a0b\u5e8f\u4e2d\u3002\u8fd9\u4e2a\u6982\u5ff5\u53ef\u4ee5\u6620\u5c04\u5230\u67d0\u4e9b\u8bed\u8a00\u4e2d\u7684\u4e0d\u540c\u672f\u8bed\uff0c\u4f8b\u5982\u201c\u6a21\u5757\u201d\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u67d0\u4e9b\u8bed \u8a00\u4e2d\uff0c\u672f\u8bed\u201c\u5305\u201d\u6307\u7684\u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002</p>"},{"location":"docs/specs/otel/glossary/#abi","title":"ABI \u517c\u5bb9","text":"<p>ABI(\u5e94\u7528\u7a0b\u5e8f\u4e8c\u8fdb\u5236\u63a5\u53e3)\u662f\u4e00\u79cd\u5728\u673a\u5668\u4ee3\u7801\u7ea7\u522b\u5b9a\u4e49\u8f6f\u4ef6\u7ec4\u4ef6\u4e4b\u95f4\u4ea4\u4e92\u7684\u63a5\u53e3\uff0c\u4f8b\u5982\u5728\u5e94 \u7528\u7a0b\u5e8f\u53ef\u6267\u884c\u6587\u4ef6\u548c\u5171\u4eab\u5bf9\u8c61\u5e93\u7684\u7f16\u8bd1\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e4b\u95f4\u3002 ABI \u517c\u5bb9\u6027\u610f\u5473\u7740\u5e93\u7684\u65b0\u7f16\u8bd1\u7248 \u672c\u53ef\u4ee5\u6b63\u786e\u5730\u94fe\u63a5\u5230\u76ee\u6807\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u7f16\u8bd1\u8be5\u53ef\u6267\u884c\u6587\u4ef6\u3002</p> <p>ABI \u517c\u5bb9\u6027\u5bf9\u4e8e\u67d0\u4e9b\u8bed\u8a00\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u63d0\u4f9b\u67d0\u79cd\u5f62\u5f0f\u7684\u673a\u5668\u7801\u7684\u8bed\u8a00\u3002\u5bf9\u4e8e\u5176\u4ed6\u8bed \u8a00\uff0cABI \u517c\u5bb9\u6027\u53ef\u80fd\u4e0d\u662f\u76f8\u5173\u9700\u6c42\u3002</p>"},{"location":"docs/specs/otel/glossary/#_10","title":"\u5e26\u5185\u548c\u5e26\u5916\u6570\u636e","text":"<p>\u5728\u7535\u4fe1\u4e2d\uff0c\u5e26\u5185\u4fe1\u4ee4\u662f\u5728\u7528\u4e8e\u6570\u636e(\u5982\u8bed\u97f3\u6216\u89c6\u9891)\u7684\u540c\u4e00\u9891\u5e26\u6216\u4fe1\u9053\u5185\u53d1\u9001\u63a7\u5236\u4fe1\u606f\u3002\u8fd9 \u4e0e\u5e26\u5916\u4fe1\u53f7\u5f62\u6210\u5bf9\u6bd4\uff0c\u5e26\u5916\u4fe1\u53f7\u901a\u8fc7\u4e0d\u540c\u7684\u901a\u9053\u53d1\u9001\uff0c\u751a\u81f3\u901a\u8fc7\u5355\u72ec\u7684\u7f51\u7edc (Wikipedia)\u3002</p> <p>\u5728 opentelementetry \u4e2d\uff0c\u6211\u4eec\u5c06\u5e26\u5185\u6570\u636e\u79f0\u4e3a\u4f5c\u4e3a\u4e1a\u52a1\u6d88\u606f\u7684\u4e00\u90e8\u5206\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u7ec4\u4ef6\u4e4b \u95f4\u4f20\u9012\u7684\u6570\u636e\uff0c\u4f8b\u5982\uff0c\u5f53\u8ddf\u8e2a\u6216\u5305\u4ee5 HTTP \u5934\u7684\u5f62\u5f0f\u5305\u542b\u5728 HTTP \u8bf7\u6c42\u4e2d\u65f6\u3002\u8fd9\u4e9b\u6570\u636e\u901a\u5e38 \u4e0d\u5305\u542b\u9065\u6d4b\u6570\u636e\uff0c\u4f46\u7528\u4e8e\u5173\u8054\u548c\u8fde\u63a5\u7531\u5404\u4e2a\u7ec4\u4ef6\u4ea7\u751f\u7684\u9065\u6d4b\u6570\u636e\u3002\u9065\u6d4b\u672c\u8eab\u88ab\u79f0\u4e3a\u5e26\u5916\u6570\u636e :\u5b83\u901a\u8fc7\u4e13\u7528\u6d88\u606f\u4ece\u5e94\u7528\u7a0b\u5e8f\u4f20\u8f93\uff0c\u901a\u5e38\u7531\u540e\u53f0\u4f8b\u7a0b\u5f02\u6b65\u4f20\u8f93\uff0c\u800c\u4e0d\u662f\u4ece\u4e1a\u52a1\u903b\u8f91\u7684\u5173\u952e\u8def \u5f84\u4f20\u8f93\u3002\u5bfc\u51fa\u5230\u9065\u6d4b\u540e\u7aef\u7684\u5ea6\u91cf\u3001\u65e5\u5fd7\u548c\u8ddf\u8e2a\u5c31\u662f\u5e26\u5916\u6570\u636e\u7684\u4f8b\u5b50\u3002</p>"},{"location":"docs/specs/otel/glossary/#_11","title":"\u624b\u52a8\u63d2\u88c5","text":"<p>\u9488\u5bf9 OpenTelemetry API \u7f16\u7801\uff0c\u5982Tracing API, Metrics API\uff0c\u6216\u5176\u4ed6\u4ece\u6700\u7ec8\u7528\u6237\u4ee3\u7801\u6216\u5171\u4eab\u6846\u67b6(\u4f8b\u5982 MongoDB, Redis \u7b49)\u6536\u96c6\u9065\u6d4b\u6570\u636e\u3002</p>"},{"location":"docs/specs/otel/glossary/#_12","title":"\u81ea\u52a8\u63d2\u88c5","text":"<p>\u6307\u4e0d\u9700\u8981\u6700\u7ec8\u7528\u6237\u4fee\u6539\u5e94\u7528\u7a0b\u5e8f\u6e90\u4ee3\u7801\u7684\u9065\u6d4b\u6536\u96c6\u65b9\u6cd5\u3002\u65b9\u6cd5\u56e0\u7f16\u7a0b\u8bed\u8a00\u800c\u5f02\uff0c\u793a\u4f8b\u5305\u62ec\u4ee3 \u7801\u64cd\u4f5c(\u5728\u7f16\u8bd1\u671f\u95f4\u6216\u8fd0\u884c\u65f6)\u3001\u7334\u5b50\u8865\u4e01\u6216\u8fd0\u884c eBPF \u7a0b\u5e8f\u3002</p> <p>Synonym: Auto-instrumentation.</p>"},{"location":"docs/specs/otel/glossary/#sdk","title":"\u9065\u6d4b SDK","text":"<p>\u8868\u793a\u5b9e\u73b0 OpenTelemetry API \u7684\u5e93\u3002</p> <p>\u89c1\u5e93\u6307\u5f15 \u548c\u5e93\u8d44\u6e90\u8bed\u4e49\u7ea6\u5b9a.</p>"},{"location":"docs/specs/otel/glossary/#_13","title":"\u6784\u9020\u51fd\u6570","text":"<p>\u6784\u9020\u51fd\u6570\u662f\u5e94\u7528\u7a0b\u5e8f\u6240\u6709\u8005\u7528\u6765\u521d\u59cb\u5316\u548c\u914d\u7f6e OpenTelemetry SDK \u548c\u8d21\u732e\u5305\u7684\u516c\u5171\u4ee3\u7801\u3002 \u6784\u9020\u51fd\u6570\u7684\u4f8b\u5b50\u5305\u62ec\u914d\u7f6e\u5bf9\u8c61\u3001\u73af\u5883\u53d8\u91cf\u548c\u6784\u9020\u51fd\u6570\u3002</p>"},{"location":"docs/specs/otel/glossary/#sdk_1","title":"SDK \u63d2\u4ef6","text":"<p>\u63d2\u4ef6\u662f\u6269\u5c55 OpenTelemetry SDK \u7684\u5e93\u3002\u63d2\u4ef6\u63a5\u53e3\u7684\u4f8b\u5b50\u6709<code>SpanProcessor</code>, <code>Exporter</code>, \u548c <code>Sampler</code> \u63a5\u53e3\u3002</p>"},{"location":"docs/specs/otel/glossary/#_14","title":"\u5bfc\u51fa\u5e93","text":"<p>\u5bfc\u51fa\u5668\u662f SDK \u63d2\u4ef6\uff0c\u5b83\u5b9e\u73b0\u4e86\u201c\u5bfc\u51fa\u5668\u201d\u63a5\u53e3\uff0c\u5e76\u5411\u6d88\u8d39\u8005\u53d1\u9001\u9065\u6d4b\u4fe1\u606f\u3002</p>"},{"location":"docs/specs/otel/glossary/#instrumented-library","title":"Instrumented Library","text":"<p>\u8868\u793a\u4e3a\u5176\u6536\u96c6\u9065\u6d4b\u4fe1\u53f7(\u8ddf\u8e2a\u3001\u5ea6\u91cf\u3001\u65e5\u5fd7)\u7684\u5e93\u3002</p> <p>\u5bf9 OpenTelemetry API \u7684\u8c03\u7528\u65e2\u53ef\u4ee5\u7531\u63d2\u88c5\u5e93\u672c\u8eab\u5b8c\u6210\uff0c\u4e5f\u53ef\u4ee5\u7531\u53e6\u4e00 \u4e2a\u63d2\u88c5\u5e93\u5b8c\u6210\u3002</p> <p>Example: <code>org.mongodb.client</code>.</p>"},{"location":"docs/specs/otel/glossary/#instrumentation-library","title":"Instrumentation Library","text":"<p>\u8868\u793a\u4e3a\u7ed9\u5b9a\u7684Instrumented library\u63d0\u4f9b\u68c0\u6d4b\u7684\u5e93\u3002\u5982\u679c Instrumented Library \u548c Instrumentation Library \u6709\u5185\u7f6e\u7684 OpenTelemetry instrumentation\uff0c\u90a3\u4e48\u5b83\u4eec\u53ef\u80fd\u662f\u540c\u4e00\u4e2a\u5e93\u3002</p> <p>\u8bf7\u53c2\u9605\u6982\u8ff0\u4e86\u89e3\u66f4\u8be6\u7ec6\u7684\u5b9a\u4e49\u548c\u547d\u540d\u6307\u5357\u3002</p> <p>Example: <code>io.opentelemetry.contrib.mongodb</code>.</p> <p>Synonyms: Instrumenting Library.</p>"},{"location":"docs/specs/otel/glossary/#instrumentation-scope","title":"Instrumentation Scope","text":"<p>\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u7684\u903b\u8f91\u5355\u5143\uff0c\u53d1\u51fa\u7684\u9065\u6d4b\u4fe1\u606f\u53ef\u4ee5\u4e0e\u5b83\u76f8\u5173\u8054\u3002\u901a\u5e38\u7531\u5f00\u53d1\u4eba\u5458\u51b3\u5b9a\u4ec0\u4e48\u8868\u793a \u5408\u7406\u7684\u68c0\u6d4b\u8303\u56f4\u3002\u6700\u5e38\u89c1\u7684\u65b9\u6cd5\u662f\u4f7f \u7528instrumentation library\u4f5c\u4e3a\u8303\u56f4\uff0c\u4f46\u662f\u5176\u4ed6\u8303\u56f4\u4e5f\u5f88 \u5e38\u89c1\uff0c\u4f8b\u5982\uff0c\u53ef\u4ee5\u9009\u62e9\u4e00\u4e2a\u6a21\u5757\u3001\u4e00\u4e2a\u5305\u6216\u4e00\u4e2a\u7c7b\u4f5c\u4e3a\u68c0\u6d4b\u8303\u56f4\u3002</p> <p>\u5982\u679c\u4ee3\u7801\u5355\u5143\u6709\u7248\u672c\uff0c\u5219\u63d2\u88c5\u8303\u56f4\u7531(name,version)\u5bf9\u5b9a\u4e49\uff0c\u5426\u5219\u7701\u7565\u7248\u672c\uff0c\u53ea\u4f7f\u7528\u540d\u79f0\u3002 \u540d\u79f0\u6216(\u540d\u79f0\u3001\u7248\u672c)\u5bf9\u552f\u4e00\u5730\u6807\u8bc6\u53d1\u51fa\u9065\u6d4b\u7684\u4ee3\u7801\u7684\u903b\u8f91\u5355\u5143\u3002\u786e\u4fdd\u552f\u4e00\u6027\u7684\u5178\u578b\u65b9\u6cd5\u662f\u4f7f \u7528\u53d1\u51fa\u4ee3\u7801\u7684\u5b8c\u5168\u9650\u5b9a\u540d(\u4f8b\u5982\uff0c\u5b8c\u5168\u9650\u5b9a\u5e93\u540d\u6216\u5b8c\u5168\u9650\u5b9a\u7c7b\u540d)\u3002</p> <p>\u4eea\u8868\u8303\u56f4\u7528\u4e8e\u83b7\u53d6\u793a\u8e2a\u5242\u6216\u4eea\u8868.</p> <p>\u68c0\u6d4b\u8303\u56f4\u53ef\u4ee5\u6709\u96f6\u4e2a\u6216\u591a\u4e2a\u9644\u52a0\u5c5e\u6027\uff0c\u8fd9\u4e9b\u5c5e\u6027\u63d0\u4f9b\u6709\u5173\u8be5\u8303\u56f4\u7684\u9644\u52a0\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u6307 \u5b9a\u5de5\u5177\u5e93\u7684\u4f5c\u7528\u57df\uff0c\u53ef\u4ee5\u8bb0\u5f55\u4e00\u4e2a\u9644\u52a0\u5c5e\u6027\u6765\u8868\u793a\u5e93\u7684 URL\u3002\u5e93\u7684\u6e90\u4ee3\u7801\u5b58\u50a8\u5728\u5b58\u50a8\u5e93\u7684 URL \u4e2d\u3002\u7531\u4e8e\u8303\u56f4\u662f\u6784\u5efa\u65f6\u7684\u6982\u5ff5\uff0c\u56e0\u6b64\u8303\u56f4\u7684\u5c5e\u6027\u4e0d\u80fd\u5728\u8fd0\u884c\u65f6\u66f4\u6539\u3002</p>"},{"location":"docs/specs/otel/glossary/#_15","title":"\u793a\u8e2a\u5668\u540d\u79f0/\u8ba1\u7b97\u5668\u540d\u79f0","text":"<p>This refers to the <code>name</code> and (optional) <code>version</code> arguments specified when creating a new <code>Tracer</code> or <code>Meter</code> (see Obtaining a Tracer/Obtaining a Meter). The name/version pair identifies the Instrumentation Scope, for example the Instrumentation Library or another unit of application in the scope of which the telemetry is emitted.</p> <p>\u8fd9\u6307\u7684\u662f <code>name</code> \u548c(\u53ef\u9009)\u5728\u521b\u5efa\u65b0\u7684\u201c\u8ddf\u8e2a\u5668\u201d\u6216\u201c\u8ba1\u7b97\u5668\u201d\u65f6\u6307\u5b9a\u7684\u201c\u7248\u672c\u201d\u53c2\u6570(\u53c2 \u89c1\u83b7\u53d6\u8ddf\u8e2a\u5668/\u83b7\u53d6\u4eea\u8868)\u3002 \u540d\u79f0/\u7248\u672c\u5bf9\u6807\u8bc6\u4e86Instrumentation Scope\uff0c\u4f8b \u5982Instrumentation Library\u6216\u9065\u6d4b\u53d1\u5c04\u8303\u56f4\u5185\u7684\u53e6\u4e00\u4e2a\u5e94 \u7528\u5355\u5143\u3002</p>"},{"location":"docs/specs/otel/glossary/#_16","title":"\u6267\u884c\u5355\u5143","text":"<p>\u987a\u5e8f\u4ee3\u7801\u6267\u884c\u7684\u6700\u5c0f\u5355\u5143\u7684\u603b\u79f0\uff0c\u7528\u4e8e\u591a\u4efb\u52a1\u7684\u4e0d\u540c\u6982\u5ff5\u3002\u4f8b\u5982\u7ebf\u7a0b\u3001\u534f\u7a0b\u6216\u7ea4\u7ef4\u3002</p>"},{"location":"docs/specs/otel/glossary/#_17","title":"\u65e5\u5fd7","text":""},{"location":"docs/specs/otel/glossary/#_18","title":"\u65e5\u5fd7\u8bb0\u5f55","text":"<p>\u8bb0\u5f55:\u4e8b\u4ef6\u7684\u8bb0\u5f55\u901a\u5e38\uff0c\u8bb0\u5f55\u5305\u62ec\u4e00\u4e2a\u65f6\u95f4\u6233\uff0c\u6307\u793a\u4e8b\u4ef6\u53d1\u751f\u7684\u65f6\u95f4\uff0c\u4ee5\u53ca\u63cf\u8ff0\u53d1\u751f\u4e86\u4ec0\u4e48 \u3001\u53d1\u751f\u5728\u54ea\u91cc\u7b49\u7684\u5176\u4ed6\u6570\u636e\u3002</p> <p>Synonyms: Log Entry.</p>"},{"location":"docs/specs/otel/glossary/#_19","title":"\u65e5\u5fd7","text":"<p>\u6709\u65f6\u7528\u4e8e\u6307\u65e5\u5fd7\u8bb0\u5f55\u7684\u96c6\u5408\u3002\u53ef\u80fd\u4f1a\u6709\u6b67\u4e49\uff0c\u56e0\u4e3a\u4eba\u4eec\u6709\u65f6\u4e5f\u4f1a\u7528\u201cLog\u201d\u6765\u6307\u4ee3\u5355\u4e00\u7684\u201cLog Record\u201d\uff0c\u56e0\u6b64\u8fd9\u4e2a\u672f\u8bed\u5e94\u8be5\u8c28\u614e\u4f7f\u7528\uff0c\u5728\u53ef\u80fd\u4ea7\u751f\u6b67\u4e49\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u5e94\u8be5\u4f7f\u7528\u989d\u5916\u7684\u9650\u5b9a \u8bcd(\u4f8b\u5982:\u201c\u65e5\u5fd7\u8bb0\u5f55\u201d)\u3002</p>"},{"location":"docs/specs/otel/glossary/#_20","title":"\u5185\u5d4c\u65e5\u5fd7","text":"<p>\u201c\u65e5\u5fd7\u8bb0\u5f55\u201d\u5d4c\u5165\u5728Span\u5bf9\u8c61\u4e2d\uff0c \u5728Events\u5217\u8868\u4e2d\u3002</p>"},{"location":"docs/specs/otel/glossary/#_21","title":"\u6807\u51c6\u65e5\u5fd7","text":"<p>\u201c\u65e5\u5fd7\u8bb0\u5f55\u201d\u6ca1\u6709\u5d4c\u5165\u5230\u201cSpan\u201d\u4e2d\uff0c\u800c\u662f\u8bb0\u5f55\u5728\u5176\u4ed6\u5730\u65b9\u3002</p>"},{"location":"docs/specs/otel/glossary/#_22","title":"\u65e5\u5fd7\u5c5e\u6027","text":"<p>\u201c\u65e5\u5fd7\u8bb0\u5f55\u201d\u4e2d\u5305\u542b\u7684\u952e/\u503c\u5bf9\u3002</p>"},{"location":"docs/specs/otel/glossary/#_23","title":"\u7ed3\u6784\u5316\u7684\u65e5\u5fd7","text":"<p>\u65e5\u5fd7\u8bb0\u5f55\u7684\u683c\u5f0f\u5177\u6709\u826f\u597d\u5b9a\u4e49\u7684\u7ed3\u6784\uff0c\u5141\u8bb8\u533a\u5206\u65e5\u5fd7\u8bb0\u5f55\u7684\u4e0d\u540c\u5143\u7d20(\u4f8b\u5982\u65f6\u95f4\u6233\uff0c\u5c5e\u6027\u7b49 )\u3002\u4f8b\u5982\uff0cSyslog \u534f\u8bae (RFC 5424)\u5b9a\u4e49\u4e86\u201c \u7ed3\u6784\u5316\u6570\u636e\u201d\u683c\u5f0f\u3002</p>"},{"location":"docs/specs/otel/glossary/#_24","title":"\u5e73\u9762\u6587\u4ef6\u65e5\u5fd7","text":"<p>\u8bb0\u5f55\u5728\u6587\u672c\u6587\u4ef6\u4e2d\u7684\u65e5\u5fd7\uff0c\u901a\u5e38\u6bcf\u6761\u65e5\u5fd7\u8bb0\u5f55\u4e00\u884c(\u5c3d\u7ba1\u4e5f\u53ef\u80fd\u6709\u591a\u884c\u8bb0\u5f55)\u3002\u4ee5\u66f4\u7ed3\u6784\u5316\u7684 \u683c\u5f0f(\u4f8b\u5982 JSON \u6587\u4ef6)\u5199\u5165\u6587\u672c\u6587\u4ef6\u7684\u65e5\u5fd7\u662f\u5426\u88ab\u8ba4\u4e3a\u662f\u5e73\u9762\u6587\u4ef6\u65e5\u5fd7\uff0c\u76ee\u524d\u8fd8\u6ca1\u6709\u901a\u7528\u7684 \u884c\u4e1a\u534f\u8bae\u3002\u5982\u679c\u8fd9\u6837\u7684\u533a\u522b\u662f\u91cd\u8981\u7684\uff0c\u5efa\u8bae\u7279\u522b\u6307\u51fa\u6765\u3002</p>"},{"location":"docs/specs/otel/glossary/#_25","title":"\u65e5\u5fd7\u9644\u52a0/\u6865\u63a5","text":"<p>\u65e5\u5fd7\u9644\u52a0\u5668\u6216\u6865\u63a5\u5668\u662f\u4e00\u4e2a\u7ec4\u4ef6\uff0c\u5b83\u4f7f\u7528Log Bridge API\u5c06\u65e5\u5fd7 \u4ece\u73b0\u6709\u7684\u65e5\u5fd7 API \u6865\u63a5\u5230 OpenTelemetry \u4e2d\u3002\u672f\u8bed\u201c\u65e5\u5fd7\u6865\u63a5\u5668\u201d\u548c\u201c\u65e5\u5fd7 appender\u201d\u53ef\u4ee5 \u4e92\u6362\u4f7f\u7528\uff0c\u8fd9\u53cd\u6620\u4e86\u8fd9\u4e9b\u7ec4\u4ef6\u5c06\u6570\u636e\u6865\u63a5\u5230 OpenTelemetry \u4e2d\uff0c\u4f46\u5728\u65e5\u5fd7\u9886\u57df\u901a\u5e38\u79f0\u4e3a\u9644 \u7740\u5668\u3002</p>"},{"location":"docs/specs/otel/library-guidelines/","title":"\u5ba2\u6237\u7aef\u8bbe\u8ba1\u539f\u5219","text":"<p>\u672c\u6587\u6863\u5b9a\u4e49\u4e86\u4e00\u4e9b\u901a\u7528\u539f\u5219\uff0c\u8fd9\u4e9b\u539f\u5219\u5c06\u5e2e\u52a9\u8bbe\u8ba1\u4eba\u5458\u521b\u5efa\u6613\u4e8e\u4f7f\u7528\u7684 OpenTelemetry \u5ba2 \u6237\u7aef\uff0c\u8fd9\u4e9b\u5ba2\u6237\u7aef\u5728\u6240\u6709\u652f\u6301\u7684\u8bed\u8a00\u4e2d\u90fd\u662f\u7edf\u4e00\u7684\uff0c\u5e76\u4e14\u4e3a\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u8868\u8fbe\u6027\u63d0\u4f9b\u8db3\u591f\u7684 \u7075\u6d3b\u6027\u3002</p> <p>OpenTelemetry \u5ba2\u6237\u7aef\u88ab\u671f\u671b\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u7684\u5b8c\u6574\u7279\u6027\uff0c\u5e76\u5141\u8bb8\u901a\u8fc7\u53ef\u6269\u5c55\u6027\u8fdb\u884c\u521b\u65b0\u548c\u5b9e \u9a8c\u3002</p> <p>\u8bf7\u5148\u9605\u8bfb\u6982\u8ff0\uff0c\u4e86\u89e3 OpenTelemetry \u7684\u57fa\u672c\u67b6\u6784\u3002</p> <p>\u672c\u6587\u6863\u4e0d\u8bd5\u56fe\u63cf\u8ff0 OpenTelemetry \u5ba2\u6237\u7aef API \u7684\u7ec6\u8282\u6216\u529f\u80fd\u3002\u6709\u5173 API \u89c4\u8303\uff0c\u8bf7\u53c2 \u9605API \u89c4\u8303\u3002</p> <p>OpenTelemetry \u5ba2\u6237\u7aef\u4f5c\u8005\u6ce8\u610f\u4e8b\u9879:</p> <p>OpenTelemetry \u89c4\u8303\u3001API \u548c SDK \u5b9e\u73b0\u6307\u5357\u6b63\u5728\u8fdb\u884c\u4e2d\u3002 \u5982\u679c\u60a8\u6ce8\u610f\u5230\u4e0d\u5b8c\u6574\u6216\u7f3a\u5931\u7684\u4fe1   \u606f\u3001\u77db\u76fe\u3001\u4e0d\u4e00\u81f4\u7684\u6837\u5f0f\u548c\u5176\u4ed6\u7f3a\u9677\uff0c\u8bf7\u901a\u8fc7\u5728\u6b64\u5b58\u50a8\u5e93\u4e2d\u521b\u5efa\u95ee\u9898\u6216   \u5728Slack\u4e2d\u53d1\u5e16\u8ba9\u89c4\u8303\u7f16\u5199\u8005   \u77e5\u9053\u3002 \u4f5c\u4e3a\u89c4\u8303\u7684\u5b9e\u73b0\u8005\uff0c\u60a8\u901a\u5e38\u4f1a\u5bf9\u5982\u4f55\u6539\u8fdb\u89c4\u8303\u6709\u5b9d\u8d35\u7684\u89c1\u89e3\u3002 \u89c4\u8303 SIG \u548c\u6280\u672f\u59d4   \u5458\u4f1a\u6210\u5458\u975e\u5e38\u91cd\u89c6\u60a8\u7684\u610f\u89c1\u5e76\u6b22\u8fce\u60a8\u7684\u53cd\u9988\u3002</p>"},{"location":"docs/specs/otel/library-guidelines/#_2","title":"\u9700\u6c42","text":"<ol> <li> <p>OpenTelemetry API \u5fc5\u987b\u5b9a\u4e49\u826f\u597d\uff0c\u5e76\u4e0e\u5b9e\u73b0\u660e\u786e\u89e3\u8026\u3002\u8fd9\u5141\u8bb8\u6700\u7ec8\u7528\u6237\u53ea\u4f7f\u7528 API \u800c    \u4e0d\u4f7f\u7528\u5b9e\u73b0(\u53c2\u89c1\u7b2c 2 \u70b9\u548c\u7b2c 3 \u70b9\uff0c\u4e86\u89e3\u4e3a\u4ec0\u4e48\u5b83\u5f88\u91cd\u8981)\u3002</p> </li> <li> <p>Third party libraries and frameworks that add instrumentation to their code    will have a dependency only on the API of OpenTelemetry client. The    developers of third party libraries and frameworks do not care (and cannot    know) what specific implementation of OpenTelemetry is used in the final    application.</p> </li> <li> <p>The developers of the final application normally decide how to configure    OpenTelemetry SDK and what extensions to use. They should be also free to    choose to not use any OpenTelemetry implementation at all, even though the    application and/or its libraries are already instrumented. The rationale is    that third-party libraries and frameworks which are instrumented with    OpenTelemetry must still be fully usable in the applications which do not    want to use OpenTelemetry (so this removes the need for framework developers    to have \"instrumented\" and \"non-instrumented\" versions of their framework).</p> </li> <li> <p>The SDK must be clearly separated into wire protocol-independent parts that    implement common logic (e.g. batching, tag enrichment by process information,    etc.) and protocol-dependent telemetry exporters. Telemetry exporters must    contain minimal functionality, thus enabling vendors to easily add support    for their specific protocol.</p> </li> <li> <p>The SDK implementation should include the following exporters:</p> </li> <li> <p>logs, metrics, trace</p> <ul> <li>OTLP (OpenTelemetry Protocol).</li> <li>Standard output (or logging) to use for debugging and testing as well as    an input for the various log proxy tools.</li> <li>In-memory (mock) exporter that accumulates telemetry data in the local    memory and allows to inspect it (useful for e.g. unit tests).</li> </ul> </li> <li>metrics<ul> <li>Prometheus.</li> </ul> </li> <li>trace<ul> <li>Jaeger.</li> <li>Zipkin.</li> </ul> </li> </ol> <p>Note: some of these support multiple protocols (e.g. gRPC, Thrift, etc). The    exact list of protocols to implement in the exporters is TBD.</p> <p>Other vendor-specific exporters (exporters that implement vendor protocols)    should not be included in OpenTelemetry clients and should be placed    elsewhere (the exact approach for storing and maintaining vendor-specific    exporters will be defined in the future).</p>"},{"location":"docs/specs/otel/library-guidelines/#opentelemetry","title":"OpenTelemetry \u5ba2\u6237\u7aef\u901a\u7528\u8bbe\u8ba1","text":"<p>Here is a generic design for an OpenTelemetry client (arrows indicate calls):</p> <p></p>"},{"location":"docs/specs/otel/library-guidelines/#_3","title":"\u9884\u671f\u7684\u4f7f\u7528","text":"<p>The OpenTelemetry client is composed of 4 types of packages: API packages, SDK packages, a Semantic Conventions package, and plugin packages. The API and the SDK are split into multiple packages, based on signal type (e.g. one for api-trace, one for api-metric, one for sdk-trace, one for sdk-metric) is considered an implementation detail as long as the API artifact(s) stay separate from the SDK artifact(s).</p> <p>Libraries, frameworks, and applications that want to be instrumented with OpenTelemetry take a dependency only on the API packages. The developers of these third-party libraries will make calls to the API to produce telemetry data.</p> <p>Applications that use third-party libraries that are instrumented with OpenTelemetry API control whether or not to install the SDK and generate telemetry data. When the SDK is not installed, the API calls should be no-ops which generate minimal overhead.</p> <p>In order to enable telemetry the application must take a dependency on the OpenTelemetry SDK. The application must also configure exporters and other plugins so that telemetry can be correctly generated and delivered to their analysis tool(s) of choice. The details of how plugins are enabled and configured are language specific.</p>"},{"location":"docs/specs/otel/library-guidelines/#api","title":"API \u548c\u6700\u5c0f\u5b9e\u73b0","text":"<p>The API package is a self-sufficient dependency, in the sense that if the end-user application or a third-party library depends only on it and does not plug a full SDK implementation then the application will still build and run without failing, although no telemetry data will be actually delivered to a telemetry backend.</p> <p>This self-sufficiency is achieved the following way.</p> <p>The API dependency contains a minimal implementation of the API. When no other implementation is explicitly included in the application no telemetry data will be collected. Here is what active components look like in this case:</p> <p></p> <p>It is important that values returned from this minimal implementation of API are valid and do not require the caller to perform extra checks (e.g. createSpan() method should not fail and should return a valid non-null Span object). The caller should not need to know and worry about the fact that minimal implementation is in effect. This minimizes the boilerplate and error handling in the instrumented code.</p> <p>It is also important that minimal implementation incurs as little performance penalty as possible, so that third-party frameworks and libraries that are instrumented with OpenTelemetry impose negligible overheads to users of such libraries that do not want to use OpenTelemetry too.</p>"},{"location":"docs/specs/otel/library-guidelines/#sdk","title":"SDK \u5b9e\u73b0","text":"<p>SDK implementation is a separate (optional) dependency. When it is plugged in it substitutes the minimal implementation that is included in the API package (exact substitution mechanism is language dependent).</p> <p>SDK implements core functionality that is required for translating API calls into telemetry data that is ready for exporting. Here is how OpenTelemetry components look like when SDK is enabled:</p> <p></p> <p>SDK defines an Exporter interface. Protocol-specific exporters that are responsible for sending telemetry data to backends must implement this interface.</p> <p>SDK also includes optional helper exporters that can be composed for additional functionality if needed.</p> <p>Library designers need to define the language-specific <code>Exporter</code> interface based on this generic specification.</p>"},{"location":"docs/specs/otel/library-guidelines/#_4","title":"\u534f\u8bae\u7684\u5bfc\u51fa\u5668","text":"<p>Telemetry backend vendors are expected to implement Exporter interface. Data received via Export() function should be serialized and sent to the backend in a vendor-specific way.</p> <p>Vendors are encouraged to keep protocol-specific exporters as simple as possible and achieve desirable additional functionality such as queuing and retrying using helpers provided by SDK.</p> <p>End users should be given the flexibility of making many of the decisions regarding the queuing, retrying, tagging, batching functionality that make the most sense for their application. For example, if an application's telemetry data must be delivered to a remote backend that has no guaranteed availability the end user may choose to use a persistent local queue and an <code>Exporter</code> to retry sending on failures. As opposed to that for an application that sends telemetry to a locally running Agent daemon, the end user may prefer to have a simpler exporting configuration without retrying or queueing.</p> <p>If additional exporters for the sdk are provided as separate libraries, the name of the library should be prefixed with the terms \"OpenTelemetry\" and \"Exporter\" in accordance with the naming conventions of the respective technology.</p> <p>For example:</p> <ul> <li>Python and Java: opentelemetry-exporter-jaeger</li> <li>Javascript: opentelemetry/exporter-jeager</li> </ul>"},{"location":"docs/specs/otel/library-guidelines/#_5","title":"\u8d44\u6e90\u53d1\u73b0","text":"<p>Cloud vendors are encouraged to provide packages to detect resource information from the environment. These MUST be implemented outside of the SDK. See Resource SDK for more details.</p>"},{"location":"docs/specs/otel/library-guidelines/#_6","title":"\u9009\u62e9\u5b9e\u73b0","text":"<p>The end-user application may decide to take a dependency on alternative implementation.</p> <p>SDK provides flexibility and extensibility that may be used by many implementations. Before developing an alternative implementation, please, review extensibility points provided by OpenTelemetry.</p> <p>An example use-case for alternate implementations is automated testing. A mock implementation can be plugged in during automated tests. For example, it can store all generated telemetry data in memory and provide a capability to inspect this stored data. This will allow the tests to verify that the telemetry is generated correctly. OpenTelemetry client authors are encouraged to provide such a mock implementation.</p> <p>Note that mocking is also possible by using SDK and a Mock <code>Exporter</code> without needing to swap out the entire SDK.</p> <p>The mocking approach chosen will depend on the testing goals and at which point exactly it is desirable to intercept the telemetry data path during the test.</p>"},{"location":"docs/specs/otel/library-guidelines/#_7","title":"\u7248\u672c\u6807\u8bc6","text":"<p>API and SDK packages must use semantic version numbering. API package version number and SDK package version number are decoupled and can be different (and they both can be also different from the Specification version number that they implement). API and SDK packages MUST be labeled with their own version number.</p> <p>This decoupling of version numbers allows OpenTelemetry client authors to make API and SDK package releases independently without the need to coordinate and match version numbers with the Specification.</p> <p>Because API and SDK package version numbers are not coupled, every API and SDK package release MUST clearly mention the Specification version number that they implement. In addition, if a particular version of SDK package is only compatible with a specific version of API package, then this compatibility information must be also published by OpenTelemetry client authors. OpenTelemetry client authors MUST include this information in the release notes. For example, the SDK package release notes may say: \"SDK 0.3.4, use with API 0.1.0, implements OpenTelemetry Specification 0.1.0\".</p> <p>TODO: How should third-party library authors who use OpenTelemetry for instrumentation guide their end users to find the correct SDK package?</p>"},{"location":"docs/specs/otel/library-guidelines/#_8","title":"\u6027\u80fd\u548c\u963b\u585e","text":"<p>See the Performance and Blocking specification for guidelines on the performance expectations that API implementations should meet, strategies for meeting these expectations, and a description of how implementations should document their behavior under load.</p>"},{"location":"docs/specs/otel/library-guidelines/#_9","title":"\u5e76\u53d1\u6027\u548c\u7ebf\u7a0b\u5b89\u5168","text":"<p>Please refer to individual API specification for guidelines on what concurrency safeties should API implementations provide and how they should be documented:</p> <ul> <li>Metrics API</li> <li>Metrics SDK</li> <li>Tracing API</li> </ul>"},{"location":"docs/specs/otel/library-layout/","title":"\u9879\u76ee\u5305\u5e03\u5c40","text":"<p>This documentation serves to document the \"look and feel\" of a basic layout for OpenTelemetry projects. This package layout is intentionally generic and it doesn't try to impose a language specific package structure.</p>"},{"location":"docs/specs/otel/library-layout/#api-package","title":"API Package","text":"<p>Here is a proposed generic package structure for OpenTelemetry API package.</p> <p>A typical top-level directory layout:</p> <pre><code>api\n   \u251c\u2500\u2500 context\n   \u2502   \u2514\u2500\u2500 propagation\n   \u251c\u2500\u2500 metrics\n   \u251c\u2500\u2500 trace\n   \u2502   \u2514\u2500\u2500 propagation\n   \u251c\u2500\u2500 baggage\n   \u2502   \u2514\u2500\u2500 propagation\n   \u251c\u2500\u2500 internal\n   \u2514\u2500\u2500 logs\n</code></pre> <p>Use of lowercase, CamelCase or Snake Case (stylized as snake_case) names depends on the language.</p>"},{"location":"docs/specs/otel/library-layout/#apicontext","title":"<code>/api/context</code>","text":"<p>This directory describes the API that provides in-process context propagation.</p>"},{"location":"docs/specs/otel/library-layout/#apimetrics","title":"<code>/api/metrics</code>","text":"<p>This directory describes the Metrics API that can be used to record application metrics.</p>"},{"location":"docs/specs/otel/library-layout/#apibaggage","title":"<code>/api/baggage</code>","text":"<p>This directory describes the Baggage API that can be used to manage context propagation and metric event attributes.</p>"},{"location":"docs/specs/otel/library-layout/#apitrace","title":"<code>/api/trace</code>","text":"<p>The Trace API consist of a few main classes:</p> <ul> <li><code>Tracer</code> is used for all operations. See Tracer   section.</li> <li><code>Span</code> is a mutable object storing information about the current operation   execution. See Span section.</li> </ul>"},{"location":"docs/specs/otel/library-layout/#apiinternal-optional","title":"<code>/api/internal</code> (Optional)","text":"<p>Library components and implementations that shouldn't be exposed to the users. If a language has an idiomatic layout for internal components, please follow the language idiomatic style.</p>"},{"location":"docs/specs/otel/library-layout/#apilogs-in-the-future","title":"<code>/api/logs</code> (In the future)","text":"<p>TODO: logs operations</p>"},{"location":"docs/specs/otel/library-layout/#sdk-package","title":"SDK Package","text":"<p>Here is a proposed generic package structure for OpenTelemetry SDK package.</p> <p>A typical top-level directory layout:</p> <pre><code>sdk\n   \u251c\u2500\u2500 context\n   \u251c\u2500\u2500 metrics\n   \u251c\u2500\u2500 resource\n   \u251c\u2500\u2500 trace\n   \u251c\u2500\u2500 baggage\n   \u251c\u2500\u2500 internal\n   \u2514\u2500\u2500 logs\n</code></pre> <p>Use of lowercase, CamelCase or Snake Case (stylized as snake_case) names depends on the language.</p>"},{"location":"docs/specs/otel/library-layout/#sdkcontext","title":"<code>/sdk/context</code>","text":"<p>This directory describes the SDK implementation for api/context.</p>"},{"location":"docs/specs/otel/library-layout/#sdkmetrics","title":"<code>/sdk/metrics</code>","text":"<p>This directory describes the SDK implementation for api/metrics.</p>"},{"location":"docs/specs/otel/library-layout/#sdkresource","title":"<code>/sdk/resource</code>","text":"<p>The resource directory primarily defines a type Resource that captures information about the entity for which stats or traces are recorded. For example, metrics exposed by a Kubernetes container can be linked to a resource that specifies the cluster, namespace, pod, and container name.</p>"},{"location":"docs/specs/otel/library-layout/#sdkbaggage","title":"<code>/sdk/baggage</code>","text":"<p>TODO</p>"},{"location":"docs/specs/otel/library-layout/#sdktrace","title":"<code>/sdk/trace</code>","text":"<p>This directory describes the Tracing SDK implementation.</p>"},{"location":"docs/specs/otel/library-layout/#sdkinternal-optional","title":"<code>/sdk/internal</code> (Optional)","text":"<p>Library components and implementations that shouldn't be exposed to the users. If a language has an idiomatic layout for internal components, please follow the language idiomatic style.</p>"},{"location":"docs/specs/otel/library-layout/#sdklogs-in-the-future","title":"<code>/sdk/logs</code> (In the future)","text":"<p>TODO: logs operations</p>"},{"location":"docs/specs/otel/overview/","title":"\u6982\u8ff0","text":"<p>\u672c\u6587\u6863\u63d0\u4f9b\u4e86 OpenTelemetry \u9879\u76ee\u7684\u6982\u8ff0\uff0c\u5e76\u5b9a\u4e49\u4e86\u91cd\u8981\u7684\u57fa\u672c\u672f\u8bed\u3002</p> <p>\u5176\u4ed6\u672f\u8bed\u5b9a\u4e49\u53ef\u5728\u8bcd\u6c47\u8868\u4e2d\u627e\u5230.</p>"},{"location":"docs/specs/otel/overview/#opentelemetry","title":"OpenTelemetry \u5ba2\u6237\u7aef\u67b6\u6784","text":"<p>\u5728\u6700\u9ad8\u7684\u4f53\u7cfb\u7ed3\u6784\u7ea7\u522b\uff0cOpenTelemetry \u5ba2\u6237\u7aef\u88ab\u7ec4\u7ec7 \u6210\u4fe1\u53f7\u3002\u6bcf\u4e2a\u4fe1\u53f7\u90fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u7279\u6b8a\u5f62\u5f0f\u7684\u53ef\u89c2\u6d4b\u6027\u3002\u4f8b\u5982 \uff0c\u8ddf\u8e2a\u3001\u5ea6\u91cf\u548c\u5305\u88b1\u662f\u4e09\u4e2a\u72ec\u7acb\u7684\u4fe1\u53f7\u3002\u4fe1\u53f7\u5171\u4eab\u4e00\u4e2a\u5171\u540c\u7684\u5b50\u7cfb\u7edf\u2014\u2014\u4e0a\u4e0b\u6587\u4f20\u64ad\u2014\u2014\u4f46\u5b83\u4eec \u5f7c\u6b64\u72ec\u7acb\u5730\u5de5\u4f5c\u3002</p> <p>\u6bcf\u4e2a\u4fe1\u53f7\u90fd\u4e3a\u8f6f\u4ef6\u63d0\u4f9b\u4e86\u4e00\u79cd\u63cf\u8ff0\u81ea\u8eab\u7684\u673a\u5236\u3002\u4ee3\u7801\u5e93\uff0c\u5982 web \u6846\u67b6\u6216\u6570\u636e\u5e93\u5ba2\u6237\u7aef\uff0c\u4f9d \u8d56\u4e8e\u5404\u79cd\u4fe1\u53f7\u6765\u63cf\u8ff0\u81ea\u5df1\u3002\u7136\u540e\u53ef\u4ee5\u5c06 OpenTelemetry \u68c0\u6d4b\u4ee3\u7801\u6df7\u5408\u5230\u8be5\u4ee3\u7801\u5e93\u4e2d\u7684\u5176\u4ed6 \u4ee3\u7801\u4e2d\u3002\u8fd9\u4f7f\u5f97 OpenTelemetry \u6210\u4e3a\u4e00 \u4e2a\u6a2a\u5207\u5173\u6ce8\u2014\u2014\u4e00\u4e2a\u4e3a\u4e86 \u63d0\u4f9b\u4ef7\u503c\u800c\u6df7\u5408\u5230\u8bb8\u591a\u5176\u4ed6\u8f6f\u4ef6\u4e2d\u7684\u8f6f\u4ef6\u3002\u6a2a\u5207\u5173\u6ce8\u70b9\uff0c\u5c31\u5176\u672c\u8d28\u800c\u8a00\uff0c\u8fdd\u53cd\u4e86\u4e00\u4e2a\u6838\u5fc3\u8bbe \u8ba1\u539f\u5219\u2014\u2014\u5173\u6ce8\u70b9\u5206\u79bb\u3002\u56e0\u6b64\uff0cOpenTelemetry \u5ba2\u6237\u7aef\u8bbe\u8ba1\u9700\u8981\u683c\u5916\u5c0f\u5fc3\uff0c\u4ee5\u907f\u514d\u4e3a\u4f9d\u8d56\u4e8e\u8fd9 \u4e9b\u6a2a\u5207 api \u7684\u4ee3\u7801\u5e93\u521b\u5efa\u95ee\u9898\u3002</p> <p>OpenTelemetry \u5ba2\u6237\u7aef\u88ab\u8bbe\u8ba1\u6210\u5c06\u6bcf\u4e2a\u4fe1\u53f7\u4e2d\u5fc5\u987b\u4f5c\u4e3a\u6a2a\u5207\u5173\u6ce8\u70b9\u5bfc\u5165\u7684\u90e8\u5206\u4e0e\u53ef\u4ee5\u72ec\u7acb\u7ba1 \u7406\u7684\u90e8\u5206\u5206\u5f00\u3002 OpenTelemetry \u5ba2\u6237\u7aef\u4e5f\u88ab\u8bbe\u8ba1\u6210\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807 \uff0c\u6bcf\u4e2a\u4fe1\u53f7\u7531\u56db\u79cd\u7c7b\u578b\u7684\u5305\u7ec4\u6210:API\u3001SDK\u3001Semantic Conventions \u548c Contrib\u3002</p>"},{"location":"docs/specs/otel/overview/#api","title":"API","text":"<p>API \u5305\u7531\u7528\u4e8e\u68c0\u6d4b\u7684\u6a2a\u5207\u516c\u5171\u63a5\u53e3\u7ec4\u6210\u3002 OpenTelemetry \u5ba2\u6237\u7aef\u4e2d\u5bfc\u5165\u7b2c\u4e09\u65b9\u5e93\u548c\u5e94\u7528\u7a0b \u5e8f\u4ee3\u7801\u7684\u4efb\u4f55\u90e8\u5206\u90fd\u88ab\u8ba4\u4e3a\u662f API \u7684\u4e00\u90e8\u5206\u3002</p>"},{"location":"docs/specs/otel/overview/#sdk","title":"SDK","text":"<p>SDK \u662f OpenTelemetry \u9879\u76ee\u63d0\u4f9b\u7684 API \u7684\u5b9e\u73b0\u3002\u5728\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0cSDK \u7531\u5e94\u7528\u7a0b\u5e8f\u6240\u6709\u8005\u5b89\u88c5\u548c\u7ba1\u7406\u3002\u8bf7\u6ce8\u610f\uff0cSDK \u5305\u542b\u989d \u5916\u7684\u516c\u5171\u63a5\u53e3\uff0c\u8fd9\u4e9b\u63a5\u53e3\u4e0d\u88ab\u8ba4\u4e3a\u662f API \u5305\u7684\u4e00\u90e8\u5206\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e0d\u662f\u6a2a\u5207\u5173\u6ce8\u70b9\u3002\u8fd9\u4e9b\u516c \u5171\u63a5\u53e3\u88ab\u5b9a\u4e49 \u4e3a\u6784\u9020\u51fd\u6570\u548c\u63d2\u4ef6\u63a5\u53e3\u3002\u5e94 \u7528\u7a0b\u5e8f\u6240\u6709\u8005\u4f7f\u7528 SDK \u6784\u9020\u51fd\u6570;\u63d2\u4ef6\u4f5c\u8005\u4f7f\u7528 SDK \u63d2\u4ef6 \u63a5\u53e3\u3002 \u5de5\u5177\u4f5c\u8005\u7edd\u5bf9\u4e0d\u80fd\u76f4\u63a5\u5f15\u7528\u4efb\u4f55 SDK \u5305 \uff0c\u53ea\u80fd\u5f15\u7528 API\u3002</p>"},{"location":"docs/specs/otel/overview/#_2","title":"\u8bed\u4e49\u7ea6\u5b9a","text":"<p>\u8bed\u4e49\u7ea6\u5b9a\u5b9a\u4e49\u4e86\u952e\u548c\u503c\uff0c\u8fd9\u4e9b\u952e\u548c\u503c\u63cf\u8ff0\u4e86\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u7684\u5e38\u89c1\u6982\u5ff5\u3001\u534f\u8bae\u548c\u64cd\u4f5c\u3002</p> <p>\u8bed\u4e49\u7ea6\u5b9a\u73b0\u5728\u4f4d\u4e8e\u5b83\u4eec\u81ea\u5df1\u7684\u5b58\u50a8\u5e93\u4e2d: https://github.com/open-telemetry/semantic-conventions</p> <p>\u6536\u96c6\u5668\u548c\u5ba2\u6237\u7aef\u5e93\u90fd\u5e94\u8be5\u81ea\u52a8\u5c06\u8bed\u4e49\u7ea6\u5b9a\u952e\u548c\u679a\u4e3e\u503c\u751f\u6210\u5e38\u91cf(\u6216\u8bed\u8a00\u4e60\u60ef\u7b49\u6548)\u3002\u5728\u8bed\u4e49\u7ea6 \u5b9a\u7a33\u5b9a\u4e4b\u524d\uff0c\u751f\u6210\u7684\u503c\u4e0d\u5e94\u8be5\u5728\u7a33\u5b9a\u7684\u5305\u4e2d\u5206\u53d1\u3002 YAML\u6587 \u4ef6\u5fc5\u987b\u4f5c\u4e3a\u751f\u6210\u7684\u771f\u5b9e\u6e90\u3002\u6bcf\u79cd\u8bed\u8a00\u5b9e\u73b0\u90fd\u5e94\u8be5 \u4e3a\u4ee3\u7801\u751f\u6210\u5668\u63d0 \u4f9b\u7279\u5b9a\u4e8e\u8bed\u8a00\u7684\u652f\u6301\u3002</p> <p>\u6b64\u5916\uff0c\u5c06\u5217\u51fa\u89c4\u8303\u6240\u9700\u7684\u5c5e\u6027\u5728\u8fd9\u91cc</p>"},{"location":"docs/specs/otel/overview/#_3","title":"\u8d21\u732e\u5305","text":"<p>OpenTelemetry \u9879\u76ee\u7ef4\u62a4\u4e0e\u6d41\u884c\u7684 OSS \u9879\u76ee\u7684\u96c6\u6210\uff0c\u8fd9\u4e9b\u9879\u76ee\u5bf9\u4e8e\u89c2\u5bdf\u73b0\u4ee3 web \u670d\u52a1\u975e\u5e38 \u91cd\u8981\u3002 API \u96c6\u6210\u7684\u793a\u4f8b\u5305\u62ec\u7528\u4e8e web \u6846\u67b6\u3001\u6570\u636e\u5e93\u5ba2\u6237\u7aef\u548c\u6d88\u606f\u961f\u5217\u7684\u63d2\u88c5\u3002\u793a\u4f8b SDK \u96c6\u6210\u5305\u62ec\u7528\u4e8e\u5c06\u9065\u6d4b\u6570\u636e\u5bfc\u51fa\u5230\u6d41\u884c\u7684\u5206\u6790\u5de5\u5177\u548c\u9065\u6d4b\u6570\u636e\u5b58\u50a8\u7cfb\u7edf\u7684\u63d2\u4ef6\u3002</p> <p>\u4e00\u4e9b\u63d2\u4ef6\uff0c\u5982 OTLP \u5bfc\u51fa\u5668\u548c TraceContext \u4f20\u64ad\u5668\uff0c\u662f OpenTelemetry \u89c4\u8303\u6240\u8981\u6c42\u7684\u3002 \u8fd9\u4e9b\u5fc5\u9700\u7684\u63d2\u4ef6\u4f5c\u4e3a SDK \u7684\u4e00\u90e8\u5206\u5305\u542b\u3002</p> <p>\u53ef\u9009\u7684\u3001\u72ec\u7acb\u4e8e SDK \u7684\u63d2\u4ef6\u548c\u5de5\u5177\u5305\u88ab\u79f0\u4e3a Contrib \u5305\u3002 API Contrib \u6307\u7684\u662f \u4ec5\u4f9d\u8d56\u4e8e API \u7684\u5305; SDK Contrib \u6307\u7684\u662f\u540c\u6837\u4f9d\u8d56\u4e8e SDK \u7684\u5305\u3002</p> <p><code>Contrib</code> \u4e00\u8bcd\u7279\u6307\u7531 OpenTelemetry \u9879\u76ee\u7ef4\u62a4\u7684\u63d2\u4ef6\u548c\u5de5\u5177\u7684\u96c6\u5408;\u5b83\u4e0d\u6d89\u53ca\u7b2c\u4e09\u65b9\u63d2\u4ef6 \u6258\u7ba1\u5728\u5176\u4ed6\u5730\u65b9\u3002</p>"},{"location":"docs/specs/otel/overview/#_4","title":"\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027","text":"<p>OpenTelemetry \u91cd\u89c6\u7a33\u5b9a\u6027\u548c\u5411\u540e\u517c\u5bb9\u6027\u3002\u8bf7\u53c2 \u9605\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027\u6307\u5357\u4e86\u89e3\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"docs/specs/otel/overview/#_5","title":"\u8ddf\u8e2a\u4fe1\u53f7","text":"<p>\u5206\u5e03\u5f0f\u8ddf\u8e2a\u662f\u4e00\u7ec4\u4e8b\u4ef6\uff0c\u7531\u5355\u4e2a\u903b\u8f91\u64cd\u4f5c\u89e6\u53d1\uff0c\u5e76\u5728\u5e94\u7528\u7a0b\u5e8f\u7684\u5404\u4e2a\u7ec4\u4ef6\u4e4b\u95f4\u8fdb\u884c\u6574\u5408\u3002\u5206 \u5e03\u5f0f\u8ddf\u8e2a\u5305\u542b\u8de8\u8fdb\u7a0b\u3001\u7f51\u7edc\u548c\u5b89\u5168\u8fb9\u754c\u7684\u4e8b\u4ef6\u3002\u5f53\u6709\u4eba\u6309\u4e0b\u6309\u94ae\u4ee5\u542f\u52a8\u7f51\u7ad9\u4e0a\u7684\u64cd\u4f5c\u65f6\uff0c\u53ef \u80fd\u4f1a\u542f\u52a8\u5206\u5e03\u5f0f\u8ddf\u8e2a\u2014\u5728\u672c\u4f8b\u4e2d\uff0c\u8ddf\u8e2a\u5c06\u8868\u793a\u4e0b\u6e38\u670d\u52a1\u4e4b\u95f4\u7684\u8c03\u7528\uff0c\u8fd9\u4e9b\u670d\u52a1\u5904\u7406\u7531\u6309\u4e0b\u6309 \u94ae\u53d1\u8d77\u7684\u8bf7\u6c42\u94fe\u3002</p>"},{"location":"docs/specs/otel/overview/#traces","title":"Traces","text":"<p>OpenTelemetry \u4e2d\u7684 Traces \u7531\u5b83\u4eec\u7684 Spans \u9690\u5f0f\u5b9a\u4e49\u3002\u7279\u522b\u5730\uff0c\u4e00\u4e2a Trace \u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f Spans \u7684\u6709\u5411\u65e0\u73af\u56fe(DAG)\uff0c\u5176\u4e2d Spans \u4e4b\u95f4\u7684\u8fb9\u88ab\u5b9a \u4e49\u4e3a\u7236/\u5b50\u5173\u7cfb\u3002</p> <p>\u4f8b\u5982\uff0c\u4e0b\u9762\u662f\u4e00\u4e2a\u7531 6 \u4e2a Spans \u7ec4\u6210\u7684 Trace \u793a\u4f8b:</p> <pre><code>Causal relationships between Spans in a single Trace\n\n        [Span A]  \u2190\u2190\u2190(the root span)\n            |\n     +------+------+\n     |             |\n [Span B]      [Span C] \u2190\u2190\u2190(Span C is a `child` of Span A)\n     |             |\n [Span D]      +---+-------+\n               |           |\n           [Span E]    [Span F]\n</code></pre> <p>\u6709\u65f6\uff0c\u7528\u65f6\u95f4\u8f74\u6765\u53ef\u89c6\u5316 Traces \u66f4\u5bb9\u6613\uff0c\u5982\u4e0b\u56fe\u6240\u793a:</p> <pre><code>Temporal relationships between Spans in a single Trace\n\n\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013\u2013\u2013\u2013\u2013\u2013\u2013|\u2013&gt; time\n\n [Span A\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n   [Span B\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n      [Span D\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n    [Span C\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]\n         [Span E\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7]        [Span F\u00b7\u00b7]\n</code></pre>"},{"location":"docs/specs/otel/overview/#spans","title":"Spans","text":"<p>\u4e00\u4e2a span \u8868\u793a\u4e8b\u52a1\u4e2d\u7684\u4e00\u4e2a\u64cd\u4f5c\u3002\u6bcf\u4e2a Span \u5c01\u88c5\u4e86\u4ee5\u4e0b\u72b6\u6001:</p> <ul> <li>\u64cd\u4f5c\u540d\u79f0</li> <li>\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4\u6233</li> <li>Attributes: \u952e\u503c\u5bf9\u5217\u8868\u3002</li> <li>\u4e00\u7ec4\u96f6\u4e2a\u6216\u591a\u4e2a**\u4e8b\u4ef6**\uff0c\u6bcf\u4e2a\u4e8b\u4ef6\u672c\u8eab\u662f\u4e00\u4e2a\u5143\u7ec4(\u65f6\u95f4\u6233\uff0c\u540d\u79f0   \uff0cAttributes)\u3002\u540d\u79f0\u5fc5\u987b\u662f\u5b57\u7b26\u4e32\u3002</li> <li>\u7236\u8282\u70b9\u7684 Span \u6807\u8bc6\u7b26\u3002</li> <li>\u94fe\u63a5\u5230\u96f6\u4e2a\u6216\u591a\u4e2a\u56e0\u679c\u76f8\u5173\u7684 Spans(\u901a\u8fc7\u8fd9\u4e9b\u76f8\u5173   Spans**\u7684**SpanContext)\u3002</li> <li>\u5f15\u7528 Span \u6240\u9700\u7684 SpanContext \u4fe1\u606f\u3002\u89c1\u4e0b\u6587\u3002</li> </ul>"},{"location":"docs/specs/otel/overview/#spancontext","title":"SpanContext","text":"<p>\u8868\u793a\u5728 Trace \u4e2d\u6807\u8bc6 Span \u7684\u6240\u6709\u4fe1\u606f\uff0c\u5e76\u4e14\u5fc5\u987b\u4f20\u64ad\u5230\u5b50 Spans \u548c\u8de8\u8fdb\u7a0b\u8fb9\u754c \u3002 SpanContext \u5305\u542b\u8ddf\u8e2a\u6807\u8bc6\u7b26\u548c\u4ece\u7236 Spans \u4f20\u64ad\u5230\u5b50 Spans \u7684\u9009\u9879\u3002</p> <ul> <li>TraceId \u662f\u8ddf\u8e2a\u7684\u6807\u8bc6\u7b26\u3002\u5b83\u662f\u4e16\u754c\u8303\u56f4\u5185\u72ec\u4e00\u65e0\u4e8c\u7684\uff0c\u51e0\u4e4e\u6709\u8db3\u591f\u7684\u6982\u7387\u7531 16 \u4e2a   \u968f\u673a\u751f\u6210\u7684\u5b57\u8282\u7ec4\u6210\u3002TraceId \u7528\u4e8e\u5728\u6240\u6709\u8fdb\u7a0b\u4e2d\u5c06\u7279\u5b9a\u8ddf\u8e2a\u7684\u6240\u6709\u8303\u56f4\u7ec4\u5408\u5728\u4e00\u8d77\u3002</li> <li>SpanId \u662f span \u7684\u6807\u8bc6\u7b26\u3002\u5b83\u662f\u5168\u5c40\u552f\u4e00\u7684\uff0c\u5b9e\u9645\u4e0a\u6709\u8db3\u591f\u7684\u6982\u7387\u7531 8 \u4e2a\u968f\u673a\u751f\u6210   \u7684\u5b57\u8282\u7ec4\u6210\u3002\u5f53\u4f20\u9012\u7ed9\u5b50 Span \u65f6\uff0c\u6b64\u6807\u8bc6\u7b26\u5c06\u6210\u4e3a\u5b50 Span \u7684\u7236 span id\u3002</li> <li>TraceFlags \u8868\u793a\u8ddf\u8e2a\u7684\u9009\u9879\u3002\u5b83\u88ab\u8868\u793a\u4e3a 1 \u5b57\u8282(\u4f4d\u56fe)\u3002</li> <li>\u91c7\u6837\u4f4d-\u8868\u793a\u8ddf\u8e2a\u662f\u5426\u91c7\u6837\u7684\u4f4d(\u63a9\u7801 <code>0x1</code>)\u3002</li> <li>Tracestate \u5728\u952e\u503c\u5bf9\u5217\u8868\u4e2d\u643a\u5e26\u8ddf\u8e2a\u7cfb\u7edf\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u3002Tracestate \u5141\u8bb8\u4e0d\u540c   \u7684\u4f9b\u5e94\u5546\u4f20\u64ad\u989d\u5916\u7684\u4fe1\u606f\u5e76\u4e0e\u4ed6\u4eec\u7684\u9057\u7559 Id \u683c\u5f0f\u8fdb\u884c\u4e92\u64cd\u4f5c\u3002\u6b32\u4e86\u89e3\u66f4\u591a\u8be6\u60c5\uff0c\u8bf7\u53c2   \u9605\u672c.</li> </ul>"},{"location":"docs/specs/otel/overview/#spans_1","title":"Spans \u95f4\u94fe\u63a5","text":"<p>\u4e00\u4e2a Span \u53ef\u4ee5\u94fe\u63a5\u5230\u96f6\u4e2a\u6216\u591a\u4e2a\u56e0\u679c\u76f8\u5173\u7684\u5176\u4ed6 Span (\u7531 SpanContext \u5b9a \u4e49)\u3002 \u94fe\u63a5 \u53ef\u4ee5\u6307\u5411 span \u5185\u7684\u5355\u4e2a Trace \u6216\u8de8\u4e0d\u540c\u7684 Trace \u3002 \u94fe \u63a5 \u53ef\u7528\u4e8e\u8868\u793a\u6279\u5904\u7406\u64cd\u4f5c\uff0c\u5176\u4e2d\u4e00\u4e2a Span \u7531\u591a\u4e2a\u521d\u59cb\u5316 Span \u53d1\u8d77\uff0c\u6bcf\u4e2a Span \u8868\u793a\u5728\u6279\u5904\u7406\u4e2d\u6b63\u5728\u5904\u7406\u7684\u5355\u4e2a\u4f20\u5165\u9879\u3002</p> <p>\u4f7f\u7528 Link \u7684\u53e6\u4e00\u4e2a\u4f8b\u5b50\u662f\u58f0\u660e\u8d77\u59cb\u8ddf\u8e2a\u548c\u540e\u7eed\u8ddf\u8e2a\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u5f53 Trace \u8fdb\u5165\u670d\u52a1 \u7684\u53ef\u4fe1\u8fb9\u754c\u5e76\u4e14\u670d\u52a1\u7b56\u7565\u9700\u8981\u751f\u6210\u65b0\u7684 Trace \u800c\u4e0d\u662f\u4fe1\u4efb\u4f20\u5165\u7684 Trace \u4e0a\u4e0b\u6587\u65f6\uff0c\u53ef\u4ee5\u4f7f \u7528\u6b64\u65b9\u6cd5\u3002\u65b0\u7684\u94fe\u63a5 Trace \u8fd8\u53ef\u4ee5\u8868\u793a\u7531\u4f17\u591a\u5feb\u901f\u4f20\u5165\u8bf7\u6c42\u4e4b\u4e00\u53d1\u8d77\u7684\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u5f02\u6b65 \u6570\u636e\u5904\u7406\u64cd\u4f5c\u3002</p> <p>\u5f53\u4f7f\u7528\u5206\u6563/\u6536\u96c6(\u4e5f\u79f0\u4e3a fork/join)\u6a21\u5f0f\u65f6\uff0c\u6839\u64cd\u4f5c\u542f\u52a8\u591a\u4e2a\u4e0b\u6e38\u5904\u7406\u64cd\u4f5c\uff0c\u5e76\u4e14\u6240\u6709\u8fd9 \u4e9b\u64cd\u4f5c\u90fd\u805a\u5408\u56de\u5355\u4e2a Span \u4e2d\u3002\u6700\u540e\u4e00\u4e2a Span \u94fe\u63a5\u5230\u5b83\u805a\u5408\u7684\u8bb8\u591a\u64cd\u4f5c\u3002\u5b83\u4eec\u90fd\u662f\u6765 \u81ea\u540c\u4e00\u4e2a Trace \u7684 span \u3002\u7c7b\u4f3c\u4e8e Span \u7684 Parent \u5b57\u6bb5\u3002\u4f46\u662f\uff0c\u5efa\u8bae\u4e0d\u8981\u5728\u8fd9 \u4e2a\u573a\u666f\u4e2d\u8bbe\u7f6e Span \u7684 parent\uff0c\u56e0\u4e3a\u5728\u8bed\u4e49\u4e0a\uff0cparent \u5b57\u6bb5\u4ee3\u8868\u4e00\u4e2a\u5355\u4e00\u7684\u7236\u573a\u666f\uff0c \u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u7236 Span \u5b8c\u5168\u5305\u542b\u5b50 Span \u3002\u5728\u5206\u6563/\u6536\u96c6\u548c\u6279\u5904\u7406\u573a\u666f\u4e2d\uff0c\u60c5\u51b5 \u5e76\u975e\u5982\u6b64\u3002</p>"},{"location":"docs/specs/otel/overview/#_6","title":"\u5ea6\u91cf\u4fe1\u53f7","text":"<p>OpenTelemetry \u5141\u8bb8\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u805a\u5408\u548c\u4e00\u7ec4\u5c5e\u6027\u8bb0\u5f55 \u539f\u59cb\u6d4b\u91cf\u6216\u5ea6\u91cf.</p> <p>\u4f7f\u7528 OpenTelemetry API \u8bb0\u5f55\u539f\u59cb\u6d4b\u91cf\uff0c\u5141\u8bb8\u6700\u7ec8\u7528\u6237\u51b3\u5b9a\u5e94\u8be5\u4e3a\u8be5\u5ea6\u91cf\u5e94\u7528\u54ea\u79cd\u805a\u5408\u7b97 \u6cd5\u4ee5\u53ca\u5b9a\u4e49\u5c5e\u6027(\u7ef4\u5ea6)\u3002\u5b83\u5c06\u5728 gRPC \u7b49\u5ba2\u6237\u7aef\u5e93\u4e2d\u7528\u4e8e\u8bb0\u5f55\u201cserver_latency\u201d\u6216 \u201creceived_bytes\u201d\u7684\u539f\u59cb\u6d4b\u91cf\u503c\u3002\u56e0\u6b64\uff0c\u6700\u7ec8\u7528\u6237\u5c06\u51b3\u5b9a\u4ece\u8fd9\u4e9b\u539f\u59cb\u6d4b\u91cf\u4e2d\u6536\u96c6\u54ea\u79cd\u7c7b\u578b\u7684 \u805a\u5408\u503c\u3002\u5b83\u53ef\u4ee5\u662f\u7b80\u5355\u7684\u5e73\u5747\u6216\u8be6\u7ec6\u7684\u76f4\u65b9\u56fe\u8ba1\u7b97\u3002</p> <p>\u4f7f\u7528 OpenTelemetry API \u7528\u9884\u5b9a\u4e49\u7684\u805a\u5408\u8bb0\u5f55\u5ea6\u91cf\u4e5f\u540c\u6837\u91cd\u8981\u3002\u5b83\u5141\u8bb8\u6536\u96c6 cpu \u548c\u5185\u5b58\u4f7f \u7528\u7b49\u503c\uff0c\u6216\u8005\u50cf\u201c\u961f\u5217\u957f\u5ea6\u201d\u8fd9\u6837\u7684\u7b80\u5355\u6307\u6807\u3002</p>"},{"location":"docs/specs/otel/overview/#_7","title":"\u8bb0\u5f55\u539f\u59cb\u6d4b\u91cf\u503c","text":"<p>\u7528\u4e8e\u8bb0\u5f55\u539f\u59cb\u6d4b\u91cf\u7684\u4e3b\u8981\u7c7b\u662f\u201cMeasure\u201d\u548c\u201cMeasurement\u201d\u3002\u53ef\u4ee5\u4f7f\u7528 OpenTelemetry API \u8bb0\u5f55\u201c\u6d4b\u91cf\u201d\u5217\u8868\u4ee5\u53ca\u9644\u52a0\u4e0a\u4e0b\u6587\u3002\u56e0\u6b64\uff0c\u7528\u6237\u53ef\u4ee5\u5b9a\u4e49\u6c47\u603b\u8fd9\u4e9b\u201c\u5ea6\u91cf\u201d\uff0c\u5e76\u4f7f\u7528\u4f20\u9012\u7684\u4e0a\u4e0b \u6587\u6765\u5b9a\u4e49\u7ed3\u679c\u5ea6\u91cf\u7684\u9644\u52a0\u5c5e\u6027\u3002</p>"},{"location":"docs/specs/otel/overview/#measure","title":"Measure","text":"<p>\u201cMeasure\u201d\u63cf\u8ff0\u4e86\u5e93\u8bb0\u5f55\u7684\u5355\u4e2a\u503c\u7684\u7c7b\u578b\u3002\u5b83\u5b9a\u4e49\u4e86\u516c\u5f00\u5ea6\u91cf\u7684\u5e93\u548c\u5e94\u7528\u7a0b\u5e8f\u4e4b\u95f4\u7684\u5951\u7ea6\uff0c \u5e94\u7528\u7a0b\u5e8f\u5c06\u8fd9\u4e9b\u5355\u72ec\u7684\u5ea6\u91cf\u805a\u5408\u5230\u4e00\u4e2a\u201cMeasure\u201d\u4e2d\u3002 \u201cMeasure\u201d\u7531\u540d\u79f0\u3001\u63cf\u8ff0\u548c\u4e00\u4e2a\u503c\u5355 \u4f4d\u6765\u6807\u8bc6\u3002</p>"},{"location":"docs/specs/otel/overview/#measurement","title":"Measurement","text":"<p>\u201cMeasurement\u201d\u63cf\u8ff0\u8981\u4e3a\u201cMeasure\u201d\u6536\u96c6\u7684\u5355\u4e2a\u503c\u3002 \u201cMeasurement\u201d\u662f API \u754c\u9762\u4e2d\u7684\u4e00\u4e2a\u7a7a \u63a5\u53e3\u3002\u8be5\u63a5\u53e3\u5728 SDK \u4e2d\u5b9a\u4e49\u3002</p>"},{"location":"docs/specs/otel/overview/#_8","title":"\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u805a\u5408\u8bb0\u5f55\u5ea6\u91cf\u6807\u51c6","text":"<p>\u6240\u6709\u7c7b\u578b\u7684\u9884\u805a\u5408\u6307\u6807\u7684\u57fa\u7c7b\u79f0\u4e3a\u201cMetric\u201d\u3002\u5b83\u5b9a\u4e49\u4e86\u57fa\u672c\u7684\u5ea6\u91cf\u5c5e\u6027\uff0c\u5982\u540d\u79f0\u548c\u5c5e\u6027\u3002\u4ece \u201cMetric\u201d\u7ee7\u627f\u7684\u7c7b\u5b9a\u4e49\u4e86\u5b83\u4eec\u7684\u805a\u5408\u7c7b\u578b\u4ee5\u53ca\u5355\u4e2a\u5ea6\u91cf\u6216\u70b9\u7684\u7ed3\u6784\u3002 API \u5b9a\u4e49\u4e86\u4ee5\u4e0b\u7c7b\u578b \u7684\u9884\u805a\u5408\u6307\u6807:</p> <ul> <li>\u8ba1\u6570\u5668\u62a5\u544a\u77ac\u65f6\u6d4b\u91cf\u3002\u8ba1\u6570\u5668\u503c\u53ef\u4ee5\u4e0a\u5347\u6216\u4fdd\u6301\u4e0d\u53d8\uff0c\u4f46\u6c38\u8fdc\u4e0d\u4f1a\u4e0b\u964d\u3002\u8ba1\u6570\u5668\u503c\u4e0d\u80fd\u4e3a   \u8d1f\u503c\u3002\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u53cd\u5ea6\u91cf\u503c\u2014\u2014<code>double</code> \u548c <code>long</code>\u3002</li> <li>\u6d4b\u91cf\u516c\u5236\u62a5\u544a\u6570\u503c\u7684\u77ac\u65f6\u6d4b\u91cf\u503c\u3002\u4eea\u8868\u53ef\u4ee5\u4e0a\u4e0b\u79fb\u52a8\u3002\u4eea\u8868\u503c\u53ef\u4ee5\u662f\u8d1f\u7684\u3002\u6709\u4e24\u79cd\u7c7b\u578b\u7684   \u6d4b\u91cf\u516c\u5236\u503c-<code>double</code> \u548c <code>long</code>\u3002</li> </ul> <p>API \u5141\u8bb8\u6784\u9020\u6240\u9009\u7c7b\u578b\u7684<code>Metric</code>\u3002 SDK \u5b9a\u4e49\u4e86\u67e5\u8be2\u8981\u5bfc\u51fa\u7684<code>Metric</code>\u5f53\u524d\u503c\u7684\u65b9\u5f0f\u3002</p> <p>\u6bcf\u79cd\u7c7b\u578b\u7684<code>Metric</code>\u90fd\u6709\u5b83\u7684 API \u6765\u8bb0\u5f55\u8981\u805a\u5408\u7684\u503c\u3002 API \u652f\u6301\u63a8\u62c9\u4e24\u79cd\u6a21\u5f0f\u8bbe \u7f6e<code>Metric</code>\u503c\u3002</p>"},{"location":"docs/specs/otel/overview/#sdk_1","title":"\u5ea6\u91cf\u6570\u636e\u6a21\u578b\u548c SDK","text":"<p>Metrics \u6570\u636e\u6a21\u578b\u5728\u8fd9\u91cc\u6307\u5b9a\uff0c\u5e76\u57fa \u4e8emetrics.proto\u3002 \u8be5\u6570\u636e\u6a21\u578b\u5b9a\u4e49\u4e86\u4e09\u79cd\u8bed\u4e49:API \u4f7f\u7528\u7684 Event \u6a21\u578b\u3001SDK \u548c OTLP \u4f7f\u7528\u7684\u52a8\u6001\u6570\u636e\u6a21\u578b\uff0c \u4ee5\u53ca\u8868\u793a\u5bfc\u51fa\u5668\u5e94\u5982\u4f55\u89e3\u91ca\u52a8\u6001\u6a21\u578b\u7684 TimeSeries \u6a21\u578b\u3002</p> <p>\u4e0d\u540c\u7684\u5bfc\u51fa\u5668\u6709\u4e0d\u540c\u7684\u529f\u80fd(\u4f8b\u5982\u652f\u6301\u54ea\u4e9b\u6570\u636e\u7c7b\u578b)\u548c\u4e0d\u540c\u7684\u7ea6\u675f(\u4f8b\u5982\u5c5e\u6027\u952e\u4e2d\u5141\u8bb8\u54ea\u4e9b \u5b57\u7b26)\u3002\u6307\u6807\u65e8\u5728\u6210\u4e3a\u53ef\u80fd\u6027\u7684\u8d85\u96c6\uff0c\u800c\u4e0d\u662f\u4efb\u4f55\u5730\u65b9\u90fd\u652f\u6301\u7684\u6700\u4f4e\u516c\u5206\u6bcd\u3002\u6240\u6709\u5bfc\u51fa\u5668\u90fd \u901a\u8fc7 OpenTelemetry SDK \u4e2d\u5b9a\u4e49\u7684 Metric Producer \u63a5\u53e3\u4ece Metrics data Model \u4e2d\u4f7f\u7528 \u6570\u636e\u3002</p> <p>\u56e0\u6b64\uff0cMetrics \u5bf9\u6570\u636e\u7684\u9650\u5236\u6700\u5c0f(\u4f8b\u5982\uff0c\u952e\u4e2d\u5141\u8bb8\u54ea\u4e9b\u5b57\u7b26)\uff0c\u5904\u7406 Metrics \u7684\u4ee3\u7801\u5e94\u8be5 \u907f\u514d\u9a8c\u8bc1\u548c\u6e05\u7406 Metrics \u6570\u636e\u3002\u76f8\u53cd\uff0c\u5c06\u6570\u636e\u4f20\u9012\u7ed9\u540e\u7aef\uff0c\u4f9d\u9760\u540e\u7aef\u6267\u884c\u9a8c\u8bc1\uff0c\u5e76\u4ece\u540e\u7aef \u4f20\u56de\u4efb\u4f55\u9519\u8bef\u3002</p> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605Metrics \u6570\u636e\u6a21\u578b\u89c4\u8303\u3002</p>"},{"location":"docs/specs/otel/overview/#_9","title":"\u65e5\u5fd7\u4fe1\u53f7","text":""},{"location":"docs/specs/otel/overview/#_10","title":"\u6570\u636e\u6a21\u578b","text":"<p>\u65e5\u5fd7\u6570\u636e\u6a21\u578b\u5b9a\u4e49\u4e86 OpenTelemetry \u5982\u4f55\u7406\u89e3\u65e5\u5fd7\u548c\u4e8b\u4ef6\u3002</p>"},{"location":"docs/specs/otel/overview/#_11","title":"\u5305\u88b1\u4fe1\u53f7","text":"<p>\u9664\u4e86\u8ddf\u8e2a\u4f20\u64ad\u4e4b\u5916\uff0cOpenTelemetry \u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u673a\u5236\u6765\u4f20\u64ad\u540d\u79f0/\u503c\u5bf9\uff0c\u79f0\u4e3a\u201c\u5305\u88b1 \u201d\u3002 \u201c\u5305\u88b1\u201d\u7528\u4e8e\u7d22\u5f15\u540c\u4e00\u4e8b\u52a1\u4e2d\u5177\u6709\u5148\u524d\u670d\u52a1\u63d0\u4f9b\u7684\u5c5e\u6027\u7684\u670d\u52a1\u4e2d\u7684\u53ef\u89c2\u5bdf\u6027\u4e8b\u4ef6\u3002\u8fd9\u6709\u52a9 \u4e8e\u5728\u8fd9\u4e9b\u4e8b\u4ef6\u4e4b\u95f4\u5efa\u7acb\u56e0\u679c\u5173\u7cfb\u3002</p> <p>\u867d\u7136\u201c\u5305\u88b1\u201d\u53ef\u4ee5\u7528\u4e8e\u5176\u4ed6\u6a2a\u5207\u5173\u6ce8\u70b9\u7684\u539f\u578b\uff0c\u4f46\u8be5\u673a\u5236\u4e3b\u8981\u662f\u4e3a\u4e86\u4f20\u8fbe OpenTelemetry \u53ef \u89c2\u5bdf\u6027\u7cfb\u7edf\u7684\u503c\u3002</p> <p>\u8fd9\u4e9b\u503c\u53ef\u4ee5\u4ece\u201c\u5305\u88b1\u201d\u4e2d\u4f7f\u7528\uff0c\u5e76\u7528\u4f5c\u5ea6\u91cf\u7684\u9644\u52a0\u5c5e\u6027\uff0c\u6216\u8005\u7528\u4e8e\u65e5\u5fd7\u548c\u8ddf\u8e2a\u7684\u9644\u52a0\u4e0a\u4e0b\u6587\u3002 \u4e00\u4e9b\u4f8b\u5b50:</p> <ul> <li>web \u670d\u52a1\u53ef\u4ee5\u4ece\u5305\u542b\u6709\u5173\u53d1\u9001\u8bf7\u6c42\u7684\u670d\u52a1\u7684\u4e0a\u4e0b\u6587\u4e2d\u83b7\u76ca</li> <li>SaaS \u63d0\u4f9b\u7a0b\u5e8f\u53ef\u4ee5\u5305\u542b\u6709\u5173\u8d1f\u8d23\u8be5\u8bf7\u6c42\u7684 API \u7528\u6237\u6216\u4ee4\u724c\u7684\u4e0a\u4e0b\u6587</li> <li>\u786e\u5b9a\u7279\u5b9a\u7684\u6d4f\u89c8\u5668\u7248\u672c\u4e0e\u56fe\u50cf\u5904\u7406\u670d\u52a1\u4e2d\u7684\u6545\u969c\u76f8\u5173\u8054</li> </ul> <p>\u4e3a\u4e86\u4e0e OpenTracing \u5411\u540e\u517c\u5bb9\uff0c\u5728\u4f7f\u7528 OpenTracing \u6865\u65f6\uff0c\u5305\u88b1\u88ab\u4f20\u64ad\u4e3a\u201c\u5305\u88b1\u201d\u3002\u4f7f\u7528\u4e0d \u540c\u6807\u51c6\u7684\u65b0\u5173\u6ce8\u70b9\u5e94\u8be5\u8003\u8651\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u6a2a\u5207\u5173\u6ce8\u70b9\u6765\u8986\u76d6\u5b83\u4eec\u7684\u7528\u4f8b; \u5b83\u4eec\u53ef\u80fd\u53d7\u76ca\u4e8e W3C \u7f16\u7801\u683c\u5f0f\uff0c\u4f46\u4f7f\u7528\u65b0\u7684 HTTP \u5934\u5728\u6574\u4e2a\u5206\u5e03\u5f0f\u8ddf\u8e2a\u4e2d\u4f20\u8f93\u6570\u636e\u3002</p>"},{"location":"docs/specs/otel/overview/#_12","title":"\u8d44\u6e90","text":"<p>\u201c\u8d44\u6e90\u201d\u6355\u83b7\u6709\u5173\u9065\u6d4b\u8bb0\u5f55\u7684\u5b9e\u4f53\u7684\u4fe1\u606f\u3002\u4f8b\u5982\uff0cKubernetes \u5bb9\u5668\u516c\u5f00\u7684\u6307\u6807\u53ef\u4ee5\u94fe\u63a5\u5230\u6307 \u5b9a\u96c6\u7fa4\u3001\u547d\u540d\u7a7a\u95f4\u3001pod \u548c\u5bb9\u5668\u540d\u79f0\u7684\u8d44\u6e90\u3002</p> <p>\u201c\u8d44\u6e90\u201d\u53ef\u4ee5\u6355\u83b7\u5b9e\u4f53\u6807\u8bc6\u7684\u6574\u4e2a\u5c42\u6b21\u7ed3\u6784\u3002\u5b83\u53ef\u4ee5\u63cf\u8ff0\u4e91\u4e2d\u7684\u4e3b\u673a\u548c\u7279\u5b9a\u5bb9\u5668\u6216\u8fdb\u7a0b\u4e2d\u8fd0\u884c \u7684\u5e94\u7528\u7a0b\u5e8f\u3002</p> <p>\u6ce8\u610f\uff0c\u4e00\u4e9b\u8fdb\u7a0b\u8bc6\u522b\u4fe1\u606f\u53ef\u4ee5\u901a\u8fc7 OpenTelemetry SDK \u81ea\u52a8\u4e0e\u9065\u6d4b\u76f8\u5173\u8054\u3002</p>"},{"location":"docs/specs/otel/overview/#_13","title":"\u4e0a\u4e0b\u6587\u4f20\u64ad","text":"<p>OpenTelemetry \u7684\u6240\u6709\u6a2a\u5207\u5173\u6ce8\u70b9\uff0c\u5982\u8ddf\u8e2a\u548c\u5ea6\u91cf\uff0c\u90fd\u5171\u4eab\u4e00\u4e2a\u5e95\u5c42\u7684\u201c\u4e0a\u4e0b\u6587\u201d\u673a\u5236\uff0c\u7528\u4e8e \u5728\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u5b58\u50a8\u72b6\u6001\u548c\u8bbf\u95ee\u6570\u636e\u3002</p> <p>\u53c2\u89c1\u80cc\u666f\u4fe1\u606f</p>"},{"location":"docs/specs/otel/overview/#_14","title":"\u4f20\u64ad\u5668","text":"<p>OpenTelemetry \u4f7f\u7528<code>Propagators</code>\u6765\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316\u6a2a\u5207\u5173\u6ce8\u70b9\u503c\uff0c\u6bd4\u5982<code>Span</code>(\u901a\u5e38\u53ea \u6709<code>SpanContext</code>\u90e8\u5206)\u548c<code>Baggage</code>\u3002\u4e0d\u540c\u7684\u201c\u4f20\u64ad\u5668\u201d\u7c7b\u578b\u5b9a\u4e49\u4e86\u7279\u5b9a\u4f20\u8f93\u6240\u65bd\u52a0\u7684\u9650\u5236\uff0c \u5e76\u7ed1\u5b9a\u5230\u6570\u636e\u7c7b\u578b\u3002</p> <p>\u4f20\u64ad\u5668 API \u76ee\u524d\u5b9a\u4e49\u4e86\u4e00\u79cd\u201c\u4f20\u64ad\u5668\u201d\u7c7b\u578b:</p> <ul> <li><code>TextMapPropagator</code> \u5411\u8f7d\u6ce2\u4e2d\u6ce8\u5165\u503c\u5e76\u4ece\u8f7d\u6ce2\u4e2d\u63d0\u53d6\u503c\u4f5c\u4e3a\u6587\u672c\u3002</li> </ul>"},{"location":"docs/specs/otel/overview/#_15","title":"\u6536\u96c6\u5668","text":"<p>OpenTelemetry \u6536\u96c6\u5668\u662f\u4e00\u7ec4\u7ec4\u4ef6\uff0c\u53ef\u4ee5\u4ece OpenTelemetry \u6216\u5176\u4ed6\u76d1\u63a7/\u8ddf\u8e2a\u5e93(Jaeger, Prometheus \u7b49)\u6d4b\u91cf\u7684\u8fdb\u7a0b\u4e2d\u6536\u96c6\u8ddf\u8e2a\u3001\u6307\u6807\u548c\u6700\u7ec8\u7684\u5176\u4ed6\u9065\u6d4b\u6570\u636e(\u4f8b\u5982\u65e5\u5fd7)\uff0c\u8fdb\u884c\u805a\u5408 \u548c\u667a\u80fd\u91c7\u6837\uff0c\u5e76\u5c06\u8ddf\u8e2a\u548c\u6307\u6807\u5bfc\u51fa\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u76d1\u63a7/\u8ddf\u8e2a\u540e\u7aef\u3002\u6536\u96c6\u5668\u5c06\u5141\u8bb8\u4e30\u5bcc\u548c\u8f6c\u6362 \u6536\u96c6\u5230\u7684\u9065\u6d4b\u6570\u636e(\u4f8b\u5982\u6dfb\u52a0\u989d\u5916\u7684\u5c5e\u6027\u6216\u5220\u9664\u4e2a\u4eba\u4fe1\u606f)\u3002</p> <p>OpenTelemetry \u6536\u96c6\u5668\u6709\u4e24\u79cd\u4e3b\u8981\u7684\u64cd\u4f5c\u6a21\u5f0f: Agent(\u4e0e\u5e94\u7528\u7a0b\u5e8f\u4e00\u8d77\u5728\u672c\u5730\u8fd0\u884c\u7684\u5b88\u62a4\u8fdb \u7a0b)\u548c Collector(\u72ec\u7acb\u8fd0\u884c\u7684\u670d\u52a1)\u3002</p> <p>\u5728 OpenTelemetry Service \u9605\u8bfb\u66f4\u591a\u5185 \u5bb9\u957f\u671f\u613f\u666f.</p>"},{"location":"docs/specs/otel/overview/#_16","title":"\u63d2\u88c5\u5e93","text":"<p>\u53c2\u89c1\u63d2\u88c5\u5e93</p> <p>\u8be5\u9879\u76ee\u7684\u7075\u611f\u662f\u901a\u8fc7\u8ba9\u6bcf\u4e2a\u5e93\u548c\u5e94\u7528\u7a0b\u5e8f\u76f4\u63a5\u8c03\u7528 OpenTelemetry API\uff0c\u4f7f\u5b83\u4eec\u6210\u4e3a\u5f00\u7bb1\u5373 \u7528\u7684\u53ef\u89c2\u5bdf\u5bf9\u8c61\u3002\u7136\u800c\uff0c\u8bb8\u591a\u5e93\u6ca1\u6709\u8fd9\u6837\u7684\u96c6\u6210\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u5355\u72ec\u7684\u5e93\u6765\u6ce8\u5165\u8fd9\u6837\u7684\u8c03\u7528 \uff0c\u4f7f\u7528\u8bf8\u5982\u5305\u88c5\u63a5\u53e3\u3001\u8ba2\u9605\u7279\u5b9a\u4e8e\u5e93\u7684\u56de\u8c03\u6216\u5c06\u73b0\u6709\u7684\u9065\u6d4b\u8f6c\u6362\u4e3a OpenTelemetry \u6a21\u578b\u7b49 \u673a\u5236\u3002</p> <p>\u4f7f OpenTelemetry \u5bf9\u53e6\u4e00\u4e2a\u5e93\u5177\u6709\u53ef\u89c2\u5bdf\u6027\u7684\u5e93\u79f0 \u4e3a\u63d2\u88c5\u5e93.</p> <p>\u63d2\u88c5\u5e93\u7684\u547d\u540d\u5e94\u8be5\u9075\u5faa\u63d2\u88c5\u5e93\u7684\u4efb\u4f55\u547d\u540d\u7ea6\u5b9a(\u4f8b\u5982: 'middleware'\u7528\u4e8e web \u6846\u67b6)\u3002</p> <p>\u5982\u679c\u6ca1\u6709\u786e\u5b9a\u7684\u540d\u79f0\uff0c\u5efa\u8bae\u4f7f\u7528\"opentelemetry-instrumentation\"\u4f5c\u4e3a\u5305\u7684\u524d\u7f00\uff0c\u540e\u9762\u8ddf \u7740\u88ab\u68c0\u6d4b\u7684\u5e93\u540d\u79f0\u672c\u8eab\u3002\u4f8b\u5b50\u5305\u62ec:</p> <ul> <li>opentelemetry-instrumentation-flask (Python)</li> <li>opentelemetry/instrumentation-grpc (Javascript)</li> </ul>"},{"location":"docs/specs/otel/performance-benchmark/","title":"API \u6027\u80fd\u57fa\u51c6","text":"<p>This document describes common performance benchmark guidelines on how to measure and report the performance of OpenTelemetry SDKs.</p> <p>The goal of this benchmark is to provide a tool to get the basic performance overhead of the OpenTelemetry SDK for given events throughput on the target platform.</p>"},{"location":"docs/specs/otel/performance-benchmark/#benchmark-configuration","title":"Benchmark Configuration","text":""},{"location":"docs/specs/otel/performance-benchmark/#span-configuration","title":"Span Configuration","text":"<ul> <li>No parent <code>Span</code> or parent <code>SpanContext</code>.</li> <li>Default Span Kind and   Status.</li> <li>Associated to a resource with attributes   <code>service.name</code>, <code>service.version</code> and 10 characters string value for each   attribute, and attribute <code>service.instance.id</code> with a unique UUID. See   Service for details.</li> <li>1 attribute with a signed 64-bit integer   value.</li> <li>1 event without any attributes.</li> <li>The <code>AlwaysOn</code> sampler should be enabled.</li> <li>Each <code>Span</code> is created and immediately ended.</li> </ul>"},{"location":"docs/specs/otel/performance-benchmark/#measurement-configuration","title":"Measurement Configuration","text":"<p>For the languages with bootstrap cost like JIT compilation, a warm-up phase is recommended to take place before the measurement, which runs under the same <code>Span</code> configuration.</p>"},{"location":"docs/specs/otel/performance-benchmark/#throughput-measurement","title":"Throughput Measurement","text":""},{"location":"docs/specs/otel/performance-benchmark/#create-spans","title":"Create Spans","text":"<p>Number of spans which could be created and exported via OTLP exporter in 1 second per logical core and average number over all logical cores, with each span containing 10 attributes, and each attribute containing two 20 characters strings, one as attribute name the other as value.</p>"},{"location":"docs/specs/otel/performance-benchmark/#instrumentation-cost","title":"Instrumentation Cost","text":""},{"location":"docs/specs/otel/performance-benchmark/#cpu-usage-measurement","title":"CPU Usage Measurement","text":"<p>With given number of span throughput specified by user, or 10,000 spans per second as default if user does not input the number, measure and report the CPU usage for SDK with both default configured simple and batching span processors together with OTLP exporter. The benchmark should create an out-of-process OTLP receiver which listens on the exporting target or adopts existing OTLP exporter which runs out-of-process, responds with success status immediately and drops the data. The collector should not add significant CPU overhead to the measurement. Because the benchmark does not include user processing logic, the total CPU consumption of benchmark program could be considered as approximation of SDK's CPU consumption.</p> <p>The total running time for one test iteration is suggested to be at least 15 seconds. The average and peak CPU usage should be reported.</p>"},{"location":"docs/specs/otel/performance-benchmark/#memory-usage-measurement","title":"Memory Usage Measurement","text":"<p>Measure dynamic memory consumption, e.g. heap, for the same scenario as above CPU Usage section with 15 seconds duration.</p>"},{"location":"docs/specs/otel/performance-benchmark/#report","title":"Report","text":""},{"location":"docs/specs/otel/performance-benchmark/#report-format","title":"Report Format","text":"<p>All the numbers above should be measured multiple times (suggest 10 times at least) and reported.</p>"},{"location":"docs/specs/otel/performance/","title":"API \u6027\u80fd\u548c\u963b\u585e","text":"<p>This document defines common principles that will help designers create OpenTelemetry clients that are safe to use.</p>"},{"location":"docs/specs/otel/performance/#key-principles","title":"Key principles","text":"<p>Here are the key principles:</p> <ul> <li>Library should not block end-user application by default.</li> <li>Library should not consume unbounded memory resource.</li> </ul> <p>Although there are inevitable overhead to achieve monitoring, API should not degrade the end-user application as possible. So that it should not block the end-user application nor consume too much memory resource.</p> <p>See also Concurrency and Thread-Safety if the implementation supports concurrency.</p>"},{"location":"docs/specs/otel/performance/#tradeoff-between-non-blocking-and-memory-consumption","title":"Tradeoff between non-blocking and memory consumption","text":"<p>Incomplete asynchronous I/O tasks or background tasks may consume memory to preserve their state. In such a case, there is a tradeoff between dropping some tasks to prevent memory starvation and keeping all tasks to prevent information loss.</p> <p>If there is such tradeoff in OpenTelemetry client, it should provide the following options to end-user:</p> <ul> <li>Prevent information loss: Preserve all information but possible to consume   many resources</li> <li>Prevent blocking: Dropping some information under overwhelming load and   show warning log to inform when information loss starts and when recovered</li> <li>Should provide option to change threshold of the dropping</li> <li>Better to provide metric that represents effective sampling ratio</li> <li>OpenTelemetry client might provide this option for Logging</li> </ul>"},{"location":"docs/specs/otel/performance/#end-user-application-should-be-aware-of-the-size-of-logs","title":"End-user application should be aware of the size of logs","text":"<p>Logging could consume much memory by default if the end-user application emits too many logs. This default behavior is intended to preserve logs rather than dropping it. To make resource usage bounded, the end-user should consider reducing logs that are passed to the exporters.</p> <p>Therefore, the OpenTelemetry client should provide a way to filter logs to capture by OpenTelemetry. End-user applications may want to log so much into log file or stdout (or somewhere else) but not want to send all of the logs to OpenTelemetry exporters.</p> <p>In a documentation of the OpenTelemetry client, it is a good idea to point out that too many logs consume many resources by default then guide how to filter logs.</p>"},{"location":"docs/specs/otel/performance/#shutdown-and-explicit-flushing-could-block","title":"Shutdown and explicit flushing could block","text":"<p>The OpenTelemetry client could block the end-user application when it shut down. On shutdown, it has to flush data to prevent information loss. The OpenTelemetry client should support user-configurable timeout if it blocks on shut down.</p> <p>If the OpenTelemetry client supports an explicit flush operation, it could block also. But should support a configurable timeout.</p>"},{"location":"docs/specs/otel/performance/#documentation","title":"Documentation","text":"<p>If language specific implementation has special characteristics that are not described in this document, such characteristics should be documented.</p>"},{"location":"docs/specs/otel/project-management/","title":"\u9879\u76ee\u7ba1\u7406","text":"<p>Some specification changes are small enough in scope such that they can be resolved with a single PR or an OTEP written by a single person. However, this is rarely the case for large, meaningful changes. Most spec work ends up being organized into projects.</p> <p>At minimum, specification projects require the following resources to be successful:</p> <ul> <li>A group of designers and subject matter experts need to dedicate a significant   amount of their work time to the project. These participants are needed to   design the spec, write a set of OTEPs, and create multiple prototypes. This   group needs to meet with each other (and with TC members) on a regular basis   to develop a successful set of proposals.</li> <li>A portion of the TC needs to be aware of and participate in the development of   the project, to review the proposals and help guide the participants through   the specification process.</li> <li>Spec approvers and the broader community need to be aware of progress being   made on the project, so they can be prepared to participate when proposals are   ready for review.</li> </ul>"},{"location":"docs/specs/otel/project-management/#project-tracking-issue","title":"Project Tracking Issue","text":"<p>Every project has a high level Project Tracking Issue, which describes the project. This issue is frequently edited and kept up to date by the working group. To create a tracking issue, please use the Project Tracking Issue template.</p> <p>The project tracking issue must contain the following information:</p>"},{"location":"docs/specs/otel/project-management/#description","title":"Description","text":"<p>Describes the goals, objectives, and requirements for the project. This include the motivations for starting the project now, as opposed to later.</p>"},{"location":"docs/specs/otel/project-management/#project-board","title":"Project Board","text":"<p>Projects should be managed using a github project board. The project board should be pre-populated with issues that cover all known deliverables, organized by timeline milestones. The project board should be linked to in the tracking issue.</p> <p>A TC member associated with the project can create the board, along with a new github label to automatically associate issues and PRs with the project. The project lead and all other relevant project members should have edit access to the board. The project lead is responsible for maintaining the board and keeping it up-to-date.</p>"},{"location":"docs/specs/otel/project-management/#deliverables","title":"Deliverables","text":"<p>A description of what this project is planning to deliver, or is in the process of delivering. This includes all OTEPs and their associated prototypes.</p> <p>In general, OTEPs are not accepted unless they come with working prototypes available to review in at least two languages. Please discuss these requirements with a TC member before submitting an OTEP.</p>"},{"location":"docs/specs/otel/project-management/#staffing-help-wanted","title":"Staffing / Help Wanted","text":"<p>Who is currently planning to work on the project? If a project requires specialized domain expertise, please list it here. If a project is missing a critical mass of people in order to begin work, please clarify.</p>"},{"location":"docs/specs/otel/project-management/#required-staffing","title":"Required staffing","text":"<p>Projects cannot be started until the following participants have been identified:</p> <ul> <li>Every project needs a project lead, who is willing to bottom line the project   and address any issues which are not handled by other project members.</li> <li>At least two sponsoring TC members. TC sponsors are dedicated to attending   meetings, reviewing proposals, and in general being aware of the state of the   project and it\u2019s technical details. TC sponsors guide the project through the   spec process, keep the tracking issue up to date, and help to ensure that   relevant community members provide input at the appropriate times.</li> <li>Engineers willing to write prototypes in at least two languages (if relevant   to project). Languages should be fairly different from each other (for   example. Java and Python).</li> <li>Maintainers or approvers from those languages committed to reviewing the   prototypes.</li> </ul>"},{"location":"docs/specs/otel/project-management/#meeting-times","title":"Meeting Times","text":"<p>Once a project is started, the working group should meet regularly for discussion. These meeting times should be posted on the OpenTelemetry public calendar.</p>"},{"location":"docs/specs/otel/project-management/#timeline","title":"Timeline","text":"<p>What is the expected timeline the project will aim to adhere to, and what resources and deliverables will be needed for each portion of the timeline? If the project has not been started, please describe this timeline in relative terms (one month in, two weeks later, etc). If a project has started, please include actual dates.</p>"},{"location":"docs/specs/otel/project-management/#labels","title":"Labels","text":"<p>The tracking issue should be properly labeled to indicate what parts of the specification it is focused on.</p>"},{"location":"docs/specs/otel/project-management/#linked-issues-and-prs","title":"Linked Issues and PRs","text":"<p>All PRs, Issues, and OTEPs related to the project should link back to the tracking issue, so that they can be easily found.</p>"},{"location":"docs/specs/otel/project-management/#specification-project-lifecycle","title":"Specification Project Lifecycle","text":"<p>All specification projects have the same lifecycle, and are tracked on the Specification Project Board, which the community can use to get a high-level view of the specification roadmap.</p> <p>The project lifecycle is as follows:</p> <ul> <li>A Project Tracking Issue is created. The tracking issue includes all the   necessary information for the TC and spec community to evaluate the breadth   and depth of the work being proposed.</li> <li>If a project is approved, it is added to the list of Potential Projects.   This list is roughly ordered in the order we expect we will start the project.</li> <li>Potential projects are moved to the list of Scheduled Projects once they   have a planned start date. Having a planned start date lets potential   contributors know when they need to make themselves available, and get   prepared to begin their work. Subject matter experts and participants who plan   to do a lot of work \u2013 such as building prototypes \u2013 benefit greatly from   having a start date, as they can plan for their participantion with their   employers and coworkers.</li> <li>Once a project is begun, it is moved to the list of Current Projects.   Projects are only started when the necessary resources are available to move   them quickly to completion. This means that the necessary subject matter   experts have been identified, and at least two TC members are committed to   review and guide the project through the specification process.</li> <li>Once all OTEPs have been approved and integrated into the spec, and the   working group is no longer meeting, projects are moved to Completed   Projects.</li> </ul>"},{"location":"docs/specs/otel/project-management/#project-board_1","title":"Project Board","text":"<p>To track our specification projects, we use a GitHub project board. This board only contains Project Tracking Issues, and is organized into the following columns:</p>"},{"location":"docs/specs/otel/project-management/#current-projects","title":"Current Projects","text":"<p>Projects that are actively being moved to completion. Projects may be in one of several different states: design, proposal review, and implementation.</p>"},{"location":"docs/specs/otel/project-management/#scheduled-projects","title":"Scheduled Projects","text":"<p>Many projects require people (such as subject matter experts) and other resources whose participation/availability must be planned out in advance. In general, projects may be able to move faster when their start date is scheduled and known in advance, so participants can prepare their schedules and do preliminary research.</p> <p>Scheduled projects are projects which have not started yet, but have a scheduled start date.</p>"},{"location":"docs/specs/otel/project-management/#potential-projects","title":"Potential Projects","text":"<p>Any project which has a tracking issue and has been approved by the TC as a needed feature for OpenTelemetry. Roughly organized by priority.</p>"},{"location":"docs/specs/otel/project-management/#completed-projects","title":"Completed Projects","text":"<p>Projects which have been successfully implemented, and no longer need any attention beyond responding to user feedback.</p>"},{"location":"docs/specs/otel/semantic-conventions/","title":"\u8bed\u4e49\u7ea6\u5b9a","text":"<p>Status: Experimental</p> <p>The Semantic Conventions define the keys and values which describe commonly observed concepts, protocols, and operations used by applications.</p> <p>OpenTelemetry defines its semantic conventions in a separate repository: https://github.com/open-telemetry/semantic-conventions.</p>"},{"location":"docs/specs/otel/semantic-conventions/#reserved-attributes","title":"Reserved Attributes","text":"<p>Semantic conventions MUST provide the following attributes:</p> <ul> <li>Resource</li> <li><code>service.name</code></li> <li><code>telemetry.sdk.language</code></li> <li><code>telemetry.sdk.name</code></li> <li><code>telemetry.sdk.version</code></li> <li>Event</li> <li><code>exception.escaped</code></li> <li><code>exception.message</code></li> <li><code>exception.stacktrace</code></li> <li><code>exception.type</code></li> </ul>"},{"location":"docs/specs/otel/semantic-conventions/#experimental-reserved-attributes","title":"Experimental Reserved Attributes","text":"<p>Semantic conventions MUST provide the following attributes:</p> <ul> <li>Resource</li> <li><code>server.address</code></li> <li><code>server.port</code></li> <li><code>service.instance.id</code></li> <li><code>url.scheme</code></li> </ul>"},{"location":"docs/specs/otel/semantic-conventions/#reserved-namespace","title":"Reserved Namespace","text":"<p>The <code>otel.*</code> namespace is reserved for defining compatibility with non-opentelemetry technologies.</p>"},{"location":"docs/specs/otel/telemetry-stability/","title":"\u9065\u6d4b\u7a33\u5b9a","text":"<p>Status: Experimental</p> Table of Contents   - [Unstable Instrumentations](#unstable-instrumentations) - [Stable Instrumentations](#stable-instrumentations)   - [Fixed Schema Telemetry Producers](#fixed-schema-telemetry-producers)   - [Schema-File Driven Telemetry Producers](#schema-file-driven-telemetry-producers)   <p>This section defines stability requirements for telemetry produced by OpenTelemetry instrumentations.</p> <p>All OpenTelemetry-authored instrumentations are labeled to be either <code>Unstable</code> or <code>Stable</code> from the perspective of the telemetry they produce.</p> <p>Adding of new metrics, spans, span events or log records and adding of new attributes to spans, span events, log records or resources are considered additive, non-breaking changes and are always allowed for <code>Unstable</code> and <code>Stable</code> instrumentations.</p> <p>Other changes in the produced telemetry are regulated by the following rules.</p>"},{"location":"docs/specs/otel/telemetry-stability/#unstable-instrumentations","title":"Unstable Instrumentations","text":"<p>Unstable telemetry-producing instrumentations (unstable instrumentations for short) SHOULD be clearly labeled so by any means the instrumentations authors consider idiomatic for their language, e.g. via version numbers, artifact names, documentation, etc.</p> <p>Unstable instrumentations provide no guarantees about the shape of the telemetry they produce and how that shape changes over time, from version to version. Span or metric names, attributes of any telemetry items may change without any restrictions. The produced telemetry MAY specify a Schema URL if the telemetry data conforms to a particular Schema.</p> <p>Unstable instrumentations authored by OpenTelemetry MAY produce additional telemetry that is not described by OpenTelemetry semantic conventions.</p> <p>TODO: decide if it is necessary to indicate on the wire if the produced telemetry is coming from an unstable instrumentation.</p>"},{"location":"docs/specs/otel/telemetry-stability/#stable-instrumentations","title":"Stable Instrumentations","text":"<p>Warning There is a moratorium on relying on schema transformations for telemetry stability.</p> <p>Stable telemetry-producing instrumentations (stable instrumentations for short) SHOULD be clearly labeled so by any means the instrumentations authors consider idiomatic for their language, e.g. via version numbers, artifact names, documentation, etc.</p> <p>Stable instrumentations fall into 2 categories: fixed-schema producers and schema-file driven producers.</p> <p>Stable instrumentations authored by OpenTelemetry SHOULD NOT produce telemetry that is not described by OpenTelemetry semantic conventions. If, however, this rule is broken the instrumentations MUST NOT change such telemetry, regardless of whether they are fixed-schema producers or schema-file driven producers. Once the produced telemetry is added to the semantic conventions, changes will be allowed as described below.</p>"},{"location":"docs/specs/otel/telemetry-stability/#fixed-schema-telemetry-producers","title":"Fixed Schema Telemetry Producers","text":"<p>Instrumentations that are labeled <code>Stable</code> and do not include the Schema URL in the produced telemetry are called Fixed Schema Telemetry Producers.</p> <p>Such instrumentations are prohibited from changing any produced telemetry. If the specification changes over time and the semantic conventions are updated, the instrumentation is still prohibited from adopting the changes. If the instrumentation wishes to adopt the semantic convention changes it must first become a Schema-File Driven Telemetry Producer by adding an appropriate Schema URL in the produced telemetry.</p>"},{"location":"docs/specs/otel/telemetry-stability/#schema-file-driven-telemetry-producers","title":"Schema-File Driven Telemetry Producers","text":"<p>Stable instrumentations that include the Schema URL in the produced telemetry are called Schema-File Driven Telemetry Producers.</p> <p>Such instrumentations are prohibited from changing the produced telemetry until the moratorium on relying on schema transformations for telemetry stability is lifted and until that date are subject to exactly the same restrictions as Fixed Schema Telemetry Producers.</p> <p>After the moratorium is lifted, stable instrumentations are allowed to change the produced telemetry if all the following conditions are fulfilled:</p> <ul> <li>The change is part of OpenTelemetry semantic conventions and is in a released   version of the specification.</li> <li>The change has a corresponding   published OpenTelemetry Schema File   that describes the change.</li> <li>The produced telemetry correctly specifies the respective Schema URL.</li> </ul>"},{"location":"docs/specs/otel/vendors/","title":"\u4f9b\u5e94\u5546","text":"Table of Contents   - [Abstract](#abstract) - [Supports OpenTelemetry](#supports-opentelemetry) - [Implements OpenTelemetry](#implements-opentelemetry) - [Qualifications](#qualifications)"},{"location":"docs/specs/otel/vendors/#abstract","title":"Abstract","text":"<p>The OpenTelemetry project consists of both a specification for the API, SDK, protocol and semantic conventions, as well as an implementation of each for a number of languages. The default SDK implementation is highly configurable and extendable, for example through Span Processors, to allow for additional logic needed by particular vendors to be added without having to implement a custom SDK. By not requiring a custom SDK means for most languages a user will already find an implementation to use and if not they'll have a well documented specification to follow for implementing in a new language.</p> <p>The goal is for users to be able to easily switch between vendors while also ensuring that any language with an OpenTelemetry SDK implementation is able to work with any vendor who claims support for OpenTelemetry.</p> <p>This document will explain what is required of a vendor to be considered to \"Support OpenTelemetry\" or \"Implements OpenTelemetry\".</p>"},{"location":"docs/specs/otel/vendors/#supports-opentelemetry","title":"Supports OpenTelemetry","text":"<p>\"Supports OpenTelemetry\" means the vendor must accept the output of the default SDK through one of two mechanisms:</p> <ul> <li>By providing an exporter for the   OpenTelemetry Collector   and / or the OpenTelemetry SDKs</li> <li>By building a receiver for the   OpenTelemetry protocol</li> </ul>"},{"location":"docs/specs/otel/vendors/#implements-opentelemetry","title":"Implements OpenTelemetry","text":"<p>A vendor with a custom SDK implementation will be listed as \"Implements OpenTelemetry\". If the custom SDK is optional then the vendor can be listed as \"Supports OpenTelemetry\".</p>"},{"location":"docs/specs/otel/vendors/#qualifications","title":"Qualifications","text":"<p>A vendor can qualify their support for OpenTelemetry with the type of telemetry they support. For example, a vendor that accepts the OpenTelemetry protocol exports for metrics only will be listed as \"Supports OpenTelemetry Metrics\" or one that implements a custom SDK only for tracing will be listed as \"Implements OpenTelemetry Tracing\".</p>"},{"location":"docs/specs/otel/versioning-and-stability/","title":"\u5ba2\u6237\u7aef\u7684\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027","text":"<p>Status: Stable</p> Table of Contents   - [\u5ba2\u6237\u7aef\u7684\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027](#\u5ba2\u6237\u7aef\u7684\u7248\u672c\u63a7\u5236\u548c\u7a33\u5b9a\u6027)   - [Design goals](#design-goals)   - [Signal lifecycle](#signal-lifecycle)     - [Experimental](#experimental)     - [Stable](#stable)       - [API Stability](#api-stability)         - [Extending Existing API Calls](#extending-existing-api-calls)       - [SDK Stability](#sdk-stability)       - [Contrib Stability](#contrib-stability)       - [Semantic Conventions Stability](#semantic-conventions-stability)       - [Telemetry Stability](#telemetry-stability)     - [Deprecated](#deprecated)     - [Removed](#removed)     - [A note on replacing signals](#a-note-on-replacing-signals)   - [Version numbers](#version-numbers)     - [Major version bump](#major-version-bump)     - [Minor version bump](#minor-version-bump)     - [Patch version bump](#patch-version-bump)     - [Language version support](#language-version-support)   - [Long Term Support](#long-term-support)     - [API support](#api-support)     - [SDK Support](#sdk-support)     - [Contrib Support](#contrib-support)   - [OpenTelemetry GA](#opentelemetry-ga)   <p>This document defines the stability guarantees offered by the OpenTelemetry clients, along with the rules and procedures for meeting those guarantees.</p> <p>In this document, the terms \"OpenTelemetry\" and \"language implementations\" both specifically refer to the OpenTelemetry clients. These terms do not refer to the specification or the Collector in this document.</p> <p>Each language implementation MUST take these versioning and stability requirements, and produce a language-specific document which details how these requirements will be met. This document SHALL be placed in the root of each repo and named <code>VERSIONING</code>.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#design-goals","title":"Design goals","text":"<p>Versioning and stability procedures are designed to meet the following goals.</p> <p>Ensure that application owners stay up to date with the latest release of the SDK. We want all users to stay up to date with the latest version of the OpenTelemetry SDK. We do not want to create hard breaks in support, of any kind, which leave users stranded on older versions. It MUST always be possible to upgrade to the latest minor version of the OpenTelemetry SDK, without creating compilation or runtime errors.</p> <p>Never create a dependency conflict between packages which rely on different versions of OpenTelemetry. Avoid breaking all stable public APIs. Backwards compatibility is a strict requirement. Instrumentation APIs cannot create a version conflict, ever. Otherwise, the OpenTelemetry API cannot be embedded in widely shared libraries, such as web frameworks. Code written against older versions of the API MUST work with all newer versions of the API. Transitive dependencies of the API cannot create a version conflict. The OpenTelemetry API cannot depend on a particular package if there is any chance that any library or application may require a different, incompatible version of that package. A library that imports the OpenTelemetry API should never become incompatible with other libraries due to a version conflict in one of OpenTelemetry's dependencies. Theoretically, APIs can be deprecated and eventually removed, but this is a process measured in years and we have no plans to do so.</p> <p>Allow for multiple levels of package stability within the same release of an OpenTelemetry component. Provide maintainers a clear process for developing new, experimental signals alongside stable signals. Different packages within the same release may have different levels of stability. This means that an implementation wishing to release stable tracing today MUST ensure that experimental metrics are factored out in such a way that breaking changes to metrics API do not destabilize the trace API packages.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#signal-lifecycle","title":"Signal lifecycle","text":"<p>The development of each signal follows a lifecycle: experimental, stable, deprecated, removed.</p> <p>The infographic below shows an example of the lifecycle of an API component.</p> <p></p>"},{"location":"docs/specs/otel/versioning-and-stability/#experimental","title":"Experimental","text":"<p>Signals start as experimental, which covers alpha, beta, and release candidate versions of the signal. While signals are experimental, breaking changes and performance issues MAY occur. Components SHOULD NOT be expected to be feature-complete. In some cases, the experiment MAY be discarded and removed entirely. Long-term dependencies SHOULD NOT be taken against experimental signals.</p> <p>OpenTelemetry clients MUST be designed in a manner that allows experimental signals to be created without breaking the stability guarantees of existing signals.</p> <p>OpenTelemetry clients MUST NOT be designed in a manner that breaks existing users when a signal transitions from experimental to stable. This would punish users of the release candidate, and hinder adoption.</p> <p>Terms which denote stability, such as \"experimental,\" MUST NOT be used as part of a directory or import name. Package version numbers MAY include a suffix, such as -alpha, -beta, -rc, or -experimental, to differentiate stable and experimental packages.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#stable","title":"Stable","text":"<p>Once an experimental signal has gone through rigorous beta testing, it MAY transition to stable. Long-term dependencies MAY now be taken against this signal.</p> <p>All signal components MAY become stable together, or MAY transition to stability component-by-component. The API MUST become stable before the other components.</p> <p>Once a signal component is marked as stable, the following rules MUST apply until the end of that signal\u2019s existence.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#api-stability","title":"API Stability","text":"<p>Backward-incompatible changes to API packages MUST NOT be made unless the major version number is incremented. All existing API calls MUST continue to compile and function against all future minor versions of the same major version.</p> <p>Languages which ship binary artifacts SHOULD offer ABI compatibility for API packages.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#extending-existing-api-calls","title":"Extending Existing API Calls","text":"<p>An existing API call MAY be extended without incrementing the major version number if the particular language allows to do it in a backward-compatible manner.</p> <p>To add a new parameter to an existing API call depending on the language several approaches are possible:</p> <ul> <li> <p>Add a new optional parameter to existing methods. This may not be the right   approach for languages where ABI stability is part of our guarantees since it   likely breaks the ABI.</p> </li> <li> <p>Add a method overload that allows passing a different set of parameters, that   include the new parameter. This is likely the preferred approach for languages   where method overloads are possible.</p> </li> </ul> <p>There may be other ways to extend existing APIs in non-breaking manner. Language maintainers SHOULD choose the idiomatic way for their language.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#sdk-stability","title":"SDK Stability","text":"<p>Public portions of SDK packages MUST remain backwards compatible. There are two categories of public features: plugin interfaces and constructors. Examples of plugins include the SpanProcessor, Exporter, and Sampler interfaces. Examples of constructors include configuration objects, environment variables, and SDK builders.</p> <p>Languages which ship binary artifacts SHOULD offer ABI compatibility for SDK packages.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#contrib-stability","title":"Contrib Stability","text":"<p>Plugins, instrumentation, and other contrib packages SHOULD be kept up to date and compatible with the latest versions of the API, SDK, and Semantic Conventions. If a release of the API, SDK, or Semantic Conventions contains changes which are relevant to a contrib package, that package SHOULD be updated and released in a timely fashion. (See limitations on instrumentation stability in Telemetry Stability.) The goal is to ensure users can update to the latest version of OpenTelemetry, and not be held back by the plugins that they depend on.</p> <p>Public portions of contrib packages (constructors, configuration, interfaces) SHOULD remain backwards compatible.</p> <p>Languages which ship binary artifacts SHOULD offer ABI compatibility for contrib packages.</p> <p>Exception: Contrib packages MAY break stability when a required downstream dependency breaks stability. For example, a database integration may break stability if the required database client breaks stability. However, it is strongly RECOMMENDED that older contrib packages remain stable. A new, incompatible version of an integration SHOULD be released as a separate contrib package, rather than break the existing contrib package.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#semantic-conventions-stability","title":"Semantic Conventions Stability","text":"<p>Warning There is a moratorium on relying on schema transformations for telemetry stability.</p> <p>Semantic conventions define a contract between the signals that instrumentation will provide and analysis tools that consumes the instrumentation (e.g. dashboards, alerts, queries, etc.).</p> <p>Changes to telemetry produced by OpenTelemetry instrumentation SHOULD avoid breaking analysis tools, such as dashboards and alerts. To achieve this, while allowing the evolution of telemetry and semantic conventions, OpenTelemetry relies on the concept of Telemetry Schemas.</p> <p>Semantic Conventions defines breaking changes as those that would break the common usage of tooling written against the telemetry it produces. That is, the portions of telemetry where specialized tooling (alerts, dashboards, e.g.) interact are expected to remain stable for that tooling after schema transformations are applied. These also assume no user interventions in the default configuration, e.g. Samplers, Views, etc.</p> <p>Semantic Conventions defines the set of fields in the OTLP data model:</p> <ul> <li>Resource</li> <li>attribute keys. (The key section of attributes key value pairs)</li> <li>InstrumentationScope</li> <li>Attribute keys<ul> <li>provided to get a tracer</li> <li>provided to get a meter</li> </ul> </li> <li>Trace</li> <li>The following data on span:<ul> <li>The span name</li> <li>The span kind</li> <li>The attribute keys provided to the span</li> <li>Whether these attributes must be provided at span start time, due to     sampling concerns.</li> </ul> </li> <li>The following data provided on span events<ul> <li>The event name</li> <li>The attribute keys provided for the event</li> </ul> </li> <li>Metrics</li> <li>The following portions of a Metric (passed when constructing     an instrument):<ul> <li>The name of the metric (defaults to instrument name).</li> <li>The kind of metric data (Gauge, Sum, Histogram, ExponentialHistogram)</li> <li>For <code>Counter</code> and <code>UpDownCounter</code> instruments, it is acceptable to     change between asynchronous and synchronous instruments, as this     preserves the metric kind.</li> <li>The unit of the metric (defaults to instrument unit).</li> </ul> </li> <li>The attribute keys on any <code>*DataPoint</code>.<ul> <li>These are provided in the API when recording a measurement, for both   synchronous and asynchronous instruments.</li> <li>These exist on <code>NumberDataPoint</code>, <code>HistogramDataPoint</code>,   <code>ExponentialHistogramDataPoint</code> and <code>SummaryDataPoint</code>.</li> </ul> </li> <li>Log Records</li> <li>The attribute keys provided on the LogRecord</li> <li>For log records that are Log Events<ul> <li>The following data provided to emit event:</li> <li>The event name (the value of the <code>event.name</code> attribute)</li> <li>The event domain (the value of the <code>event.domain</code> attribute)</li> </ul> </li> </ul> <p>Things not listed in the above are not expected to remain stable via semantic convention and are allowed (or expected) to change. A few examples:</p> <ul> <li>The values of attributes</li> <li>Specifically for <code>enums</code> the list of allowed values is expected to change     overtime.</li> <li>Even for <code>enums</code> that limit allowed values to semconv, some may need to     updated values in the future. Tooling should expect unknown values.</li> <li>The links attached to a span</li> <li>The recorded measurement type (float or integer) of a metric is not enforced   and allowed to change.</li> <li>The description of a metric instrument.</li> <li>The values being recorded by an instrument.</li> </ul> <p>The list of telemetry fields which are covered by stability guarantees MAY be extended.</p> <p>Changes to semantic conventions in this specification are allowed, provided that the changes can be described by schema files. The following changes can be currently described and are allowed:</p> <ul> <li>Renaming of span, metric, log and resource attributes.</li> <li>Renaming of metrics.</li> <li>Renaming of span events.</li> </ul> <p>All such changes MUST be described in the OpenTelemetry Schema File Format and published in this repository. For details see how OpenTelemetry Schemas are published.</p> <p>See the Telemetry Stability document for details on how instrumentations can use schemas to change the instrumentation they produce.</p> <p>Exception: Some resource attributes are embedded in various locations of the Specification, e.g. the <code>service.*</code> attributes which are required by SDKs to be produced and have corresponding environment variables defined in general SDK configuration. These resource attributes MUST NOT be ever changed. They are considered a hard-coded part of this specification.</p> <p>In addition to the 3 types of changes described above there are certain types that are always allowed. Such changes do not need to be described (and are not described) by schema files. Here is the list of such changes:</p> <ul> <li>Adding new attributes to the existing semantic conventions for resources,   spans, span events or log records.</li> <li>Adding new attributes to existing metrics that do not \"break apart\" existing   timeseries, such that alert thresholds would break / need to change.</li> <li>Adding semantic conventions for new types of resources, spans, span events,   metrics or log records.</li> </ul> <p>Any other changes to semantic conventions are currently prohibited. Other types of changes MAY be introduced in the future versions of this specification. This is only allowed if OpenTelemetry introduces a new schema file format that is capable of describing such changes.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#telemetry-stability","title":"Telemetry Stability","text":"<p>For stability of telemetry produced by instrumentation see the Telemetry Stability document.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#deprecated","title":"Deprecated","text":"<p>Signals MAY eventually be replaced. When this happens, they are marked as deprecated.</p> <p>Signals SHALL only be marked as deprecated when the replacement becomes stable. Deprecated code MUST abide by the same support guarantees as stable code.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#removed","title":"Removed","text":"<p>Support is ended by the removal of a signal from the release. The release MUST make a major version bump when this happens.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#a-note-on-replacing-signals","title":"A note on replacing signals","text":"<p>Note that we currently have no plans for creating a major version of OpenTelemetry past v1.0.</p> <p>For clarity, it is still possible to create new, backwards incompatible versions of existing signals without actually moving to v2.0 and breaking support.</p> <p>For example, imagine we develop a new, better tracing API - let's call it AwesomeTrace. We will never mutate the current tracing API into AwesomeTrace. Instead, AwesomeTrace would be added as an entirely new signal which coexists and interoperates with the current tracing signal. This would make adding AwesomeTrace a minor version bump, not v2.0. v2.0 would mark the end of support for current tracing, not the addition of AwesomeTrace. And we don't want to ever end that support, if we can help it.</p> <p>This is not actually a theoretical example. OpenTelemetry already supports two tracing APIs: OpenTelemetry and OpenTracing. We invented a new tracing API, but continue to support the old one.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#version-numbers","title":"Version numbers","text":"<p>OpenTelemetry clients follow Semantic Versioning 2.0.0, with the following clarifications.</p> <p>OpenTelemetry clients have four components: API, SDK, Semantic Conventions, and Contrib.</p> <p>For the purposes of versioning, all code within a component MUST treated as if it were part of a single package, and versioned with the same version number, except for Contrib, which may be a collection of packages versioned separately.</p> <ul> <li>All stable API packages MUST version together, across all signals. Stable   signals MUST NOT have separate version numbers. There is one version number   that applies to all signals that are included in the API release that is   labeled with that particular version number.</li> <li>SDK packages for all signals MUST version together, across all signals.   Signals MUST NOT have separate version numbers. There is one version number   that applies to all signals that are included in the SDK release that is   labeled with that particular version number.</li> <li>Semantic Conventions are a single package with a single version number.</li> <li>Each contrib package MAY have it's own version number.</li> <li>The API, SDK, Semantic Conventions, and contrib components have independent   version numbers. For example, the latest version of <code>opentelemetry-python-api</code>   MAY be at v1.2.3 while the latest version of <code>opentelemetry-python-sdk</code> is at   v2.3.1.</li> <li>Different language implementations have independent version numbers. For   example, it is fine to have <code>opentelemetry-python-api</code> at v1.2.8 when   <code>opentelemetry-java-api</code> is at v1.3.2.</li> <li>Language implementations have version numbers which are independent of the   specification they implement. For example, it is fine for v1.8.2 of   <code>opentelemetry-python-api</code> to implement v1.1.1 of the specification.</li> </ul> <p>Exception: in some languages, package managers may react poorly to experimental packages having a version higher than 0.X. In these cases, experimental signals MAY version independently from stable signals, in order to retain a 0.X version number. When a signal becomes stable, the version MUST be bumped to match the other stable signals in the release.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#major-version-bump","title":"Major version bump","text":"<p>Major version bumps MUST occur when there is a breaking change to a stable interface or a deprecated signal is removed. Major version bumps SHOULD NOT occur for changes which do not result in a drop in support of some form.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#minor-version-bump","title":"Minor version bump","text":"<p>Most changes to OpenTelemetry clients result in a minor version bump.</p> <ul> <li>New backward-compatible functionality added to any component.</li> <li>Breaking changes to internal SDK components.</li> <li>Breaking changes to experimental signals.</li> <li>New experimental signals are added.</li> <li>Experimental signals become stable.</li> <li>Stable signals are deprecated.</li> </ul>"},{"location":"docs/specs/otel/versioning-and-stability/#patch-version-bump","title":"Patch version bump","text":"<p>Patch versions make no changes which would require recompilation or potentially break application code. The following are examples of patch fixes.</p> <ul> <li>Bug fixes which don't require minor version bump per rules above.</li> <li>Security fixes.</li> <li>Documentation.</li> </ul> <p>Currently, the OpenTelemetry project does NOT have plans to backport bug and security fixes to prior minor versions of the SDK. Security and bug fixes MAY only be applied to the latest minor version. We are committed to making it feasible for end users to stay up to date with the latest version of the OpenTelemetry SDK.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#language-version-support","title":"Language version support","text":"<p>Each language implementation SHOULD define how the removal of a supported language/runtime version affects its versioning. As a rule of thumb, it SHOULD follow the conventions in the given ecosystem.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#long-term-support","title":"Long Term Support","text":""},{"location":"docs/specs/otel/versioning-and-stability/#api-support","title":"API support","text":"<p>Major versions of the API MUST be supported for a minimum of three years after the release of the next major API version. API support is defined as follows.</p> <ul> <li> <p>API stability, as defined above, MUST be maintained.</p> </li> <li> <p>A version of the SDK which supports the latest minor version of the last major   version of the API will continue to be maintained during LTS. Bug and security   fixes MUST be backported. Additional feature development is NOT RECOMMENDED.</p> </li> <li> <p>Contrib packages available when the API is versioned MUST continue to be   maintained for the duration of LTS. Bug and security fixes will be backported.   Additional feature development is NOT RECOMMENDED.</p> </li> </ul>"},{"location":"docs/specs/otel/versioning-and-stability/#sdk-support","title":"SDK Support","text":"<p>SDK stability, as defined above, will be maintained for a minimum of one year after the release of the next major SDK version.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#contrib-support","title":"Contrib Support","text":"<p>Contrib stability, as defined above, will be maintained for a minimum of one year after the release of the next major version of a contrib package.</p>"},{"location":"docs/specs/otel/versioning-and-stability/#opentelemetry-ga","title":"OpenTelemetry GA","text":"<p>The term \u201cOpenTelemetry GA\u201d refers to the point at which OpenTracing and OpenCensus will be fully deprecated. The minimum requirements for declaring GA are as followed.</p> <ul> <li>A stable version of both tracing and metrics MUST be released in at least four   languages.</li> <li>CI/CD, performance, and integration tests MUST be implemented for these   languages.</li> </ul>"},{"location":"docs/specs/otel/baggage/","title":"Baggage","text":""},{"location":"docs/specs/otel/baggage/api/","title":"Baggage API","text":"<p>Status: Stable, Feature-freeze</p>"},{"location":"docs/specs/otel/baggage/api/#_1","title":"\u6982\u8ff0","text":"<p><code>Baggage</code>\u7528\u4e8e\u6ce8\u91ca\u9065\u6d4b\uff0c\u5c06\u4e0a\u4e0b\u6587\u548c\u4fe1\u606f\u6dfb\u52a0\u5230\u5ea6\u91cf\u3001\u8ddf\u8e2a\u548c\u65e5\u5fd7\u4e2d\u3002\u5b83\u662f\u4e00\u7ec4\u63cf\u8ff0\u7528\u6237 \u5b9a\u4e49\u5c5e\u6027\u7684\u540d\u79f0/\u503c\u5bf9\u3002 <code>Baggage</code>\u4e2d\u7684\u6bcf\u4e2a\u540d\u79f0\u5fc5\u987b\u4e0e\u4e00\u4e2a\u503c\u76f8\u5173\u8054\u3002</p> <p><code>Baggage</code> API \u5305\u62ec:</p> <ul> <li><code>Baggage</code></li> <li>\u5728\u201c\u4e0a\u4e0b\u6587\u201d\u4e2d\u4e0e<code>Baggage</code>\u4ea4\u4e92\u7684\u51fd\u6570</li> </ul> <p>The functions described here are one way to approach interacting with the <code>Baggage</code> via having struct/object that represents the entire Baggage content. Depending on language idioms, a language API MAY implement these functions by interacting with the baggage via the <code>Context</code> directly.</p> <p>The Baggage API MUST be fully functional in the absence of an installed SDK. This is required in order to enable transparent cross-process Baggage propagation. If a Baggage propagator is installed into the API, it will work with or without an installed SDK.</p> <p>The <code>Baggage</code> container MUST be immutable, so that the containing <code>Context</code> also remains immutable.</p>"},{"location":"docs/specs/otel/baggage/api/#_2","title":"\u64cd\u4f5c","text":""},{"location":"docs/specs/otel/baggage/api/#get-value","title":"Get Value","text":"<p>To access the value for a name/value pair set by a prior event, the Baggage API MUST provide a function that takes the name as input, and returns a value associated with the given name, or null if the given name is not present.</p> <p>REQUIRED parameters:</p> <p><code>Name</code> the name to return the value for.</p>"},{"location":"docs/specs/otel/baggage/api/#get-all-values","title":"Get All Values","text":"<p>Returns the name/value pairs in the <code>Baggage</code>. The order of name/value pairs MUST NOT be significant. Based on the language specifics, the returned value can be either an immutable collection or an iterator on the immutable collection of name/value pairs in the <code>Baggage</code>.</p>"},{"location":"docs/specs/otel/baggage/api/#set-value","title":"Set Value","text":"<p>To record the value for a name/value pair, the Baggage API MUST provide a function which takes a name, and a value as input. Returns a new <code>Baggage</code> that contains the new value. Depending on language idioms, a language API MAY implement these functions by using a <code>Builder</code> pattern and exposing a way to construct a <code>Builder</code> from a <code>Baggage</code>.</p> <p>REQUIRED parameters:</p> <p><code>Name</code> The name for which to set the value, of type string.</p> <p><code>Value</code> The value to set, of type string.</p> <p>OPTIONAL parameters:</p> <p><code>Metadata</code> Optional metadata associated with the name-value pair. This should be an opaque wrapper for a string with no semantic meaning. Left opaque to allow for future functionality.</p>"},{"location":"docs/specs/otel/baggage/api/#remove-value","title":"Remove Value","text":"<p>To delete a name/value pair, the Baggage API MUST provide a function which takes a name as input. Returns a new <code>Baggage</code> which no longer contains the selected name. Depending on language idioms, a language API MAY implement these functions by using a <code>Builder</code> pattern and exposing a way to construct a <code>Builder</code> from a <code>Baggage</code>.</p> <p>REQUIRED parameters:</p> <p><code>Name</code> the name to remove.</p>"},{"location":"docs/specs/otel/baggage/api/#_3","title":"\u73af\u5883\u76f8\u4e92\u4f5c\u7528","text":"<p>This section defines all operations within the Baggage API that interact with the <code>Context</code>.</p> <p>If an implementation of this API does not operate directly on the <code>Context</code>, it MUST provide the following functionality to interact with a <code>Context</code> instance:</p> <ul> <li>Extract the <code>Baggage</code> from a <code>Context</code> instance</li> <li>Insert the <code>Baggage</code> to a <code>Context</code> instance</li> </ul> <p>The functionality listed above is necessary because API users SHOULD NOT have access to the Context Key used by the Baggage API implementation.</p> <p>If the language has support for implicitly propagated <code>Context</code> (see here), the API SHOULD also provide the following functionality:</p> <ul> <li>Get the currently active <code>Baggage</code> from the implicit context. This is   equivalent to getting the implicit context, then extracting the <code>Baggage</code> from   the context.</li> <li>Set the currently active <code>Baggage</code> to the implicit context. This is equivalent   to getting the implicit context, then inserting the <code>Baggage</code> to the context.</li> </ul> <p>All the above functionalities operate solely on the context API, and they MAY be exposed as static methods on the baggage module, as static methods on a class inside the baggage module (it MAY be named <code>BaggageUtilities</code>), or on the <code>Baggage</code> class. This functionality SHOULD be fully implemented in the API when possible.</p>"},{"location":"docs/specs/otel/baggage/api/#_4","title":"\u6e05\u9664\u8bed\u5883\u4e2d\u7684\u5305\u88b1","text":"<p>To avoid sending any name/value pairs to an untrusted process, the Baggage API MUST provide a way to remove all baggage entries from a context.</p> <p>This functionality can be implemented by having the user set an empty <code>Baggage</code> object/struct into the context, or by providing an API that takes a <code>Context</code> as input, and returns a new <code>Context</code> with no <code>Baggage</code> associated.</p>"},{"location":"docs/specs/otel/baggage/api/#_5","title":"\u4f20\u64ad","text":"<p><code>Baggage</code> MAY be propagated across process boundaries or across any arbitrary boundaries (process, $OTHER_BOUNDARY1, $OTHER_BOUNDARY2, etc) for various reasons.</p> <p>The API layer or an extension package MUST include the following <code>Propagator</code>s:</p> <ul> <li>A <code>TextMapPropagator</code> implementing the   W3C Baggage Specification.</li> </ul> <p>See Propagators Distribution for how propagators are to be distributed.</p> <p>Note: The W3C baggage specification does not currently assign semantic meaning to the optional metadata.</p> <p>On <code>extract</code>, the propagator should store all metadata as a single metadata instance per entry. On <code>inject</code>, the propagator should append the metadata per the W3C specification format. Refer to the API Propagators Operation section for the additional requirements these operations need to follow.</p>"},{"location":"docs/specs/otel/baggage/api/#_6","title":"\u89e3\u51b3\u51b2\u7a81","text":"<p>\u5982\u679c\u6dfb\u52a0\u4e86\u65b0\u7684\u540d\u79f0/\u503c\u5bf9\uff0c\u5e76\u4e14\u5176\u540d\u79f0\u4e0e\u73b0\u6709\u540d\u79f0\u76f8\u540c\uff0c\u5219\u5fc5\u987b\u4f18\u5148\u8003\u8651\u65b0\u7684\u540d\u79f0/\u503c\u5bf9\u3002 \u8be5\u503c\u5c06\u88ab\u66ff\u6362\u4e3a\u6dfb\u52a0\u7684\u503c(\u65e0\u8bba\u8be5\u503c\u662f\u672c\u5730\u751f\u6210\u7684\u8fd8\u662f\u4ece\u8fdc\u7aef\u5bf9\u7b49\u4f53\u63a5\u6536\u7684)\u3002</p>"},{"location":"docs/specs/otel/common/","title":"Index","text":""},{"location":"docs/specs/otel/common/#_1","title":"\u901a\u7528\u89c4\u8303\u6982\u5ff5","text":"<p>Status: Stable, Feature-freeze</p>"},{"location":"docs/specs/otel/common/#attribute","title":"Attribute","text":"<p>\u4e00\u4e2a\u201c\u5c5e\u6027\u201d\u662f\u4e00\u4e2a\u952e\u503c\u5bf9\uff0c\u5b83\u5fc5\u987b\u5177\u6709\u4ee5\u4e0b\u5c5e\u6027:</p> <ul> <li>\u5c5e\u6027\u952e\u5fc5\u987b\u662f\u4e00\u4e2a\u975e<code>null</code>\u548c\u975e\u7a7a\u5b57\u7b26\u4e32\u3002</li> <li>\u5c5e\u6027\u503c\u4e3a:</li> <li>\u57fa\u672c\u7c7b\u578b:\u5b57\u7b26\u4e32\u3001\u5e03\u5c14\u503c\u3001\u53cc\u7cbe\u5ea6\u6d6e\u70b9\u6570(IEEE 754-1985)\u6216\u6709\u7b26\u53f7 64 \u4f4d\u6574\u6570\u3002</li> <li>\u539f\u59cb\u7c7b\u578b\u503c\u7684\u6570\u7ec4\u3002\u6570\u7ec4\u5fc5\u987b\u662f\u540c\u6784\u7684\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4e0d\u80fd\u5305\u542b\u4e0d\u540c\u7c7b\u578b\u7684\u503c\u3002</li> </ul> <p>\u5bf9\u4e8e\u539f\u751f\u4e0d\u652f\u6301\u975e\u5b57\u7b26\u4e32\u503c\u7684\u534f\u8bae\uff0c\u975e\u5b57\u7b26\u4e32\u503c\u5e94\u8be5\u8868\u793a\u4e3a json \u7f16\u7801\u7684\u5b57\u7b26\u4e32\u3002\u4f8b\u5982\uff0c\u8868 \u8fbe\u5f0f<code>int64(100)</code>\u5c06\u88ab\u7f16\u7801\u4e3a<code>100</code>\uff0c<code>float64(1.5)</code>\u5c06\u88ab\u7f16\u7801\u4e3a<code>1.5</code>\uff0c\u4efb\u4f55\u7c7b\u578b\u7684\u7a7a\u6570\u7ec4 \u5c06\u88ab\u7f16\u7801\u4e3a<code>[]</code>\u3002</p> <p>Attribute values expressing a numerical value of zero, an empty string, or an empty array are considered meaningful and MUST be stored and passed on to processors / exporters.</p> <p>Attribute values of <code>null</code> are not valid and attempting to set a <code>null</code> value is undefined behavior.</p> <p><code>null</code> values SHOULD NOT be allowed in arrays. However, if it is impossible to make sure that no <code>null</code> values are accepted (e.g. in languages that do not have appropriate compile-time type checking), <code>null</code> values within arrays MUST be preserved as-is (i.e., passed on to span processors / exporters as <code>null</code>). If exporters do not support exporting <code>null</code> values, they MAY replace those values by 0, <code>false</code>, or empty strings. This is required for map/dictionary structures represented as two arrays with indices that are kept in sync (e.g., two attributes <code>header_keys</code> and <code>header_values</code>, both containing an array of strings to represent a mapping <code>header_keys[i] -&gt; header_values[i]</code>).</p> <p>See Attribute Naming for naming guidelines.</p> <p>See Requirement Level for requirement levels guidelines.</p> <p>See this document to find out how to map values obtained outside OpenTelemetry into OpenTelemetry attribute values.</p>"},{"location":"docs/specs/otel/common/#_2","title":"\u5c5e\u6027\u9650\u5236","text":"<p>\u6267\u884c\u9519\u8bef\u7684\u4ee3\u7801\u53ef\u80fd\u4f1a\u4ea7\u751f\u610f\u60f3\u4e0d\u5230\u7684\u5c5e\u6027\u3002\u5982\u679c\u5bf9\u5c5e\u6027\u6ca1\u6709\u9650\u5236\uff0c\u5b83\u4eec\u4f1a\u8fc5\u901f\u8017\u5c3d\u53ef\u7528\u5185 \u5b58\uff0c\u5bfc\u81f4\u96be\u4ee5\u5b89\u5168\u6062\u590d\u7684\u5d29\u6e83\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cSDK \u5e94\u8be5\u6309\u7167\u4e0b\u9762\u7684\u53ef\u914d\u7f6e\u53c2\u6570\u5217\u8868\u5e94\u7528\u622a\u65ad \u3002</p> <p>\u5982\u679c SDK \u63d0\u4f9b\u4e86\u4e00\u79cd\u65b9\u6cd5:</p> <ul> <li>\u8bbe\u7f6e\u4e00\u4e2a\u5c5e\u6027\u503c\u957f\u5ea6\u9650\u5236\uff0c\u8fd9\u6837\u5bf9\u4e8e\u6bcf\u4e2a\u5c5e\u6027\u503c:</li> <li>\u5982\u679c\u5b83\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\uff0c\u5982\u679c\u5b83\u8d85\u8fc7\u4e86\u8fd9\u4e2a\u9650\u5236(\u5c06\u5176\u4e2d\u7684\u4efb\u4f55\u5b57\u7b26\u8ba1\u6570\u4e3a 1)\uff0csdk \u5fc5\u987b     \u622a\u65ad\u8be5\u503c\uff0c\u4f7f\u5176\u957f\u5ea6\u6700\u591a\u7b49\u4e8e\u9650\u5236\uff0c</li> <li>\u5982\u679c\u5b83\u662f\u5b57\u7b26\u4e32\u6570\u7ec4\uff0c\u5219\u5206\u522b\u5bf9\u6bcf\u4e2a\u503c\u5e94\u7528\u4e0a\u8ff0\u89c4\u5219\u3002</li> <li>\u5426\u5219\u4e00\u4e2a\u503c\u7edd\u5bf9\u4e0d\u80fd\u88ab\u622a\u65ad;</li> <li>\u8bbe\u7f6e\u552f\u4e00\u5c5e\u6027\u952e\u7684\u9650\u5236\u5982\u4e0b:</li> <li>\u5bf9\u4e8e\u6bcf\u4e2a\u552f\u4e00\u7684\u5c5e\u6027\u952e\uff0c\u6dfb\u52a0\u5b83\u5c06\u5bfc\u81f4\u8d85\u8fc7\u9650\u5236\uff0cSDK \u5fc5\u987b\u4e22\u5f03\u8be5\u952e/\u503c\u5bf9\u3002</li> </ul> <p>There MAY be a log emitted to indicate to the user that an attribute was truncated or discarded. To prevent excessive logging, the log MUST NOT be emitted more than once per record on which an attribute is set.</p> <p>If the SDK implements the limits above, it MUST provide a way to change these limits programmatically. Names of the configuration options SHOULD be the same as in the list below.</p> <p>An SDK MAY implement model-specific limits, for example <code>SpanAttributeCountLimit</code> or <code>LogRecordAttributeCountLimit</code>. If both a general and a model-specific limit are implemented, then the SDK MUST first attempt to use the model-specific limit, if it isn't set, then the SDK MUST attempt to use the general limit. If neither are defined, then the SDK MUST try to use the model-specific limit default value, followed by the global limit default value.</p>"},{"location":"docs/specs/otel/common/#_3","title":"\u53ef\u914d\u7f6e\u53c2\u6570","text":"<ul> <li><code>AttributeCountLimit</code> (Default=128) - \u6bcf\u6761\u8bb0\u5f55\u5141\u8bb8\u7684\u6700\u5927\u5c5e\u6027\u8ba1\u6570;</li> <li><code>AttributeValueLengthLimit</code> (Default=Infinity) - \u5141\u8bb8\u7684\u6700\u5927\u5c5e\u6027\u503c\u957f\u5ea6;</li> </ul>"},{"location":"docs/specs/otel/common/#_4","title":"\u514d\u7a0e\u5b9e\u4f53","text":"<p>\u8d44\u6e90\u5c5e\u6027\u5e94\u8be5\u4e0d\u53d7\u4e0a\u9762\u63cf\u8ff0\u7684\u9650\u5236\uff0c\u56e0\u4e3a\u8d44\u6e90\u4e0d\u5bb9\u6613\u53d7\u5230\u5bfc\u81f4\u8fc7\u591a\u5c5e\u6027\u8ba1\u6570\u6216\u5927\u5c0f\u7684\u573a\u666f( \u81ea\u52a8\u68c0\u6d4b)\u7684\u5f71\u54cd\u3002\u8d44\u6e90\u6bcf\u6279\u53ea\u53d1\u9001\u4e00\u6b21\uff0c\u800c\u4e0d\u662f\u6bcf\u4e2a\u8de8\u5ea6\u53d1\u9001\u4e00\u6b21\uff0c\u56e0\u6b64\u5728\u8d44\u6e90\u4e0a\u62e5\u6709\u66f4 \u591a/\u66f4\u5927\u7684\u5c5e\u6027\u76f8\u5bf9\u66f4\u4fbf\u5b9c\u3002\u8d44\u6e90\u5728\u8bbe\u8ba1\u4e0a\u4e5f\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u5b83\u4eec\u901a\u5e38\u4e0e\u9650\u5236\u4e00\u8d77\u4f20\u9012\u7ed9 TracerProvider\u3002\u8fd9\u4f7f\u5f97\u4e3a\u8d44\u6e90\u5b9e\u73b0\u5c5e\u6027\u9650\u5236\u53d8\u5f97\u5f88\u5c34\u5c2c\u3002</p> <p>\u5c5e\u4e8e\u5ea6\u91cf\u7684\u5c5e\u6027\u6b64\u65f6\u4e0d\u53d7\u4e0a\u8ff0\u9650\u5236\u7684\u9650\u5236\uff0c \u5982\u5ea6\u91cf\u5c5e\u6027\u9650\u5236\u4e2d\u6240\u8ff0\u3002</p>"},{"location":"docs/specs/otel/common/#_5","title":"\u5c5e\u6027\u96c6\u5408","text":"<p>Resources, Metrics data points, Spans, Span Events, Span Links and Log Records may contain a collection of attributes. The keys in each such collection are unique, i.e. there MUST NOT exist more than one key-value pair with the same key. The enforcement of uniqueness may be performed in a variety of ways as it best fits the limitations of the particular implementation.</p> <p>Normally for the telemetry generated using OpenTelemetry SDKs the attribute key-value pairs are set via an API that either accepts a single key-value pair or a collection of key-value pairs. Setting an attribute with the same key as an existing attribute SHOULD overwrite the existing attribute's value. See for example Span's SetAttribute API.</p> <p>A typical implementation of SetAttribute API will enforce the uniqueness by overwriting any existing attribute values pending to be exported, so that when the Span is eventually exported the exporters see only unique attributes. The OTLP format in particular requires that exported Resources, Spans, Metric data points and Log Records contain only unique attributes.</p> <p>Some other implementations may use a streaming approach where every SetAttribute API call immediately results in that individual attribute value being exported using a streaming wire protocol. In such cases the enforcement of uniqueness will likely be the responsibility of the recipient of this data.</p>"},{"location":"docs/specs/otel/common/attribute-naming/","title":"\u5c5e\u6027\u547d\u540d","text":"<p>Status: Stable</p> Table of Contents   - [Name Pluralization guidelines](#name-pluralization-guidelines) - [Name Reuse Prohibition](#name-reuse-prohibition) - [Recommendations for OpenTelemetry Authors](#recommendations-for-opentelemetry-authors) - [Recommendations for Application Developers](#recommendations-for-application-developers) - [otel.\\* Namespace](#otel-namespace)   <p>This section applies to Resource, Span, Log, and Metric attribute names (also known as the \"attribute keys\"). For brevity within this section when we use the term \"name\" without an adjective it is implied to mean \"attribute name\".</p> <p>Every name MUST be a valid Unicode sequence.</p> <p>Note: we merely require that the names are represented as Unicode sequences. This specification does not define how exactly the Unicode sequences are encoded. The encoding can vary from one programming language to another and from one wire format to another. Use the idiomatic way to represent Unicode in the particular programming language or wire format.</p> <p>Names SHOULD follow these rules:</p> <ul> <li> <p>Use namespacing to avoid name clashes. Delimit the namespaces using a dot   character. For example <code>service.version</code> denotes the service version where   <code>service</code> is the namespace and <code>version</code> is an attribute in that namespace.</p> </li> <li> <p>Namespaces can be nested. For example <code>telemetry.sdk</code> is a namespace inside   top-level <code>telemetry</code> namespace and <code>telemetry.sdk.name</code> is an attribute   inside <code>telemetry.sdk</code> namespace. Note: the fact that an entity is located   within another entity is typically not a sufficient reason for forming nested   namespaces. The purpose of a namespace is to avoid name clashes, not to   indicate entity hierarchies. This purpose should primarily drive the decision   about forming nested namespaces.</p> </li> <li> <p>For each multi-word dot-delimited component of the attribute name separate the   words by underscores (i.e. use snake_case). For example   <code>http.response.status_code</code> denotes the status code in the http namespace.</p> </li> <li> <p>Names SHOULD NOT coincide with namespaces. For example if   <code>service.instance.id</code> is an attribute name then it is no longer valid to have   an attribute named <code>service.instance</code> because <code>service.instance</code> is already a   namespace. Because of this rule be careful when choosing names: every existing   name prohibits existence of an equally named namespace in the future, and vice   versa: any existing namespace prohibits existence of an equally named   attribute key in the future.</p> </li> </ul>"},{"location":"docs/specs/otel/common/attribute-naming/#name-pluralization-guidelines","title":"Name Pluralization guidelines","text":"<ul> <li> <p>When an attribute represents a single entity, the attribute name SHOULD be   singular. Examples: <code>host.name</code>, <code>db.user</code>, <code>container.id</code>.</p> </li> <li> <p>When attribute can represent multiple entities, the attribute name SHOULD be   pluralized and the value type SHOULD be an array. E.g. <code>process.command_args</code>   might include multiple values: the executable name and command arguments.</p> </li> <li> <p>When an attribute represents a measurement,   Metric Name Pluralization Guidelines   SHOULD be followed for the attribute name.</p> </li> </ul>"},{"location":"docs/specs/otel/common/attribute-naming/#name-reuse-prohibition","title":"Name Reuse Prohibition","text":"<p>A new attribute MUST NOT be added with the same name as an attribute that existed in the past but was renamed (with a corresponding schema file).</p> <p>When introducing a new attribute name check all existing schema files to make sure the name does not appear as a key of any \"rename_attributes\" section (keys denote old attribute names in rename operations).</p>"},{"location":"docs/specs/otel/common/attribute-naming/#recommendations-for-opentelemetry-authors","title":"Recommendations for OpenTelemetry Authors","text":"<ul> <li> <p>All names that are part of OpenTelemetry semantic conventions SHOULD be part   of a namespace.</p> </li> <li> <p>When coming up with a new semantic convention make sure to check existing   namespaces for Resources,   Spans, and   Metrics to see if the new name   fits.</p> </li> <li> <p>When a new namespace is necessary consider whether it should be a top-level   namespace (e.g. <code>service</code>) or a nested namespace (e.g. <code>service.instance</code>).</p> </li> <li> <p>Semantic conventions exist for four areas: for Resource, Span, Log, and Metric   attribute names. In addition, for spans we have two more areas: Event and Link   attribute names. Identical namespaces or names in all these areas MUST have   identical meanings. For example the <code>http.request.method</code> span attribute name   denotes exactly the same concept as the <code>http.request.method</code> metric   attribute, has the same data type and the same set of possible values (in both   cases it records the value of the HTTP protocol's request method as a string).</p> </li> <li> <p>Semantic conventions MUST limit names to printable Basic Latin characters   (more precisely to   U+0021 .. U+007E   subset of Unicode code points). It is recommended to further limit names to   the following Unicode code points: Latin alphabet, Numeric, Underscore, Dot   (as namespace delimiter).</p> </li> </ul>"},{"location":"docs/specs/otel/common/attribute-naming/#recommendations-for-application-developers","title":"Recommendations for Application Developers","text":"<p>As an application developer when you need to record an attribute first consult existing semantic conventions for Resources, Spans, and Metrics. If an appropriate name does not exists you will need to come up with a new name. To do that consider a few options:</p> <ul> <li> <p>The name is specific to your company and may be possibly used outside the   company as well. To avoid clashes with names introduced by other companies (in   a distributed system that uses applications from multiple vendors) it is   recommended to prefix the new name by your company's reverse domain name, e.g.   <code>com.acme.shopname</code>.</p> </li> <li> <p>The name is specific to your application that will be used internally only. If   you already have an internal company process that helps you to ensure no name   clashes happen then feel free to follow it. Otherwise it is recommended to   prefix the attribute name by your application name, provided that the   application name is reasonably unique within your organization (e.g.   <code>myuniquemapapp.longitude</code> is likely fine). Make sure the application name   does not clash with an existing semantic convention namespace.</p> </li> <li> <p>The name may be generally applicable to applications in the industry. In that   case consider submitting a proposal to this specification to add a new name to   the semantic conventions, and if necessary also to add a new namespace.</p> </li> </ul> <p>It is recommended to limit names to printable Basic Latin characters (more precisely to U+0021 .. U+007E subset of Unicode code points).</p>"},{"location":"docs/specs/otel/common/attribute-naming/#otel-namespace","title":"otel.* Namespace","text":"<p>Attribute names that start with <code>otel.</code> are reserved to be defined by OpenTelemetry specification. These are typically used to express OpenTelemetry concepts in formats that don't have a corresponding concept.</p> <p>For example, the <code>otel.scope.name</code> attribute is used to record the instrumentation scope name, which is an OpenTelemetry concept that is natively represented in OTLP, but does not have an equivalent in other telemetry formats and protocols.</p> <p>Any additions to the <code>otel.*</code> namespace MUST be approved as part of OpenTelemetry specification.</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/","title":"\u8bed\u4e49\u7ea6\u5b9a\u7684\u5c5e\u6027\u9700\u6c42\u7ea7\u522b","text":"<p>Status: Stable</p> <p>This section applies to Log, Metric, Resource, and Span, and describes requirement levels for attributes defined in semantic conventions.</p> <p>Attribute requirement levels apply to the instrumentation library.</p> <p>The following attribute requirement levels are specified:</p> <ul> <li>\u8bed\u4e49\u7ea6\u5b9a\u7684\u5c5e\u6027\u9700\u6c42\u7ea7\u522b</li> <li>Required</li> <li>Conditionally Required</li> <li>Recommended</li> <li>Opt-In</li> <li>Performance suggestions</li> </ul> <p>The requirement level for an attribute is specified by semantic conventions depending on attribute availability across instrumented entities, performance, security, and other factors. When specifying requirement levels, a semantic convention MUST take into account signal-specific requirements.</p> <p>For example, Metric attributes that may have high cardinality can only be defined with <code>Opt-In</code> level.</p> <p>A semantic convention that refers to an attribute from another semantic convention MAY modify the requirement level within its own scope. Otherwise, requirement level from the referred semantic convention applies.</p> <p>For example, Database semantic convention references <code>network.transport</code> attribute defined in General attributes with <code>Conditionally Required</code> level on it.</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/#required","title":"Required","text":"<p>All instrumentations MUST populate the attribute. A semantic convention defining a Required attribute expects an absolute majority of instrumentation libraries and applications are able to efficiently retrieve and populate it, and can additionally meet requirements for cardinality, security, and any others specific to the signal defined by the convention. <code>http.request.method</code> is an example of a Required attribute.</p> <p>Note: Consumers of telemetry can detect if a telemetry item follows a specific semantic convention by checking for the presence of a <code>Required</code> attribute defined by such convention. For example, the presence of the <code>db.system</code> attribute on a span can be used as an indication that the span follows database semantics.</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/#conditionally-required","title":"Conditionally Required","text":"<p>All instrumentations MUST populate the attribute when the given condition is satisfied. The semantic convention of a <code>Conditionally Required</code> attribute MUST clarify the condition under which the attribute is to be populated.</p> <p><code>http.route</code> is an example of a conditionally required attribute that is populated when the instrumented HTTP framework provides route information for the instrumented request. Some low-level HTTP server implementations do not support routing and corresponding instrumentations can't populate the attribute.</p> <p>When a <code>Conditionally Required</code> attribute's condition is not satisfied, and there is no requirement to populate the attribute, semantic conventions MAY provide special instructions on how to handle it. If no instructions are given and if instrumentation can populate the attribute, instrumentation SHOULD use the <code>Opt-In</code> requirement level on the attribute.</p> <p>For example, <code>server.address</code> is <code>Conditionally Required</code> by the Database convention when available. When <code>server.socket.address</code> is available instead, instrumentation can do a DNS lookup, cache and populate <code>server.address</code>, but only if the user explicitly enables the instrumentation to do so, considering the performance issues that DNS lookups introduce.</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/#recommended","title":"Recommended","text":"<p>Instrumentations SHOULD add the attribute by default if it's readily available and can be efficiently populated. Instrumentations MAY offer a configuration option to disable Recommended attributes.</p> <p>Instrumentations that decide not to populate <code>Recommended</code> attributes due to performance, security, privacy, or other consideration by default, SHOULD allow for users to opt-in to emit them as defined for the <code>Opt-In</code> requirement level (if the attributes are logically applicable).</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/#opt-in","title":"Opt-In","text":"<p>Instrumentations SHOULD populate the attribute if and only if the user configures the instrumentation to do so. Instrumentation that doesn't support configuration MUST NOT populate <code>Opt-In</code> attributes.</p> <p>This attribute requirement level is recommended for attributes that are particularly expensive to retrieve or might pose a security or privacy risk. These should therefore only be enabled explicitly by a user making an informed decision.</p>"},{"location":"docs/specs/otel/common/attribute-requirement-level/#performance-suggestions","title":"Performance suggestions","text":"<p>Here are several examples of expensive operations to be avoided by default:</p> <ul> <li>DNS lookups to populate <code>server.address</code> when only an IP address is available   to the instrumentation. Caching lookup results does not solve the issue for   all possible cases and should be avoided by default too.</li> <li>forcing an <code>http.route</code> calculation before the HTTP framework calculates it</li> <li>reading response stream to find <code>http.response.body.size</code> when   <code>Content-Length</code> header is not available</li> </ul>"},{"location":"docs/specs/otel/common/attribute-type-mapping/","title":"\u5c06\u4efb\u610f\u6570\u636e\u6620\u5c04\u5230 OTLP AnyValue","text":"<p>Status: Experimental</p> <p>This document defines how to map (convert) arbitrary data (e.g. in-memory objects) to OTLP's AnyValue.</p> <p>The mapping is needed when OpenTelemetry needs to convert a value produced outside OpenTelemetry into a value that can be exported using an OTLP exporter, or otherwise be converted to be used inside OpenTelemetry boundaries. Example use cases are the following:</p> <ul> <li>In the Logging SDKs, to convert values received from logging   libraries into OpenTelemetry representation.</li> <li>In the Collector, to convert values received from various data sources into   pdata   internal representation.</li> </ul>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#converting-to-anyvalue","title":"Converting to AnyValue","text":"<p>AnyValue is capable of representing primitive and structured data of certain types.</p> <p>Implementations that have source data in any form, such as in-memory objects or data coming from other formats that needs to be converted to AnyValue SHOULD follow the rules described below.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#primitive-values","title":"Primitive Values","text":""},{"location":"docs/specs/otel/common/attribute-type-mapping/#integer-values","title":"Integer Values","text":"<p>Integer values which are within the range of 64 bit signed numbers [-263..263-1] SHOULD be converted to AnyValue's int_value field.</p> <p>Integer values which are outside the range of 64 bit signed numbers SHOULD be converted to AnyValue's string_value field using decimal representation.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#enumerations","title":"Enumerations","text":"<p>Values, which belong to a limited enumerated set (e.g. a Java enum), SHOULD be converted to AnyValue's string_value field with the value of the string set to the symbolic name of the enumeration.</p> <p>If it is not possible to obtain the symbolic name of the enumeration, the implementation SHOULD map enumeration values to AnyValue's int_value field set to the enum's ordinal value, when such an ordinal number is naturally obtainable.</p> <p>If it is also not possible to obtain the ordinal value, the enumeration SHOULD be converted to AnyValue's bytes_value field in any manner that the implementation deems reasonable.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#floating-point-values","title":"Floating Point Values","text":"<p>Floating point values which are within the range and precision of IEEE 754 64-bit floating point numbers (including IEEE 32-bit floating point values) SHOULD be converted to AnyValue's double_value field.</p> <p>Floating point values which are outside the range or precision of IEEE 754 64-bit floating point numbers (e.g. IEEE 128-bit floating bit values) SHOULD be converted to AnyValue's string_value field using decimal floating point representation.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#string-values","title":"String Values","text":"<p>String values which are valid UTF-8 sequences SHOULD be converted to AnyValue's string_value field.</p> <p>String values which are not valid Unicode sequences SHOULD be converted to AnyValue's bytes_value with the bytes representing the string in the original order and format of the source string.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#byte-sequences","title":"Byte Sequences","text":"<p>Byte sequences (e.g. Go's <code>[]byte</code> slice or raw byte content of a file) SHOULD be converted to AnyValue's bytes_value field.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#composite-values","title":"Composite Values","text":""},{"location":"docs/specs/otel/common/attribute-type-mapping/#array-values","title":"Array Values","text":"<p>Values that represent ordered sequences of other values (such as arrays, vectors, ordered lists, slices) SHOULD be converted to AnyValue's array_value field. String values and byte sequences are an exception from this rule (see above).</p> <p>The rules described in this document should be applied recursively to each element of the array.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#associative-arrays-with-unique-keys","title":"Associative Arrays With Unique Keys","text":"<p>Values that represent associative arrays with unique keys (also often known as maps, dictionaries or key-value stores) SHOULD be converted to AnyValue's kvlist_value field.</p> <p>If the keys of the source array are not strings, they MUST be converted to strings by any means available, often via a toString() or stringify functions available in programming languages. The conversion function MUST be chosen in a way that ensures that the resulting string keys are unique in the target array.</p> <p>The value part of each element of the source array SHOULD be converted to AnyValue recursively.</p> <p>For example a JSON object <code>{\"a\": 123, \"b\": \"def\"}</code> SHOULD be converted to</p> <pre><code>AnyValue{\n    kvlist_value:KeyValueList{\n        values:[\n            KeyValue{key:\"a\",value:AnyValue{int_value:123}},\n            KeyValue{key:\"b\",value:AnyValue{string_value:\"def\"}},\n        ]\n    }\n}\n</code></pre> <p>The rules described in this document should be applied recursively to each value of the associative array.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#associative-arrays-with-non-unique-keys","title":"Associative Arrays With Non-Unique Keys","text":"<p>Values that represent an associative arrays with non-unique keys where multiple values may be associated with the same key (also sometimes known as multimaps, multidicts) SHOULD be converted to AnyValue's kvlist_value field.</p> <p>The resulting kvlist_value field MUST list each key only once and the value of each element of kvlist_value field MUST be an array represented using AnyValue's array_value field, each element of the array_value representing one value of source array associated with the given key.</p> <p>For example an associative array shown in the following table:</p> Key Value \"abc\" 123 \"def\" \"foo\" \"def\" \"bar\" <p>SHOULD be converted to:</p> <pre><code>AnyValue{\n    kvlist_value:KeyValueList{\n        values:[\n            KeyValue{\n                key:\"abc\",\n                value:AnyValue{array_value:ArrayValue{values[\n                    AnyValue{int_value:123}\n                ]}}\n            },\n            KeyValue{\n                key:\"def\",\n                value:AnyValue{array_value:ArrayValue{values[\n                    AnyValue{string_value:\"foo\"},\n                    AnyValue{string_value:\"bar\"}\n                ]}}\n            },\n        ]\n    }\n}\n</code></pre> <p>The rules described in this document should be applied recursively to each value of the associative array.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#sets","title":"Sets","text":"<p>Unordered collections of unique values (such as Java Sets, C++ sets, Python Sets) SHOULD be converted to AnyValue's array_value field, where each element of the set becomes an element of the array.</p> <p>The rules described in this document should be applied recursively to each value of the set.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#other-values","title":"Other Values","text":"<p>Any other values not listed above SHOULD be converted to AnyValue's string_value field if the source data can be serialized to a string (can be stringified) using toString() or stringify functions available in programming languages.</p> <p>If the source data cannot be serialized to a string then the value SHOULD be converted AnyValue's bytes_value field by serializing it into a byte sequence by any means available.</p> <p>If the source data cannot be serialized neither to a string nor to a byte sequence then it SHOULD by converted to an empty AnyValue.</p>"},{"location":"docs/specs/otel/common/attribute-type-mapping/#empty-values","title":"Empty Values","text":"<p>If the source data has no type associated with it and is empty, null, nil or otherwise indicates absence of data it SHOULD be converted to an empty AnyValue, where all the fields are unset.</p> <p>Empty values which has a type associated with them (e.g. empty associative array) SHOULD be converted using the corresponding rules defined for the types above.</p>"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/","title":"OpenTelemetry \u8f6c\u6362\u5230\u975e otlp \u683c\u5f0f","text":"<p>Status: Stable</p> <p>All OpenTelemetry concepts and data recorded using OpenTelemetry API can be directly and precisely represented using corresponding messages and fields of OTLP format. However, for other formats this is not always the case. Sometimes a format will not have a native way to represent a particular OpenTelemetry concept or a field of a concept.</p> <p>This document defines the transformation between OpenTelemetry and formats other than OTLP, for OpenTelemetry fields and concepts that have no direct semantic equivalent in those other formats.</p> <p>Note: when a format has a direct semantic equivalent for a particular field or concept then the recommendation in this document MUST be ignored.</p> <p>See also additional specific transformation rules for Jaeger and Zipkin. The specific rules for Jaeger and Zipkin take precedence over the generic rules defined in this document.</p>"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#mappings","title":"Mappings","text":""},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#instrumentationscope","title":"InstrumentationScope","text":"<p>OpenTelemetry <code>InstrumentationScope</code>'s fields MUST be reported as key-value pairs associated with the Span, Metric Data Point or LogRecord using the following mapping:</p> Attribute Type Description Examples Requirement Level <code>otel.scope.name</code> string The name of the instrumentation scope - (<code>InstrumentationScope.Name</code> in OTLP). <code>io.opentelemetry.contrib.mongodb</code> Recommended <code>otel.scope.version</code> string The version of the instrumentation scope - (<code>InstrumentationScope.Version</code> in OTLP). <code>1.0.0</code> Recommended <p>The following deprecated aliases MUST also be reported with exact same values for backward compatibility reasons:</p> Attribute Type Description Examples Requirement Level <code>otel.library.name</code> string Deprecated, use the <code>otel.scope.name</code> attribute. <code>io.opentelemetry.contrib.mongodb</code> Recommended <code>otel.library.version</code> string Deprecated, use the <code>otel.scope.version</code> attribute. <code>1.0.0</code> Recommended"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#span-status","title":"Span Status","text":"<p>Span <code>Status</code> MUST be reported as key-value pairs associated with the Span, unless the <code>Status</code> is <code>UNSET</code>. In the latter case it MUST NOT be reported.</p> <p>The following table defines the OpenTelemetry <code>Status</code>'s mapping to Span's key-value pairs:</p> Attribute Type Description Examples Requirement Level <code>otel.status_code</code> string Name of the code, either \"OK\" or \"ERROR\". MUST NOT be set if the status code is UNSET. <code>OK</code> Recommended <code>otel.status_description</code> string Description of the Status if it has a value, otherwise not set. <code>resource not found</code> Recommended <p><code>otel.status_code</code> MUST be one of the following:</p> Value Description <code>OK</code> The operation has been validated by an Application developer or Operator to have completed successfully. <code>ERROR</code> The operation contains an error."},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#dropped-attributes-count","title":"Dropped Attributes Count","text":"<p>OpenTelemetry dropped attributes count MUST be reported as a key-value pair associated with the corresponding data entity (e.g. Span, Span Link, Span Event, Metric data point, LogRecord, etc). The key name MUST be <code>otel.dropped_attributes_count</code>.</p> <p>This key-value pair should only be recorded when it contains a non-zero value.</p>"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#dropped-events-count","title":"Dropped Events Count","text":"<p>OpenTelemetry Span's dropped events count MUST be reported as a key-value pair associated with the Span. The key name MUST be <code>otel.dropped_events_count</code>.</p> <p>This key-value pair should only be recorded when it contains a non-zero value.</p>"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#dropped-links-count","title":"Dropped Links Count","text":"<p>OpenTelemetry Span's dropped links count MUST be reported as a key-value pair associated with the Span. The key name MUST be <code>otel.dropped_links_count</code>.</p> <p>This key-value pair should only be recorded when it contains a non-zero value.</p>"},{"location":"docs/specs/otel/common/mapping-to-non-otlp/#instrumentation-scope-attributes","title":"Instrumentation Scope Attributes","text":"<p>Exporters to formats that don't have a concept that is equivalent to the Scope SHOULD record the attributes at the most suitable place in their corresponding format, typically at the Span, Metric or LogRecord equivalent.</p>"},{"location":"docs/specs/otel/common/url/","title":"URL \u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>Status: Experimental</p> <p>\u672c\u6587\u6863\u5b9a\u4e49\u4e86\u63cf\u8ff0 URL \u53ca\u5176\u7ec4\u4ef6\u7684\u8bed\u4e49\u7ea6\u5b9a\u3002</p>"},{"location":"docs/specs/otel/common/url/#attributes","title":"Attributes","text":"Attribute Type Description Examples Requirement Level <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>https</code>; <code>ftp</code>; <code>telnet</code> Recommended <code>url.full</code> string Absolute URL describing a network resource according to RFC3986 [1] <code>https://www.foo.bar/search?q=OpenTelemetry#SemConv</code>; <code>//localhost</code> Recommended <code>url.path</code> string The URI path component [2] <code>/search</code> Recommended <code>url.query</code> string The URI query component [3] <code>q=OpenTelemetry</code> Recommended <code>url.fragment</code> string The URI fragment component <code>SemConv</code> Recommended <p>[1]: For network calls, URL usually has <code>scheme://host[:port][path][?query][#fragment]</code> format, where the fragment is not transmitted over HTTP, but if it is known, it should be included nevertheless. <code>url.full</code> MUST NOT contain credentials passed via URL in form of <code>https://username:password@www.example.com/</code>. In such case username and password should be redacted and attribute's value should be <code>https://REDACTED:REDACTED@www.example.com/</code>. <code>url.full</code> SHOULD capture the absolute URL when it is available (or can be reconstructed) and SHOULD NOT be validated or modified except for sanitizing purposes.</p> <p>[2]: When missing, the value is assumed to be <code>/</code></p> <p>[3]: Sensitive content provided in query string SHOULD be scrubbed when instrumentations can identify it.</p>"},{"location":"docs/specs/otel/common/url/#_1","title":"\u654f\u611f\u4fe1\u606f","text":"<p>Capturing URL and its components MAY impose security risk. User and password information, when they are provided in User Information subcomponent, MUST NOT be recorded.</p> <p>Instrumentations that are aware of specific sensitive query string parameters MUST scrub their values before capturing <code>url.query</code> attribute. For example, native instrumentation of a client library that passes credentials or user location in URL, must scrub corresponding properties.</p> <p>Note: Applications and telemetry consumers should scrub sensitive information from URL attributes on collected telemetry. In systems unable to identify sensitive information, certain attribute values may be redacted entirely.</p>"},{"location":"docs/specs/otel/compatibility/","title":"\u517c\u5bb9","text":""},{"location":"docs/specs/otel/compatibility/logging_trace_context/","title":"\u4ee5\u975e OTLP \u65e5\u5fd7\u683c\u5f0f\u8ddf\u8e2a\u4e0a\u4e0b\u6587","text":"<p>Status: Experimental</p> Table of Contents   - [Overview](#overview)   - [Syslog RFC5424](#syslog-rfc5424)   - [Plain Text Formats](#plain-text-formats)   - [JSON Formats](#json-formats)   - [Other Structured Formats](#other-structured-formats)"},{"location":"docs/specs/otel/compatibility/logging_trace_context/#overview","title":"Overview","text":"<p>OTLP Logs Records have top level fields representing trace context. This document defines how trace context should be recorded in non-OTLP Log Formats. To summarize, the following field names should be used in legacy formats:</p> <ul> <li>\"trace_id\" for TraceId, hex-encoded.</li> <li>\"span_id\" for SpanId, hex-encoded.</li> <li>\"trace_flags\" for trace flags,   formatted according to W3C traceflags format.</li> </ul> <p>All 3 fields are optional (see the data model for details of which combination of fields is considered valid).</p>"},{"location":"docs/specs/otel/compatibility/logging_trace_context/#syslog-rfc5424","title":"Syslog RFC5424","text":"<p>Trace id, span id and traceflags SHOULD be recorded via SD-ID \"opentelemetry\".</p> <p>For example:</p> <pre><code>[opentelemetry trace_id=\"102981ABCD2901\" span_id=\"abcdef1010\" trace_flags=\"01\"]\n</code></pre>"},{"location":"docs/specs/otel/compatibility/logging_trace_context/#plain-text-formats","title":"Plain Text Formats","text":"<p>The fields SHOULD be recorded according to the customary approach used for a particular format (e.g. field:value format for LTSV). For example:</p> <pre><code>host:192.168.0.1&lt;TAB&gt;trace_id:102981ABCD2901&lt;TAB&gt;span_id:abcdef1010&lt;TAB&gt;time:[01/Jan/2010:10:11:23 -0400]&lt;TAB&gt;req:GET /health HTTP/1.0&lt;TAB&gt;status:200\n</code></pre>"},{"location":"docs/specs/otel/compatibility/logging_trace_context/#json-formats","title":"JSON Formats","text":"<p>The fields SHOULD be recorded as top-level fields in the JSON structure. For example:</p> <pre><code>{\n\"timestamp\": 1581385157.14429,\n\"body\": \"Incoming request\",\n\"trace_id\": \"102981ABCD2901\",\n\"span_id\": \"abcdef1010\"\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/logging_trace_context/#other-structured-formats","title":"Other Structured Formats","text":"<p>The fields SHOULD be recorded as top-level structured attributes of the log record as it is customary for the particular format.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/","title":"OpenCensus \u517c\u5bb9","text":"<p>Status: Stable, Unless otherwise specified.</p> <p>The OpenTelemetry project aims to provide backwards compatibility with the OpenCensus project in order to ease migration of instrumented codebases.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#migration-path","title":"Migration Path","text":""},{"location":"docs/specs/otel/compatibility/opencensus/#breaking-changes-when-migrating-to-opentelemetry","title":"Breaking Changes when migrating to OpenTelemetry","text":"<p>Migrating from OpenCensus to OpenTelemetry may require breaking changes to the telemetry produced because of:</p> <ul> <li>Different or new semantic conventions for names and attributes (e.g.   <code>grpc.io/server/server_latency</code>   vs   <code>rpc.server.duration</code>)</li> <li>Data model differences (e.g. OpenCensus supports   SumOfSquaredDeviations,   OTLP does not)</li> <li>Instrumentation API feature differences (e.g. OpenCensus supports   context-based attributes),   OTel does not)</li> <li>Differences between equivalent OC and OTel exporters (e.g. the OpenTelemetry   Prometheus exporter   adds type and unit suffixes;   OpenCensus   does not)</li> </ul> <p>This migration path groups most breaking changes into the installation of bridges. This gives users control over introducing the initial breaking change, and makes subsequent migrations of instrumentation (including in third-party libraries) non-breaking.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#migration-plans","title":"Migration Plans","text":""},{"location":"docs/specs/otel/compatibility/opencensus/#migrating-from-the-opencensus-agent-and-protocol","title":"Migrating from the OpenCensus Agent and Protocol","text":"<p>Starting with a deployment of the OC agent, using the OC protocol, migrate by:</p> <ol> <li>Deploy the OpenTelemetry collector with OpenCensus and OTLP receivers and    equivalent processors and exporters.</li> <li>breaking: For each workload sending the OC protocol, change to sending to    the OpenTelemetry collector's OpenCensus receiver.</li> <li>Remove the deployment of the OC Agent.</li> <li>For each workload, migrate the application from OpenCensus to OpenTelemetry,    following the guidance below, and use the OTLP exporter.</li> </ol>"},{"location":"docs/specs/otel/compatibility/opencensus/#migrating-an-application-using-bridges","title":"Migrating an Application using Bridges","text":"<p>Starting with an application using entirely OpenCensus instrumention for traces and metrics, it can be migrated by:</p> <ol> <li>Migrate the exporters (SDK)</li> <li>Install the OpenTelemetry SDK, with an equivalent exporter<ol> <li>If using an OpenCensus exporter, switch to using an OTLP exporter</li> </ol> </li> <li>Install equivalent OpenTelemetry resource detectors</li> <li>Install OpenTelemetry       <code>W3c TraceContext</code>       propagator, which is the equivalent of the OpenCensus' <code>TextFormat</code>       propagator.</li> <li>breaking: Install the metrics and trace bridges</li> <li>Remove initialization of OpenCensus exporters</li> <li>Migrate the instrumentation (API)</li> <li>breaking: For OpenCensus instrumentation packages, migrate to the       OpenTelemetry equivalent.<ol> <li>If migrating gRPC, enable the      <code>BinaryPropagation</code>      propagator if the language supports it. Otherwise, enable OpenCensus      <code>BinaryPropagation</code> on the OpenTelemetry gRPC instrumentation.</li> </ol> </li> <li>For external dependencies, wait for it to migrate to OpenTelemetry, and       update the dependency.</li> <li>For instrumentation which is part of the application, migrate it following       the \"library\" guidance below.</li> <li>Clean up: Remove the metrics and trace bridges</li> </ol>"},{"location":"docs/specs/otel/compatibility/opencensus/#migrating-libraries-using-oc-instrumentation","title":"Migrating Libraries using OC Instrumentation","text":""},{"location":"docs/specs/otel/compatibility/opencensus/#in-place-migration","title":"In-place Migration","text":"<p>Libraries which want a simple migration can choose to replace instrumentation in-place.</p> <p>Starting with a library using OpenCensus Instrumentation:</p> <ol> <li>Annouce to users the library's transition from OpenCensus to OpenTelemetry,    and recommend users adopt OC bridges.</li> <li>Change unit tests to use the OC bridges, and use OpenTelemetry unit testing    frameworks.</li> <li>After a notification period, migrate instrumentation line-by-line to    OpenTelemetry. The notification period should be long for popular libraries.</li> <li>Remove the OC bridge from unit tests.</li> </ol>"},{"location":"docs/specs/otel/compatibility/opencensus/#migration-via-config","title":"Migration via Config","text":"<p>Libraries which are eager to add native OpenTelemetry instrumentation sooner, and/or want to provide extended support for OpenCensus may choose to provide users the option to use OpenCensus instrumentation or OpenTelemetry instrumentation.</p> <p>Starting with a library using OpenCensus Instrumentation:</p> <ol> <li>Change unit tests to use the OC bridges, and use OpenTelemetry unit testing    frameworks.</li> <li>Add configuration allowing users to enable OpenTelemetry instrumentation and    disable OpenCensus instrumentation.</li> <li>Add OpenTelemetry instrumentation gated by the configuration, and tested    using the same sets of unit tests.</li> <li>After a notification period, switch to using OpenTelemetry instrumentation by    default.</li> <li>After a deprecation period, remove the option to use OpenCensus    instrumentation.</li> </ol>"},{"location":"docs/specs/otel/compatibility/opencensus/#trace-bridge","title":"Trace Bridge","text":"<p>The trace bridge is provided as a shim layer implementing the OpenCensus Trace API using the OpenTelemetry Trace API. This layer MUST NOT rely on implementation specific details of the OpenTelemetry SDK.</p> <p>More specifically, the intention is to allow OpenCensus instrumentation to be recorded using OpenTelemetry. This Shim Layer MUST NOT publicly expose any upstream OpenTelemetry API.</p> <p>The OpenCensus Shim and the OpenTelemetry API/SDK are expected to be consumed simultaneously in a running service, in order to ease migration from the former to the latter. It is expected that application owners will begin the migration process towards OpenTelemetry via the shim and adding new telemetry information via OpenTelemetry. Slowly, libraries and integrations will also migrate towards opentelemetry until the shim is no longer necessary.</p> <p>For example, an application may have traces today of the following variety:</p> <pre><code>|-- Application - Configured OpenCensus --------------------------------- |\n    |--  gRPC -&gt; Using OpenCensus to generate Trace A  --------- |\n      |--  Application -&gt; Using OpenCensus to generate a sub Trace B-- |\n</code></pre> <p>In this case, the application should be able to update its outer layer to OpenTelemetry, without having to wait for all downstream dependencies to have updated to OpenTelemetry (or deal with incompatibilities therein). The Application also doesn't need to rewrite any of its own instrumentation.</p> <pre><code>|-- Application - Configured Otel w/ OpenCensus Shim ------------------- |\n    |--  gRPC -&gt; Using OpenCensus to generate Trace A  --------- |\n      |--  Application -&gt; Using OpenCensus to generate a sub Trace B-- |\n</code></pre> <p>Next, the application can update its own instrumentation in a piecemeal fashion:</p> <pre><code>|-- Application - Configured Otel w/ OpenCensus Shim ---------------------- |\n    |--  gRPC -&gt; Using OpenCensus to generate Trace A  --------- |\n      |--  Application -&gt; Using OpenTelemetry to generate a sub Trace B-- |\n</code></pre> <p>This layer of Otel -&gt; OpenCensus -&gt; Otel tracing can be thought of as the \"OpenTelemetry sandwich\" problem, and is the key motivating factor for this specification.</p> <p>Finally, the Application would update all usages of OpenCensus to OpenTelemetry.</p> <pre><code>|-- Application - Configured Otel standalone ----------------------------- |\n    |--  gRPC -&gt; Using Otel to generate Trace A  --------- |\n      |--  Application -&gt; Using OpenTelemetry to generate a sub Trace B-- |\n</code></pre>"},{"location":"docs/specs/otel/compatibility/opencensus/#requirements","title":"Requirements","text":"<p>The OpenTelemetry&lt;-&gt;OpenCensus trace bridge has the following requirements:</p> <ul> <li>OpenCensus has no hard dependency on OpenTelemetry</li> <li>Minimal changes to OpenCensus for implementation</li> <li>Easy for users to use, ideally no change to their code</li> <li>Maintain parent-child span relationship between applications and libraries</li> <li>Maintain span link relationships between applications and libraries</li> <li>This component MUST be an optional dependency.</li> </ul>"},{"location":"docs/specs/otel/compatibility/opencensus/#creating-spans-in-opencensus","title":"Creating Spans in OpenCensus","text":"<p>When the shim is in place, all OpenCensus Spans MUST be sent through an OpenTelemetry <code>Tracer</code> as specified for the OpenTelemetry API.</p> <p>This mechanism SHOULD be seamless to the user in languages that allow discovery and auto injection of dependencies.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#methods-on-spans","title":"Methods on Spans","text":"<p>All specified methods in OpenCensus will delegate to the underlying <code>Span</code> of OpenTelemetry.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#resources","title":"Resources","text":"<p>Note: resources appear not to be usable in the \"API\" section of OpenCensus.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#known-incompatibilities","title":"Known Incompatibilities","text":"<p>Below are listed known incompatibilities between OpenTelemetry and OpenCensus specifications. Applications leveraging unspecified behavior from OpenCensus that is specified incompatibly within OpenTelemetry are not eligible for using the OpenCensus &lt;-&gt; OpenTelemetry bridge.</p> <ol> <li>In OpenCensus, there is    no specification    when parent spans can be specified on a child span. OpenTelemetry specifies    that    parent spans must be specified during span creation.    This leads to some issues with OpenCensus APIs that allowed flexible    specification of parent spans post-initialization.</li> <li>Links added to spans after the spans are created. This is    not supported in OpenTelemetry, therefore    OpenCensus spans that have links added to them after creation will be mapped    to OpenTelemetry spans without the links.</li> <li>OpenTelemetry specifies that samplers are    attached to an entire Trace provider while    OpenCensus allows custom samplers per span.</li> <li>TraceFlags in both OpenCensus and OpenTelemetry only specify the single    <code>sampled</code> flag (OpenTelemetry,    OpenCensus).    Some OpenCensus APIs support \"debug\" and \"defer\" tracing flags in addition to    \"sampled\". In this case, the OpenCensus bridge will do its best to support    and translate unspecified flags into the closest OpenTelemetry equivalent.</li> </ol>"},{"location":"docs/specs/otel/compatibility/opencensus/#opencensus-binary-context-propagation","title":"OpenCensus Binary Context Propagation","text":"<p>The shim will provide an OpenCensus <code>BinaryPropogator</code> implementation which maps OpenCenus binary trace context format to an OpenTelemetry SpanContext.</p> <p>This adapter MUST provide an implementation of OpenCensus <code>BinaryPropogator</code> to write OpenCensus binary format using OpenTelemetry's context. This implementation may be drawn from OpenCensus if applicable.</p> <p>The <code>BinaryPropagator</code> MUST be a TextMapPropagator if possible in the language. Otherwise, the <code>BinaryPropagator</code> MUST be a library, which is expected to be used by OpenTelemetry gRPC instrumentation. gRPC instrumentation in those languages SHOULD NOT enable the <code>BinaryPropagator</code> by default, but SHOULD provide configuration to allow users to enable it.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#metrics-stats","title":"Metrics / Stats","text":"<p>OpenTelemetry will provide an OpenCensus-Metrics-Shim component which implements the OpenTelemetry MetricProducer interface. When Produce() is invoked, the shim collects metrics from the OpenCensus global state, converts the metrics to an OpenTelemetry metrics batch, and returns.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#requirements_1","title":"Requirements","text":"<ul> <li>This component MUST be an optional dependency</li> <li>MUST NOT require OpenTelemetry to be included in OpenCensus API distributions</li> <li>SHOULD NOT require OpenCensus to depend on OpenTelemetry at runtime</li> <li>MUST require few or no changes to OpenCensus</li> <li>MUST be compatible with push and pull exporters</li> <li>MUST support Gauges, Counters, Cumulative Histograms, and Summaries</li> <li>Is NOT REQUIRED to support Gauge Histograms</li> <li>MUST support exemplar span context in language that provide utilities for   recording span context in exemplars</li> </ul>"},{"location":"docs/specs/otel/compatibility/opencensus/#resource","title":"Resource","text":"<p>The shim MUST discard the resource attached to OpenCensus metrics, and insert the resource provided during initialization, or fall back to the the default OpenTelemetry resource.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#instrumentation-scope","title":"Instrumentation Scope","text":"<p>The shim MUST add an instrumentation scope name and version which identifies the shim.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#usage","title":"Usage","text":"<p>The shim can be passed as an option to an OpenTelemetry MetricReader when configuring the OpenTelemetry SDK. This enables the bridge to work with both push and pull metric exporters.</p>"},{"location":"docs/specs/otel/compatibility/opencensus/#known-incompatibilities_1","title":"Known Incompatibilities","text":"<ul> <li>OpenTelemetry does not support OpenCensus' GaugeHistogram type; these metrics   MUST be dropped when using the bridge.</li> <li>OpenTelemetry does not currently support context-based attributes (tags).</li> <li>OpenTelemetry does not support OpenCensus' SumOfSquaredDeviation field; this   is dropped when using the bridge.</li> </ul>"},{"location":"docs/specs/otel/compatibility/opentracing/","title":"OpenTracing \u517c\u5bb9","text":"<p>Status: Stable.</p> Table of Contents  - [Abstract](#abstract) - [Language version support](#language-version-support) - [Create an OpenTracing Tracer Shim](#create-an-opentracing-tracer-shim) - [Tracer Shim](#tracer-shim)   - [Start a new Span](#start-a-new-span)   - [Inject](#inject)   - [Extract](#extract)   - [Close](#close) - [Span Shim and SpanContext Shim relationship](#span-shim-and-spancontext-shim-relationship) - [Span Shim](#span-shim)   - [Get Context](#get-context)   - [Get Baggage Item](#get-baggage-item)   - [Set Baggage Item](#set-baggage-item)   - [Set Tag](#set-tag)   - [Log](#log)   - [Finish](#finish) - [SpanContext Shim](#spancontext-shim)   - [Get Baggage Items](#get-baggage-items) - [ScopeManager Shim](#scopemanager-shim)   - [Activate a Span](#activate-a-span)   - [Get the active Span](#get-the-active-span) - [Span References](#span-references) - [In-process propagation exceptions](#in-process-propagation-exceptions)   - [Implicit and Explicit support mismatch](#implicit-and-explicit-support-mismatch)"},{"location":"docs/specs/otel/compatibility/opentracing/#abstract","title":"Abstract","text":"<p>The OpenTelemetry project aims to provide backwards compatibility with the OpenTracing project in order to ease migration of instrumented codebases.</p> <p>This functionality will be provided as a bridge layer implementing the OpenTracing API using the OpenTelemetry API. This layer MUST NOT rely on implementation specific details of any SDK.</p> <p>More specifically, the intention is to allow OpenTracing instrumentation to be recorded using OpenTelemetry. This Shim Layer MUST NOT publicly expose any upstream OpenTelemetry API.</p> <p>This functionality MUST be defined in its own OpenTracing Shim Layer, not in the OpenTracing nor the OpenTelemetry API or SDK.</p> <p>Semantic convention mapping SHOULD NOT be performed, with the exception of error mapping, as described in the Set Tag and Log sections.</p> <p>Consuming both the OpenTracing Shim and the OpenTelemetry API in the same codebase is not recommended for the following scenarios:</p> <ul> <li>If the OpenTracing-instrumented code consumes baggage, as the <code>Baggage</code> itself   may not be properly propagated. See   Span Shim and SpanContext Shim relationship.</li> <li>For languages with implicit in-process propagation support in   OpenTelemetry and none in OpenTracing (e.g. Javascript), as it breaks the   expected propagation semantics and may lead to incorrect <code>Context</code> usage and   incorrect traces. See   Implicit and Explicit support mismatch.</li> </ul>"},{"location":"docs/specs/otel/compatibility/opentracing/#language-version-support","title":"Language version support","text":"<p>Users are encouraged to check and update their language and runtime components before using the Shim layer, as the OpenTelemetry APIs and SDKs may have higher version requirements than their OpenTracing counterparts.</p> <p>For details, see the Language version support section of Migrating from OpenTracing.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#create-an-opentracing-tracer-shim","title":"Create an OpenTracing Tracer Shim","text":"<p>This operation is used to create a new OpenTracing <code>Tracer</code>:</p> <p>This operation MUST accept the following parameters:</p> <ul> <li>An OpenTelemetry <code>TracerProvider</code>. This operation MUST use this   <code>TracerProvider</code> to obtain a <code>Tracer</code> with the name <code>opentracing-shim</code> along   with the current shim library version.</li> <li>OpenTelemetry <code>Propagator</code>s to be used to perform injection and extraction for   the the OpenTracing <code>TextMap</code> and <code>HTTPHeaders</code> formats. If not specified, no   <code>Propagator</code> values will be stored in the Shim, and the global OpenTelemetry   <code>TextMap</code> propagator will be used for both OpenTracing <code>TextMap</code> and   <code>HTTPHeaders</code> formats.</li> </ul> <p>The API MUST return an OpenTracing <code>Tracer</code>.</p> <pre><code>// Create a Tracer Shim relying on the global propagators.\ncreateTracerShim(tracerProvider);\n// Create a Tracer Shim using:\n// 1) TraceContext propagator for TextMap\n// 2) Jaeger propagator for HttPHeaders.\ncreateTracerShim(tracerProvider, OTPropagatorsBuilder()\n.setTextMap(W3CTraceContextPropagator.getInstance())\n.setHttpHeaders(JaegerPropagator.getInstance())\n.build());\n</code></pre> <p>See OpenTracing Propagation Formats.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#tracer-shim","title":"Tracer Shim","text":""},{"location":"docs/specs/otel/compatibility/opentracing/#start-a-new-span","title":"Start a new Span","text":"<p>Parameters:</p> <ul> <li>The operation name, a string.</li> <li>An optional list of Span references.</li> <li>An optional list of tags.</li> <li>An optional explicit start timestamp, a numeric value.</li> </ul> <p>For OpenTracing languages implementing the ScopeManager interface, the following parameters are defined as well:</p> <ul> <li>An optional boolean specifying whether the current <code>Span</code> should be ignored as   automatic parent.</li> </ul> <p>If a list of <code>Span</code> references is specified, the first <code>SpanContext</code> with Child Of type in the entire list is used as parent, else the the first <code>SpanContext</code> is used as parent. All values in the list MUST be added as Links with the reference type value as a <code>Link</code> attribute, i.e. opentracing.ref_type set to <code>follows_from</code> or <code>child_of</code>.</p> <p>If a list of <code>Span</code> references is specified, the union of their <code>Baggage</code> values MUST be used as the initial <code>Baggage</code> of the newly created <code>Span</code>. It is unspecified which <code>Baggage</code> value is used in the case of repeated keys. If no such lisf of references is specified, the current <code>Baggage</code> MUST be used as the initial value of the newly created <code>Span</code>.</p> <p>If an initial set of tags is specified, the values MUST be set at the creation time of the OpenTelemetry <code>Span</code>, as opposed to setting them after the <code>Span</code> is already created. This is done in order to make those values available to any pre-<code>Span</code>-creation hook, e.g. the reference SDK performs a sampling step that consults <code>Span</code> information, including the initial tags/attributes, to decide whether to sample or not.</p> <p>If an initial set of tags is specified and the OpenTracing <code>error</code> tag is included, after the OpenTelemetry <code>Span</code> was created the Shim layer MUST perform the same error handling as described in the Set Tag operation.</p> <p>If an explicit start timestamp is specified, a conversion MUST be done to match the OpenTracing and OpenTelemetry units.</p> <p>The API MUST return an OpenTracing <code>Span</code>.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#inject","title":"Inject","text":"<p>Parameters:</p> <ul> <li>A <code>SpanContext</code>.</li> <li>A <code>Format</code> descriptor.</li> <li>A carrier.</li> </ul> <p>Inject the underlying OpenTelemetry <code>Span</code> and <code>Baggage</code> using either the explicitly registered or the global OpenTelemetry <code>Propagator</code>s, as configured at construction time.</p> <ul> <li><code>TextMap</code> and <code>HttpHeaders</code> formats MUST use their explicitly specified   <code>TextMapPropagator</code>, if any, or else use the global <code>TextMapPropagator</code>.</li> </ul> <p>It MUST inject any non-empty <code>Baggage</code> even amidst no valid <code>SpanContext</code>.</p> <p>Errors MAY be raised if the specified <code>Format</code> is not recognized, depending on the specific OpenTracing Language API (e.g. Go and Python do, but Java may not).</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#extract","title":"Extract","text":"<p>Parameters:</p> <ul> <li>A <code>Format</code> descriptor.</li> <li>A carrier.</li> </ul> <p>Extract the underlying OpenTelemetry <code>Span</code> and <code>Baggage</code> using either the explicitly registered or the global OpenTelemetry <code>Propagator</code>s, as configured at construction time.</p> <ul> <li><code>TextMap</code> and <code>HttpHeaders</code> formats MUST use their explicitly specified   <code>TextMapPropagator</code>, if any, or else use the global <code>TextMapPropagator</code>.</li> </ul> <p>If the extracted <code>SpanContext</code> is invalid AND the extracted <code>Baggage</code> is empty, this operation MUST return a null value, and otherwise it MUST return a <code>SpanContext</code> Shim instance with the extracted values.</p> <pre><code>if (!extractedSpanContext.isValid() &amp;&amp; extractedBaggage.isEmpty()) {\nreturn null;\n}\nreturn SpanContextShim(extractedSpanContext, extractedBaggage);\n</code></pre> <p>Errors MAY be raised if either the <code>Format</code> is not recognized or no value could be extracted, depending on the specific OpenTracing Language API (e.g. Go and Python do, but Java may not).</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#close","title":"Close","text":"<p>OPTIONAL operation. If this operation is implemented for a specific OpenTracing language, it MUST close the underlying <code>TracerProvider</code> if it implements a \"closeable\" interface or method; otherwise it MUST be defined as a no-op operation.</p> <p>The Shim layer MUST protect against errors or exceptions raised while closing the underlying <code>TracerProvider</code>.</p> <p>Note: Users are advised against calling this operation more than once per <code>TracerProvider</code> as it may have unexpected side effects, limitations or race conditions, e.g. a single Shim <code>Tracer</code> being closed multiple times or multiple Shim <code>Tracer</code> having their close operation being called.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#span-shim-and-spancontext-shim-relationship","title":"Span Shim and SpanContext Shim relationship","text":"<p>As per the OpenTracing Specification, the OpenTracing <code>SpanContext</code> Shim MUST contain <code>Baggage</code> data and it MUST be immutable.</p> <p>In turn, the OpenTracing <code>Span</code> Shim MUST contain a <code>SpanContext</code> Shim. When updating its associated baggage, the OpenTracing <code>Span</code> MUST set its OpenTracing <code>SpanContext</code> Shim to a new instance containing the updated <code>Baggage</code>.</p> <p>This is a simple graphical representation of the mentioned objects:</p> <pre><code>  Span Shim\n  +- OpenTelemetry Span (read-only)\n  +- SpanContext Shim\n        +- OpenTelemetry SpanContext (read-only)\n        +- OpenTelemetry Baggage (read-only)\n</code></pre> <p>The OpenTracing Shim properly performs in-process and inter-process propagation of the OpenTelemetry <code>Span</code> along its associated <code>Baggage</code> leveraging the hierarchy of objects shown above.</p> <p>As OpenTelemetry is not aware of this association, the related <code>Baggage</code> may not be properly propagated if the OpenTelemetry API is consumed along the OpenTracing Shim in the same codebase, as shown in the example below:</p> <pre><code>// methodOne consumes the OpenTelemetry API.\nvoid methodOne(Span span) {\ntry (Scope scope = span.makeCurrent()) {\nmethodTwo();\n}\n}\n// methodTwo consumes the OpenTracing Shim.\nvoid methodTwo() {\nio.opentracing.Span span = io.opentracing.util.GlobalTracer.get()\n.activeSpan();\n// Correctly set in the underlying io.opentelemetry.api.trace.Span\nspan.setTag(\"tag\", \"value\");\n// Value is set in the Shim layer -- it may not be later propagated\n// as OpenTelemetry is not aware of the Baggage associated\n// to this Span.\nspan.setBaggageItem(\"baggage\", \"item\");\n}\n</code></pre> <p>Operations accessing the associated <code>Baggage</code> MUST be safe to be called concurrently.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#span-shim","title":"Span Shim","text":"<p>The OpenTracing <code>Span</code> operations MUST be implemented using the underlying OpenTelemetry <code>Span</code> and <code>Baggage</code> values with the help of a <code>SpanContext</code> Shim object.</p> <p>The <code>Log</code> operations MUST be implemented using the OpenTelemetry <code>Span</code>'s <code>Add Events</code> operations.</p> <p>The <code>Set Tag</code> operations MUST be implemented using the OpenTelemetry <code>Span</code>'s <code>Set Attributes</code> operations.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#get-context","title":"Get Context","text":"<p>Returns the current <code>SpanContext</code> Shim.</p> <p>This operation MUST be safe to be called concurrently.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#get-baggage-item","title":"Get Baggage Item","text":"<p>Parameters:</p> <ul> <li>The baggage key, a string.</li> </ul> <p>Returns the value for the specified key in the OpenTelemetry <code>Baggage</code> of the current <code>SpanContext</code> Shim, or null if none exists.</p> <p>This operation MUST be safe to be called concurrently.</p> <pre><code>String getBaggageItem(String key) {\nsynchronized(this) {\n// Get the current SpanContext's Baggage.\nio.opentelemetry.baggage.Baggage baggage = this.spanContextShim.getBaggage();\n// Return the value for key.\nreturn baggage.getEntryValue(key);\n}\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/opentracing/#set-baggage-item","title":"Set Baggage Item","text":"<p>Parameters:</p> <ul> <li>The baggage key, a string.</li> <li>The baggage value, a string.</li> </ul> <p>Creates a new <code>SpanContext</code> Shim with a new OpenTelemetry <code>Baggage</code> containing the specified <code>Baggage</code> key/value pair, and sets it as the current instance for this <code>Span</code> Shim.</p> <p>This operation MUST be safe to be called concurrently.</p> <pre><code>void setBaggageItem(String key, String value) {\nsynchronized(this) {\n// Add value/key to the existing Baggage.\nBaggage newBaggage = this.spanContextShim.getBaggage().toBuilder()\n.put(key, value)\n.build();\n// Create a new SpanContext with the updated Baggage.\nSpanContextShim newSpanContextShim = this.spanContextShim\n.newWithBaggage(newBaggage);\n// Update our SpanContext instance.\nthis.spanContextShim = newSpanContextShim;\n}\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/opentracing/#set-tag","title":"Set Tag","text":"<p>Parameters:</p> <ul> <li>The tag key, a string.</li> <li>The tag value, which must be either a string, a boolean value, or a numeric   type.</li> </ul> <p>Calls <code>Set Attribute</code> on the underlying OpenTelemetry <code>Span</code> with the specified key/value pair.</p> <p>The <code>error</code> tag MUST be mapped to StatusCode:</p> <ul> <li><code>true</code> maps to <code>Error</code>.</li> <li><code>false</code> maps to <code>Ok</code>.</li> <li>no value being set maps to <code>Unset</code>.</li> </ul> <p>If the type of the specified value is not supported by the OTel API, the value MUST be converted to a string.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#log","title":"Log","text":"<p>Parameters:</p> <ul> <li>A set of key/value pairs, where keys must be strings, and the values may have   any type.</li> </ul> <p>Calls <code>Add Events</code> on the underlying OpenTelemetry <code>Span</code> with the specified key/value pair set.</p> <p>The <code>Add Event</code>'s <code>name</code> parameter MUST be the value with the <code>event</code> key in the pair set, or else fallback to use the <code>log</code> literal string.</p> <p>If pair set contains an <code>event=error</code> entry, the values MUST be mapped to an <code>Event</code> with the conventions outlined in the Exception semantic conventions document:</p> <ul> <li>If an entry with <code>error.object</code> key exists and the value is a   language-specific error object, a call to <code>RecordException(e)</code> is performed   along the rest of the specified key/value pair set as additional event   attributes.</li> <li>Else, a call to <code>AddEvent</code> is performed with <code>name</code> being set to <code>exception</code>,   along the specified key/value pair set as additional event attributes,   including mapping of the following key/value pairs:</li> <li><code>error.kind</code> maps to <code>exception.type</code>.</li> <li><code>message</code> maps to <code>exception.message</code>.</li> <li><code>stack</code> maps to <code>exception.stacktrace</code>.</li> </ul> <p>If an explicit timestamp is specified, a conversion MUST be done to match the OpenTracing and OpenTelemetry units.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#finish","title":"Finish","text":"<p>Calls <code>End</code> on the underlying OpenTelemetry <code>Span</code>.</p> <p>If an explicit timestamp is specified, a conversion MUST be done to match the OpenTracing and OpenTelemetry units.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#spancontext-shim","title":"SpanContext Shim","text":"<p><code>SpanContext</code> Shim MUST be immutable and MUST contain the associated <code>SpanContext</code> and <code>Baggage</code> values.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#get-baggage-items","title":"Get Baggage Items","text":"<p>Returns a dictionary, collection, or iterator (depending on the requirements of the OpenTracing API for a specific language) backed by the associated OpenTelemetry <code>Baggage</code> values.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#scopemanager-shim","title":"ScopeManager Shim","text":"<p>For OpenTracing languages implementing the <code>ScopeManager</code> interface, its operations MUST be implemented using the OpenTelemetry Context Propagation API in order to get and set active <code>Context</code> instances.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#activate-a-span","title":"Activate a Span","text":"<p>Parameters:</p> <ul> <li>A <code>Span</code>.</li> </ul> <p>Stores the <code>Span</code> Shim and its underlying <code>Span</code> and <code>Baggage</code> in a new <code>Context</code>, which is then set as the currently active instance.</p> <p>If the specified <code>Span</code> is null, it MUST be set to a NonRecordableSpan wrapping an invalid <code>SpanContext</code>, to signal there is no active <code>Span</code> nor <code>Baggage</code>.</p> <pre><code>Scope activate(Span span) {\nif (span == null) {\nspan = new SpanShim(io.opentelemetry.api.trace.Span.getInvalid());\n}\nSpanShim spanShim = (SpanShim)span;\n// Put the associated Span and Baggage in a new Context.\nContext context = Context.current()\n.withValue(spanShim)\n.withValue(spanShim.getSpan())\n.withValue(spanShim.getBaggage());\n// Set context as the current instance.\nreturn context.makeCurrent();\n}\n</code></pre> <p>Unsampled OpenTelemetry <code>Span</code>s can be perfectly activated, as they have valid <code>SpanContext</code>s (albeit with the <code>sampled</code> flag set to <code>false</code>):</p> <pre><code>// The underlying OpenTelemetry TracerProvider's Sampler\n// decided to NOT sample this Span, hence\n// io.opentelemetry.api.trace.Span.getSpanContext().isSampled() == false.\nSpan span = tracer.buildSpan(\"operationName\").start();\ntry (Scope scope = tracer.scopeManager().activate(span)) {\n// tracer.scopeManager().activeSpan() == span\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/opentracing/#get-the-active-span","title":"Get the active Span","text":"<p>Returns a <code>Span</code> Shim wrapping the currently active OpenTelemetry <code>Span</code>.</p> <p>This operation MUST immediately return null if the current OpenTelemetry <code>Span</code>'s <code>SpanContext</code> is invalid and the current <code>Baggage</code> is empty, to signal there is no active <code>Span</code> nor <code>Baggage</code>.</p> <p>If the current OpenTelemetry <code>Span</code>'s <code>SpanContext</code> is invalid but the current <code>Baggage</code> is not empty, this operation MUST return a new <code>Span</code> Shim containing a no-op OpenTelemetry <code>Span</code> and the non-empty <code>Baggage</code>.</p> <p>If there are matching OpenTelemetry <code>Span</code> and <code>Span</code> Shim objects in the current <code>Context</code>, the <code>Span</code> Shim MUST be returned. Else, a new <code>Span</code> Shim containing the current OpenTelemetry <code>Span</code> and <code>Baggage</code> MUST be returned.</p> <pre><code>Span active() {\nio.opentelemetry.api.trace.Span span = Span.fromContext(Context.current());\nio.opentelemetry.api.baggage.Baggage baggage = Baggage.fromContext(Context.current());\nSpanShim spanShim = SpanShim.fromContext(Context.current());\n// There is no actual currently active Span.\nif (!span.getSpanContext().isValid()) {\n// Immediately return null if there is no Baggage.\nif (baggage.isEmpty()) {\nreturn null;\n}\n// Else return a no-op Span with the Baggage.\nreturn SpanShim(baggage);\n}\n// Span was activated through the Shim layer, re-use it.\nif (spanShim != null &amp;&amp; spanShim.getSpan() == span) {\nreturn spanShim;\n}\n// Span was NOT activated through the Shim layer,\n// do a best effort with the current values.\nnew SpanShim(span, baggage);\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/opentracing/#span-references","title":"Span References","text":"<p>As defined in the OpenTracing Specification, a <code>Span</code> may reference zero or more other <code>SpanContext</code>s that are causally related. The reference information itself consists of a <code>SpanContext</code> and the reference type.</p> <p>OpenTracing defines two types of references:</p> <ul> <li>Child Of: The parent <code>Span</code> depends on the child <code>Span</code> in some capacity.</li> <li>Follows From: The parent <code>Span</code> does not depend in any way on the result   of their child <code>Span</code>s.</li> </ul> <p>OpenTelemetry does not define strict equivalent semantics for these references. These reference types must not be confused with the Link functionality. This information is however preserved as the <code>opentracing.ref_type</code> attribute.</p>"},{"location":"docs/specs/otel/compatibility/opentracing/#in-process-propagation-exceptions","title":"In process Propagation exceptions","text":""},{"location":"docs/specs/otel/compatibility/opentracing/#implicit-and-explicit-support-mismatch","title":"Implicit and Explicit support mismatch","text":"<p>For languages with implicit in-process propagation support in OpenTelemetry and none in OpenTracing (i.e. no ScopeManager support), the Shim MUST only use explicit context propagation in its operations (e.g. when starting a new <code>Span</code>). This is done to easily comply with the explicit-only propagation semantics of the OpenTracing API:</p> <pre><code>// Tracer Shim\nstartSpan(name: string, options: SpanOptions = {}): Span {\nconst otelSpanOptions = ...;\nif (!options.childOf &amp;&amp; !options.references) {\n// Do NOT get nor set the current Context/Span,\n// as it is part of the implicit propagation support.\notelSpanOptions.root = true;\n}\n...\n}\n</code></pre> <p>Using both the OpenTracing Shim and the OpenTelemetry API in the same codebase may result in traces using the incorrect parent <code>Span</code>, given the different implicit/explicit propagation expectations. For this case, the Shim MAY offer experimental integration with the OpenTelemetry implicit in-process propagation via an explicit setting, warning the user incorrect parent values may be consumed:</p> <pre><code>// Tracer Shim\nstartSpan(name: string, options: SpanOptions = {}): Span {\nconst otelSpanOptions = ...;\nif (!options.childOf &amp;&amp; !options.references) {\nif (otShimOptions.supportImplicitPropagation) {\n// Allow OpenTelemetry to consume the current Context\n// to fetch the parent Span.\notelSpanOptions.root = false;\n}\n...\n}\n...\n}\n</code></pre>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/","title":"\u666e\u7f57\u7c73\u4fee\u65af\u548c OpenMetrics \u517c\u5bb9\u6027","text":"<p>Status: Experimental</p> Table of Contents   - [Prometheus Metric points to OTLP](#prometheus-metric-points-to-otlp)   - [Metric Metadata](#metric-metadata)   - [Counters](#counters)   - [Gauges](#gauges)   - [Info](#info)   - [StateSet](#stateset)   - [Unknown-typed](#unknown-typed)   - [Histograms](#histograms)   - [Summaries](#summaries)   - [Dropped Types](#dropped-types)   - [Start Time](#start-time)   - [Exemplars](#exemplars)   - [Instrumentation Scope](#instrumentation-scope)   - [Resource Attributes](#resource-attributes) - [OTLP Metric points to Prometheus](#otlp-metric-points-to-prometheus)   - [Metric Metadata](#metric-metadata-1)   - [Instrumentation Scope](#instrumentation-scope-1)   - [Gauges](#gauges-1)   - [Sums](#sums)   - [Histograms](#histograms-1)   - [Exponential Histograms](#exponential-histograms)   - [Summaries](#summaries-1)   - [Metric Attributes](#metric-attributes)   - [Exemplars](#exemplars-1)   - [Resource Attributes](#resource-attributes-1)   <p>This section denotes how to convert metrics scraped in the Prometheus exposition or OpenMetrics formats to the OpenTelemetry metric data model and how to create Prometheus metrics from OpenTelemetry metric data. Since OpenMetrics has a superset of Prometheus' types, \"Prometheus\" is taken to mean \"Prometheus or OpenMetrics\". \"OpenMetrics\" refers to OpenMetrics-only concepts.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#prometheus-metric-points-to-otlp","title":"Prometheus Metric points to OTLP","text":""},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#metric-metadata","title":"Metric Metadata","text":"<p>The OpenMetrics MetricFamily Name MUST be added as the Name of the OTLP metric after the removal of unit and type suffixes described below.</p> <p>The OpenMetrics UNIT metadata, if present, MUST be converted to the unit of the OTLP metric. After trimming type-specific suffixes, such as <code>_total</code> for counters, the unit MUST be trimmed from the suffix as well, if the metric suffix matches the unit. The unit SHOULD be translated from Prometheus conventions to OpenTelemetry conventions by:</p> <ul> <li>Converting from full words to abbreviations (e.g. \"milliseconds\" to \"ms\").</li> <li>Special case: Converting \"ratio\" to \"1\".</li> <li>Converting \"foo_per_bar\" to \"foo/bar\".</li> </ul> <p>The OpenMetrics HELP metadata, if present, MUST be added as the description of the OTLP metric.</p> <p>The OpenMetrics TYPE metadata, if present, MUST be used to determine the OTLP data type, and dictates type-specific conversion rules listed below. Metric families without type metadata follow rules for unknown-typed metrics below.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#counters","title":"Counters","text":"<p>A Prometheus Counter MUST be converted to an OTLP Sum with <code>is_monotonic</code> equal to <code>true</code>. If the counter has a <code>_total</code> suffix, it MUST be removed.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#gauges","title":"Gauges","text":"<p>A Prometheus Gauge MUST be converted to an OTLP Gauge.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#info","title":"Info","text":"<p>An OpenMetrics Info metric MUST be converted to an OTLP Non-Monotonic Sum unless it is the target_info metric, which is used to populate resource attributes. An OpenMetrics Info can be thought of as a special-case of the OpenMetrics Gauge which has a value of 1, and whose labels generally stays constant over the life of the process. It is converted to a Non-Monotonic Sum, rather than a Gauge, because the value of 1 is intended to be viewed as a count, which should be summed together when aggregating away labels. If it has an <code>_info</code> suffix, the suffix MUST be removed from the metric name.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#stateset","title":"StateSet","text":"<p>An OpenMetrics StateSet metric MUST be converted to an OTLP Non-Monotonic Sum. An OpenMetrics StateSet can be thought of as a special-case of the OpenMetrics Gauge which has a 0 or 1 value, and has one metric point for every possible state. It is converted to a Non-Monotonic Sum, rather than a Gauge, because the value of 1 is intended to be viewed as a count, which should be summed together when aggregating away labels.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#unknown-typed","title":"Unknown-typed","text":"<p>A Prometheus Unknown MUST be converted to an OTLP Gauge.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#histograms","title":"Histograms","text":"<p>A Prometheus Histogram MUST be converted to an OTLP Histogram.</p> <p>Multiple Prometheus histogram metrics MUST be merged together into a single OTLP Histogram:</p> <ul> <li>The <code>le</code> label on the <code>_bucket</code>-suffixed metric is used to identify and order   histogram bucket boundaries. Each Prometheus line produces one bucket count on   the resulting histogram. Each value for the <code>le</code> label except <code>+Inf</code> produces   one bucket boundary.</li> <li>Lines with <code>_count</code> and <code>_sum</code> suffixes are used to determine the histogram's   count and sum.</li> <li>If <code>_count</code> is not present, the metric MUST be dropped.</li> <li>If <code>_sum</code> is not present, the histogram's sum MUST be unset.</li> </ul>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#summaries","title":"Summaries","text":"<p>Prometheus Summary MUST be converted to an OTLP Summary.</p> <p>Multiple Prometheus metrics are merged together into a single OTLP Summary:</p> <ul> <li>The <code>quantile</code> label on non-suffixed metrics is used to identify quantile   points in summary metrics. Each Prometheus line produces one quantile on the   resulting summary.</li> <li>Lines with <code>_count</code> and <code>_sum</code> suffixes are used to determine the summary's   count and sum.</li> <li>If <code>_count</code> is not present, the metric MUST be dropped.</li> <li>If <code>_sum</code> is not present, the summary's sum MUST be   set to zero.</li> </ul>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#dropped-types","title":"Dropped Types","text":"<p>The following Prometheus types MUST be dropped:</p> <ul> <li>OpenMetrics GaugeHistogram</li> </ul>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#start-time","title":"Start Time","text":"<p>Prometheus Cumulative metrics can include the start time using the <code>_created</code> metric as specified in OpenMetrics. When converting Prometheus Counters to OTLP, conversion SHOULD use <code>_created</code> where available. When no <code>_created</code> metric is available, conversion MUST follow Cumulative streams: handling unknown start time by default. Conversion MAY offer configuration, disabled by default, which allows using the <code>process_start_time_seconds</code> metric to provide the start time. Using <code>process_start_time_seconds</code> is only correct when all counters on the target start after the process and are not reset while the process is running.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#exemplars","title":"Exemplars","text":"<p>OpenMetrics Exemplars can be attached to Prometheus Histogram bucket metric points and counter metric points. Exemplars on histogram buckets SHOULD be converted to exemplars on OpenTelemetry histograms. Exemplars on counter metric points SHOULD be converted to exemplars on OpenTelemetry sums. If present, the timestamp MUST be added to the OpenTelemetry exemplar. The Trace ID and Span ID SHOULD be retrieved from the <code>trace_id</code> and <code>span_id</code> label keys, respectively. All labels not used for the trace and span ids MUST be added to the OpenTelemetry exemplar as attributes.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#instrumentation-scope","title":"Instrumentation Scope","text":"<p>Each <code>otel_scope_info</code> metric point present in a batch of metrics SHOULD be dropped from the incoming scrape, and converted to an instrumentation scope. The <code>otel_scope_name</code> and <code>otel_scope_version</code> labels, if present, MUST be converted to the Name and Version of the Instrumentation Scope. Additional labels MUST be added as scope attributes, with keys and values unaltered. Other metrics in the batch which have <code>otel_scope_name</code> and <code>otel_scope_version</code> labels that match an instrumentation scope MUST be placed within the matching instrumentation scope, and MUST remove those labels. For example, the OpenMetrics metrics:</p> <pre><code># TYPE otel_scope_info info\notel_scope_info{otel_scope_name=\"go.opentelemetry.io.contrib.instrumentation.net.http.otelhttp\",otel_scope_version=\"v0.24.0\",library_mascot=\"bear\"} 1\n# TYPE http_server_duration counter\nhttp_server_duration{otel_scope_name=\"go.opentelemetry.io.contrib.instrumentation.net.http.otelhttp\",otel_scope_version=\"v0.24.0\"...} 1\n</code></pre> <p>becomes:</p> <pre><code># within a resource_metrics\nscope_metrics:\nscope:\nname: go.opentelemetry.io.contrib.instrumentation.net.http.otelhttp\nversion: v0.24.0\nattributes:\nlibrary_mascot: bear\nmetrics:\n- name: http_server_duration\ndata:\nsum:\ndata_points:\n- value: 1\n</code></pre> <p>Metrics which are not found to be associated with an instrumentation scope MUST all be placed within an empty instrumentation scope, and MUST not have any labels removed.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#resource-attributes","title":"Resource Attributes","text":"<p>When scraping a Prometheus endpoint, resource attributes MUST be added to the scraped metrics to distinguish them from metrics from other Prometheus endpoints. In particular, <code>service.name</code> and <code>service.instance.id</code>, are needed to ensure Prometheus exporters can disambiguate metrics using <code>job</code> and <code>instance</code> labels as described below.</p> <p>The following attributes MUST be associated with scraped metrics as resource attributes, and MUST NOT be added as metric attributes:</p> OTLP Resource Attribute Description <code>service.name</code> The configured name of the service that the target belongs to <code>service.instance.id</code> A unique identifier of the target. By default, it should be the <code>&lt;host&gt;:&lt;port&gt;</code> of the scraped URL <p>The following attributes SHOULD be associated with scraped metrics as resource attributes, and MUST NOT be added as metric attributes:</p> OTLP Resource Attribute Description <code>server.address</code> The <code>&lt;host&gt;</code> portion of the target's URL that was scraped <code>server.port</code> The <code>&lt;port&gt;</code> portion of the target's URL that was scraped <code>url.scheme</code> <code>http</code> or <code>https</code> <p>In addition to the attributes above, the target_info metric is used to supply additional resource attributes. If present, target*info MUST be dropped from the batch of metrics, and all labels from the target_info metric MUST be converted to resource attributes attached to all other metrics which are part of the scrape. By default, label keys and values MUST NOT be altered (such as replacing <code>*</code>with<code>.</code> characters in keys).</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#otlp-metric-points-to-prometheus","title":"OTLP Metric points to Prometheus","text":""},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#metric-metadata_1","title":"Metric Metadata","text":"<p>Prometheus SDK exporters MUST NOT allow duplicate UNIT, HELP, or TYPE comments for the same metric name to be returned in a single scrape of the Prometheus endpoint. Exporters MUST drop entire metrics to prevent conflicting TYPE comments, but SHOULD NOT drop metric points as a result of conflicting UNIT or HELP comments. Instead, all but one of the conflicting UNIT and HELP comments (but not metric points) SHOULD be dropped. If dropping a comment or metric points, the exporter SHOULD warn the user through error logging.</p> <p>The Name of an OTLP metric MUST be added as the OpenMetrics MetricFamily Name, with unit and type suffixes added as described below. The metric name is required to match the regex: <code>[a-zA-Z_:]([a-zA-Z0-9_:])*</code>. Invalid characters in the metric name MUST be replaced with the <code>_</code> character. Multiple consecutive <code>_</code> characters MUST be replaced with a single <code>_</code> character.</p> <p>The Unit of an OTLP metric point SHOULD be converted to the equivalent unit in Prometheus when possible. This includes:</p> <ul> <li>Converting from abbreviations to full words (e.g. \"ms\" to \"milliseconds\").</li> <li>Dropping the portions of the Unit within brackets (e.g. {packets}). Brackets   MUST NOT be included in the resulting unit. A \"count of foo\" is considered   unitless in Prometheus.</li> <li>Special case: Converting \"1\" to \"ratio\".</li> <li>Converting \"foo/bar\" to \"foo_per_bar\".</li> </ul> <p>The resulting unit SHOULD be added to the metric as OpenMetrics UNIT metadata and as a suffix to the metric name unless the metric name already contains the unit, or the unit MUST be omitted. The unit suffix comes before any type-specific suffixes.</p> <p>The description of an OTLP metrics point MUST be added as OpenMetrics HELP metadata.</p> <p>The data point type of an OTLP metric MUST be added as OpenMetrics TYPE metadata. It also dictates type-specific conversion rules listed below.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#instrumentation-scope_1","title":"Instrumentation Scope","text":"<p>Prometheus exporters SHOULD generate an Info-typed metric named <code>otel_scope_info</code>. If present, Instrumentation Scope <code>name</code> and <code>version</code> MUST be added as <code>otel_scope_name</code> and <code>otel_scope_version</code> labels. Scope attributes MUST also be added as labels following the rules described in the <code>Metric Attributes</code> section below.</p> <p>Prometheus exporters MUST add the scope name as the <code>otel_scope_name</code> label and the scope version as the <code>otel_scope_version</code> label on all metric points by default, based on the scope the original data point was nested in.</p> <p>Prometheus exporters SHOULD provide a configuration option to disable the <code>otel_scope_info</code> metric and <code>otel_scope_</code> labels.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#gauges_1","title":"Gauges","text":"<p>An OpenTelemetry Gauge MUST be converted to a Prometheus Gauge.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#sums","title":"Sums","text":"<p>OpenTelemetry Sums follows this logic:</p> <ul> <li>If the aggregation temporality is cumulative and the sum is monotonic, it MUST   be converted to a Prometheus Counter.</li> <li>If the aggregation temporality is cumulative and the sum is non-monotonic, it   MUST be converted to a Prometheus Gauge.</li> <li>If the aggregation temporality is delta and the sum is monotonic, it SHOULD be   converted to a cumulative temporality and become a Prometheus Counter. The   following behaviors are expected:</li> <li>The new data point type must be the same as the accumulated data point type.</li> <li>The new data point's start time must match the time of the accumulated data     point. If not, see     detecting alignment issues.</li> <li>Otherwise, it MUST be dropped.</li> </ul> <p>Monotonic Sum metric points MUST have <code>_total</code> added as a suffix to the metric name. Monotonic Sum metric points with <code>StartTimeUnixNano</code> should export the <code>{name}_created</code> metric as well.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#histograms_1","title":"Histograms","text":"<p>An OpenTelemetry Histogram with a cumulative aggregation temporality MUST be converted to a Prometheus metric family with the following metrics:</p> <ul> <li>A single <code>{name}_count</code> metric denoting the count field of the histogram. All   attributes of the histogram point are converted to Prometheus labels.</li> <li><code>{name}_sum</code> metric denoting the sum field of the histogram, reported only if   the sum is positive and monotonic. The sum is positive and monotonic when all   buckets are positive. All attributes of the histogram point are converted to   Prometheus labels.</li> <li>A series of <code>{name}_bucket</code> metric points that contain all attributes of the   histogram point recorded as labels. Additionally, a label, denoted as <code>le</code> is   added denoting the bucket boundary. The label's value is the stringified   floating point value of bucket boundaries, ordered from lowest to highest. The   value of each point is the sum of the count of all histogram buckets up the   the boundary reported in the <code>le</code> label. These points will include a single   exemplar that falls within <code>le</code> label and no other <code>le</code> labelled point. The   final bucket metric MUST have an <code>+Inf</code> threshold.</li> <li>Histograms with <code>StartTimeUnixNano</code> set should export the <code>{name}_created</code>   metric as well.</li> </ul> <p>OpenTelemetry Histograms with Delta aggregation temporality SHOULD be aggregated into a Cumulative aggregation temporality and follow the logic above, or MUST be dropped.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#exponential-histograms","title":"Exponential Histograms","text":"<p>An OpenTelemetry Exponential Histogram with a cumulative aggregation temporality MUST be converted to a Prometheus Native Histogram as follows:</p> <ul> <li><code>Scale</code> is converted to the Native Histogram <code>Schema</code>. Currently,   valid values   for <code>schema</code> are -4 &lt;= n &lt;= 8. If <code>Scale</code> is &gt; 8 then Exponential Histogram   data points SHOULD be downscaled to a scale accepted by Prometheus (in range   [-4,8]). Any data point unable to be rescaled to an acceptable range MUST be   dropped.</li> <li><code>Count</code> is converted to Native Histogram <code>Count</code> if the <code>NoRecordedValue</code> flag   is set to <code>false</code>, otherwise, Native Histogram <code>Count</code> is set to the Stale NaN   value.</li> <li><code>Sum</code> is converted to the Native Histogram <code>Sum</code> if <code>Sum</code> is set and the   <code>NoRecordedValue</code> flag is set to <code>false</code>, otherwise, Native Histogram <code>Sum</code> is   set to the Stale NaN value.</li> <li><code>TimeUnixNano</code> is converted to the Native Histogram <code>Timestamp</code> after   converting nanoseconds to milliseconds.</li> <li><code>ZeroCount</code> is converted directly to the Native Histogram <code>ZeroCount</code>.</li> <li><code>ZeroThreshold</code>, if set, is converted to the Native Histogram <code>ZeroThreshold</code>.   Otherwise, it is set to the default value <code>1e-128</code>.</li> <li>The dense bucket layout represented by <code>Positive</code> bucket counts and <code>Offset</code>   is converted to the Native Histogram sparse layout represented by   <code>PositiveSpans</code> and <code>PositiveDeltas</code>. The same holds for the <code>Negative</code> bucket   counts and <code>Offset</code>. Note that Prometheus Native Histograms buckets are   indexed by upper boundary while Exponential Histograms are indexed by lower   boundary, the result being that the Offset fields are different-by-one.</li> <li><code>Min</code> and <code>Max</code> are not used.</li> <li><code>StartTimeUnixNano</code> is not used.</li> </ul> <p>OpenTelemetry Exponential Histogram metrics with the delta aggregation temporality are dropped.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#summaries_1","title":"Summaries","text":"<p>An OpenTelemetry Summary MUST be converted to a Prometheus metric family with the following metrics:</p> <ul> <li>A single <code>{name}_count</code> metric denoting the count field of the summary. All   attributes of the summary point are converted to Prometheus labels.</li> <li><code>{name}_sum</code> metric denoting the sum field of the summary, reported only if   the sum is positive and monotonic. All attributes of the summary point are   converted to Prometheus labels.</li> <li>A series of <code>{name}</code> metric points that contain all attributes of the summary   point recorded as labels. Additionally, a label, denoted as <code>quantile</code> is   added denoting a reported quantile point, and having its value be the   stringified floating point value of quantiles (between 0.0 and 1.0), starting   from lowest to highest, and all being non-negative. The value of each point is   the computed value of the quantile point.</li> <li>Summaries with <code>StartTimeUnixNano</code> set should export the <code>{name}_created</code>   metric as well.</li> </ul>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#metric-attributes","title":"Metric Attributes","text":"<p>OpenTelemetry Metric Attributes MUST be converted to Prometheus labels. String Attribute values are converted directly to Metric Attributes, and non-string Attribute values MUST be converted to string attributes following the attribute specification. Prometheus metric label keys are required to match the following regex: <code>[a-zA-Z_]([a-zA-Z0-9_])*</code>. Metrics from OpenTelemetry with unsupported Attribute names MUST replace invalid characters with the <code>_</code> character. Multiple consecutive <code>_</code> characters MUST be replaced with a single <code>_</code> character. This may cause ambiguity in scenarios where multiple similar-named attributes share invalid characters at the same location. In such unlikely cases, if multiple key-value pairs are converted to have the same Prometheus key, the values MUST be concatenated together, separated by <code>;</code>, and ordered by the lexicographical order of the original keys.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#exemplars_1","title":"Exemplars","text":"<p>Exemplars on OpenTelemetry Histograms and Monotonic Sums SHOULD be converted to OpenMetrics exemplars. Exemplars on other OpenTelemetry data points MUST be dropped. For Prometheus push exporters, multiple exemplars are able to be added to each bucket, so all exemplars SHOULD be converted. For Prometheus pull endpoints, only a single exemplar is able to be added to each bucket, so the largest exemplar from each bucket MUST be used, if attaching exemplars. If no exemplars exist on a bucket, the highest exemplar from a lower bucket MUST be used, even though it is a duplicate of another bucket's exemplar. OpenMetrics Exemplars MUST use the <code>trace_id</code> and <code>span_id</code> keys for the trace and span IDs, respectively. Timestamps MUST be added as timestamps on the OpenMetrics exemplar, and <code>filtered_attributes</code> MUST be added as labels on the OpenMetrics exemplar unless they would exceed the OpenMetrics limit on characters.</p>"},{"location":"docs/specs/otel/compatibility/prometheus_and_openmetrics/#resource-attributes_1","title":"Resource Attributes","text":"<p>In SDK Prometheus (pull) exporters, resource attributes SHOULD be converted to a single <code>target_info</code> metric; otherwise, they MUST be dropped, and MUST NOT be attached as labels to other metric families. The target_info metric MUST be an info-typed metric whose labels MUST include the resource attributes, and MUST NOT include any other labels. There MUST be at most one target_info metric exposed on an SDK Prometheus endpoint.</p> <p>In the Collector's Prometheus pull and push (remote-write) exporters, it is possible for metrics from multiple targets to be sent together, so targets must be disambiguated from one another. However, the Prometheus exposition format and remote-write formats do not include a notion of resource, and expect metric labels to distinguish scraped targets. By convention, <code>job</code> and <code>instance</code> labels distinguish targets and are expected to be present on metrics exposed on a Prometheus pull exporter (a \"federated\" Prometheus endpoint) or pushed via Prometheus remote-write. In OTLP, the <code>service.name</code>, <code>service.namespace</code>, and <code>service.instance.id</code> triplet is required to be unique, which makes them good candidates to use to construct <code>job</code> and <code>instance</code>. In the collector Prometheus exporters, the <code>service.name</code> and <code>service.namespace</code> attributes MUST be combined as <code>&lt;service.namespace&gt;/&lt;service.name&gt;</code>, or <code>&lt;service.name&gt;</code> if namespace is empty, to form the <code>job</code> metric label. The <code>service.instance.id</code> attribute, if present, MUST be converted to the <code>instance</code> label; otherwise, <code>instance</code> should be added with an empty value. Other resource attributes SHOULD be converted to a target_info metric, or MUST be dropped. The target_info metric is an info-typed metric whose labels MUST include the resource attributes, and MUST NOT include any other labels other than <code>job</code> and <code>instance</code>. There MUST be at most one target_info metric exported for each unique combination of <code>job</code> and <code>instance</code>.</p> <p>If info-typed metric families are not yet supported by the language Prometheus client library, a gauge-typed metric family named target_info with a constant value of 1 MUST be used instead.</p> <p>To convert OTLP resource attributes to Prometheus labels, string Attribute values are converted directly to labels, and non-string Attribute values MUST be converted to string attributes following the attribute specification.</p>"},{"location":"docs/specs/otel/configuration/","title":"\u914d\u7f6e","text":""},{"location":"docs/specs/otel/configuration/file-configuration/","title":"File configuration","text":""},{"location":"docs/specs/otel/configuration/file-configuration/#_1","title":"\u6587\u4ef6\u914d\u7f6e","text":"<p>Status: Experimental</p> <ul> <li>Overview</li> <li>Configuration Model</li> <li>Stability Definition</li> <li>Configuration file</li> <li>SDK Configuration</li> <li>In-Memory Configuration Model</li> <li>Operations</li> <li>References</li> </ul>"},{"location":"docs/specs/otel/configuration/file-configuration/#overview","title":"Overview","text":"<p>File configuration provides a mechanism for configuring OpenTelemetry which is more expressive and full-featured than the environment variable based scheme, and language agnostic in a way not possible with programmatic configuration.</p> <p>File configuration defines a Configuration Model, which can be expressed in a configuration file. Configuration files can be validated against the Configuration Schema, and interpreted to produce configured OpenTelemetry components.</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#configuration-model","title":"Configuration Model","text":"<p>The configuration model is defined in opentelemetry-configuration using the JSON Schema.</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#stability-definition","title":"Stability Definition","text":"<p>TODO: define stability guarantees and backwards compatibility</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#configuration-file","title":"Configuration file","text":"<p>A configuration file is a file representation of the Configuration Model.</p> <p>TODO: define acceptable file formats</p> <p>TODO: define environment variable substitution</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#sdk-configuration","title":"SDK Configuration","text":"<p>SDK configuration defines the interfaces and operations that SDKs are expected to expose to enable file based configuration.</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#in-memory-configuration-model","title":"In-Memory Configuration Model","text":"<p>SDKs SHOULD provide an in-memory representation of the Configuration Model. In general, SDKs are encouraged to provide this in-memory representation in a manner that is idiomatic for their language. If an SDK needs to expose a class or interface, the name <code>Configuration</code> is RECOMMENDED.</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#operations","title":"Operations","text":"<p>TODO: define how to parse configuration file to configuration model</p> <p>TODO: define how to apply configuration model to produce configured sdk components</p>"},{"location":"docs/specs/otel/configuration/file-configuration/#references","title":"References","text":"<ul> <li>Configuration proposal   (OTEP #225)</li> </ul>"},{"location":"docs/specs/otel/configuration/sdk-configuration/","title":"Sdk configuration","text":""},{"location":"docs/specs/otel/configuration/sdk-configuration/#sdk","title":"\u9ed8\u8ba4 SDK \u914d\u7f6e","text":"Table of Contents   - [Abstract](#abstract) - [Configuration Interface](#configuration-interface)   - [Programmatic](#programmatic)   - [Environment Variables](#environment-variables)   - [Configuration File](#configuration-file)   - [Other Mechanisms](#other-mechanisms)"},{"location":"docs/specs/otel/configuration/sdk-configuration/#abstract","title":"Abstract","text":"<p>The default Open Telemetry SDK (hereafter referred to as \"The SDK\") is highly configurable. This specification outlines the mechanisms by which the SDK can be configured. It does not attempt to specify the details of what can be configured.</p>"},{"location":"docs/specs/otel/configuration/sdk-configuration/#configuration-interface","title":"Configuration Interface","text":""},{"location":"docs/specs/otel/configuration/sdk-configuration/#programmatic","title":"Programmatic","text":"<p>The SDK MUST provide a programmatic interface for all configuration. This interface SHOULD be written in the language of the SDK itself. All other configuration mechanisms SHOULD be built on top of this interface.</p> <p>An example of this programmatic interface is accepting a well-defined struct on an SDK builder class. From that, one could build a CLI that accepts a file (YAML, JSON, TOML, ...) and then transforms into that well-defined struct consumable by the programmatic interface.</p>"},{"location":"docs/specs/otel/configuration/sdk-configuration/#environment-variables","title":"Environment Variables","text":"<p>See OpenTelemetry Environment Variable Specification.</p>"},{"location":"docs/specs/otel/configuration/sdk-configuration/#configuration-file","title":"Configuration File","text":"<p>See File Configuration.</p>"},{"location":"docs/specs/otel/configuration/sdk-configuration/#other-mechanisms","title":"Other Mechanisms","text":"<p>Additional configuration mechanisms SHOULD be provided in whatever language/format/style is idiomatic for the language of the SDK. The SDK can include as many configuration mechanisms as appropriate.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/","title":"Sdk environment variables","text":""},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#opentelemetry","title":"OpenTelemetry \u73af\u5883\u53d8\u91cf\u89c4\u8303","text":"<p>Status: Mixed</p> <p>The goal of this specification is to unify the environment variable names between different OpenTelemetry SDK implementations. SDKs MAY choose to allow configuration via the environment variables in this specification, but are not required to. If they do, they SHOULD use the names listed in this document.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#parsing-empty-value","title":"Parsing empty value","text":"<p>Status: Stable</p> <p>The SDK MUST interpret an empty value of an environment variable the same way as when the variable is unset.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#special-configuration-types","title":"Special configuration types","text":"<p>Status: Stable</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#boolean-value","title":"Boolean value","text":"<p>Any value that represents a Boolean MUST be set to true only by the case-insensitive string <code>\"true\"</code>, meaning <code>\"True\"</code> or <code>\"TRUE\"</code> are also accepted, as true. An SDK MUST NOT extend this definition and define additional values that are interpreted as true. Any value not explicitly defined here as a true value, including unset and empty values, MUST be interpreted as false. If any value other than a true value, case-insensitive string <code>\"false\"</code>, empty, or unset is used, a warning SHOULD be logged to inform users about the fallback to false being applied. All Boolean environment variables SHOULD be named and defined such that false is the expected safe default behavior. Renaming or changing the default value MUST NOT happen without a major version upgrade.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#numeric-value","title":"Numeric value","text":"<p>If an SDK chooses to support an integer-valued environment variable, it SHOULD support nonnegative values between 0 and 2\u00b3\u00b9 \u2212 1 (inclusive). Individual SDKs MAY choose to support a larger range of values.</p> <p>The following paragraph was added after stabilization and the requirements are thus qualified as \"SHOULD\" to allow SDKs to avoid breaking changes. For new implementations, these should be treated as MUST requirements.</p> <p>For variables accepting a numeric value, if the user provides a value the SDK cannot parse, or which is outside the valid range for the configuration item, the SDK SHOULD generate a warning and gracefully ignore the setting, i.e., treat them as not set. In particular, SDKs SHOULD NOT assign a custom interpretation e.g. to negative values if a negative value does not naturally apply to a configuration and does not have an explicitly specified meaning, but treat it like any other invalid value.</p> <p>For example, a value specifying a buffer size must naturally be non-negative. Treating a negative value as \"buffer everything\" would be an example of such a discouraged custom interpretation. Instead the default buffer size should be used.</p> <p>Note that this could make a difference even if the custom interpretation is identical with the default value, because it might reset a value set from other configuration sources with the default.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#enum-value","title":"Enum value","text":"<p>For variables which accept a known value out of a set, i.e., an enum value, SDK implementations MAY support additional values not listed here. For variables accepting an enum value, if the user provides a value the SDK does not recognize, the SDK MUST generate a warning and gracefully ignore the setting.</p> <p>If a null object (empty, no-op) value is acceptable, then the enum value representing it MUST be <code>\"none\"</code>.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#duration","title":"Duration","text":"<p>Any value that represents a duration, for example a timeout, MUST be an integer representing a number of milliseconds. The value is non-negative - if a negative value is provided, the SDK MUST generate a warning, gracefully ignore the setting and use the default value if it is defined.</p> <p>For example, the value <code>12000</code> indicates 12000 milliseconds, i.e., 12 seconds.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#general-sdk-configuration","title":"General SDK Configuration","text":"<p>Status: Stable</p> Name Description Default Notes OTEL_SDK_DISABLED Disable the SDK for all signals false Boolean value. If \"true\", a no-op SDK implementation will be used for all telemetry signals. Any other value or absence of the variable will have no effect and the SDK will remain enabled. This setting has no effect on propagators configured through the OTEL_PROPAGATORS variable. OTEL_RESOURCE_ATTRIBUTES Key-value pairs to be used as resource attributes See Resource semantic conventions for details. See Resource SDK for more details. OTEL_SERVICE_NAME Sets the value of the <code>service.name</code> resource attribute If <code>service.name</code> is also provided in <code>OTEL_RESOURCE_ATTRIBUTES</code>, then <code>OTEL_SERVICE_NAME</code> takes precedence. OTEL_LOG_LEVEL Log level used by the SDK logger \"info\" OTEL_PROPAGATORS Propagators to be used as a comma-separated list \"tracecontext,baggage\" Values MUST be deduplicated in order to register a <code>Propagator</code> only once. OTEL_TRACES_SAMPLER Sampler to be used for traces \"parentbased_always_on\" See Sampling OTEL_TRACES_SAMPLER_ARG String value to be used as the sampler argument The specified value will only be used if OTEL_TRACES_SAMPLER is set. Each Sampler type defines its own expected input, if any. Invalid or unrecognized input MUST be logged and MUST be otherwise ignored, i.e. the SDK MUST behave as if OTEL_TRACES_SAMPLER_ARG is not set. <p>Known values for <code>OTEL_PROPAGATORS</code> are:</p> <ul> <li><code>\"tracecontext\"</code>: W3C Trace Context</li> <li><code>\"baggage\"</code>: W3C Baggage</li> <li><code>\"b3\"</code>: B3 Single</li> <li><code>\"b3multi\"</code>: B3 Multi</li> <li><code>\"jaeger\"</code>:   Jaeger</li> <li><code>\"xray\"</code>:   AWS X-Ray   (third party)</li> <li><code>\"ottrace\"</code>:   OT Trace (third   party)</li> <li><code>\"none\"</code>: No automatically configured propagator.</li> </ul> <p>Known values for <code>OTEL_TRACES_SAMPLER</code> are:</p> <ul> <li><code>\"always_on\"</code>: <code>AlwaysOnSampler</code></li> <li><code>\"always_off\"</code>: <code>AlwaysOffSampler</code></li> <li><code>\"traceidratio\"</code>: <code>TraceIdRatioBased</code></li> <li><code>\"parentbased_always_on\"</code>: <code>ParentBased(root=AlwaysOnSampler)</code></li> <li><code>\"parentbased_always_off\"</code>: <code>ParentBased(root=AlwaysOffSampler)</code></li> <li><code>\"parentbased_traceidratio\"</code>: <code>ParentBased(root=TraceIdRatioBased)</code></li> <li><code>\"parentbased_jaeger_remote\"</code>: <code>ParentBased(root=JaegerRemoteSampler)</code></li> <li><code>\"jaeger_remote\"</code>: <code>JaegerRemoteSampler</code></li> <li><code>\"xray\"</code>:   AWS X-Ray Centralized Sampling   (third party)</li> </ul> <p>Depending on the value of <code>OTEL_TRACES_SAMPLER</code>, <code>OTEL_TRACES_SAMPLER_ARG</code> may be set as follows:</p> <ul> <li>For <code>traceidratio</code> and <code>parentbased_traceidratio</code> samplers: Sampling   probability, a number in the [0..1] range, e.g. \"0.25\". Default is 1.0 if   unset.</li> <li>For <code>jaeger_remote</code> and <code>parentbased_jaeger_remote</code>: The value is a comma   separated list:</li> <li><code>endpoint</code>: the endpoint in form of <code>scheme://host:port</code> of gRPC server that     serves the sampling strategy for the service     (sampling.proto).</li> <li><code>pollingIntervalMs</code>: in milliseconds indicating how often the sampler will     poll the backend for updates to sampling strategy.</li> <li><code>initialSamplingRate</code>: in the [0..1] range, which is used as the sampling     probability when the backend cannot be reached to retrieve a sampling     strategy. This value stops having an effect once a sampling strategy is     retrieved successfully, as the remote strategy will be used until a new     update is retrieved.</li> <li>Example:     <code>endpoint=http://localhost:14250,pollingIntervalMs=5000,initialSamplingRate=0.25</code></li> </ul>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#batch-span-processor","title":"Batch Span Processor","text":"<p>Status: Stable</p> Name Description Default Notes OTEL_BSP_SCHEDULE_DELAY Delay interval (in milliseconds) between two consecutive exports 5000 OTEL_BSP_EXPORT_TIMEOUT Maximum allowed time (in milliseconds) to export data 30000 OTEL_BSP_MAX_QUEUE_SIZE Maximum queue size 2048 OTEL_BSP_MAX_EXPORT_BATCH_SIZE Maximum batch size 512 Must be less than or equal to OTEL_BSP_MAX_QUEUE_SIZE"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#batch-logrecord-processor","title":"Batch LogRecord Processor","text":"<p>Status: Stable</p> Name Description Default Notes OTEL_BLRP_SCHEDULE_DELAY Delay interval (in milliseconds) between two consecutive exports 1000 OTEL_BLRP_EXPORT_TIMEOUT Maximum allowed time (in milliseconds) to export data 30000 OTEL_BLRP_MAX_QUEUE_SIZE Maximum queue size 2048 OTEL_BLRP_MAX_EXPORT_BATCH_SIZE Maximum batch size 512 Must be less than or equal to OTEL_BLRP_MAX_QUEUE_SIZE"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#attribute-limits","title":"Attribute Limits","text":"<p>SDKs SHOULD only offer environment variables for the types of attributes, for which that SDK implements truncation mechanism.</p> <p>See the SDK Attribute Limits section for the definition of the limits.</p> Name Description Default Notes OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT Maximum allowed attribute value size no limit OTEL_ATTRIBUTE_COUNT_LIMIT Maximum allowed span attribute count 128"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#span-limits","title":"Span Limits","text":"<p>Status: Stable</p> <p>See the SDK Span Limits section for the definition of the limits.</p> Name Description Default Notes OTEL_SPAN_ATTRIBUTE_VALUE_LENGTH_LIMIT Maximum allowed attribute value size no limit OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT Maximum allowed span attribute count 128 OTEL_SPAN_EVENT_COUNT_LIMIT Maximum allowed span event count 128 OTEL_SPAN_LINK_COUNT_LIMIT Maximum allowed span link count 128 OTEL_EVENT_ATTRIBUTE_COUNT_LIMIT Maximum allowed attribute per span event count 128 OTEL_LINK_ATTRIBUTE_COUNT_LIMIT Maximum allowed attribute per span link count 128"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#logrecord-limits","title":"LogRecord Limits","text":"<p>Status: Stable</p> <p>See the SDK LogRecord Limits section for the definition of the limits.</p> Name Description Default Notes OTEL_LOGRECORD_ATTRIBUTE_VALUE_LENGTH_LIMIT Maximum allowed attribute value size no limit OTEL_LOGRECORD_ATTRIBUTE_COUNT_LIMIT Maximum allowed log record attribute count 128"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#otlp-exporter","title":"OTLP Exporter","text":"<p>See OpenTelemetry Protocol Exporter Configuration Options.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#jaeger-exporter","title":"Jaeger Exporter","text":"<p>Status: Deprecated</p> <p>Jaeger exporter support will be removed from OpenTelemetry in July 2023.</p> <p>Note: Jaeger supports the OpenTelemetry protocol natively and most users should export to Jaeger using OTLP. These environment variables remain here only for backwards compatibility and will be removed in a future version. SDKs MAY include Jaeger exporters, but Jaeger export is not required.</p> <p>The <code>OTEL_EXPORTER_JAEGER_PROTOCOL</code> environment variable MAY by used to specify the transport protocol. The value MUST be one of:</p> <ul> <li><code>http/thrift.binary</code> - Thrift over HTTP</li> <li><code>grpc</code> - gRPC</li> <li><code>udp/thrift.compact</code> - Thrift with compact encoding over UDP</li> <li><code>udp/thrift.binary</code> - Thrift with binary encoding over UDP</li> </ul> <p>The default transport protocol SHOULD be <code>http/thrift.binary</code> unless SDKs have good reasons to choose other as the default (e.g. for backward compatibility reasons).</p> <p>Environment variables specific for the <code>http/thrift.binary</code> transport protocol:</p> Name Description Default OTEL_EXPORTER_JAEGER_ENDPOINT Full URL of the Jaeger HTTP endpoint <code>http://localhost:14268/api/traces</code> OTEL_EXPORTER_JAEGER_TIMEOUT Maximum time (in milliseconds) the Jaeger exporter will wait for each batch export 10000 OTEL_EXPORTER_JAEGER_USER Username to be used for HTTP basic authentication OTEL_EXPORTER_JAEGER_PASSWORD Password to be used for HTTP basic authentication <p>Environment variables specific for the <code>grpc</code> transport protocol:</p> Name Description Default OTEL_EXPORTER_JAEGER_ENDPOINT URL of the Jaeger gRPC endpoint <code>http://localhost:14250</code> OTEL_EXPORTER_JAEGER_TIMEOUT Maximum time (in milliseconds) the Jaeger exporter will wait for each batch export 10000 OTEL_EXPORTER_JAEGER_USER Username to be used for HTTP basic authentication OTEL_EXPORTER_JAEGER_PASSWORD Password to be used for HTTP basic authentication <p>Environment variables specific for the <code>udp/thrift.compact</code> transport protocol:</p> Name Description Default OTEL_EXPORTER_JAEGER_AGENT_HOST Hostname of the Jaeger agent <code>localhost</code> OTEL_EXPORTER_JAEGER_AGENT_PORT <code>udp/thrift.compact</code> port of the Jaeger agent <code>6831</code> <p>Environment variables specific for the <code>udp/thrift.binary</code> transport protocol:</p> Name Description Default OTEL_EXPORTER_JAEGER_AGENT_HOST Hostname of the Jaeger agent <code>localhost</code> OTEL_EXPORTER_JAEGER_AGENT_PORT <code>udp/thrift.binary</code> port of the Jaeger agent <code>6832</code>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#zipkin-exporter","title":"Zipkin Exporter","text":"<p>Status: Stable</p> Name Description Default OTEL_EXPORTER_ZIPKIN_ENDPOINT Endpoint for Zipkin traces <code>http://localhost:9411/api/v2/spans</code> OTEL_EXPORTER_ZIPKIN_TIMEOUT Maximum time (in milliseconds) the Zipkin exporter will wait for each batch export 10000 <p>Additionally, the following environment variables are reserved for future usage in Zipkin Exporter configuration:</p> <ul> <li><code>OTEL_EXPORTER_ZIPKIN_PROTOCOL</code></li> </ul> <p>This will be used to specify whether or not the exporter uses v1 or v2, json, thrift or protobuf. As of 1.0 of the specification, there is no specified default, or configuration via environment variables.</p>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#prometheus-exporter","title":"Prometheus Exporter","text":"<p>Status: Experimental</p> Name Description Default OTEL_EXPORTER_PROMETHEUS_HOST Host used by the Prometheus exporter \"localhost\" OTEL_EXPORTER_PROMETHEUS_PORT Port used by the Prometheus exporter 9464"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#exporter-selection","title":"Exporter Selection","text":"<p>Status: Stable</p> <p>We define environment variables for setting one or more exporters per signal.</p> Name Description Default OTEL_TRACES_EXPORTER Trace exporter to be used \"otlp\" OTEL_METRICS_EXPORTER Metrics exporter to be used \"otlp\" OTEL_LOGS_EXPORTER Logs exporter to be used \"otlp\" <p>The SDK MAY accept a comma-separated list to enable setting multiple exporters.</p> <p>Known values for <code>OTEL_TRACES_EXPORTER</code> are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"jaeger\"</code>: export in Jaeger data model</li> <li><code>\"zipkin\"</code>: Zipkin (Defaults to   protobuf   format)</li> <li><code>\"none\"</code>: No automatically configured exporter for traces.</li> </ul> <p>Known values for <code>OTEL_METRICS_EXPORTER</code> are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"prometheus\"</code>:   Prometheus</li> <li><code>\"none\"</code>: No automatically configured exporter for metrics.</li> </ul> <p>Known values for <code>OTEL_LOGS_EXPORTER</code> are:</p> <ul> <li><code>\"otlp\"</code>: OTLP</li> <li><code>\"none\"</code>: No automatically configured exporter for logs.</li> </ul>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#metrics-sdk-configuration","title":"Metrics SDK Configuration","text":"<p>Status: Mixed</p> Name Description Default Notes <code>OTEL_METRICS_EXEMPLAR_FILTER</code> Filter for which measurements can become Exemplars. <code>\"trace_based\"</code> <p>Known values for <code>OTEL_METRICS_EXEMPLAR_FILTER</code> are:</p> <ul> <li><code>\"always_on\"</code>: AlwaysOn</li> <li><code>\"always_off\"</code>: AlwaysOff</li> <li><code>\"trace_based\"</code>: TraceBased</li> </ul>"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#periodic-exporting-metricreader","title":"Periodic exporting MetricReader","text":"<p>Status: Stable</p> <p>Environment variables specific for the push metrics exporters (OTLP, stdout, in-memory) that use periodic exporting MetricReader.</p> Name Description Default Notes <code>OTEL_METRIC_EXPORT_INTERVAL</code> The time interval (in milliseconds) between the start of two export attempts. 60000 <code>OTEL_METRIC_EXPORT_TIMEOUT</code> Maximum allowed time (in milliseconds) to export data. 30000"},{"location":"docs/specs/otel/configuration/sdk-environment-variables/#language-specific-environment-variables","title":"Language Specific Environment Variables","text":"<p>To ensure consistent naming across projects, this specification recommends that language specific environment variables are formed using the following convention:</p> <pre><code>OTEL_{LANGUAGE}_{FEATURE}\n</code></pre>"},{"location":"docs/specs/otel/context/","title":"Index","text":""},{"location":"docs/specs/otel/context/#_1","title":"\u4e0a\u4e0b\u6587","text":"<p>Status: Stable, Feature-freeze.</p>"},{"location":"docs/specs/otel/context/#_2","title":"\u6982\u8ff0","text":"<p>A <code>Context</code> is a propagation mechanism which carries execution-scoped values across API boundaries and between logically associated execution units. Cross-cutting concerns access their data in-process using the same shared <code>Context</code> object.</p> <p>A <code>Context</code> MUST be immutable, and its write operations MUST result in the creation of a new <code>Context</code> containing the original values and the specified values updated.</p> <p>Languages are expected to use the single, widely used <code>Context</code> implementation if one exists for them. In the cases where an extremely clear, pre-existing option is not available, OpenTelemetry MUST provide its own <code>Context</code> implementation. Depending on the language, its usage may be either explicit or implicit.</p> <p>Users writing instrumentation in languages that use <code>Context</code> implicitly are discouraged from using the <code>Context</code> API directly. In those cases, users will manipulate <code>Context</code> through cross-cutting concerns APIs instead, in order to perform operations such as setting trace or baggage entries for a specified <code>Context</code>.</p> <p>A <code>Context</code> is expected to have the following operations, with their respective language differences:</p>"},{"location":"docs/specs/otel/context/#_3","title":"\u521b\u5efa\u5bc6\u94a5","text":"<p>Keys are used to allow cross-cutting concerns to control access to their local state. They are unique such that other libraries which may use the same context cannot accidentally use the same key. It is recommended that concerns mediate data access via an API, rather than provide direct public access to their keys.</p> <p>The API MUST accept the following parameter:</p> <ul> <li>The key name. The key name exists for debugging purposes and does not uniquely   identify the key. Multiple calls to <code>CreateKey</code> with the same name SHOULD NOT   return the same value unless language constraints dictate otherwise. Different   languages may impose different restrictions on the expected types, so this   parameter remains an implementation detail.</li> </ul> <p>The API MUST return an opaque object representing the newly created key.</p>"},{"location":"docs/specs/otel/context/#_4","title":"\u83b7\u5f97\u7684\u4ef7\u503c","text":"<p>Concerns can access their local state in the current execution state represented by a <code>Context</code>.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>The <code>Context</code>.</li> <li>The key.</li> </ul> <p>The API MUST return the value in the <code>Context</code> for the specified key.</p>"},{"location":"docs/specs/otel/context/#set-value","title":"Set value","text":"<p>Concerns can record their local state in the current execution state represented by a <code>Context</code>.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>The <code>Context</code>.</li> <li>The key.</li> <li>The value to be set.</li> </ul> <p>The API MUST return a new <code>Context</code> containing the new value.</p>"},{"location":"docs/specs/otel/context/#_5","title":"\u53ef\u9009\u7684\u5168\u5c40\u64cd\u4f5c","text":"<p>These operations are expected to only be implemented by languages using <code>Context</code> implicitly, and thus are optional. These operations SHOULD only be used to implement automatic scope switching and define higher level APIs by SDK components and OpenTelemetry instrumentation libraries.</p>"},{"location":"docs/specs/otel/context/#_6","title":"\u83b7\u53d6\u5f53\u524d\u4e0a\u4e0b\u6587","text":"<p>The API MUST return the <code>Context</code> associated with the caller's current execution unit.</p>"},{"location":"docs/specs/otel/context/#attach-context","title":"Attach Context","text":"<p>Associates a <code>Context</code> with the caller's current execution unit.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>The <code>Context</code>.</li> </ul> <p>The API MUST return a value that can be used as a <code>Token</code> to restore the previous <code>Context</code>.</p> <p>Note that every call to this operation should result in a corresponding call to Detach Context.</p>"},{"location":"docs/specs/otel/context/#detach-context","title":"Detach Context","text":"<p>Resets the <code>Context</code> associated with the caller's current execution unit to the value it had before attaching a specified <code>Context</code>.</p> <p>This operation is intended to help making sure the correct <code>Context</code> is associated with the caller's current execution unit. Users can rely on it to identify a wrong call order, i.e. trying to detach a <code>Context</code> that is not the current instance. In this case the operation can emit a signal to warn users of the wrong call order, such as logging an error or returning an error value.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>A <code>Token</code> that was returned by a previous call to attach a <code>Context</code>.</li> </ul> <p>The API MAY return a value used to check whether the operation was successful or not.</p>"},{"location":"docs/specs/otel/context/api-propagators/","title":"\u7e41\u6b96\u4e13\u5bb6 API","text":"<p>Status: Stable, Feature-Freeze</p> Table of Contents   - [Overview](#overview) - [Propagator Types](#propagator-types)   - [Carrier](#carrier)   - [Operations](#operations)     - [Inject](#inject)     - [Extract](#extract) - [TextMap Propagator](#textmap-propagator)   - [Fields](#fields)   - [TextMap Inject](#textmap-inject)     - [Setter argument](#setter-argument)       - [Set](#set)   - [TextMap Extract](#textmap-extract)     - [Getter argument](#getter-argument)       - [Keys](#keys)       - [Get](#get) - [Injectors and Extractors as Separate Interfaces](#injectors-and-extractors-as-separate-interfaces) - [Composite Propagator](#composite-propagator)   - [Create a Composite Propagator](#create-a-composite-propagator)   - [Composite Extract](#composite-extract)   - [Composite Inject](#composite-inject) - [Global Propagators](#global-propagators)   - [Get Global Propagator](#get-global-propagator)   - [Set Global Propagator](#set-global-propagator) - [Propagators Distribution](#propagators-distribution)   - [B3 Requirements](#b3-requirements)     - [B3 Extract](#b3-extract)     - [B3 Inject](#b3-inject)     - [Fields](#fields-1)     - [Configuration](#configuration)"},{"location":"docs/specs/otel/context/api-propagators/#overview","title":"Overview","text":"<p>Cross-cutting concerns send their state to the next process using <code>Propagator</code>s, which are defined as objects used to read and write context data to and from messages exchanged by the applications. Each concern creates a set of <code>Propagator</code>s for every supported <code>Propagator</code> type.</p> <p><code>Propagator</code>s leverage the <code>Context</code> to inject and extract data for each cross-cutting concern, such as traces and <code>Baggage</code>.</p> <p>Propagation is usually implemented via a cooperation of library-specific request interceptors and <code>Propagators</code>, where the interceptors detect incoming and outgoing requests and use the <code>Propagator</code>'s extract and inject operations respectively.</p> <p>The Propagators API is expected to be leveraged by users writing instrumentation libraries.</p>"},{"location":"docs/specs/otel/context/api-propagators/#propagator-types","title":"Propagator Types","text":"<p>A <code>Propagator</code> type defines the restrictions imposed by a specific transport and is bound to a data type, in order to propagate in-band context data across process boundaries.</p> <p>The Propagators API currently defines one <code>Propagator</code> type:</p> <ul> <li><code>TextMapPropagator</code> is a type that inject values into and extracts values from   carriers as string key/value pairs.</li> </ul> <p>A binary <code>Propagator</code> type will be added in the future (see #437).</p>"},{"location":"docs/specs/otel/context/api-propagators/#carrier","title":"Carrier","text":"<p>A carrier is the medium used by <code>Propagator</code>s to read values from and write values to. Each specific <code>Propagator</code> type defines its expected carrier type, such as a string map or a byte array.</p> <p>Carriers used at Inject are expected to be mutable.</p>"},{"location":"docs/specs/otel/context/api-propagators/#operations","title":"Operations","text":"<p><code>Propagator</code>s MUST define <code>Inject</code> and <code>Extract</code> operations, in order to write values to and read values from carriers respectively. Each <code>Propagator</code> type MUST define the specific carrier type and MAY define additional parameters.</p>"},{"location":"docs/specs/otel/context/api-propagators/#inject","title":"Inject","text":"<p>Injects the value into a carrier. For example, into the headers of an HTTP request.</p> <p>Required arguments:</p> <ul> <li>A <code>Context</code>. The Propagator MUST retrieve the appropriate value from the   <code>Context</code> first, such as <code>SpanContext</code>, <code>Baggage</code> or another cross-cutting   concern context.</li> <li>The carrier that holds the propagation fields. For example, an outgoing   message or HTTP request.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#extract","title":"Extract","text":"<p>Extracts the value from an incoming request. For example, from the headers of an HTTP request.</p> <p>If a value can not be parsed from the carrier, for a cross-cutting concern, the implementation MUST NOT throw an exception and MUST NOT store a new value in the <code>Context</code>, in order to preserve any previously existing valid value.</p> <p>Required arguments:</p> <ul> <li>A <code>Context</code>.</li> <li>The carrier that holds the propagation fields. For example, an incoming   message or HTTP request.</li> </ul> <p>Returns a new <code>Context</code> derived from the <code>Context</code> passed as argument, containing the extracted value, which can be a <code>SpanContext</code>, <code>Baggage</code> or another cross-cutting concern context.</p>"},{"location":"docs/specs/otel/context/api-propagators/#textmap-propagator","title":"TextMap Propagator","text":"<p><code>TextMapPropagator</code> performs the injection and extraction of a cross-cutting concern value as string key/values pairs into carriers that travel in-band across process boundaries.</p> <p>The carrier of propagated data on both the client (injector) and server (extractor) side is usually an HTTP request.</p> <p>In order to increase compatibility, the key/value pairs MUST only consist of US-ASCII characters that make up valid HTTP header fields as per RFC 7230.</p> <p><code>Getter</code> and <code>Setter</code> are optional helper components used for extraction and injection respectively, and are defined as separate objects from the carrier to avoid runtime allocations, by removing the need for additional interface-implementing-objects wrapping the carrier in order to access its contents.</p> <p><code>Getter</code> and <code>Setter</code> MUST be stateless and allowed to be saved as constants, in order to effectively avoid runtime allocations.</p>"},{"location":"docs/specs/otel/context/api-propagators/#fields","title":"Fields","text":"<p>The predefined propagation fields. If your carrier is reused, you should delete the fields here before calling Inject.</p> <p>Fields are defined as string keys identifying format-specific components in a carrier.</p> <p>For example, if the carrier is a single-use or immutable request object, you don't need to clear fields as they couldn't have been set before. If it is a mutable, retryable object, successive calls should clear these fields first.</p> <p>The use cases of this are:</p> <ul> <li>allow pre-allocation of fields, especially in systems like gRPC Metadata</li> <li>allow a single-pass over an iterator</li> </ul> <p>Returns list of fields that will be used by the <code>TextMapPropagator</code>.</p> <p>Observe that some <code>Propagator</code>s may define, besides the returned values, additional fields with variable names. To get a full list of fields for a specific carrier object, use the Keys operation.</p>"},{"location":"docs/specs/otel/context/api-propagators/#textmap-inject","title":"TextMap Inject","text":"<p>Injects the value into a carrier. The required arguments are the same as defined by the base Inject operation.</p> <p>Optional arguments:</p> <ul> <li>A <code>Setter</code> to set a propagation key/value pair. Propagators MAY invoke it   multiple times in order to set multiple pairs. This is an additional argument   that languages are free to define to help inject data into the carrier.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#setter-argument","title":"Setter argument","text":"<p>Setter is an argument in <code>Inject</code> that sets values into given fields.</p> <p><code>Setter</code> allows a <code>TextMapPropagator</code> to set propagated fields into a carrier.</p> <p>One of the ways to implement it is <code>Setter</code> class with <code>Set</code> method as described below.</p>"},{"location":"docs/specs/otel/context/api-propagators/#set","title":"Set","text":"<p>Replaces a propagated field with the given value.</p> <p>Required arguments:</p> <ul> <li>the carrier holding the propagation fields. For example, an outgoing message   or an HTTP request.</li> <li>the key of the field.</li> <li>the value of the field.</li> </ul> <p>The implementation SHOULD preserve casing (e.g. it should not transform <code>Content-Type</code> to <code>content-type</code>) if the used protocol is case insensitive, otherwise it MUST preserve casing.</p>"},{"location":"docs/specs/otel/context/api-propagators/#textmap-extract","title":"TextMap Extract","text":"<p>Extracts the value from an incoming request. The required arguments are the same as defined by the base Extract operation.</p> <p>Optional arguments:</p> <ul> <li>A <code>Getter</code> invoked for each propagation key to get. This is an additional   argument that languages are free to define to help extract data from the   carrier.</li> </ul> <p>Returns a new <code>Context</code> derived from the <code>Context</code> passed as argument.</p>"},{"location":"docs/specs/otel/context/api-propagators/#getter-argument","title":"Getter argument","text":"<p>Getter is an argument in <code>Extract</code> that get value from given field</p> <p><code>Getter</code> allows a <code>TextMapPropagator</code> to read propagated fields from a carrier.</p> <p>One of the ways to implement it is <code>Getter</code> class with <code>Get</code> and <code>Keys</code> methods as described below. Languages may decide on alternative implementations and expose corresponding methods as delegates or other ways.</p>"},{"location":"docs/specs/otel/context/api-propagators/#keys","title":"Keys","text":"<p>The <code>Keys</code> function MUST return the list of all the keys in the carrier.</p> <p>Required arguments:</p> <ul> <li>The carrier of the propagation fields, such as an HTTP request.</li> </ul> <p>The <code>Keys</code> function can be called by <code>Propagator</code>s which are using variable key names in order to iterate over all the keys in the specified carrier.</p> <p>For example, it can be used to detect all keys following the <code>uberctx-{user-defined-key}</code> pattern, as defined by the Jaeger Propagation Format.</p>"},{"location":"docs/specs/otel/context/api-propagators/#get","title":"Get","text":"<p>The Get function MUST return the first value of the given propagation key or return null if the key doesn't exist.</p> <p>Required arguments:</p> <ul> <li>the carrier of propagation fields, such as an HTTP request.</li> <li>the key of the field.</li> </ul> <p>The Get function is responsible for handling case sensitivity. If the getter is intended to work with a HTTP request object, the getter MUST be case insensitive.</p>"},{"location":"docs/specs/otel/context/api-propagators/#injectors-and-extractors-as-separate-interfaces","title":"Injectors and Extractors as Separate Interfaces","text":"<p>Languages can choose to implement a <code>Propagator</code> type as a single object exposing <code>Inject</code> and <code>Extract</code> methods, or they can opt to divide the responsibilities further into individual <code>Injector</code>s and <code>Extractor</code>s. A <code>Propagator</code> can be implemented by composing individual <code>Injector</code>s and <code>Extractors</code>.</p>"},{"location":"docs/specs/otel/context/api-propagators/#composite-propagator","title":"Composite Propagator","text":"<p>Implementations MUST offer a facility to group multiple <code>Propagator</code>s from different cross-cutting concerns in order to leverage them as a single entity.</p> <p>A composite propagator can be built from a list of propagators, or a list of injectors and extractors. The resulting composite <code>Propagator</code> will invoke the <code>Propagator</code>s, <code>Injector</code>s, or <code>Extractor</code>s, in the order they were specified.</p> <p>Each composite <code>Propagator</code> will implement a specific <code>Propagator</code> type, such as <code>TextMapPropagator</code>, as different <code>Propagator</code> types will likely operate on different data types.</p> <p>There MUST be functions to accomplish the following operations.</p> <ul> <li>Create a composite propagator</li> <li>Extract from a composite propagator</li> <li>Inject into a composite propagator</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#create-a-composite-propagator","title":"Create a Composite Propagator","text":"<p>Required arguments:</p> <ul> <li>A list of <code>Propagator</code>s or a list of <code>Injector</code>s and <code>Extractor</code>s.</li> </ul> <p>Returns a new composite <code>Propagator</code> with the specified <code>Propagator</code>s.</p>"},{"location":"docs/specs/otel/context/api-propagators/#composite-extract","title":"Composite Extract","text":"<p>Required arguments:</p> <ul> <li>A <code>Context</code>.</li> <li>The carrier that holds propagation fields.</li> </ul> <p>If the <code>TextMapPropagator</code>'s <code>Extract</code> implementation accepts the optional <code>Getter</code> argument, the following arguments are REQUIRED, otherwise they are OPTIONAL:</p> <ul> <li>The instance of <code>Getter</code> invoked for each propagation key to get.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#composite-inject","title":"Composite Inject","text":"<p>Required arguments:</p> <ul> <li>A <code>Context</code>.</li> <li>The carrier that holds propagation fields.</li> </ul> <p>If the <code>TextMapPropagator</code>'s <code>Inject</code> implementation accepts the optional <code>Setter</code> argument, the following arguments are REQUIRED, otherwise they are OPTIONAL:</p> <ul> <li>The <code>Setter</code> to set a propagation key/value pair. Propagators MAY invoke it   multiple times in order to set multiple pairs.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#global-propagators","title":"Global Propagators","text":"<p>The OpenTelemetry API MUST provide a way to obtain a propagator for each supported <code>Propagator</code> type. Instrumentation libraries SHOULD call propagators to extract and inject the context on all remote calls. Propagators, depending on the language, MAY be set up using various dependency injection techniques or available as global accessors.</p> <p>Note: It is a discouraged practice, but certain instrumentation libraries might use proprietary context propagation protocols or be hardcoded to use a specific one. In such cases, instrumentation libraries MAY choose not to use the API-provided propagators and instead hardcode the context extraction and injection logic.</p> <p>The OpenTelemetry API MUST use no-op propagators unless explicitly configured otherwise. Context propagation may be used for various telemetry signals - traces, metrics, logging and more. Therefore, context propagation MAY be enabled for any of them independently. For instance, a span exporter may be left unconfigured, although the trace context propagation was configured to enrich logs or metrics.</p> <p>Platforms such as ASP.NET may pre-configure out-of-the-box propagators. If pre-configured, <code>Propagator</code>s SHOULD default to a composite <code>Propagator</code> containing the W3C Trace Context Propagator and the Baggage <code>Propagator</code> specified in the Baggage API. These platforms MUST also allow pre-configured propagators to be disabled or overridden.</p>"},{"location":"docs/specs/otel/context/api-propagators/#get-global-propagator","title":"Get Global Propagator","text":"<p>This method MUST exist for each supported <code>Propagator</code> type.</p> <p>Returns a global <code>Propagator</code>. This usually will be composite instance.</p>"},{"location":"docs/specs/otel/context/api-propagators/#set-global-propagator","title":"Set Global Propagator","text":"<p>This method MUST exist for each supported <code>Propagator</code> type.</p> <p>Sets the global <code>Propagator</code> instance.</p> <p>Required parameters:</p> <ul> <li>A <code>Propagator</code>. This usually will be a composite instance.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#propagators-distribution","title":"Propagators Distribution","text":"<p>The official list of propagators that MUST be maintained by the OpenTelemetry organization and MUST be distributed as OpenTelemetry extension packages:</p> <ul> <li>W3C TraceContext. MAY alternatively be   distributed as part of the OpenTelemetry API.</li> <li>W3C Baggage. MAY alternatively be distributed   as part of the OpenTelemetry API.</li> <li>B3.</li> <li>Jaeger.</li> </ul> <p>This is a list of additional propagators that MAY be maintained and distributed as OpenTelemetry extension packages:</p> <ul> <li>OT Trace.   Propagation format used by the OpenTracing Basic Tracers. It MUST NOT use   <code>OpenTracing</code> in the resulting propagator name as it is not widely adopted   format in the OpenTracing ecosystem.</li> <li>OpenCensus BinaryFormat.   Propagation format used by OpenCensus, which describes how to format the span   context into the binary format, and does not prescribe a key. It is commonly   used with OpenCensus gRPC using the <code>grpc-trace-bin</code> propagation key.</li> </ul> <p>Additional <code>Propagator</code>s implementing vendor-specific protocols such as AWS X-Ray trace header protocol MUST NOT be maintained or distributed as part of the Core OpenTelemetry repositories.</p>"},{"location":"docs/specs/otel/context/api-propagators/#b3-requirements","title":"B3 Requirements","text":"<p>B3 has both single and multi-header encodings. It also has semantics that do not map directly to OpenTelemetry such as a debug trace flag, and allowing spans from both sides of request to share the same id. To maximize compatibility between OpenTelemetry and Zipkin implementations, the following guidelines have been established for B3 context propagation.</p>"},{"location":"docs/specs/otel/context/api-propagators/#b3-extract","title":"B3 Extract","text":"<p>When extracting B3, propagators:</p> <ul> <li>MUST attempt to extract B3 encoded using single and multi-header formats. The   single-header variant takes precedence over the multi-header version.</li> <li>MUST preserve a debug trace flag, if received, and propagate it with   subsequent requests. Additionally, an OpenTelemetry implementation MUST set   the sampled trace flag when the debug flag is set.</li> <li>MUST NOT reuse <code>X-B3-SpanId</code> as the id for the server-side span.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#b3-inject","title":"B3 Inject","text":"<p>When injecting B3, propagators:</p> <ul> <li>MUST default to injecting B3 using the single-header format</li> <li>MUST provide configuration to change the default injection format to B3   multi-header</li> <li>MUST NOT propagate <code>X-B3-ParentSpanId</code> as OpenTelemetry does not support   reusing the same id for both sides of a request.</li> </ul>"},{"location":"docs/specs/otel/context/api-propagators/#fields_1","title":"Fields","text":"<p>Fields MUST return the header names that correspond to the configured format, i.e., the headers used for the inject operation.</p>"},{"location":"docs/specs/otel/context/api-propagators/#configuration","title":"Configuration","text":"Option Extract Order Inject Format Specification B3 Single Single, Multi Single Link B3 Multi Single, Multi Multi Link"},{"location":"docs/specs/otel/logs/","title":"Index","text":""},{"location":"docs/specs/otel/logs/#opentelemetry","title":"OpenTelemetry \u65e5\u5fd7","text":"Table of Contents   - [Introduction](#introduction) - [Limitations of non-OpenTelemetry Solutions](#limitations-of-non-opentelemetry-solutions) - [OpenTelemetry Solution](#opentelemetry-solution) - [Log Correlation](#log-correlation) - [Legacy and Modern Log Sources](#legacy-and-modern-log-sources)   - [System Logs](#system-logs)   - [Infrastructure Logs](#infrastructure-logs)   - [Third-party Application Logs](#third-party-application-logs)   - [Legacy First-Party Applications Logs](#legacy-first-party-applications-logs)     - [Via File or Stdout Logs](#via-file-or-stdout-logs)     - [Direct to Collector](#direct-to-collector)   - [New First-Party Application Logs](#new-first-party-application-logs) - [OpenTelemetry Collector](#opentelemetry-collector) - [Auto-Instrumenting Existing Logging](#auto-instrumenting-existing-logging) - [Specifications](#specifications)"},{"location":"docs/specs/otel/logs/#introduction","title":"Introduction","text":"<p>Of all telemetry signals logs have probably the biggest legacy. Most programming languages have built-in logging capabilities or well-known, widely used logging libraries.</p> <p>For metrics and traces OpenTelemetry takes the approach of a clean-sheet design, specifies a new API and provides full implementations of this API in multiple languages.</p> <p>Our approach with logs is somewhat different. For OpenTelemetry to be successful in logging space we need to support existing legacy of logs and logging libraries, while offering improvements and better integration with the rest of observability world where possible.</p> <p>This is in essence the philosophy behind OpenTelemetry's logs support. We embrace existing logging solutions and make sure OpenTelemetry works nicely with existing logging libraries, log collection and processing solutions.</p>"},{"location":"docs/specs/otel/logs/#limitations-of-non-opentelemetry-solutions","title":"Limitations of non-OpenTelemetry Solutions","text":"<p>Unfortunately existing logging solutions are currently weakly integrated with the rest of the observability signals. Logs typically have limited support in tracing and monitoring tools in the form of links that use available and often incomplete correlation information (such as the time and origin attributes). This correlation may be fragile because attributes are often added to logs, traces and metrics via different means (e.g. using different collection agents). There is no standardized way to include the information about the origin and source of logs (such as the application and the location/infrastructure where the application runs) that is uniform with traces and metrics and allows all telemetry data to be fully correlated in a precise and robust manner.</p> <p>Similarly, logs have no standardized way to propagate and record the request execution context. In distributed systems this often results in a disjoint set of logs collected from different components of the system.</p> <p>This is how a typical non-OpenTelemetry observability collection pipeline looks like today:</p> <p></p> <p>There are often different libraries and different collection agents, using different protocols and data models, with telemetry data ending up in separate backends that don't know how to work well together.</p>"},{"location":"docs/specs/otel/logs/#opentelemetry-solution","title":"OpenTelemetry Solution","text":"<p>Distributed tracing introduced the notion of trace context propagation.</p> <p>Fundamentally, though, nothing prevents the logs to adopt the same context propagation concepts. If the recorded logs contained trace context identifiers (such as trace and span ids or user-defined baggage) it would result in much richer correlation between logs and traces, as well as correlation between logs emitted by different components of a distributed system. This would make logs significantly more valuable in distributed systems.</p> <p>This is one of the promising evolutionary directions for observability tools. Standardizing log correlation with traces and metrics, adding support for distributed context propagation for logs, unification of source attribution of logs, traces and metrics will increase the individual and combined value of observability information for legacy and modern systems. This is the vision of OpenTelemetry's collection of logs, traces and metrics:</p> <p></p> <p>We emit logs, traces and metrics in a way that is compliant with OpenTelemetry data models, send the data through OpenTelemetry Collector, where it can be enriched and processed in a uniform manner. For example, Collector can add to all telemetry data coming from a Kubernetes Pod several attributes that describe the pod and it can be done automatically using k8sprocessor without the need for the Application to do anything special. Most importantly such enrichment is completely uniform for all 3 signals. The Collector guarantees that logs, traces and metrics have precisely the same attribute names and values describing the Kubernetes Pod that they come from. This enables exact and unambiguous correlation of the signals by the Pod in the backend.</p> <p>For traces and metrics OpenTelemetry defines a new API that application developers must use to emit traces and metrics.</p> <p>For logs we did not take the same path. We realized that there is a much bigger and more diverse legacy in logging space. There are many existing logging libraries in different languages, each having their own API. Many programming languages have established standards for using particular logging libraries. For example in Java world there are several highly popular and widely used logging libraries, such as Log4j or Logback.</p> <p>There are also countless existing prebuilt applications or systems that emit logs in certain formats. Operators of such applications have no or limited control on how the logs are emitted. OpenTelemetry needs to support these logs.</p> <p>Given the above state of the logging space we took the following approach:</p> <ul> <li> <p>OpenTelemetry defines a log data model. The purpose of the   data model is to have a common understanding of what a LogRecord is, what data   needs to be recorded, transferred, stored and interpreted by a logging system.</p> </li> <li> <p>Newly designed logging systems are expected to emit logs according to   OpenTelemetry's log data model. More on this   later.</p> </li> <li> <p>Existing log formats can be unambiguously mapped to   OpenTelemetry log data model. OpenTelemetry Collector can read such logs and   translate them to OpenTelemetry log data model.</p> </li> <li> <p>OpenTelemetry defines a Logs Bridge API for   emitting LogRecords. Application   developers are NOT encouraged to call this API directly. It is provided for   library authors to build log appender,   which use the API to bridge between existing logging libraries and the   OpenTelemetry log data model. Existing logging libraries generally provide a   much richer set of features than what is defined in OpenTelemetry. It is NOT a   goal of OpenTelemetry to ship a feature-rich logging library.</p> </li> <li> <p>OpenTelemetry defines an SDK implementation of the   Bridge API, which enables configuration of   processing and   exporting LogRecords.</p> </li> </ul> <p>This approach allows OpenTelemetry to read existing system and application logs, provides a way for newly built application to emit rich, structured, OpenTelemetry-compliant logs, and ensures that all logs are eventually represented according to a uniform log data model on which the backends can operate.</p> <p>Later in this document we will discuss in more details how various log sources are handled by OpenTelemetry, but first we need to describe in more details an important concept: the log correlation.</p>"},{"location":"docs/specs/otel/logs/#log-correlation","title":"Log Correlation","text":"<p>Logs can be correlated with the rest of observability data in a few dimensions:</p> <ul> <li> <p>By the time of execution. Logs, traces and metrics can record the moment   of time or the range of time the execution took place. This is the most basic   form of correlation.</p> </li> <li> <p>By the execution context, also known as the trace context. It is a   standard practice to record the execution context (trace and span ids as well   as user-defined context) in the spans. OpenTelemetry extends this practice to   logs where possible by including TraceId and   SpanId in the LogRecords. This allows to   directly correlate logs and traces that correspond to the same execution   context. It also allows to correlate logs from different components of a   distributed system that participated in the particular request execution.</p> </li> <li> <p>By the origin of the telemetry, also known as the Resource context.   OpenTelemetry traces and metrics contain information about the Resource they   come from. We extend this practice to logs by including the   Resource in LogRecords.</p> </li> </ul> <p>These 3 correlations can be the foundation of powerful navigational, filtering, querying and analytical capabilities. OpenTelemetry aims to record and collects logs in a manner that enables such correlations.</p>"},{"location":"docs/specs/otel/logs/#legacy-and-modern-log-sources","title":"Legacy and Modern Log Sources","text":"<p>It is important to distinguish several sorts of legacy and modern log sources. Firstly, this directly affects how exactly we get access to these logs and how we collect them. Secondly, we have varying levels of control over how these logs are generated and whether we can amend the information that can be included in the logs.</p> <p>Below we list several categories of logs and describe what can be possibly done for each category to have better experience in the observability solutions.</p>"},{"location":"docs/specs/otel/logs/#system-logs","title":"System Logs","text":"<p>These are logs generated by the operating system and over which we have no control. We cannot change the format or affect what information is included. Examples of system format are Syslog and Windows Event Logs.</p> <p>System logs are written at the host level (which may be physical, virtual or containerized) and have a predefined format and content (note that applications may also be able to write records to standard system logs: this case is covered below in the Third-Party Applications section).</p> <p>System operations recorded in the logs can be a result of a request execution. However system logs either do not include any data about the trace context or if included it is highly idiosyncratic and thus difficult to identify, parse and use. This makes it nearly impossible to perform trace context correlation for system logs. However we can and should automatically enrich system logs with the resource context - the information about the host that is available during collection. This can include the host name, IP address, container or pod name, etc. This information should be added to the Resource field of collected log data.</p> <p>OpenTelemetry Collector can read system logs (link TBD) and automatically enrich them with Resource information using the resourcedetection processor.</p>"},{"location":"docs/specs/otel/logs/#infrastructure-logs","title":"Infrastructure Logs","text":"<p>These are logs generated by various infrastructure components, such as Kubernetes events (if you are wondering why events are discussed in the context of logs see Event API Overview). Like system logs, the infrastructure logs lack a trace context and can be enriched by the resource context - information about the node, pod, container, etc.</p> <p>OpenTelemetry Collector or other agents can be used to query logs from most common infrastructure controllers.</p>"},{"location":"docs/specs/otel/logs/#third-party-application-logs","title":"Third-party Application Logs","text":"<p>Applications typically write logs to standard output, to files or other specialized medium (e.g. Windows Event Logs for applications). These logs can be in many different formats, spanning a spectrum along these variations:</p> <ul> <li> <p>Free-form text formats with no easily automatable and reliable way to parse   structured data from them.</p> </li> <li> <p>Better specified and sometimes customizable formats that can be parsed to   extract structured data (such as Apache logs or RFC5424 Syslog).</p> </li> <li> <p>Formally structured formats (e.g. JSON files with well-defined schema or   Windows Event Log).</p> </li> </ul> <p>The collection system needs to be able to discover most commonly used applications and have parsers that can convert these logs into a structured format. Like system and infrastructure logs, application logs often lack request context but can be enriched by resource context, including the attributes that describe the host and infrastructure as well as application-level attributes (such as the application name, version, name of the database - if it is a DBMS, etc).</p> <p>OpenTelemetry recommends to collect application logs using Collector's filelog receiver. Alternatively, another log collection agent, such as FluentBit, can collect logs, then send to OpenTelemetry Collector where the logs can be further processed and enriched.</p>"},{"location":"docs/specs/otel/logs/#legacy-first-party-applications-logs","title":"Legacy First-Party Applications Logs","text":"<p>These are applications that are created in-house. People tasked with setting up log collection infrastructure sometimes are able to modify these applications to alter how logs are written and what information is included in the logs. For example, the application\u2019s log formatters may be reconfigured to output json instead of plain text and by doing so help improve the reliability of log collection.</p> <p>More significant modifications to these applications can be done manually by their developers, such as addition of the trace context to every log statement, however this is likely going to be vanishingly rare due to the effort required.</p> <p>As opposed to manual efforts we have an interesting opportunity to \"upgrade\" application logs in a less laborious way by providing full or semi auto-instrumenting solutions that modify trace logging libraries used by the application to automatically output the trace context such as the trace id or span id with every log statement. The trace context can be automatically extracted from incoming requests if standard compliant request propagation is used, e.g. via W3C TraceContext. In addition, the requests outgoing from the application may be injected with the same trace context data, thus resulting in context propagation through the application and creating an opportunity to have full trace context in logs collected from all applications that can be instrumented in this manner.</p> <p>Some logging libraries are designed to be extended in this manner relatively easily. There is no need to actually modify the libraries, instead we can implement \"log appender\" or \"log bridge\" components for such libraries and implement the additional LogRecord enrichment in these components.</p> <p>There are typically 2 ways to collect logs from these applications.</p>"},{"location":"docs/specs/otel/logs/#via-file-or-stdout-logs","title":"Via File or Stdout Logs","text":"<p>The first approach, assuming the logs are written to files or to standard output, requires ability to read file logs, tail them, work correctly when log rotation is used, optionally also parse the logs to convert them into more structured formats. Parings requires support for different parser types, which can also be configured to parse custom formats as well as ability to add custom parsers. Examples of common formats that parsers need to support are: CSV, Common Log Format, Labeled Tab-separated Values (LTSV), Key/Value Pair format, JSON, etc. To support this approach OpenTelemetry recommends to collect logs using OpenTelemetry Collector.</p> <p></p> <p>Alternatively, if the Collector does not have the necessary file reading and parsing capabilities, another log collection agent, such as FluentBit can collect the logs, then send the logs to OpenTelemetry Collector.</p> <p></p> <p>The benefit of using an intermediary medium is that how logs are produced and where they are written by the application requires no or minimal changes. The downside is that it requires the often non-trivial log file reading and parsing functionality. Parsing may also be not reliable if the output format is not well-defined. For details on recording and parsing trace context, see Trace Context in Non-OTLP Log Formats.</p>"},{"location":"docs/specs/otel/logs/#direct-to-collector","title":"Direct to Collector","text":"<p>The second approach is to modify the application so that the logs are output via a network protocol, e.g. via OTLP. The most convenient way to achieve this is to provide addons or extensions to the commonly used logging libraries. The addons implement sending over such network protocols, which would then typically require small, localized changes to the application code to change the logging target.</p> <p></p> <p>The application logs will be also enriched by the resource context, similarly to how it is done for third-party applications and so will potentially have full correlation information across all context dimensions.</p> <p>The downside of this approach is that the simplicity of having the logs in a local file is lost (e.g. ability to easily inspect the log file locally) and requires a full buy-in in OpenTelemetry's logging approach. This approach also only works if the destination that the logs need to be delivered is able to receive logs via the network protocol that OpenTelemetry can send in.</p> <p>The benefits of this approach is that it emits the logs in well-defined, formal, highly structured format, removes all complexity associated with file logs, such as parsers, log tailing and rotation. It also enables the possibility to send logs directly to the logging backend without using a log collection agent.</p> <p>To facilitate both approaches described above OpenTelemetry provides a Bridge API and SDK, which can be used together with existing logging libraries to automatically inject the trace context in the emitted logs, and provide an easy way to send the logs via OTLP. Instead of modifying each logging statement, log appenders use the API to bridge logs from existing logging libraries to the OpenTelemetry data model, where the SDK controls how the logs are processed and exported. Application developers only need to configure the Appender and SDK at application startup.</p>"},{"location":"docs/specs/otel/logs/#new-first-party-application-logs","title":"New First-Party Application Logs","text":"<p>These are greenfield developments. OpenTelemetry provides recommendations and best practices about how to emit logs (along with traces and metrics) from these applications. For applicable languages and frameworks the auto-instrumentation or simple configuration of a logging library to use an OpenTelemetry log appender will still be the easiest way to emit context-enriched logs. As already described earlier we provide extensions to some popular logging libraries languages to support the manual instrumentation cases. The extensions will support the inclusion of the trace context in the logs and allow to send logs using OTLP protocol to the backend or to the Collector, bypassing the need to have the logs represented as text files. Emitted logs are automatically augmented by application-specific resource context (e.g. process id, programming language, logging library name and version, etc). Full correlation across all context dimensions will be available for these logs.</p> <p>This is how a typical new application uses OpenTelemetry API, SDK and the existing log libraries:</p> <p></p>"},{"location":"docs/specs/otel/logs/#opentelemetry-collector","title":"OpenTelemetry Collector","text":"<p>To enable log collection according to this specification we use OpenTelemetry Collector.</p> <p>The following functionality exists to enable log collection:</p> <ul> <li> <p>Support for log data type and log pipelines based on the   log data model. This includes processors such as   attributesprocessor   that can operate on log data.</p> </li> <li> <p>Ability to read logs from text files, tail the files, understand common log   rotation schemes, watch directories for log file creation, ability to   checkpoint file positions and resume reading from checkpoints. This ability is   implemented by using Collector's   filelog receiver   or using an externally running agent (such as FluentBit).</p> </li> <li> <p>Ability to parse logs in common text formats and to allow end users to   customize parsing formats and add custom parsers as needed. Collector's   parsers   or parsing in the external agent is used for this.</p> </li> <li> <p>Ability to receive logs via common network protocols for logs, such as Syslog   and interpret them according to semantic conventions defined in this   specification. FluentBit or similar agent is used for this. Over time some of   this functionality may be migrated directly to the Collector.</p> </li> <li> <p>Ability to send logs via common network protocols for logs, such as Syslog, or   vendor-specific log formats. Collector contains exporters that directly   implement this ability.</p> </li> </ul>"},{"location":"docs/specs/otel/logs/#auto-instrumenting-existing-logging","title":"Auto-Instrumenting Existing Logging","text":"<p>We can provide auto-instrumentation for most popular logging libraries. The auto-instrumented logging statements will do the following:</p> <ul> <li> <p>Read incoming trace context (this is part of broader instrumentation that   auto-instrumenting libraries perform).</p> </li> <li> <p>Configure logging libraries to use trace id and span id fields from request   context as logging context and automatically include them in all logged   statements.</p> </li> </ul> <p>This is possible to do for certain languages (e.g. in Java) and we can reuse existing open-source libraries that do this.</p> <p>A further optional modification would be to auto-instrument loggers to send logs directly to the backend via OTLP instead or in addition to writing to a file or standard output.</p>"},{"location":"docs/specs/otel/logs/#specifications","title":"Specifications","text":"<ul> <li>Logs Bridge API</li> <li>Logs SDK</li> <li>Logs Data Model</li> <li>Event API</li> <li>Semantic Conventions</li> <li>Trace Context in non-OTLP Log Formats</li> </ul>"},{"location":"docs/specs/otel/logs/bridge-api/","title":"\u65e5\u5fd7\u6865 API","text":"<p>Status: Stable</p> <p>Note: this document defines a log backend API. The API is not intended to be called by application developers directly. It is provided for logging library authors to build log appenders, which use this API to bridge between existing logging libraries and the OpenTelemetry log data model.</p> <p>The Logs Bridge API consist of these main classes:</p> <ul> <li>LoggerProvider is the entry point of the API. It provides   access to <code>Logger</code>s.</li> <li>Logger is the class responsible for emitting logs as   LogRecords.</li> </ul> <pre><code>graph TD\n    A[LoggerProvider] --&gt;|Get| B(Logger)\n    B --&gt;|Emit| C(LogRecord)</code></pre>"},{"location":"docs/specs/otel/logs/bridge-api/#loggerprovider","title":"LoggerProvider","text":"<p><code>Logger</code>s can be accessed with a <code>LoggerProvider</code>.</p> <p>In implementations of the API, the <code>LoggerProvider</code> is expected to be the stateful object that holds any configuration.</p> <p>Normally, the <code>LoggerProvider</code> is expected to be accessed from a central place. Thus, the API SHOULD provide a way to set/register and access a global default <code>LoggerProvider</code>.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#loggerprovider-operations","title":"LoggerProvider operations","text":"<p>The <code>LoggerProvider</code> MUST provide the following functions:</p> <ul> <li>Get a <code>Logger</code></li> </ul>"},{"location":"docs/specs/otel/logs/bridge-api/#get-a-logger","title":"Get a Logger","text":"<p>This API MUST accept the following parameters:</p> <ul> <li> <p><code>name</code>: This name uniquely identifies the   instrumentation scope, such as the   instrumentation library (e.g.   <code>io.opentelemetry.contrib.mongodb</code>), package, module or class name. If an   application or library has built-in OpenTelemetry instrumentation, both   Instrumented library and   Instrumentation library may refer to   the same library. In that scenario, the <code>name</code> denotes a module name or   component name within that library or application.</p> </li> <li> <p><code>version</code> (optional): Specifies the version of the instrumentation scope if   the scope has a version (e.g. a library version). Example value: 1.0.0.</p> </li> <li> <p><code>schema_url</code> (optional): Specifies the Schema URL that should be recorded in   the emitted telemetry.</p> </li> <li> <p><code>attributes</code> (optional): Specifies the instrumentation scope attributes to   associate with emitted telemetry. This API MUST be structured to accept a   variable number of attributes, including none.</p> </li> </ul> <p><code>Logger</code>s are identified by <code>name</code>, <code>version</code>, and <code>schema_url</code> fields. When more than one <code>Logger</code> of the same <code>name</code>, <code>version</code>, and <code>schema_url</code> is created, it is unspecified whether or under which conditions the same or different <code>Logger</code> instances are returned. It is a user error to create Loggers with different <code>attributes</code> but the same identity.</p> <p>The term identical applied to <code>Logger</code>s describes instances where all identifying fields are equal. The term distinct applied to <code>Logger</code>s describes instances where at least one identifying field has a different value.</p> <p>The effect of associating a Schema URL with a <code>Logger</code> MUST be that the telemetry emitted using the <code>Logger</code> will be associated with the Schema URL, provided that the emitted data format is capable of representing such association.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#logger","title":"Logger","text":"<p>The <code>Logger</code> is responsible for emitting <code>LogRecord</code>s.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#logger-operations","title":"Logger operations","text":"<p>The <code>Logger</code> MUST provide functions to:</p> <ul> <li>Emit a <code>LogRecord</code></li> </ul>"},{"location":"docs/specs/otel/logs/bridge-api/#emit-a-logrecord","title":"Emit a LogRecord","text":"<p>The effect of calling this API is to emit a <code>LogRecord</code> to the processing pipeline.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>Timestamp</li> <li>Observed Timestamp. If unspecified   the implementation SHOULD set it equal to the current time.</li> <li>The Context associated with the <code>LogRecord</code>. The API   MAY implicitly use the current Context as a default behavior.</li> <li>Severity Number</li> <li>Severity Text</li> <li>Body</li> <li>Attributes</li> </ul> <p>All parameters are optional.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#optional-and-required-parameters","title":"Optional and required parameters","text":"<p>The operations defined include various parameters, some of which are marked optional. Parameters not marked optional are required.</p> <p>For each optional parameter, the API MUST be structured to accept it, but MUST NOT obligate a user to provide it.</p> <p>For each required parameter, the API MUST be structured to obligate a user to provide it.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#concurrency-requirements","title":"Concurrency requirements","text":"<p>For languages which support concurrent execution the Logs Bridge APIs provide specific guarantees and safeties.</p> <p>LoggerProvider - all methods are safe to be called concurrently.</p> <p>Logger - all methods are safe to be called concurrently.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#artifact-naming","title":"Artifact Naming","text":"<p>The Logs Bridge API is not intended to be called by application developers directly, and SHOULD include documentation that discourages direct use. However, in the event OpenTelemetry were to add a user facing API, the Logs Bridge API would be a natural starting point. Therefore, Log Bridge API artifact, package, and class names MUST NOT include the terms \"bridge\", \"appender\", or any other qualifier that would prevent evolution into a user facing API.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#usage","title":"Usage","text":""},{"location":"docs/specs/otel/logs/bridge-api/#how-to-create-a-log4j-log-appender","title":"How to Create a Log4J Log Appender","text":"<p>A log appender implementation can be used to bridge logs into the Log SDK OpenTelemetry LogRecordExporters. This approach is typically used for applications which are fine with changing the log transport and is one of the supported log collection approaches.</p> <p>The log appender implementation will typically acquire a Logger from the global LoggerProvider at startup time, then call Emit LogRecord for <code>LogRecord</code>s received from the application.</p> <p>Implicit Context Injection and Explicit Context Injection describe how an Appender injects <code>TraceContext</code> into <code>LogRecord</code>s.</p> <p></p> <p>This same approach can be also used for example for:</p> <ul> <li>Python logging library by creating a Handler.</li> <li>Go zap logging library by implementing the Core interface. Note that since   there is no implicit Context in Go it is not possible to get and use the   active Span.</li> </ul> <p>Log appenders can be created in OpenTelemetry language libraries by OpenTelemetry maintainers, or by 3rd parties for any logging library that supports a similar extension mechanism. This specification recommends each OpenTelemetry language library to include out-of-the-box Appender implementation for at least one popular logging library.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#implicit-context-injection","title":"Implicit Context Injection","text":"<p>When Context is implicitly available (e.g. in Java) the Appender can rely on automatic context propagation by NOT explicitly setting <code>Context</code> when calling emit a LogRecord.</p> <p>Some log libraries have mechanisms specifically tailored for injecting contextual information info logs, such as MDC in Log4j. When available, it may be preferable to use these mechanisms to set the Context. A log appender can then fetch the Context and explicitly set it when calling emit a LogRecord. This allows the correct Context to be included even when log records are emitted asynchronously, which can otherwise lead the Context to be incorrect.</p> <p>TODO: clarify how works or doesn't work when the log statement call site and the log appender are executed on different threads.</p>"},{"location":"docs/specs/otel/logs/bridge-api/#explicit-context-injection","title":"Explicit Context Injection","text":"<p>In order for <code>TraceContext</code> to be recorded in <code>LogRecord</code>s in languages where the Context must be provided explicitly (e.g. Go), the end user must capture the Context and explicitly pass it to the logging subsystem. The log appender must take this Context and explicitly set it when calling emit a LogRecord.</p> <p>Support for OpenTelemetry for logging libraries in these languages typically can be implemented in the form of logger wrappers that can capture the context once, when the span is created and then use the wrapped logger to execute log statements in a normal way. The wrapper will be responsible for injecting the captured context in the logs.</p> <p>This specification does not define how exactly it is achieved since the actual mechanism depends on the language and the particular logging library used. In any case the wrappers are expected to make use of the Trace Context API to get the current active span.</p> <p>See an example of how it can be done for zap logging library for Go.</p>"},{"location":"docs/specs/otel/logs/data-model-appendix/","title":"\u6570\u636e\u6a21\u578b\u9644\u5f55","text":"<p>Note: this document is NOT a spec, it is provided to support the Logs Data Model specification. These examples provided purely for demonstrative purposes and are not exhaustive or canonical; please refer to the respective exporter documentation if exact details are required.</p> <ul> <li>Appendix A. Example Mappings</li> <li>RFC5424 Syslog</li> <li>Windows Event Log</li> <li>SignalFx Events</li> <li>Splunk HEC</li> <li>Log4j</li> <li>Zap</li> <li>Apache HTTP Server access log</li> <li>CloudTrail Log Event</li> <li>Google Cloud Logging</li> <li>Elastic Common Schema</li> <li>Appendix B: <code>SeverityNumber</code> example mappings</li> </ul>"},{"location":"docs/specs/otel/logs/data-model-appendix/#appendix-a-example-mappings","title":"Appendix A. Example Mappings","text":"<p>This section contains examples of mapping of other events and logs formats to this data model.</p>"},{"location":"docs/specs/otel/logs/data-model-appendix/#rfc5424-syslog","title":"RFC5424 Syslog","text":"Property Type Description Maps to Unified Model Field TIMESTAMP Timestamp Time when an event occurred measured by the origin clock. Timestamp SEVERITY enum Defines the importance of the event. Example: `Debug` Severity FACILITY enum Describes where the event originated. A predefined list of Unix processes. Part of event source identity. Example: `mail system` Attributes[\"syslog.facility\"] VERSION number Meta: protocol version, orthogonal to the event. Attributes[\"syslog.version\"] HOSTNAME string Describes the location where the event originated. Possible values are FQDN, IP address, etc. Resource[\"host.hostname\"] APP-NAME string User-defined app name. Part of event source identity. Resource[\"service.name\"] PROCID string Not well defined. May be used as a meta field for protocol operation purposes or may be part of event source identity. Attributes[\"syslog.procid\"] MSGID string Defines the type of the event. Part of event source identity. Example: \"TCPIN\" Attributes[\"syslog.msgid\"] STRUCTURED-DATA array of maps of string to string A variety of use cases depending on the SDID: Can describe event source identity Can include data that describes particular occurrence of the event. Can be meta-information, e.g. quality of timestamp value. SDID origin.swVersion map to Resource[\"service.version\"]  SDID origin.ip map to attribute[\"client.address\"]  Rest of SDIDs -&gt; Attributes[\"syslog.*\"] MSG string Free-form text message about the event. Typically human readable. Body"},{"location":"docs/specs/otel/logs/data-model-appendix/#windows-event-log","title":"Windows Event Log","text":"Property Type Description Maps to Unified Model Field TimeCreated Timestamp The time stamp that identifies when the event was logged. Timestamp Level enum Contains the severity level of the event. Severity Computer string The name of the computer on which the event occurred. Resource[\"host.hostname\"] EventID uint The identifier that the provider used to identify the event. Attributes[\"winlog.event_id\"] Message string The message string. Body Rest of the fields. any All other fields in the event. Attributes[\"winlog.*\"]"},{"location":"docs/specs/otel/logs/data-model-appendix/#signalfx-events","title":"SignalFx Events","text":"Field Type Description Maps to Unified Model Field Timestamp Timestamp Time when the event occurred measured by the origin clock. Timestamp EventType string Short machine understandable string describing the event type. SignalFx specific concept. Non-namespaced. Example: k8s Event Reason field. Attributes[\"com.splunk.signalfx.event_type\"] Category enum Describes where the event originated and why. SignalFx specific concept. Example: AGENT.  Attributes[\"com.splunk.signalfx.event_category\"] Dimensions map&lt;string, string&gt; Helps to define the identity of the event source together with EventType and Category. Multiple occurrences of events coming from the same event source can happen across time and they all have the value of Dimensions.  Resource Properties map&lt;string, any&gt; Additional information about the specific event occurrence. Unlike Dimensions which are fixed for a particular event source, Properties can have different values for each occurrence of the event coming from the same event source. Attributes"},{"location":"docs/specs/otel/logs/data-model-appendix/#splunk-hec","title":"Splunk HEC","text":"<p>We apply this mapping from HEC to the unified model:</p> Field Type Description Maps to Unified Model Field time numeric, string The event time in epoch time format, in seconds. Timestamp host string The host value to assign to the event data. This is typically the host name of the client that you are sending data from. Resource[\"host.name\"] source string The source value to assign to the event data. For example, if you are sending data from an app you are developing, you could set this key to the name of the app. Resource[\"com.splunk.source\"] sourcetype string The sourcetype value to assign to the event data. Resource[\"com.splunk.sourcetype\"] event any The JSON representation of the raw body of the event. It can be a string, number, string array, number array, JSON object, or a JSON array. Body fields map&lt;string, any&gt; Specifies a JSON object that contains explicit custom fields. Attributes index string The name of the index by which the event data is to be indexed. The index you specify here must be within the list of allowed indexes if the token has the indexes parameter set. Attributes[\"com.splunk.index\"] <p>When mapping from the unified model to HEC, we apply this additional mapping:</p> Unified model element Type Description Maps to HEC SeverityText string The severity of the event as a human-readable string. fields['otel.log.severity.text'] SeverityNumber string The severity of the event as a number. fields['otel.log.severity.number'] Name string Short event identifier that does not contain varying parts. fields['otel.log.name'] TraceId string Request trace id. fields['trace_id'] SpanId string Request span id. fields['span_id'] TraceFlags string W3C trace flags. fields['trace_flags']"},{"location":"docs/specs/otel/logs/data-model-appendix/#log4j","title":"Log4j","text":"Field Type Description Maps to Unified Model Field Instant Timestamp Time when an event occurred measured by the origin clock. Timestamp Level enum Log level. Severity Message string Human readable message. Body All other fields any Structured data. Attributes"},{"location":"docs/specs/otel/logs/data-model-appendix/#zap","title":"Zap","text":"Field Type Description Maps to Unified Model Field ts Timestamp Time when an event occurred measured by the origin clock. Timestamp level enum Logging level. Severity caller string Calling function's filename and line number.  Attributes, key=TBD msg string Human readable message. Body All other fields any Structured data. Attributes"},{"location":"docs/specs/otel/logs/data-model-appendix/#apache-http-server-access-log","title":"Apache HTTP Server access log","text":"Field Type Description Maps to Unified Model Field %t Timestamp Time when an event occurred measured by the origin clock. Timestamp %a string Client address Attributes[\"client.address\"] %A string Server address Attributes[\"server.socket.address\"] %h string Client hostname. Attributes[\"server.address\"] %m string The request method. Attributes[\"http.request.method\"] %v,%p,%U,%q string Multiple fields that can be composed into URL. Attributes[\"url.full\"] %&gt;s string Response status. Attributes[\"http.response.status_code\"] All other fields any Structured data. Attributes, key=TBD"},{"location":"docs/specs/otel/logs/data-model-appendix/#cloudtrail-log-event","title":"CloudTrail Log Event","text":"Field Type Description Maps to Unified Model Field eventTime string The date and time the request was made, in coordinated universal time (UTC). Timestamp eventSource string The service that the request was made to. This name is typically a short form of the service name without spaces plus .amazonaws.com. Resource[\"service.name\"]? awsRegion string The AWS region that the request was made to, such as us-east-2. Resource[\"cloud.region\"] sourceIPAddress string The IP address that the request was made from. Attributes[\"client.address\"] errorCode string The AWS service error if the request returns an error. Attributes[\"cloudtrail.error_code\"] errorMessage string If the request returns an error, the description of the error. Body All other fields * Attributes[\"cloudtrail.*\"]"},{"location":"docs/specs/otel/logs/data-model-appendix/#google-cloud-logging","title":"Google Cloud Logging","text":"Field Type Description Maps to Unified Model Field timestamp string The time the event described by the log entry occurred. Timestamp resource MonitoredResource The monitored resource that produced this log entry. Resource log_name string The URL-encoded LOG_ID suffix of the log_name field identifies which log stream this entry belongs to. Attributes[\"gcp.log_name\"] json_payload google.protobuf.Struct The log entry payload, represented as a structure that is expressed as a JSON object. Body proto_payload google.protobuf.Any The log entry payload, represented as a protocol buffer. Body text_payload string The log entry payload, represented as a Unicode string (UTF-8). Body severity LogSeverity The severity of the log entry. Severity trace string The trace associated with the log entry, if any. TraceId span_id string The span ID within the trace associated with the log entry. SpanId labels map A set of user-defined (key, value) data that provides additional information about the log entry. Attributes http_request HttpRequest The HTTP request associated with the log entry, if any. Attributes[\"gcp.http_request\"] All other fields Attributes[\"gcp.*\"]"},{"location":"docs/specs/otel/logs/data-model-appendix/#elastic-common-schema","title":"Elastic Common Schema","text":"Field Type Description Maps to Unified Model Field @timestamp datetime Time the event was recorded Timestamp message string Any type of message Body labels key/value Arbitrary labels related to the event Attributes[*] tags array of string List of values related to the event ? trace.id string Trace ID TraceId span.id* string Span ID SpanId agent.ephemeral_id string Ephemeral ID created by agent **Resource agent.id string Unique identifier of this agent **Resource agent.name string Name given to the agent Resource[\"telemetry.sdk.name\"] agent.type string Type of agent Resource[\"telemetry.sdk.language\"] agent.version string Version of agent Resource[\"telemetry.sdk.version\"] source.ip, client.ip string The IP address that the request was made from. Attributes[\"client.address\"] cloud.account.id string ID of the account in the given cloud Resource[\"cloud.account.id\"] cloud.availability_zone string Availability zone in which this host is running. Resource[\"cloud.zone\"] cloud.instance.id string Instance ID of the host machine. **Resource cloud.instance.name string Instance name of the host machine. **Resource cloud.machine.type string Machine type of the host machine. **Resource cloud.provider string Name of the cloud provider. Example values are aws, azure, gcp, or digitalocean. Resource[\"cloud.provider\"] cloud.region string Region in which this host is running. Resource[\"cloud.region\"] cloud.image.id* string Resource[\"host.image.name\"] container.id string Unique container id Resource[\"container.id\"] container.image.name string Name of the image the container was built on. Resource[\"container.image.name\"] container.image.tag Array of string Container image tags. **Resource container.labels key/value Image labels. Attributes[*] container.name string Container name. Resource[\"container.name\"] container.runtime string Runtime managing this container. Example: \"docker\" **Resource destination.address string Destination address for the event Attributes[\"destination.address\"] error.code string Error code describing the error. Attributes[\"error.code\"] error.id string Unique identifier for the error. Attributes[\"error.id\"] error.message string Error message. Attributes[\"error.message\"] error.stack_trace string The stack trace of this error in plain text. Attributes[\"error.stack_trace] host.architecture string Operating system architecture **Resource host.domain string Name of the domain of which the host is a member.  For example, on Windows this could be the host\u2019s Active Directory domain or NetBIOS domain name. For Linux this could be the domain of the host\u2019s LDAP provider. **Resource host.hostname string Hostname of the host.  It normally contains what the hostname command returns on the host machine. Resource[\"host.hostname\"] host.id string Unique host id. Resource[\"host.id\"] host.ip Array of string Host IP Resource[\"host.ip\"] host.mac array of string MAC addresses of the host Resource[\"host.mac\"] host.name string Name of the host.  It may contain what hostname returns on Unix systems, the fully qualified, or a name specified by the user.  Resource[\"host.name\"] host.type string Type of host. Resource[\"host.type\"] host.uptime string Seconds the host has been up. ? service.ephemeral_id   string Ephemeral identifier of this service **Resource service.id string Unique identifier of the running service. If the service is comprised of many nodes, the service.id should be the same for all nodes. **Resource service.name string Name of the service data is collected from. Resource[\"service.name\"] service.node.name string Specific node serving that service Resource[\"service.instance.id\"] service.state string Current state of the service. Attributes[\"service.state\"] service.type string The type of the service data is collected from. **Resource service.version string Version of the service the data was collected from. Resource[\"service.version\"] <p>* Not yet formalized into ECS.</p> <p>** A resource that doesn\u2019t exist in the OpenTelemetry resource semantic convention.</p> <p>This is a selection of the most relevant fields. See for the full reference for an exhaustive list.</p>"},{"location":"docs/specs/otel/logs/data-model-appendix/#appendix-b-severitynumber-example-mappings","title":"Appendix B: <code>SeverityNumber</code> example mappings","text":"Syslog WinEvtLog Log4j Zap java.util.logging .NET (Microsoft.Extensions.Logging) SeverityNumber TRACE FINEST LogLevel.Trace TRACE Debug Verbose DEBUG Debug FINER LogLevel.Debug DEBUG FINE DEBUG2 CONFIG DEBUG3 Informational Information INFO Info INFO LogLevel.Information INFO Notice INFO2 Warning Warning WARN Warn WARNING LogLevel.Warning WARN Error Error ERROR Error SEVERE LogLevel.Error ERROR Critical Critical Dpanic ERROR2 Alert Panic ERROR3 Emergency FATAL Fatal LogLevel.Critical FATAL"},{"location":"docs/specs/otel/logs/data-model/","title":"\u65e5\u5fd7\u6570\u636e\u6a21\u578b","text":"<p>Status: Stable</p> Table of Contents   - [Design Notes](#design-notes)   - [Requirements](#requirements)   - [Definitions Used in this Document](#definitions-used-in-this-document)     - [Type `any`](#type-any)     - [Type `map`](#type-mapstring-any)   - [Field Kinds](#field-kinds) - [Log and Event Record Definition](#log-and-event-record-definition)   - [Field: `Timestamp`](#field-timestamp)   - [Field: `ObservedTimestamp`](#field-observedtimestamp)   - [Trace Context Fields](#trace-context-fields)     - [Field: `TraceId`](#field-traceid)     - [Field: `SpanId`](#field-spanid)     - [Field: `TraceFlags`](#field-traceflags)   - [Severity Fields](#severity-fields)     - [Field: `SeverityText`](#field-severitytext)     - [Field: `SeverityNumber`](#field-severitynumber)     - [Mapping of `SeverityNumber`](#mapping-of-severitynumber)     - [Reverse Mapping](#reverse-mapping)     - [Error Semantics](#error-semantics)     - [Displaying Severity](#displaying-severity)     - [Comparing Severity](#comparing-severity)   - [Field: `Body`](#field-body)   - [Field: `Resource`](#field-resource)   - [Field: `InstrumentationScope`](#field-instrumentationscope)   - [Field: `Attributes`](#field-attributes)     - [Errors and Exceptions](#errors-and-exceptions) - [Example Log Records](#example-log-records) - [Example Mappings](#example-mappings) - [References](#references)   <p>This is a data model and semantic conventions that allow to represent logs from various sources: application log files, machine generated events, system logs, etc. Existing log formats can be unambiguously mapped to this data model. Reverse mapping from this data model is also possible to the extent that the target log format has equivalent capabilities.</p> <p>The purpose of the data model is to have a common understanding of what a log record is, what data needs to be recorded, transferred, stored and interpreted by a logging system.</p> <p>This proposal defines a data model for Standalone Logs.</p>"},{"location":"docs/specs/otel/logs/data-model/#design-notes","title":"Design Notes","text":""},{"location":"docs/specs/otel/logs/data-model/#requirements","title":"Requirements","text":"<p>The Data Model was designed to satisfy the following requirements:</p> <ul> <li> <p>It should be possible to unambiguously map existing log formats to this Data   Model. Translating log data from an arbitrary log format to this Data Model   and back should ideally result in identical data.</p> </li> <li> <p>Mappings of other log formats to this Data Model should be semantically   meaningful. The Data Model must preserve the semantics of particular elements   of existing log formats.</p> </li> <li> <p>Translating log data from an arbitrary log format A to this Data Model and   then translating from the Data Model to another log format B ideally must   result in a meaningful translation of log data that is no worse than a   reasonable direct translation from log format A to log format B.</p> </li> <li> <p>It should be possible to efficiently represent the Data Model in concrete   implementations that require the data to be stored or transmitted. We   primarily care about 2 aspects of efficiency: CPU usage for   serialization/deserialization and space requirements in serialized form. This   is an indirect requirement that is affected by the specific representation of   the Data Model rather than the Data Model itself, but is still useful to keep   in mind.</p> </li> </ul> <p>The Data Model aims to successfully represent 3 sorts of logs and events:</p> <ul> <li> <p>System Formats. These are logs and events generated by the operating system   and over which we have no control - we cannot change the format or affect what   information is included (unless the data is generated by an application which   we can modify). An example of system format is Syslog.</p> </li> <li> <p>Third-party Applications. These are generated by third-party applications. We   may have certain control over what information is included, e.g. customize the   format. An example is Apache log file.</p> </li> <li> <p>First-party Applications. These are applications that we develop and we have   some control over how the logs and events are generated and what information   we include in the logs. We can likely modify the source code of the   application if needed.</p> </li> </ul>"},{"location":"docs/specs/otel/logs/data-model/#definitions-used-in-this-document","title":"Definitions Used in this Document","text":"<p>In this document we refer to types <code>any</code> and <code>map&lt;string, any&gt;</code>, defined as follows.</p>"},{"location":"docs/specs/otel/logs/data-model/#type-any","title":"Type <code>any</code>","text":"<p>Value of type <code>any</code> can be one of the following:</p> <ul> <li> <p>A scalar value: number, string or boolean,</p> </li> <li> <p>A byte array,</p> </li> <li> <p>An array (a list) of <code>any</code> values,</p> </li> <li> <p>A <code>map&lt;string, any&gt;</code>.</p> </li> </ul>"},{"location":"docs/specs/otel/logs/data-model/#type-mapstring-any","title":"Type <code>map&lt;string, any&gt;</code>","text":"<p>Value of type <code>map&lt;string, any&gt;</code> is a map of string keys to <code>any</code> values. The keys in the map are unique (duplicate keys are not allowed). The representation of the map is language-dependent.</p> <p>Arbitrary deep nesting of values for arrays and maps is allowed (essentially allows to represent an equivalent of a JSON object).</p>"},{"location":"docs/specs/otel/logs/data-model/#field-kinds","title":"Field Kinds","text":"<p>This Data Model defines a logical model for a log record (irrespective of the physical format and encoding of the record). Each record contains 2 kinds of fields:</p> <ul> <li> <p>Named top-level fields of specific type and meaning.</p> </li> <li> <p>Fields stored as <code>map&lt;string, any&gt;</code>, which can contain arbitrary values of   different types. The keys and values for well-known fields follow semantic   conventions for key names and possible values that allow all parties that work   with the field to have the same interpretation of the data. See references to   semantic conventions for <code>Resource</code> and <code>Attributes</code> fields and examples in   Appendix A.</p> </li> </ul> <p>The reasons for having these 2 kinds of fields are:</p> <ul> <li> <p>Ability to efficiently represent named top-level fields, which are almost   always present (e.g. when using encodings like Protocol Buffers where fields   are enumerated but not named on the wire).</p> </li> <li> <p>Ability to enforce types of named fields, which is very useful for compiled   languages with type checks.</p> </li> <li> <p>Flexibility to represent less frequent data as <code>map&lt;string, any&gt;</code>. This   includes well-known data that has standardized semantics as well as arbitrary   custom data that the application may want to include in the logs.</p> </li> </ul> <p>When designing this data model we followed the following reasoning to make a decision about when to use a top-level named field:</p> <ul> <li> <p>The field needs to be either mandatory for all records or be frequently   present in well-known log and event formats (such as <code>Timestamp</code>) or is   expected to be often present in log records in upcoming logging systems (such   as <code>TraceId</code>).</p> </li> <li> <p>The field\u2019s semantics must be the same for all known log and event formats and   can be mapped directly and unambiguously to this data model.</p> </li> </ul> <p>Both of the above conditions were required to give the field a place in the top-level structure of the record.</p>"},{"location":"docs/specs/otel/logs/data-model/#log-and-event-record-definition","title":"Log and Event Record Definition","text":"<p>Appendix A contains many examples that show how existing log formats map to the fields defined below. If there are questions about the meaning of the field reviewing the examples may be helpful.</p> <p>Here is the list of fields in a log record:</p> Field Name Description Timestamp Time when the event occurred. ObservedTimestamp Time when the event was observed. TraceId Request trace id. SpanId Request span id. TraceFlags W3C trace flag. SeverityText The severity text (also known as log level). SeverityNumber Numerical value of the severity. Body The body of the log record. Resource Describes the source of the log. InstrumentationScope Describes the scope that emitted the log. Attributes Additional information about the event. <p>Below is the detailed description of each field.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-timestamp","title":"Field: <code>Timestamp</code>","text":"<p>Type: Timestamp, uint64 nanoseconds since Unix epoch.</p> <p>Description: Time when the event occurred measured by the origin clock, i.e. the time at the source. This field is optional, it may be missing if the source timestamp is unknown.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-observedtimestamp","title":"Field: <code>ObservedTimestamp</code>","text":"<p>Type: Timestamp, uint64 nanoseconds since Unix epoch.</p> <p>Description: Time when the event was observed by the collection system. For events that originate in OpenTelemetry (e.g. using OpenTelemetry Logging SDK) this timestamp is typically set at the generation time and is equal to Timestamp. For events originating externally and collected by OpenTelemetry (e.g. using Collector) this is the time when OpenTelemetry's code observed the event measured by the clock of the OpenTelemetry code. This field SHOULD be set once the event is observed by OpenTelemetry.</p> <p>For converting OpenTelemetry log data to formats that support only one timestamp or when receiving OpenTelemetry log data by recipients that support only one timestamp internally the following logic is recommended:</p> <ul> <li>Use <code>Timestamp</code> if it is present, otherwise use <code>ObservedTimestamp</code>.</li> </ul>"},{"location":"docs/specs/otel/logs/data-model/#trace-context-fields","title":"Trace Context Fields","text":""},{"location":"docs/specs/otel/logs/data-model/#field-traceid","title":"Field: <code>TraceId</code>","text":"<p>Type: byte sequence.</p> <p>Description: Request trace id as defined in W3C Trace Context. Can be set for logs that are part of request processing and have an assigned trace id. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-spanid","title":"Field: <code>SpanId</code>","text":"<p>Type: byte sequence.</p> <p>Description: Span id. Can be set for logs that are part of a particular processing span. If SpanId is present TraceId SHOULD be also present. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-traceflags","title":"Field: <code>TraceFlags</code>","text":"<p>Type: byte.</p> <p>Description: Trace flag as defined in W3C Trace Context specification. At the time of writing the specification defines one flag - the SAMPLED flag. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#severity-fields","title":"Severity Fields","text":""},{"location":"docs/specs/otel/logs/data-model/#field-severitytext","title":"Field: <code>SeverityText</code>","text":"<p>Type: string.</p> <p>Description: severity text (also known as log level). This is the original string representation of the severity as it is known at the source. If this field is missing and <code>SeverityNumber</code> is present then the short name that corresponds to the <code>SeverityNumber</code> may be used as a substitution. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-severitynumber","title":"Field: <code>SeverityNumber</code>","text":"<p>Type: number.</p> <p>Description: numerical value of the severity, normalized to values described in this document. This field is optional.</p> <p><code>SeverityNumber</code> is an integer number. Smaller numerical values correspond to less severe events (such as debug events), larger numerical values correspond to more severe events (such as errors and critical events). The following table defines the meaning of <code>SeverityNumber</code> value:</p> SeverityNumber range Range name Meaning 1-4 TRACE A fine-grained debugging event. Typically disabled in default configurations. 5-8 DEBUG A debugging event. 9-12 INFO An informational event. Indicates that an event happened. 13-16 WARN A warning event. Not an error but is likely more important than an informational event. 17-20 ERROR An error event. Something went wrong. 21-24 FATAL A fatal error such as application or system crash. <p>Smaller numerical values in each range represent less important (less severe) events. Larger numerical values in each range represent more important (more severe) events. For example <code>SeverityNumber=17</code> describes an error that is less critical than an error with <code>SeverityNumber=20</code>.</p>"},{"location":"docs/specs/otel/logs/data-model/#mapping-of-severitynumber","title":"Mapping of <code>SeverityNumber</code>","text":"<p>Mappings from existing logging systems and formats (or source format for short) must define how severity (or log level) of that particular format corresponds to <code>SeverityNumber</code> of this data model based on the meaning given for each range in the above table.</p> <p>If the source format has more than one severity that matches a single range in this table then the severities of the source format must be assigned numerical values from that range according to how severe (important) the source severity is.</p> <p>For example if the source format defines \"Error\" and \"Critical\" as error events and \"Critical\" is a more important and more severe situation then we can choose the following <code>SeverityNumber</code> values for the mapping: \"Error\"-&gt;17, \"Critical\"-&gt;18.</p> <p>If the source format has only a single severity that matches the meaning of the range then it is recommended to assign that severity the smallest value of the range.</p> <p>For example if the source format has an \"Informational\" log level and no other log levels with similar meaning then it is recommended to use <code>SeverityNumber=9</code> for \"Informational\".</p> <p>Source formats that do not define a concept of severity or log level MAY omit <code>SeverityNumber</code> and <code>SeverityText</code> fields. Backend and UI may represent log records with missing severity information distinctly or may interpret log records with missing <code>SeverityNumber</code> and <code>SeverityText</code> fields as if the <code>SeverityNumber</code> was set equal to INFO (numeric value of 9).</p>"},{"location":"docs/specs/otel/logs/data-model/#reverse-mapping","title":"Reverse Mapping","text":"<p>When performing a reverse mapping from <code>SeverityNumber</code> to a specific format and the <code>SeverityNumber</code> has no corresponding mapping entry for that format then it is recommended to choose the target severity that is in the same severity range and is closest numerically.</p> <p>For example Zap has only one severity in the INFO range, called \"Info\". When doing reverse mapping all <code>SeverityNumber</code> values in INFO range (numeric 9-12) will be mapped to Zap\u2019s \"Info\" level.</p>"},{"location":"docs/specs/otel/logs/data-model/#error-semantics","title":"Error Semantics","text":"<p>If <code>SeverityNumber</code> is present and has a value of ERROR (numeric 17) or higher then it is an indication that the log record represents an erroneous situation. It is up to the reader of this value to make a decision on how to use this fact (e.g. UIs may display such errors in a different color or have a feature to find all erroneous log records).</p> <p>If the log record represents an erroneous event and the source format does not define a severity or log level concept then it is recommended to set <code>SeverityNumber</code> to ERROR (numeric 17) during the mapping process. If the log record represents a non-erroneous event the <code>SeverityNumber</code> field may be omitted or may be set to any numeric value less than ERROR (numeric 17). The recommended value in this case is INFO (numeric 9). See Appendix B for more mapping examples.</p>"},{"location":"docs/specs/otel/logs/data-model/#displaying-severity","title":"Displaying Severity","text":"<p>The following table defines the recommended short name for each <code>SeverityNumber</code> value. The short name can be used for example for representing the <code>SeverityNumber</code> in the UI:</p> SeverityNumber Short Name 1 TRACE 2 TRACE2 3 TRACE3 4 TRACE4 5 DEBUG 6 DEBUG2 7 DEBUG3 8 DEBUG4 9 INFO 10 INFO2 11 INFO3 12 INFO4 13 WARN 14 WARN2 15 WARN3 16 WARN4 17 ERROR 18 ERROR2 19 ERROR3 20 ERROR4 21 FATAL 22 FATAL2 23 FATAL3 24 FATAL4 <p>When an individual log record is displayed it is recommended to show both <code>SeverityText</code> and <code>SeverityNumber</code> values. A recommended combined string in this case begins with the short name followed by <code>SeverityText</code> in parenthesis.</p> <p>For example \"Informational\" Syslog record will be displayed as INFO (Informational). When for a particular log record the <code>SeverityNumber</code> is defined but the <code>SeverityText</code> is missing it is recommended to only show the short name, e.g. INFO.</p> <p>When drop down lists (or other UI elements that are intended to represent the possible set of values) are used for representing the severity it is preferable to display the short name in such UI elements.</p> <p>For example a dropdown list of severities that allows filtering log records by severities is likely to be more usable if it contains the short names of <code>SeverityNumber</code> (and thus has a limited upper bound of elements) compared to a dropdown list, which lists all distinct <code>SeverityText</code> values that are known to the system (which can be a large number of elements, often differing only in capitalization or abbreviated, e.g. \"Info\" vs \"Information\").</p>"},{"location":"docs/specs/otel/logs/data-model/#comparing-severity","title":"Comparing Severity","text":"<p>In the contexts where severity participates in less-than / greater-than comparisons <code>SeverityNumber</code> field should be used. <code>SeverityNumber</code> can be compared to another <code>SeverityNumber</code> or to numbers in the 1..24 range (or to the corresponding short names).</p>"},{"location":"docs/specs/otel/logs/data-model/#field-body","title":"Field: <code>Body</code>","text":"<p>Type: any.</p> <p>Description: A value containing the body of the log record (see the description of <code>any</code> type above). Can be for example a human-readable string message (including multi-line) describing the event in a free form or it can be a structured data composed of arrays and maps of other values. First-party Applications SHOULD use a string message. However, a structured body SHOULD be used to preserve the semantics of structured logs emitted by Third-party Applications. Can vary for each occurrence of the event coming from the same source. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-resource","title":"Field: <code>Resource</code>","text":"<p>Type: <code>map&lt;string, any&gt;</code>.</p> <p>Description: Describes the source of the log, aka resource. Multiple occurrences of events coming from the same event source can happen across time and they all have the same value of <code>Resource</code>. Can contain for example information about the application that emits the record or about the infrastructure where the application runs. Data formats that represent this data model may be designed in a manner that allows the <code>Resource</code> field to be recorded only once per batch of log records that come from the same source. SHOULD follow OpenTelemetry semantic conventions for Resources. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-instrumentationscope","title":"Field: <code>InstrumentationScope</code>","text":"<p>Type: (Name,Version) tuple of strings.</p> <p>Description: the instrumentation scope. Multiple occurrences of events coming from the same scope can happen across time and they all have the same value of <code>InstrumentationScope</code>. For log sources which define a logger name (e.g. Java Logger Name) the Logger Name SHOULD be recorded as the Instrumentation Scope name.</p> <p>Version is optional. Name SHOULD be specified if version is specified, otherwise Name is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#field-attributes","title":"Field: <code>Attributes</code>","text":"<p>Type: <code>map&lt;string, any&gt;</code>.</p> <p>Description: Additional information about the specific event occurrence. Unlike the <code>Resource</code> field, which is fixed for a particular source, <code>Attributes</code> can vary for each occurrence of the event coming from the same source. Can contain information about the request context (other than TraceId/SpanId). SHOULD follow OpenTelemetry semantic conventions for Log Attributes or semantic conventions for Span Attributes. This field is optional.</p>"},{"location":"docs/specs/otel/logs/data-model/#errors-and-exceptions","title":"Errors and Exceptions","text":"<p>Additional information about errors and/or exceptions that are associated with a log record MAY be included in the structured data in the <code>Attributes</code> section of the record. If included, they MUST follow the OpenTelemetry semantic conventions for exception-related attributes.</p>"},{"location":"docs/specs/otel/logs/data-model/#example-log-records","title":"Example Log Records","text":"<p>For example log records see JSON File serialization.</p>"},{"location":"docs/specs/otel/logs/data-model/#example-mappings","title":"Example Mappings","text":"<p>For example log format mappings, see the Data Model Appendix.</p>"},{"location":"docs/specs/otel/logs/data-model/#references","title":"References","text":"<ul> <li> <p>Log Data Model   OTEP 0097</p> </li> <li> <p>Draft discussion of Data Model</p> </li> <li> <p>Discussion of Severity field</p> </li> </ul>"},{"location":"docs/specs/otel/logs/event-api/","title":"\u4e8b\u4ef6 API \u63a5\u53e3","text":"<p>Status: Experimental</p> Table of Contents   - [Overview](#overview) - [EventLogger](#eventlogger)   - [EventLogger Operations](#eventlogger-operations)     - [Create EventLogger](#create-eventlogger)     - [Emit Event](#emit-event)"},{"location":"docs/specs/otel/logs/event-api/#overview","title":"Overview","text":"<p>Wikipedia\u2019s definition of log file:</p> <p>In computing, a log file is a file that records either events that occur in an operating system or other software runs.</p> <p>From OpenTelemetry's perspective LogRecords and Events are both represented using the same data model.</p> <p>However, OpenTelemetry does recognize a subtle semantic difference between LogRecords and Events: Events are LogRecords which have a <code>name</code> and <code>domain</code>. Within a particular <code>domain</code>, the <code>name</code> uniquely defines a particular class or type of event. Events with the same <code>domain</code> / <code>name</code> follow the same schema which assists in analysis in observability platforms. Events are described in more detail in the semantic conventions.</p> <p>While the logging space has a diverse legacy with many existing logging libraries in different languages, there is not ubiquitous alignment with OpenTelemetry events. In some logging libraries, producing records shaped as OpenTelemetry events is clunky or error-prone.</p> <p>The Event API offers convenience methods for emitting LogRecords that conform to the semantic conventions for Events. Unlike the Logs Bridge API, application developers and instrumentation authors are encouraged to call this API directly.</p>"},{"location":"docs/specs/otel/logs/event-api/#eventlogger","title":"EventLogger","text":"<p>The <code>EventLogger</code> is the entrypoint of the Event API, and is responsible for emitting <code>Events</code> as <code>LogRecord</code>s.</p>"},{"location":"docs/specs/otel/logs/event-api/#eventlogger-operations","title":"EventLogger Operations","text":"<p>The <code>EventLogger</code> MUST provide functions to:</p>"},{"location":"docs/specs/otel/logs/event-api/#create-eventlogger","title":"Create EventLogger","text":"<p>New <code>EventLogger</code> instances are created though a constructor or factory method on <code>EventLogger</code>.</p> <p>Parameters:</p> <ul> <li><code>logger</code> - the delegate Logger used to emit <code>Events</code>   as <code>LogRecord</code>s.</li> <li><code>event_domain</code> - the domain of emitted events, used to set the <code>event.domain</code>   attribute.</li> </ul>"},{"location":"docs/specs/otel/logs/event-api/#emit-event","title":"Emit Event","text":"<p>Emit a <code>LogRecord</code> representing an <code>Event</code> to the delegate <code>Logger</code>.</p> <p>This function MAY be named <code>logEvent</code>.</p> <p>Parameters:</p> <ul> <li><code>event_name</code> - the Event name. This argument MUST be recorded as a <code>LogRecord</code>   attribute with the key <code>event.name</code>. Care MUST be taken by the implementation   to not override or delete this attribute while the Event is emitted to   preserve its identity.</li> <li><code>logRecord</code> - the LogRecord   representing the Event.</li> </ul> <p>Implementation Requirements:</p> <p>The implementation MUST emit the <code>logRecord</code> to the <code>logger</code> specified when creating the EventLogger after making the following changes:</p> <ul> <li>The <code>event_domain</code> specified when   creating the EventLogger MUST be set as the   <code>event.domain</code> attribute on the <code>logRecord</code>.</li> <li>The <code>event_name</code> MUST be set as the <code>event.name</code> attribute on the <code>logRecord</code>.</li> </ul>"},{"location":"docs/specs/otel/logs/noop/","title":"Noop","text":""},{"location":"docs/specs/otel/logs/noop/#api","title":"\u65e5\u5fd7\u6865 API \u65e0\u64cd\u4f5c\u5b9e\u73b0","text":"<p>Status: Experimental</p>  Table of Contents    - [LoggerProvider](#loggerprovider)   - [Logger Creation](#logger-creation) - [Logger](#logger)   - [Emit LogRecord](#emit-logrecord)   <p>Users of OpenTelemetry need a way to disable the API from actually performing any operations. The No-Op OpenTelemetry API implementation (henceforth referred to as the No-Op) provides users with this functionally. It implements the OpenTelemetry Logs Bridge API so that no telemetry is produced and computation resources are minimized.</p> <p>All language implementations of OpenTelemetry MUST provide a No-Op.</p> <p>The Logs Bridge API defines classes with various operations. All No-Op classes MUST NOT hold configuration or operational state. All No-op operations MUST accept all defined parameters, MUST NOT validate any arguments received, and MUST NOT return any non-empty error or log any message.</p>"},{"location":"docs/specs/otel/logs/noop/#loggerprovider","title":"LoggerProvider","text":"<p>The No-Op MUST allow the creation of multiple <code>LoggerProviders</code>s.</p> <p>Since all <code>LoggerProviders</code>s hold the same empty state, a No-Op MAY provide the same <code>LoggerProvider</code> instances to all creation requests.</p>"},{"location":"docs/specs/otel/logs/noop/#logger-creation","title":"Logger Creation","text":"<p>New <code>Logger</code> instances are always created with a LoggerProvider. Therefore, <code>LoggerProvider</code> MUST allow for the creation of <code>Logger</code>s. All <code>Logger</code>s created MUST be an instance of the No-Op Logger.</p> <p>Since all <code>Logger</code>s will hold the same empty state, a <code>LoggerProvider</code> MAY return the same <code>Logger</code> instances to all creation requests.</p>"},{"location":"docs/specs/otel/logs/noop/#logger","title":"Logger","text":""},{"location":"docs/specs/otel/logs/noop/#emit-logrecord","title":"Emit LogRecord","text":"<p>The No-Op <code>Logger</code> MUST allow for emitting LogRecords.</p>"},{"location":"docs/specs/otel/logs/sdk/","title":"\u65e5\u5fd7 SDK","text":"<p>Status: Stable</p> Table of Contents   - [\u65e5\u5fd7 SDK](#\u65e5\u5fd7-sdk)   - [LoggerProvider](#loggerprovider)     - [LoggerProvider Creation](#loggerprovider-creation)     - [Logger Creation](#logger-creation)     - [Shutdown](#shutdown)     - [ForceFlush](#forceflush)   - [Logger](#logger)   - [Additional LogRecord interfaces](#additional-logrecord-interfaces)     - [ReadableLogRecord](#readablelogrecord)     - [ReadWriteLogRecord](#readwritelogrecord)   - [LogRecord Limits](#logrecord-limits)   - [LogRecordProcessor](#logrecordprocessor)     - [LogRecordProcessor operations](#logrecordprocessor-operations)       - [OnEmit](#onemit)       - [ShutDown](#shutdown-1)       - [ForceFlush](#forceflush-1)     - [Built-in processors](#built-in-processors)       - [Simple processor](#simple-processor)       - [Batching processor](#batching-processor)   - [LogRecordExporter](#logrecordexporter)     - [LogRecordExporter operations](#logrecordexporter-operations)       - [Export](#export)       - [ForceFlush](#forceflush-2)       - [Shutdown](#shutdown-2)   <p>Users of OpenTelemetry need a way for instrumentation interactions with the OpenTelemetry API to actually produce telemetry. The OpenTelemetry Logging SDK (henceforth referred to as the SDK) is an implementation of the OpenTelemetry API that provides users with this functionally.</p> <p>All language implementations of OpenTelemetry MUST provide an SDK.</p>"},{"location":"docs/specs/otel/logs/sdk/#loggerprovider","title":"LoggerProvider","text":"<p>A <code>LoggerProvider</code> MUST provide a way to allow a Resource to be specified. If a <code>Resource</code> is specified, it SHOULD be associated with all the <code>LogRecord</code>s produced by any <code>Logger</code> from the <code>LoggerProvider</code>.</p>"},{"location":"docs/specs/otel/logs/sdk/#loggerprovider-creation","title":"LoggerProvider Creation","text":"<p>The SDK SHOULD allow the creation of multiple independent <code>LoggerProviders</code>s.</p>"},{"location":"docs/specs/otel/logs/sdk/#logger-creation","title":"Logger Creation","text":"<p>New <code>Logger</code> instances are always created through a <code>LoggerProvider</code> (see Bridge API). The <code>name</code>, <code>version</code> (optional), <code>schema_url</code> (optional), and <code>attributes</code> (optional) supplied to the <code>LoggerProvider</code> must be used to create an <code>InstrumentationScope</code> instance which is stored on the created <code>Logger</code>.</p> <p>In the case where an invalid <code>name</code> (null or empty string) is specified, a working <code>Logger</code> MUST be returned as a fallback rather than returning null or throwing an exception, its <code>name</code> SHOULD keep the original invalid value, and a message reporting that the specified value is invalid SHOULD be logged.</p> <p>Configuration (i.e. LogRecordProcessors) MUST be managed solely by the <code>LoggerProvider</code> and the SDK MUST provide some way to configure all options that are implemented by the SDK. This MAY be done at the time of <code>LoggerProvider</code> creation if appropriate.</p> <p>The <code>LoggerProvider</code> MAY provide methods to update the configuration. If configuration is updated (e.g., adding a <code>LogRecordProcessor</code>), the updated configuration MUST also apply to all already returned <code>Logger</code>s (i.e. it MUST NOT matter whether a <code>Logger</code> was obtained from the <code>LoggerProvider</code> before or after the configuration change). Note: Implementation-wise, this could mean that <code>Logger</code> instances have a reference to their <code>LoggerProvider</code> and access configuration only via this reference.</p>"},{"location":"docs/specs/otel/logs/sdk/#shutdown","title":"Shutdown","text":"<p>This method provides a way for provider to do any cleanup required.</p> <p><code>Shutdown</code> MUST be called only once for each <code>LoggerProvider</code> instance. After the call to <code>Shutdown</code>, subsequent attempts to get a <code>Logger</code> are not allowed. SDKs SHOULD return a valid no-op <code>Logger</code> for these calls, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> MAY be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the shutdown timeout configurable.</p> <p><code>Shutdown</code> MUST be implemented at least by invoking <code>Shutdown</code> on all registered LogRecordProcessors.</p>"},{"location":"docs/specs/otel/logs/sdk/#forceflush","title":"ForceFlush","text":"<p>This method provides a way for provider to notify the registered LogRecordProcessors to immediately export all <code>ReadableLogRecords</code> that have not yet been exported.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out. <code>ForceFlush</code> SHOULD return some ERROR status if there is an error condition; and if there is no error condition, it SHOULD return some NO ERROR status, language implementations MAY decide how to model ERROR and NO ERROR.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> MAY be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the flush timeout configurable.</p> <p><code>ForceFlush</code> MUST invoke <code>ForceFlush</code> on all registered LogRecordProcessors.</p>"},{"location":"docs/specs/otel/logs/sdk/#logger","title":"Logger","text":"<p>Note that <code>Logger</code>s should not be responsible for configuration. This should be the responsibility of the <code>LoggerProvider</code> instead.</p>"},{"location":"docs/specs/otel/logs/sdk/#additional-logrecord-interfaces","title":"Additional LogRecord interfaces","text":"<p>In addition to the definition for LogRecord, the following <code>LogRecord</code>-like interfaces are defined in the SDK:</p>"},{"location":"docs/specs/otel/logs/sdk/#readablelogrecord","title":"ReadableLogRecord","text":"<p>A function receiving this as an argument MUST be able to access all the information added to the LogRecord. It MUST also be able to access the Instrumentation Scope and Resource information (implicitly) associated with the <code>LogRecord</code>.</p> <p>The trace context fields MUST be populated from the resolved <code>Context</code> (either the explicitly passed <code>Context</code> or the current <code>Context</code>) when emitted.</p> <p>Counts for attributes due to collection limits MUST be available for exporters to report as described in the transformation to non-OTLP formats specification.</p> <p>Note: Typically this will be implemented with a new interface or (immutable) value type.</p>"},{"location":"docs/specs/otel/logs/sdk/#readwritelogrecord","title":"ReadWriteLogRecord","text":"<p>A function receiving this as an argument MUST be able to write to the full LogRecord and additionally MUST be able to retrieve all information that was added to the <code>LogRecord</code> (as with ReadableLogRecord).</p>"},{"location":"docs/specs/otel/logs/sdk/#logrecord-limits","title":"LogRecord Limits","text":"<p><code>LogRecord</code> attributes MUST adhere to the common rules of attribute limits.</p> <p>If the SDK implements attribute limits it MUST provide a way to change these limits, via a configuration to the <code>LoggerProvider</code>, by allowing users to configure individual limits like in the Java example below.</p> <p>The options MAY be bundled in a class, which then SHOULD be called <code>LogRecordLimits</code>.</p> <pre><code>public interface LogRecordLimits {\npublic int getAttributeCountLimit();\npublic int getAttributeValueLengthLimit();\n}\n</code></pre> <p>Configurable parameters:</p> <ul> <li>all common options applicable to attributes</li> </ul> <p>There SHOULD be a message printed in the SDK's log to indicate to the user that an attribute was discarded due to such a limit. To prevent excessive logging, the message MUST be printed at most once per <code>LogRecord</code> (i.e., not per discarded attribute).</p>"},{"location":"docs/specs/otel/logs/sdk/#logrecordprocessor","title":"LogRecordProcessor","text":"<p><code>LogRecordProcessor</code> is an interface which allows hooks for <code>LogRecord</code> emitting.</p> <p>Built-in processors are responsible for batching and conversion of <code>LogRecord</code>s to exportable representation and passing batches to exporters.</p> <p><code>LogRecordProcessors</code> can be registered directly on SDK <code>LoggerProvider</code> and they are invoked in the same order as they were registered.</p> <p>Each processor registered on <code>LoggerProvider</code> is part of a pipeline that consists of a processor and optional exporter. The SDK MUST allow each pipeline to end with an individual exporter.</p> <p>The SDK MUST allow users to implement and configure custom processors and decorate built-in processors for advanced scenarios such as enriching with attributes.</p> <p>The following diagram shows <code>LogRecordProcessor</code>'s relationship to other components in the SDK:</p> <pre><code>  +-----+------------------------+   +------------------------------+   +-------------------------+\n  |     |                        |   |                              |   |                         |\n  |     |                        |   | Batching LogRecordProcessor  |   |    LogRecordExporter    |\n  |     |                        +---&gt; Simple LogRecordProcessor    +---&gt;     (OtlpExporter)      |\n  |     |                        |   |                              |   |                         |\n  | SDK | Logger.emit(LogRecord) |   +------------------------------+   +-------------------------+\n  |     |                        |\n  |     |                        |\n  |     |                        |\n  |     |                        |\n  |     |                        |\n  +-----+------------------------+\n</code></pre>"},{"location":"docs/specs/otel/logs/sdk/#logrecordprocessor-operations","title":"LogRecordProcessor operations","text":""},{"location":"docs/specs/otel/logs/sdk/#onemit","title":"OnEmit","text":"<p><code>OnEmit</code> is called when a <code>LogRecord</code> is emitted. This method is called synchronously on the thread that emitted the <code>LogRecord</code>, therefore it SHOULD NOT block or throw exceptions.</p> <p>Parameters:</p> <ul> <li><code>logRecord</code> - a ReadWriteLogRecord for the emitted   <code>LogRecord</code>.</li> <li><code>context</code> - the resolved <code>Context</code> (the explicitly passed <code>Context</code> or the   current <code>Context</code>)</li> </ul> <p>Returns: <code>Void</code></p> <p>A <code>LogRecordProcessor</code> may freely modify <code>logRecord</code> for the duration of the <code>OnEmit</code> call. If <code>logRecord</code> is needed after <code>OnEmit</code> returns (i.e. for asynchronous processing) only reads are permitted.</p>"},{"location":"docs/specs/otel/logs/sdk/#shutdown_1","title":"ShutDown","text":"<p>Shuts down the processor. Called when the SDK is shut down. This is an opportunity for the processor to do any cleanup required.</p> <p><code>Shutdown</code> SHOULD be called only once for each <code>LogRecordProcessor</code> instance. After the call to <code>Shutdown</code>, subsequent calls to <code>OnEmit</code> are not allowed. SDKs SHOULD ignore these calls gracefully, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> MUST include the effects of <code>ForceFlush</code>.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors can decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/logs/sdk/#forceflush_1","title":"ForceFlush","text":"<p>This is a hint to ensure that any tasks associated with <code>LogRecord</code>s for which the <code>LogRecordProcessor</code> had already received events prior to the call to <code>ForceFlush</code> SHOULD be completed as soon as possible, preferably before returning from this method.</p> <p>In particular, if any <code>LogRecordProcessor</code> has any associated exporter, it SHOULD try to call the exporter's <code>Export</code> with all <code>LogRecord</code>s for which this was not already done and then invoke <code>ForceFlush</code> on it. The built-in LogRecordProcessors MUST do so. If a timeout is specified (see below), the <code>LogRecordProcessor</code> MUST prioritize honoring the timeout over finishing all calls. It MAY skip or abort some or all Export or ForceFlush calls it has made to achieve this goal.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD only be called in cases where it is absolutely necessary, such as when using some FaaS providers that may suspend the process after an invocation, but before the <code>LogRecordProcessor</code> exports the emitted <code>LogRecord</code>s.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors can decide if they want to make the flush timeout configurable.</p>"},{"location":"docs/specs/otel/logs/sdk/#built-in-processors","title":"Built-in processors","text":"<p>The standard OpenTelemetry SDK MUST implement both simple and batch processors, as described below. Other common processing scenarios SHOULD be first considered for implementation out-of-process in OpenTelemetry Collector.</p>"},{"location":"docs/specs/otel/logs/sdk/#simple-processor","title":"Simple processor","text":"<p>This is an implementation of <code>LogRecordProcessor</code> which passes finished logs and passes the export-friendly <code>ReadableLogRecord</code> representation to the configured LogRecordExporter, as soon as they are finished.</p> <p>Configurable parameters:</p> <ul> <li><code>exporter</code> - the exporter where the <code>LogRecord</code>s are pushed.</li> </ul>"},{"location":"docs/specs/otel/logs/sdk/#batching-processor","title":"Batching processor","text":"<p>This is an implementation of the <code>LogRecordProcessor</code> which create batches of <code>LogRecord</code>s and passes the export-friendly <code>ReadableLogRecord</code> representations to the configured <code>LogRecordExporter</code>.</p> <p>Configurable parameters:</p> <ul> <li><code>exporter</code> - the exporter where the <code>LogRecord</code>s are pushed.</li> <li><code>maxQueueSize</code> - the maximum queue size. After the size is reached logs are   dropped. The default value is <code>2048</code>.</li> <li><code>scheduledDelayMillis</code> - the delay interval in milliseconds between two   consecutive exports. The default value is <code>1000</code>.</li> <li><code>exportTimeoutMillis</code> - how long the export can run before it is cancelled.   The default value is <code>30000</code>.</li> <li><code>maxExportBatchSize</code> - the maximum batch size of every export. It must be   smaller or equal to <code>maxQueueSize</code>. The default value is <code>512</code>.</li> </ul>"},{"location":"docs/specs/otel/logs/sdk/#logrecordexporter","title":"LogRecordExporter","text":"<p><code>LogRecordExporter</code> defines the interface that protocol-specific exporters must implement so that they can be plugged into OpenTelemetry SDK and support sending of telemetry data.</p> <p>The goal of the interface is to minimize burden of implementation for protocol-dependent telemetry exporters. The protocol exporter is expected to be primarily a simple telemetry data encoder and transmitter.</p>"},{"location":"docs/specs/otel/logs/sdk/#logrecordexporter-operations","title":"LogRecordExporter operations","text":"<p>A <code>LogRecordExporter</code> MUST support the following functions:</p>"},{"location":"docs/specs/otel/logs/sdk/#export","title":"Export","text":"<p>Exports a batch of ReadableLogRecords. Protocol exporters that will implement this function are typically expected to serialize and transmit the data to the destination.</p> <p><code>Export</code> will never be called concurrently for the same exporter instance. Depending on the implementation the result of the export may be returned to the Processor not in the return value of the call to <code>Export</code> but in a language specific way for signaling completion of an asynchronous task. This means that while an instance of an exporter will never have it <code>Export</code> called concurrently it does not mean that the task of exporting can not be done concurrently. How this is done is outside the scope of this specification. Each implementation MUST document the concurrency characteristics the SDK requires of the exporter.</p> <p><code>Export</code> MUST NOT block indefinitely, there MUST be a reasonable upper limit after which the call must time out with an error result (<code>Failure</code>).</p> <p>Concurrent requests and retry logic is the responsibility of the exporter. The default SDK's <code>LogRecordProcessors</code> SHOULD NOT implement retry logic, as the required logic is likely to depend heavily on the specific protocol and backend the logs are being sent to. For example, the OpenTelemetry Protocol (OTLP) specification defines logic for both sending concurrent requests and retrying requests.</p> <p>Parameters:</p> <ul> <li><code>batch</code> - a batch of ReadableLogRecords. The exact data   type of the batch is language specific, typically it is some kind of list,   e.g. for logs in Java it will be typically <code>Collection&lt;LogRecordData&gt;</code>.</li> </ul> <p>Returns: <code>ExportResult</code></p> <p>The return of <code>Export</code> is implementation specific. In what is idiomatic for the language the Exporter must send an <code>ExportResult</code> to the Processor. <code>ExportResult</code> has values of either <code>Success</code> or <code>Failure</code>:</p> <ul> <li><code>Success</code> - The batch has been successfully exported. For protocol exporters   this typically means that the data is sent over the wire and delivered to the   destination server.</li> <li><code>Failure</code> - exporting failed. The batch must be dropped. For example, this can   happen when the batch contains bad data and cannot be serialized.</li> </ul> <p>For example, in Java the return of <code>Export</code> would be a Future which when completed returns the <code>ExportResult</code> object. While in Erlang the exporter sends a message to the processor with the <code>ExportResult</code> for a particular batch.</p>"},{"location":"docs/specs/otel/logs/sdk/#forceflush_2","title":"ForceFlush","text":"<p>This is a hint to ensure that the export of any <code>ReadableLogRecords</code> the exporter has received prior to the call to <code>ForceFlush</code> SHOULD be completed as soon as possible, preferably before returning from this method.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD only be called in cases where it is absolutely necessary, such as when using some FaaS providers that may suspend the process after an invocation, but before the exporter exports the <code>ReadlableLogRecords</code>.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the flush timeout configurable.</p>"},{"location":"docs/specs/otel/logs/sdk/#shutdown_2","title":"Shutdown","text":"<p>Shuts down the exporter. Called when SDK is shut down. This is an opportunity for exporter to do any cleanup required.</p> <p>Shutdown SHOULD be called only once for each <code>LogRecordExporter</code> instance. After the call to <code>Shutdown</code> subsequent calls to <code>Export</code> are not allowed and SHOULD return a Failure result.</p> <p><code>Shutdown</code> SHOULD NOT block indefinitely (e.g. if it attempts to flush the data and the destination is unavailable). OpenTelemetry SDK authors MAY decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/","title":"\u65e5\u5fd7\u5c5e\u6027\u8bed\u4e49\u7ea6\u5b9a","text":"<p>Note</p> <p>\u8bed\u4e49\u7ea6\u5b9a\u6b63\u5728\u8f6c\u79fb\u5230\u4e00\u4e2a\u65b0\u7684\u4f4d\u7f6e.</p> <p>\u4e0d\u5141\u8bb8\u5bf9\u672c\u6587\u6863\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u3002</p> <p>Status: Experimental</p> <p>\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u65e5\u5fd7\u7684\u8bed\u4e49\u7ea6\u5b9a:</p> <ul> <li>General: \u53ef\u7528\u4e8e\u63cf\u8ff0\u65e5\u5fd7\u8bb0\u5f55\u7684\u4e00\u822c\u8bed\u4e49\u5c5e\u6027\u3002</li> <li>Log Media: \u53ef\u7528\u4e8e\u63cf\u8ff0\u65e5\u5fd7\u6e90\u7684\u8bed\u4e49\u5c5e\u6027\u3002</li> </ul> <p>\u5b9a\u4e49\u4e86\u4e8b\u4ef6\u7684\u4ee5\u4e0b\u8bed\u4e49\u7ea6\u5b9a:</p> <ul> <li>Events: \u4f7f\u7528\u65e5\u5fd7\u6570\u636e\u6a21\u578b\u8868\u793a\u4e8b\u4ef6\u65f6\u5fc5\u987b\u4f7f\u7528\u7684\u8bed\u4e49\u5c5e\u6027\u3002</li> </ul> <p>\u9664\u4e86\u65e5\u5fd7\u7684\u8bed\u4e49\u7ea6\u5b9a \uff0ctrace\u548cmetrics\uff0c OpenTelemetry \u8fd8\u7528\u81ea\u5df1 \u7684\u8d44\u6e90\u8bed\u4e49\u7ea6\u5b9a\u5b9a\u4e49\u4e86\u8986 \u76d6Resources\u7684\u6982\u5ff5\u3002</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/events/","title":"\u4e8b\u4ef6\u5c5e\u6027\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document describes the attributes of standalone Events that are represented in the data model by <code>LogRecord</code>s. Events are recorded as LogRecords that are shaped in a special way: Event LogRecords have the attributes <code>event.domain</code> and <code>event.name</code> (and possibly other LogRecord attributes).</p> <p>The <code>event.domain</code> attribute is used to logically separate events from different systems. For example, to record Events from browser apps, mobile apps and Kubernetes, we could use <code>browser</code>, <code>device</code> and <code>k8s</code> as the domain for their Events. This provides a clean separation of semantics for events in each of the domains.</p> <p>Within a particular domain, the <code>event.name</code> attribute identifies the event. Events with same domain and name are structurally similar to one another. For example, some domains could have well-defined schema for their events based on event names.</p> <p>When recording events from an existing system as OpenTelemetry Events, it is possible that the existing system does not have the equivalent of a name or requires multiple fields to identify the structure of the events. In such cases, OpenTelemetry recommends using a combination of one or more fields as the name such that the name identifies the event structurally. It is also recommended that the event names have low-cardinality, so care must be taken to use fields that identify the class of Events but not the instance of the Event.</p> Attribute Type Description Examples Requirement Level <code>event.name</code> string The name identifies the event. <code>click</code>; <code>exception</code> Required <code>event.domain</code> string The domain identifies the business context for the events. [1] <code>browser</code> Required <p>[1]: Events across different domains may have same <code>event.name</code>, yet be unrelated events.</p> <p><code>event.domain</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>browser</code> Events from browser apps <code>device</code> Events from mobile apps <code>k8s</code> Events from Kubernetes"},{"location":"docs/specs/otel/logs/semantic_conventions/exceptions/","title":"\u5f02\u5e38\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions for recording exceptions on logs and events emitted through the Logger API.</p> <ul> <li>Recording an Exception</li> <li>Attributes</li> <li>Stacktrace Representation</li> </ul>"},{"location":"docs/specs/otel/logs/semantic_conventions/exceptions/#recording-an-exception","title":"Recording an Exception","text":"<p>Exceptions SHOULD be recorded as attributes on the LogRecord passed to the Logger emit operations. Exceptions MAY be recorded on \"logs\" or \"events\" depending on the context.</p> <p>To encapsulate proper handling of exceptions API authors MAY provide a constructor, <code>RecordException</code> method/extension, or similar helper mechanism on the <code>LogRecord</code> class/structure or wherever it makes the most sense depending on the language runtime.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/exceptions/#attributes","title":"Attributes","text":"<p>The table below indicates which attributes should be added to the LogRecord and their types.</p> Attribute Type Description Examples Requirement Level <code>exception.message</code> string The exception message. <code>Division by zero</code>; <code>Can't convert 'int' object to str implicitly</code> See below <code>exception.stacktrace</code> string A stacktrace as a string in the natural representation for the language runtime. The representation is to be determined and documented by each language SIG. <code>Exception in thread \"main\" java.lang.RuntimeException: Test exception\\n at com.example.GenerateTrace.methodB(GenerateTrace.java:13)\\n at com.example.GenerateTrace.methodA(GenerateTrace.java:9)\\n at com.example.GenerateTrace.main(GenerateTrace.java:5)</code> Recommended <code>exception.type</code> string The type of the exception (its fully-qualified class name, if applicable). The dynamic type of the exception should be preferred over the static type in languages that support it. <code>java.net.ConnectException</code>; <code>OSError</code> See below <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>exception.type</code></li> <li><code>exception.message</code></li> </ul>"},{"location":"docs/specs/otel/logs/semantic_conventions/exceptions/#stacktrace-representation","title":"Stacktrace Representation","text":"<p>Same as Trace Semantic Conventions for Exceptions - Stacktrace Representation.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/feature-flags/","title":"\u7279\u5f81\u6807\u5fd7\u8ba1\u7b97\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions for recording feature flag evaluations as a log record emitted through the Logger API. This is useful when a flag is evaluated outside of a transaction context such as when the application loads or on a timer. To record a flag evaluation as a part of a transaction context, consider recording it as a span event.</p> <p>For more information about why it is useful to capture feature flag evaluations, refer to the motivation section of the trace semantic convention for feature flag evaluations.</p> <ul> <li>\u7279\u5f81\u6807\u5fd7\u8ba1\u7b97\u7684\u8bed\u4e49\u7ea6\u5b9a</li> <li>Recording an Evaluation</li> <li>Attributes</li> </ul>"},{"location":"docs/specs/otel/logs/semantic_conventions/feature-flags/#recording-an-evaluation","title":"Recording an Evaluation","text":"<p>Feature flag evaluations SHOULD be recorded as attributes on the LogRecord passed to the Logger emit operations. Evaluations MAY be recorded on \"logs\" or \"events\" depending on the context.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/feature-flags/#attributes","title":"Attributes","text":"<p>The table below indicates which attributes should be added to the LogRecord and their types.</p> <p>The event name MUST be <code>feature_flag</code>.</p> Attribute Type Description Examples Requirement Level <code>feature_flag.key</code> string The unique identifier of the feature flag. <code>logo-color</code> Required <code>feature_flag.provider_name</code> string The name of the service provider that performs the flag evaluation. <code>Flag Manager</code> Recommended <code>feature_flag.variant</code> string SHOULD be a semantic identifier for a value. If one is unavailable, a stringified version of the value can be used. [1] <code>red</code>; <code>true</code>; <code>on</code> Recommended <p>[1]: A semantic identifier, commonly referred to as a variant, provides a means for referring to a value without including the value itself. This can provide additional context for understanding the meaning behind a value. For example, the variant <code>red</code> maybe be used for the value <code>#c05543</code>.</p> <p>A stringified version of the value can be used in situations where a semantic identifier is unavailable. String representation of the value should be determined by the implementer.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/general/","title":"\u901a\u7528\u5c5e\u6027","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>The attributes described in this section are rather generic. They may be used in any Log Record they apply to.</p> <ul> <li>General log identification attributes</li> </ul>"},{"location":"docs/specs/otel/logs/semantic_conventions/general/#general-log-identification-attributes","title":"General log identification attributes","text":"<p>These attributes may be used for identifying a Log Record.</p> Attribute Type Description Examples Requirement Level <code>log.record.uid</code> string A unique identifier for the Log Record. [1] <code>01ARZ3NDEKTSV4RRFFQ69G5FAV</code> Opt-In <p>[1]: If an id is provided, other log records with the same id will be considered duplicates and can be removed safely. This means, that two distinguishable log records MUST have different values. The id MAY be an Universally Unique Lexicographically Sortable Identifier (ULID), but other identifiers (e.g. UUID) may be used as needed.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/media/","title":"\u65e5\u5fd7\u5a92\u4f53\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document describes attributes for log media in OpenTelemetry. Log media are mechanisms by which logs are transmitted. Types of media include files, streams, network protocols, and os-specific logging services such as journald and Windows Event Log.</p> Table of Contents   - [Log Media](#log-media)   - [Log File](#log-file)   - [I/O Stream](#io-stream)"},{"location":"docs/specs/otel/logs/semantic_conventions/media/#log-media","title":"Log Media","text":"<p>Note: The OpenTelemetry specification defines a Resource as <code>an immutable representation of the entity producing telemetry</code>. The following attributes do not describe entities that produce telemetry. Rather, they describe mechanisms of log transmission. As such, these should be recorded as Log Record attributes when applicable. They should not be recorded as Resource attributes.</p>"},{"location":"docs/specs/otel/logs/semantic_conventions/media/#log-file","title":"Log File","text":"<p>Description: A file to which log was emitted.</p> Name Notes and examples <code>log.file.name</code> The basename of the file. Example: <code>audit.log</code> <code>log.file.path</code> The full path to the file. Example: <code>/var/log/mysql/audit.log</code> <code>log.file.name_resolved</code> The basename of the file, with symlinks resolved. Example: <code>uuid.log</code> <code>log.file.path_resolved</code> The full path to the file, with symlinks resolved. Example: <code>/var/lib/docker/uuid.log</code>"},{"location":"docs/specs/otel/logs/semantic_conventions/media/#io-stream","title":"I/O Stream","text":"<p>Description: The I/O stream to which the log was emitted.</p> Name Notes and examples <code>log.iostream</code> The stream associated with the log. SHOULD be one of: <code>stdout</code>, <code>stderr</code>"},{"location":"docs/specs/otel/metrics/","title":"Index","text":""},{"location":"docs/specs/otel/metrics/#opentelemetry","title":"OpenTelemetry \u6307\u6807","text":"Table of Contents   - [Overview](#overview)   - [Design Goals](#design-goals)   - [Concepts](#concepts)     - [API](#api)     - [SDK](#sdk)     - [Programming Model](#programming-model) - [Specifications](#specifications) - [References](#references)"},{"location":"docs/specs/otel/metrics/#overview","title":"Overview","text":""},{"location":"docs/specs/otel/metrics/#design-goals","title":"Design Goals","text":"<p>Given there are many well-established metrics solutions that exist today, it is important to understand the goals of OpenTelemetry\u2019s metrics effort:</p> <ul> <li> <p>Being able to connect metrics to other signals. For example, metrics and   traces can be correlated via exemplars, and metrics attributes can be enriched   via Baggage and Context.   Additionally, Resource can be applied to   logs/metrics/traces   in a consistent way.</p> </li> <li> <p>Providing a path for OpenCensus customers to   migrate to OpenTelemetry. This was the original goal of OpenTelemetry -   converging OpenCensus and OpenTracing. We will focus on providing the   semantics and capability, instead of doing a 1-1 mapping of the APIs.</p> </li> <li> <p>Working with existing metrics instrumentation protocols and standards. The   minimum goal is to provide full support for   Prometheus and   StatsD - users should be able to use   OpenTelemetry clients and Collector to collect and   export metrics, with the ability to achieve the same functionality as their   native clients.</p> </li> </ul>"},{"location":"docs/specs/otel/metrics/#concepts","title":"Concepts","text":""},{"location":"docs/specs/otel/metrics/#api","title":"API","text":"<p>The OpenTelemetry Metrics API (\"the API\" hereafter) serves two purposes:</p> <ul> <li>Capturing raw measurements efficiently and simultaneously.</li> <li>Decoupling the instrumentation from the SDK, allowing the SDK to be   specified/included in the application.</li> </ul> <p>When no SDK is explicitly included/enabled in the application, no telemetry data will be collected. Please refer to the overall OpenTelemetry API concept and API and Minimal Implementation for more information.</p>"},{"location":"docs/specs/otel/metrics/#sdk","title":"SDK","text":"<p>The OpenTelemetry Metrics SDK (\"the SDK\" hereafter) implements the API, providing functionality and extensibility such as configuration, aggregation, processors and exporters.</p> <p>OpenTelemetry requires a separation of the API from the SDK, so that different SDKs can be configured at run time. Please refer to the overall OpenTelemetry SDK concept for more information.</p>"},{"location":"docs/specs/otel/metrics/#programming-model","title":"Programming Model","text":"<pre><code>+------------------+\n| MeterProvider    |                 +-----------------+             +--------------+\n|   Meter A        | Measurements... |                 | Metrics...  |              |\n|     Instrument X +-----------------&gt; In-memory state +-------------&gt; MetricReader |\n|     Instrument Y |                 |                 |             |              |\n|   Meter B        |                 +-----------------+             +--------------+\n|     Instrument Z |\n|     ...          |                 +-----------------+             +--------------+\n|     ...          | Measurements... |                 | Metrics...  |              |\n|     ...          +-----------------&gt; In-memory state +-------------&gt; MetricReader |\n|     ...          |                 |                 |             |              |\n|     ...          |                 +-----------------+             +--------------+\n+------------------+\n</code></pre>"},{"location":"docs/specs/otel/metrics/#specifications","title":"Specifications","text":"<ul> <li>Metrics API</li> <li>Metrics SDK</li> <li>Metrics Data Model and Protocol</li> <li>Metrics Requirement Levels</li> <li>Semantic Conventions</li> </ul>"},{"location":"docs/specs/otel/metrics/#references","title":"References","text":"<ul> <li>Scenarios for Metrics API/SDK Prototyping   (OTEP 146)</li> </ul>"},{"location":"docs/specs/otel/metrics/api/","title":"Api","text":""},{"location":"docs/specs/otel/metrics/api/#api","title":"\u6307\u6807 API","text":"<p>Status: Stable, except where otherwise specified</p> Table of Contents   - [Overview](#overview) - [MeterProvider](#meterprovider)   - [MeterProvider operations](#meterprovider-operations)     - [Get a Meter](#get-a-meter) - [Meter](#meter)   - [Meter operations](#meter-operations) - [Instrument](#instrument)   - [General characteristics](#general-characteristics)     - [Instrument name syntax](#instrument-name-syntax)     - [Instrument unit](#instrument-unit)     - [Instrument description](#instrument-description)     - [Instrument advice](#instrument-advice)     - [Synchronous and Asynchronous instruments](#synchronous-and-asynchronous-instruments)       - [Synchronous Instrument API](#synchronous-instrument-api)       - [Asynchronous Instrument API](#asynchronous-instrument-api)   - [Counter](#counter)     - [Counter creation](#counter-creation)     - [Counter operations](#counter-operations)       - [Add](#add)   - [Asynchronous Counter](#asynchronous-counter)     - [Asynchronous Counter creation](#asynchronous-counter-creation)     - [Asynchronous Counter operations](#asynchronous-counter-operations)   - [Histogram](#histogram)     - [Histogram creation](#histogram-creation)     - [Histogram operations](#histogram-operations)       - [Record](#record)   - [Asynchronous Gauge](#asynchronous-gauge)     - [Asynchronous Gauge creation](#asynchronous-gauge-creation)     - [Asynchronous Gauge operations](#asynchronous-gauge-operations)   - [UpDownCounter](#updowncounter)     - [UpDownCounter creation](#updowncounter-creation)     - [UpDownCounter operations](#updowncounter-operations)       - [Add](#add-1)   - [Asynchronous UpDownCounter](#asynchronous-updowncounter)     - [Asynchronous UpDownCounter creation](#asynchronous-updowncounter-creation)     - [Asynchronous UpDownCounter operations](#asynchronous-updowncounter-operations) - [Measurement](#measurement)   - [Multiple-instrument callbacks](#multiple-instrument-callbacks) - [Compatibility requirements](#compatibility-requirements) - [Concurrency requirements](#concurrency-requirements)"},{"location":"docs/specs/otel/metrics/api/#overview","title":"Overview","text":"<p>The Metrics API consists of these main components:</p> <ul> <li>MeterProvider is the entry point of the API. It provides   access to <code>Meters</code>.</li> <li>Meter is the class responsible for creating <code>Instruments</code>.</li> <li>Instrument is responsible for reporting   Measurements.</li> </ul> <p>Here is an example of the object hierarchy inside a process instrumented with the metrics API:</p> <pre><code>+-- MeterProvider(default)\n    |\n    +-- Meter(name='io.opentelemetry.runtime', version='1.0.0')\n    |   |\n    |   +-- Instrument&lt;Asynchronous Gauge, int&gt;(name='cpython.gc', attributes=['generation'], unit='kB')\n    |   |\n    |   +-- instruments...\n    |\n    +-- Meter(name='io.opentelemetry.contrib.mongodb.client', version='2.3.0')\n        |\n        +-- Instrument&lt;Counter, int&gt;(name='client.exception', attributes=['type'], unit='1')\n        |\n        +-- Instrument&lt;Histogram, double&gt;(name='client.duration', attributes=['server.address', 'server.port'], unit='ms')\n        |\n        +-- instruments...\n\n+-- MeterProvider(custom)\n    |\n    +-- Meter(name='bank.payment', version='23.3.5')\n        |\n        +-- instruments...\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#meterprovider","title":"MeterProvider","text":"<p><code>Meter</code>s can be accessed with a <code>MeterProvider</code>.</p> <p>In implementations of the API, the <code>MeterProvider</code> is expected to be the stateful object that holds any configuration.</p> <p>Normally, the <code>MeterProvider</code> is expected to be accessed from a central place. Thus, the API SHOULD provide a way to set/register and access a global default <code>MeterProvider</code>.</p>"},{"location":"docs/specs/otel/metrics/api/#meterprovider-operations","title":"MeterProvider operations","text":"<p>The <code>MeterProvider</code> MUST provide the following functions:</p> <ul> <li>Get a <code>Meter</code></li> </ul>"},{"location":"docs/specs/otel/metrics/api/#get-a-meter","title":"Get a Meter","text":"<p>This API MUST accept the following parameters:</p> <ul> <li><code>name</code>: This name uniquely identifies the   instrumentation scope, such as the   instrumentation library (e.g.   <code>io.opentelemetry.contrib.mongodb</code>), package, module or class name. If an   application or library has built-in OpenTelemetry instrumentation, both   Instrumented library and   Instrumentation library can refer to   the same library. In that scenario, the <code>name</code> denotes a module name or   component name within that library or application.</li> <li><code>version</code>: Specifies the version of the instrumentation scope if the scope has   a version (e.g. a library version). Example value: <code>1.0.0</code>.</li> </ul> <p>Users can provide a <code>version</code>, but it is up to their discretion. Therefore,   this API needs to be structured to accept a <code>version</code>, but MUST NOT obligate a   user to provide one.</p> <ul> <li>[since 1.4.0] <code>schema_url</code>: Specifies the Schema URL that should be recorded   in the emitted telemetry.</li> </ul> <p>Users can provide a <code>schema_url</code>, but it is up to their discretion. Therefore,   this API needs to be structured to accept a <code>schema_url</code>, but MUST NOT   obligate a user to provide one.</p> <ul> <li>[since 1.13.0] <code>attributes</code>: Specifies the instrumentation scope attributes to   associate with emitted telemetry.</li> </ul> <p>Users can provide attributes to associate with the instrumentation scope, but   it is up to their discretion. Therefore, this API MUST be structured to accept   a variable number of attributes, including none.</p> <p>Meters are identified by <code>name</code>, <code>version</code>, and <code>schema_url</code> fields. When more than one <code>Meter</code> of the same <code>name</code>, <code>version</code>, and <code>schema_url</code> is created, it is unspecified whether or under which conditions the same or different <code>Meter</code> instances are returned. It is a user error to create Meters with different attributes but the same identity.</p> <p>The term identical applied to Meters describes instances where all identifying fields are equal. The term distinct applied to Meters describes instances where at least one identifying field has a different value.</p>"},{"location":"docs/specs/otel/metrics/api/#meter","title":"Meter","text":"<p>The meter is responsible for creating Instruments.</p> <p>Note: <code>Meter</code> SHOULD NOT be responsible for the configuration. This should be the responsibility of the <code>MeterProvider</code> instead.</p>"},{"location":"docs/specs/otel/metrics/api/#meter-operations","title":"Meter operations","text":"<p>The <code>Meter</code> MUST provide functions to create new Instruments:</p> <ul> <li>Create a new Counter</li> <li>Create a new Asynchronous Counter</li> <li>Create a new Histogram</li> <li>Create a new Asynchronous Gauge</li> <li>Create a new UpDownCounter</li> <li>Create a new Asynchronous UpDownCounter</li> </ul> <p>Also see the respective sections below for more information on instrument creation.</p>"},{"location":"docs/specs/otel/metrics/api/#instrument","title":"Instrument","text":"<p>Instruments are used to report Measurements. Each Instrument will have the following fields:</p> <ul> <li>The <code>name</code> of the Instrument</li> <li>The <code>kind</code> of the Instrument - whether it is a Counter or one of   the other kinds, whether it is synchronous or asynchronous</li> <li>An optional <code>unit</code> of measure</li> <li>An optional <code>description</code></li> <li>Optional <code>advice</code> (experimental)</li> </ul> <p>Instruments are associated with the Meter during creation. Instruments are identified by all of these fields.</p> <p>Language-level features such as the distinction between integer and floating point numbers SHOULD be considered as identifying.</p>"},{"location":"docs/specs/otel/metrics/api/#general-characteristics","title":"General characteristics","text":""},{"location":"docs/specs/otel/metrics/api/#instrument-name-syntax","title":"Instrument name syntax","text":"<p>The instrument name syntax is defined below using the Augmented Backus-Naur Form:</p> <pre><code>instrument-name = ALPHA 0*62 (\"_\" / \".\" / \"-\" / ALPHA / DIGIT)\nALPHA = %x41-5A / %x61-7A; A-Z / a-z\nDIGIT = %x30-39 ; 0-9\n</code></pre> <ul> <li>They are not null or empty strings.</li> <li>They are case-insensitive, ASCII strings.</li> <li>The first character must be an alphabetic character.</li> <li>Subsequent characters must belong to the alphanumeric characters, '_', '.',   and '-'.</li> <li>They can have a maximum length of 63 characters.</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#instrument-unit","title":"Instrument unit","text":"<p>The <code>unit</code> is an optional string provided by the author of the Instrument. The API SHOULD treat it as an opaque string.</p> <ul> <li>It MUST be case-sensitive (e.g. <code>kb</code> and <code>kB</code> are different units), ASCII   string.</li> <li>It can have a maximum length of 63 characters. The number 63 is chosen to   allow the unit strings (including the <code>\\0</code> terminator on certain language   runtimes) to be stored and compared as fixed size array/struct when   performance is critical.</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#instrument-description","title":"Instrument description","text":"<p>The <code>description</code> is an optional free-form text provided by the author of the instrument. The API MUST treat it as an opaque string.</p> <ul> <li>It MUST support   BMP (Unicode Plane 0),   which is basically only the first three bytes of UTF-8 (or <code>utf8mb3</code>).   OpenTelemetry API authors MAY decide if they want to   support more Unicode   Planes.</li> <li>It MUST support at least 1023 characters.   OpenTelemetry API authors MAY decide if they want to   support more.</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#instrument-advice","title":"Instrument advice","text":"<p>Status: Experimental</p> <p><code>advice</code> are an optional set of recommendations provided by the author of the Instrument, aimed at assisting implementations in providing useful output with minimal configuration.</p> <ul> <li>Implementations MAY ignore <code>advice</code>. However, OpenTelemetry SDKs handle   <code>advice</code> as described here.</li> <li><code>advice</code> parameters may be general, or vary by instrument <code>kind</code>.</li> <li><code>Histogram</code>:<ul> <li><code>ExplicitBucketBoundaries</code> (<code>double[]</code>): The recommended set of bucket   boundaries to use if aggregating to   explicit bucket Histogram metric data point.</li> </ul> </li> </ul>"},{"location":"docs/specs/otel/metrics/api/#synchronous-and-asynchronous-instruments","title":"Synchronous and Asynchronous instruments","text":"<p>Instruments are categorized on whether they are synchronous or asynchronous:</p> <ul> <li> <p>Synchronous instruments (e.g. Counter) are meant to be invoked   inline with application/business processing logic. For example, an HTTP client   could use a Counter to record the number of bytes it has received.   Measurements recorded by synchronous instruments can be   associated with the Context.</p> </li> <li> <p>Asynchronous instruments (e.g. Asynchronous Gauge) give   the user a way to register callback function, and the callback function will   be invoked only on demand (see SDK collection for   reference). For example, a piece of embedded software could use an   asynchronous gauge to collect the temperature from a sensor every 15 seconds,   which means the callback function will only be invoked every 15 seconds.   Measurements recorded by asynchronous instruments cannot be   associated with the Context.</p> </li> </ul> <p>Please note that the term synchronous and asynchronous have nothing to do with the asynchronous pattern.</p>"},{"location":"docs/specs/otel/metrics/api/#synchronous-instrument-api","title":"Synchronous Instrument API","text":"<p>The API to construct synchronous instruments MUST accept the following parameters:</p> <ul> <li>A <code>name</code> of the Instrument.</li> </ul> <p>The <code>name</code> needs to be provided by a user. If possible, the API SHOULD be   structured so a user is obligated to provide this parameter. If it is not   possible to structurally enforce this obligation, the API MUST be documented   in a way to communicate to users that this parameter is needed.</p> <p>The API SHOULD be documented in a way to communicate to users that the <code>name</code>   parameter needs to conform to the   instrument name syntax. The API SHOULD NOT validate   the <code>name</code>; that is left to implementations of the API.</p> <ul> <li>A <code>unit</code> of measure.</li> </ul> <p>Users can provide a <code>unit</code>, but it is up to their discretion. Therefore, this   API needs to be structured to accept a <code>unit</code>, but MUST NOT obligate a user to   provide one.</p> <p>The <code>unit</code> parameter needs to support the   instrument unit rule. Meaning, the API MUST accept a   case-sensitive string that supports ASCII character encoding and can hold at   least 63 characters. The API SHOULD NOT validate the <code>unit</code>.</p> <ul> <li>A <code>description</code> describing the Instrument in human-readable terms.</li> </ul> <p>Users can provide a <code>description</code>, but it is up to their discretion.   Therefore, this API needs to be structured to accept a <code>description</code>, but MUST   NOT obligate a user to provide one.</p> <p>The <code>description</code> needs to support the   instrument description rule. Meaning, the API MUST   accept a string that supports at least   BMP (Unicode Plane 0)   encoded characters and hold at least 1023 characters.</p> <ul> <li><code>advice</code> for implementations.</li> </ul> <p>Users can provide <code>advice</code>, but its up to their discretion. Therefore, this   API needs to be structured to accept <code>advice</code>, but MUST NOT obligate the user   to provide it.</p> <p><code>advice</code> needs to be structured as described in   instrument advice, with parameters that are general and   specific to a particular instrument <code>kind</code>. The API SHOULD NOT validate   <code>advice</code>.</p>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-instrument-api","title":"Asynchronous Instrument API","text":"<p>Asynchronous instruments have associated <code>callback</code> functions which are responsible for reporting Measurements. Callback functions will be called only when the Meter is being observed. The order of callback execution is not specified.</p> <p>The API to construct asynchronous instruments MUST accept the following parameters:</p> <ul> <li>A <code>name</code> of the Instrument.</li> </ul> <p>The <code>name</code> needs to be provided by a user. If possible, the API SHOULD be   structured so a user is obligated to provide this parameter. If it is not   possible to structurally enforce this obligation, the API MUST be documented   in a way to communicate to users that this parameter is needed.</p> <p>The API SHOULD be documented in a way to communicate to users that the <code>name</code>   parameter needs to conform to the   instrument name syntax. The API SHOULD NOT validate   the <code>name</code>, that is left to implementations of the API.</p> <ul> <li>A <code>unit</code> of measure.</li> </ul> <p>Users can provide a <code>unit</code>, but it is up to their discretion. Therefore, this   API needs to be structured to accept a <code>unit</code>, but MUST NOT obligate a user to   provide one.</p> <p>The <code>unit</code> parameter needs to support the   instrument unit rule. Meaning, the API MUST accept a   case-sensitive string that supports ASCII character encoding and can hold at   least 63 characters. The API SHOULD NOT validate the <code>unit</code>.</p> <ul> <li>A <code>description</code> describing the Instrument in human-readable terms.</li> </ul> <p>Users can provide a <code>description</code>, but it is up to their discretion.   Therefore, this API needs to be structured to accept a <code>description</code>, but MUST   NOT obligate a user to provide one.</p> <p>The <code>description</code> needs to support the   instrument description rule. Meaning, the API MUST   accept a string that supports at least   BMP (Unicode Plane 0)   encoded characters and hold at least 1023 characters.</p> <ul> <li><code>callback</code> functions that report Measurements of the created   instrument.</li> </ul> <p>Users can provide <code>callback</code> functions, but it is up to their discretion.   Therefore, this API MUST be structured to accept a variable number of   <code>callback</code> functions, including none.</p> <p>The API MUST support creation of asynchronous instruments by passing zero or more <code>callback</code> functions to be permanently registered to the newly created instrument.</p> <p>A Callback is the conceptual entity created each time a <code>callback</code> function is registered through an OpenTelemetry API.</p> <p>The API SHOULD support registration of <code>callback</code> functions associated with asynchronous instruments after they are created.</p> <p>Where the API supports registration of <code>callback</code> functions after asynchronous instrumentation creation, the user MUST be able to undo registration of the specific callback after its registration by some means.</p> <p>Every currently registered Callback associated with a set of instruments MUST be evaluated exactly once during collection prior to reading data for that instrument set.</p> <p>Callback functions MUST be documented as follows for the end user:</p> <ul> <li>Callback functions SHOULD be reentrant safe. The SDK expects to evaluate   callbacks for each MetricReader independently.</li> <li>Callback functions SHOULD NOT take an indefinite amount of time.</li> <li>Callback functions SHOULD NOT make duplicate observations (more than one   <code>Measurement</code> with the same <code>attributes</code>) across all registered callbacks.</li> </ul> <p>The resulting behavior when a callback violates any of these RECOMMENDATIONS is explicitly not specified at the API level.</p> <p>OpenTelemetry API authors MAY decide what is the idiomatic approach for capturing measurements from callback functions. Here are some examples:</p> <ul> <li>Return a list (or tuple, generator, enumerator, etc.) of individual   <code>Measurement</code> values.</li> <li>Pass an Observable Result as a formal parameter of the callback, where   <code>result.Observe()</code> captures individual <code>Measurement</code> values.</li> </ul> <p>Callbacks registered at the time of instrument creation MUST apply to the single instruments which is under construction.</p> <p>Callbacks registered after the time of instrument creation MAY be associated with multiple instruments.</p> <p>Idiomatic APIs for multiple-instrument Callbacks MUST distinguish the instrument associated with each observed <code>Measurement</code> value.</p> <p>Multiple-instrument Callbacks MUST be associated at the time of registration with a declared set of asynchronous instruments from the same <code>Meter</code> instance. This requirement that Instruments be declaratively associated with Callbacks allows an SDK to execute only those Callbacks that are necessary to evaluate instruments that are in use by a configured View.</p> <p>The API MUST treat observations from a single Callback as logically taking place at a single instant, such that when recorded, observations from a single callback MUST be reported with identical timestamps.</p> <p>The API SHOULD provide some way to pass <code>state</code> to the callback. OpenTelemetry API authors MAY decide what is the idiomatic approach (e.g. it could be an additional parameter to the callback function, or captured by the lambda closure, or something else).</p>"},{"location":"docs/specs/otel/metrics/api/#counter","title":"Counter","text":"<p><code>Counter</code> is a synchronous Instrument which supports non-negative increments.</p> <p>Example uses for <code>Counter</code>:</p> <ul> <li>count the number of bytes received</li> <li>count the number of requests completed</li> <li>count the number of accounts created</li> <li>count the number of checkpoints run</li> <li>count the number of HTTP 5xx errors</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#counter-creation","title":"Counter creation","text":"<p>There MUST NOT be any API for creating a <code>Counter</code> other than with a <code>Meter</code>. This MAY be called <code>CreateCounter</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateUInt64Counter</code>, <code>CreateDoubleCounter</code>, <code>CreateCounter&lt;UInt64&gt;</code>, <code>CreateCounter&lt;double&gt;</code>.</p> <p>See the general requirements for synchronous instruments.</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\nexception_counter = meter.create_counter(name=\"exceptions\", description=\"number of exceptions caught\", value_type=int)\n</code></pre> <pre><code>// C#\nvar counterExceptions = meter.CreateCounter&lt;UInt64&gt;(\"exceptions\", description=\"number of exceptions caught\");\nreadonly struct PowerConsumption\n{\n[HighCardinality]\nstring customer;\n};\nvar counterPowerUsed = meter.CreateCounter&lt;double, PowerConsumption&gt;(\"power_consumption\", unit=\"kWh\");\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#counter-operations","title":"Counter operations","text":""},{"location":"docs/specs/otel/metrics/api/#add","title":"Add","text":"<p>Increment the Counter by a fixed amount.</p> <p>This API SHOULD NOT return a value (it MAY return a dummy value if required by certain programming languages or systems, for example <code>null</code>, <code>undefined</code>).</p> <p>This API MUST accept the following parameter:</p> <ul> <li>A numeric increment value.</li> </ul> <p>The increment value needs to be provided by a user. If possible, this API   SHOULD be structured so a user is obligated to provide this parameter. If it   is not possible to structurally enforce this obligation, this API MUST be   documented in a way to communicate to users that this parameter is needed.</p> <p>The increment value is expected to be non-negative. This API SHOULD be   documented in a way to communicate to users that this value is expected to be   non-negative. This API SHOULD NOT validate this value, that is left to   implementations of the API.</p> <ul> <li>Attributes to associate with the increment   value.</li> </ul> <p>Users can provide attributes to associate with the increment value, but it is   up to their discretion. Therefore, this API MUST be structured to accept a   variable number of attributes, including none.</p> <p>The OpenTelemetry API authors MAY decide to allow flexible attributes to be passed in as arguments. If the attribute names and types are provided during the counter creation, the OpenTelemetry API authors MAY allow attribute values to be passed in using a more efficient way (e.g. strong typed struct allocated on the callstack, tuple). The API MUST allow callers to provide flexible attributes at invocation time rather than having to register all the possible attribute names during the instrument creation. Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\nexception_counter.add(1, {\"exception_type\": \"IOError\", \"handled_by_user\": True})\nexception_counter.add(1, exception_type=\"IOError\", handled_by_user=True)\n</code></pre> <pre><code>// C#\ncounterExceptions.Add(1, (\"exception_type\", \"FileLoadException\"), (\"handled_by_user\", true));\ncounterPowerUsed.Add(13.5, new PowerConsumption { customer = \"Tom\" });\ncounterPowerUsed.Add(200, new PowerConsumption { customer = \"Jerry\" }, (\"is_green_energy\", true));\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-counter","title":"Asynchronous Counter","text":"<p>Asynchronous Counter is an asynchronous Instrument which reports monotonically increasing value(s) when the instrument is being observed.</p> <p>Example uses for Asynchronous Counter:</p> <ul> <li>CPU time, which could be reported for   each thread, each process or the entire system. For example \"the CPU time for   process A running in user mode, measured in seconds\".</li> <li>The number of page faults for each   process.</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-counter-creation","title":"Asynchronous Counter creation","text":"<p>There MUST NOT be any API for creating an Asynchronous Counter other than with a <code>Meter</code>. This MAY be called <code>CreateObservableCounter</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateUInt64ObservableCounter</code>, <code>CreateDoubleObservableCounter</code>, <code>CreateObservableCounter&lt;UInt64&gt;</code>, <code>CreateObservableCounter&lt;double&gt;</code>.</p> <p>It is highly recommended that implementations use the name <code>ObservableCounter</code> (or any language idiomatic variation, e.g. <code>observable_counter</code>) unless there is a strong reason not to do so. Please note that the name has nothing to do with asynchronous pattern and observer pattern.</p> <p>See the general requirements for asynchronous instruments.</p> <p>Note: Unlike Counter.Add() which takes the increment/delta value, the callback function reports the absolute value of the counter. To determine the reported rate the counter is changing, the difference between successive measurements is used.</p> <p>OpenTelemetry API authors MAY decide what is the idiomatic approach. Here are some examples:</p> <ul> <li>Return a list (or tuple, generator, enumerator, etc.) of <code>Measurement</code>s.</li> <li>Use an observable result argument to allow individual <code>Measurement</code>s to be   reported.</li> </ul> <p>User code is recommended not to provide more than one <code>Measurement</code> with the same <code>attributes</code> in a single callback. If it happens, OpenTelemetry SDK authors MAY decide how to handle it in the SDK. For example, during the callback invocation if two measurements <code>value=1, attributes={pid:4, bitness:64}</code> and <code>value=2, attributes={pid:4, bitness:64}</code> are reported, OpenTelemetry SDK authors MAY decide to simply let them pass through (so the downstream consumer can handle duplication), drop the entire data, pick the last one, or something else. The API MUST treat observations from a single callback as logically taking place at a single instant, such that when recorded, observations from a single callback MUST be reported with identical timestamps.</p> <p>The API SHOULD provide some way to pass <code>state</code> to the callback. OpenTelemetry API authors MAY decide what is the idiomatic approach (e.g. it could be an additional parameter to the callback function, or captured by the lambda closure, or something else).</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\ndef pf_callback():\n# Note: in the real world these would be retrieved from the operating system\nreturn (\n(8,        (\"pid\", 0),   (\"bitness\", 64)),\n(37741921, (\"pid\", 4),   (\"bitness\", 64)),\n(10465,    (\"pid\", 880), (\"bitness\", 32)),\n)\nmeter.create_observable_counter(name=\"PF\", description=\"process page faults\", pf_callback)\n</code></pre> <pre><code># Python\ndef pf_callback(result):\n# Note: in the real world these would be retrieved from the operating system\nresult.Observe(8,        (\"pid\", 0),   (\"bitness\", 64))\nresult.Observe(37741921, (\"pid\", 4),   (\"bitness\", 64))\nresult.Observe(10465,    (\"pid\", 880), (\"bitness\", 32))\nmeter.create_observable_counter(name=\"PF\", description=\"process page faults\", pf_callback)\n</code></pre> <pre><code>// C#\n// A simple scenario where only one value is reported\ninterface IAtomicClock\n{\nUInt64 GetCaesiumOscillates();\n}\nIAtomicClock clock = AtomicClock.Connect();\nmeter.CreateObservableCounter&lt;UInt64&gt;(\"caesium_oscillates\", () =&gt; clock.GetCaesiumOscillates());\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-counter-operations","title":"Asynchronous Counter operations","text":"<p>Asynchronous Counter uses an idiomatic interface for reporting measurements through a <code>callback</code>, which is registered during Asynchronous Counter creation.</p> <p>For callback functions registered after an asynchronous instrument is created, the API is required to support a mechanism for unregistration. For example, the object returned from <code>register_callback</code> can support an <code>unregister()</code> method directly.</p> <pre><code># Python\nclass Device:\n\"\"\"A device with one counter\"\"\"\ndef __init__(self, meter, x):\nself.x = x\ncounter = meter.create_observable_counter(name=\"usage\", description=\"count of items used\")\nself.cb = counter.register_callback(self.counter_callback)\ndef counter_callback(self, result):\nresult.Observe(self.read_counter(), {'x', self.x})\ndef read_counter(self):\nreturn 100  # ...\ndef stop(self):\nself.cb.unregister()\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#histogram","title":"Histogram","text":"<p><code>Histogram</code> is a synchronous Instrument which can be used to report arbitrary values that are likely to be statistically meaningful. It is intended for statistics such as histograms, summaries, and percentile.</p> <p>Example uses for <code>Histogram</code>:</p> <ul> <li>the request duration</li> <li>the size of the response payload</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#histogram-creation","title":"Histogram creation","text":"<p>There MUST NOT be any API for creating a <code>Histogram</code> other than with a <code>Meter</code>. This MAY be called <code>CreateHistogram</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateUInt64Histogram</code>, <code>CreateDoubleHistogram</code>, <code>CreateHistogram&lt;UInt64&gt;</code>, <code>CreateHistogram&lt;double&gt;</code>.</p> <p>See the general requirements for synchronous instruments.</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\nhttp_server_duration = meter.create_histogram(\nname=\"http.server.duration\",\ndescription=\"measures the duration of the inbound HTTP request\",\nunit=\"ms\",\nvalue_type=float)\n</code></pre> <pre><code>// C#\nvar httpServerDuration = meter.CreateHistogram&lt;double&gt;(\n\"http.server.duration\",\ndescription: \"measures the duration of the inbound HTTP request\",\nunit: \"ms\"\n);\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#histogram-operations","title":"Histogram operations","text":""},{"location":"docs/specs/otel/metrics/api/#record","title":"Record","text":"<p>Updates the statistics with the specified amount.</p> <p>This API SHOULD NOT return a value (it MAY return a dummy value if required by certain programming languages or systems, for example <code>null</code>, <code>undefined</code>).</p> <p>This API MUST accept the following parameter:</p> <ul> <li>A numeric value to record.</li> </ul> <p>The value needs to be provided by a user. If possible, this API SHOULD be   structured so a user is obligated to provide this parameter. If it is not   possible to structurally enforce this obligation, this API MUST be documented   in a way to communicate to users that this parameter is needed.</p> <p>The value is expected to be non-negative. This API SHOULD be documented in a   way to communicate to users that this value is expected to be non-negative.   This API SHOULD NOT validate this value, that is left to implementations of   the API.</p> <ul> <li>Attributes to associate with the value.</li> </ul> <p>Users can provide attributes to associate with the value, but it is up to   their discretion. Therefore, this API MUST be structured to accept a variable   number of attributes, including none.</p> <p>OpenTelemetry API authors MAY decide to allow flexible attributes to be passed in as individual arguments. OpenTelemetry API authors MAY allow attribute values to be passed in using a more efficient way (e.g. strong typed struct allocated on the callstack, tuple). Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\nhttp_server_duration.Record(50, {\"http.request.method\": \"POST\", \"url.scheme\": \"https\"})\nhttp_server_duration.Record(100, http_method=\"GET\", http_scheme=\"http\")\n</code></pre> <pre><code>// C#\nhttpServerDuration.Record(50, (\"http.request.method\", \"POST\"), (\"url.scheme\", \"https\"));\nhttpServerDuration.Record(100, new HttpRequestAttributes { method = \"GET\", scheme = \"http\" });\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-gauge","title":"Asynchronous Gauge","text":"<p>Asynchronous Gauge is an asynchronous Instrument which reports non-additive value(s) (e.g. the room temperature - it makes no sense to report the temperature value from multiple rooms and sum them up) when the instrument is being observed.</p> <p>Note: if the values are additive (e.g. the process heap size - it makes sense to report the heap size from multiple processes and sum them up, so we get the total heap usage), use Asynchronous Counter or Asynchronous UpDownCounter.</p> <p>Example uses for Asynchronous Gauge:</p> <ul> <li>the current room temperature</li> <li>the CPU fan speed</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-gauge-creation","title":"Asynchronous Gauge creation","text":"<p>There MUST NOT be any API for creating an Asynchronous Gauge other than with a <code>Meter</code>. This MAY be called <code>CreateObservableGauge</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateUInt64ObservableGauge</code>, <code>CreateDoubleObservableGauge</code>, <code>CreateObservableGauge&lt;UInt64&gt;</code>, <code>CreateObservableGauge&lt;double&gt;</code>.</p> <p>It is highly recommended that implementations use the name <code>ObservableGauge</code> (or any language idiomatic variation, e.g. <code>observable_gauge</code>) unless there is a strong reason not to do so. Please note that the name has nothing to do with asynchronous pattern and observer pattern.</p> <p>See the general requirements for asynchronous instruments.</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\ndef cpu_frequency_callback():\n# Note: in the real world these would be retrieved from the operating system\nreturn (\n(3.38, (\"cpu\", 0), (\"core\", 0)),\n(3.51, (\"cpu\", 0), (\"core\", 1)),\n(0.57, (\"cpu\", 1), (\"core\", 0)),\n(0.56, (\"cpu\", 1), (\"core\", 1)),\n)\nmeter.create_observable_gauge(\nname=\"cpu.frequency\",\ndescription=\"the real-time CPU clock speed\",\ncallback=cpu_frequency_callback,\nunit=\"GHz\",\nvalue_type=float)\n</code></pre> <pre><code># Python\ndef cpu_frequency_callback(result):\n# Note: in the real world these would be retrieved from the operating system\nresult.Observe(3.38, (\"cpu\", 0), (\"core\", 0))\nresult.Observe(3.51, (\"cpu\", 0), (\"core\", 1))\nresult.Observe(0.57, (\"cpu\", 1), (\"core\", 0))\nresult.Observe(0.56, (\"cpu\", 1), (\"core\", 1))\nmeter.create_observable_gauge(\nname=\"cpu.frequency\",\ndescription=\"the real-time CPU clock speed\",\ncallback=cpu_frequency_callback,\nunit=\"GHz\",\nvalue_type=float)\n</code></pre> <pre><code>// C#\n// A simple scenario where only one value is reported\nmeter.CreateObservableGauge&lt;double&gt;(\"temperature\", () =&gt; sensor.GetTemperature());\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-gauge-operations","title":"Asynchronous Gauge operations","text":"<p>Asynchronous Gauge uses an idiomatic interface for reporting measurements through a <code>callback</code>, which is registered during Asynchronous Gauge creation.</p> <p>For callback functions registered after an asynchronous instrument is created, the API is required to support a mechanism for unregistration. For example, the object returned from <code>register_callback</code> can support an <code>unregister()</code> method directly.</p> <pre><code># Python\nclass Device:\n\"\"\"A device with one gauge\"\"\"\ndef __init__(self, meter, x):\nself.x = x\ngauge = meter.create_observable_gauge(name=\"pressure\", description=\"force/area\")\nself.cb = gauge.register_callback(self.gauge_callback)\ndef gauge_callback(self, result):\nresult.Observe(self.read_gauge(), {'x', self.x})\ndef read_gauge(self):\nreturn 100  # ...\ndef stop(self):\nself.cb.unregister()\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#updowncounter","title":"UpDownCounter","text":"<p><code>UpDownCounter</code> is a synchronous Instrument which supports increments and decrements.</p> <p>Note: if the value is monotonically increasing, use Counter instead.</p> <p>Example uses for <code>UpDownCounter</code>:</p> <ul> <li>the number of active requests</li> <li>the number of items in a queue</li> </ul> <p>An <code>UpDownCounter</code> is intended for scenarios where the absolute values are not pre-calculated, or fetching the \"current value\" requires extra effort. If the pre-calculated value is already available or fetching the snapshot of the \"current value\" is straightforward, use Asynchronous UpDownCounter instead.</p> <p>UpDownCounter supports counting the size of a collection incrementally, e.g. reporting the number of items in a concurrent bag by the \"color\" and \"material\" properties as they are added and removed.</p> Color Material Count Red Aluminum 1 Red Steel 2 Blue Aluminum 0 Blue Steel 5 Yellow Aluminum 0 Yellow Steel 3 <pre><code># Python\nitems_counter = meter.create_up_down_counter(\nname=\"store.inventory\",\ndescription=\"the number of the items available\")\ndef restock_item(color, material):\ninventory.add_item(color=color, material=material)\nitems_counter.add(1, {\"color\": color, \"material\": material})\nreturn true\ndef sell_item(color, material):\nsucceeded = inventory.take_item(color=color, material=material)\nif succeeded:\nitems_counter.add(-1, {\"color\": color, \"material\": material})\nreturn succeeded\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#updowncounter-creation","title":"UpDownCounter creation","text":"<p>There MUST NOT be any API for creating an <code>UpDownCounter</code> other than with a <code>Meter</code>. This MAY be called <code>CreateUpDownCounter</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateInt64UpDownCounter</code>, <code>CreateDoubleUpDownCounter</code>, <code>CreateUpDownCounter&lt;Int64&gt;</code>, <code>CreateUpDownCounter&lt;double&gt;</code>.</p> <p>See the general requirements for synchronous instruments.</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\ncustomers_in_store = meter.create_up_down_counter(\nname=\"grocery.customers\",\ndescription=\"measures the current customers in the grocery store\",\nvalue_type=int)\n</code></pre> <pre><code>// C#\nvar customersInStore = meter.CreateUpDownCounter&lt;int&gt;(\n\"grocery.customers\",\ndescription: \"measures the current customers in the grocery store\",\n);\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#updowncounter-operations","title":"UpDownCounter operations","text":""},{"location":"docs/specs/otel/metrics/api/#add_1","title":"Add","text":"<p>Increment or decrement the UpDownCounter by a fixed amount.</p> <p>This API SHOULD NOT return a value (it MAY return a dummy value if required by certain programming languages or systems, for example <code>null</code>, <code>undefined</code>).</p> <p>This API MUST accept the following parameter:</p> <ul> <li>A numeric value to add.</li> </ul> <p>The value needs to be provided by a user. If possible, this API SHOULD be   structured so a user is obligated to provide this parameter. If it is not   possible to structurally enforce this obligation, this API MUST be documented   in a way to communicate to users that this parameter is needed.</p> <ul> <li>Attributes to associate with the value.</li> </ul> <p>Users can provide attributes to associate with the value, but it is up to   their discretion. Therefore, this API MUST be structured to accept a variable   number of attributes, including none.</p> <p>OpenTelemetry API authors MAY decide to allow flexible attributes to be passed in as individual arguments. OpenTelemetry API authors MAY allow attribute values to be passed in using a more efficient way (e.g. strong typed struct allocated on the callstack, tuple). Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\ncustomers_in_store.add(1, {\"account.type\": \"commercial\"})\ncustomers_in_store.add(-1, account_type=\"residential\")\n</code></pre> <pre><code>// C#\ncustomersInStore.Add(1, (\"account.type\", \"commercial\"));\ncustomersInStore.Add(-1, new Account { Type = \"residential\" });\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-updowncounter","title":"Asynchronous UpDownCounter","text":"<p>Asynchronous UpDownCounter is an asynchronous Instrument which reports additive value(s) (e.g. the process heap size - it makes sense to report the heap size from multiple processes and sum them up, so we get the total heap usage) when the instrument is being observed.</p> <p>Note: if the value is monotonically increasing, use Asynchronous Counter instead; if the value is non-additive, use Asynchronous Gauge instead.</p> <p>Example uses for Asynchronous UpDownCounter:</p> <ul> <li>the process heap size</li> <li>the approximate number of items in a lock-free circular buffer</li> </ul>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-updowncounter-creation","title":"Asynchronous UpDownCounter creation","text":"<p>There MUST NOT be any API for creating an Asynchronous UpDownCounter other than with a <code>Meter</code>. This MAY be called <code>CreateObservableUpDownCounter</code>. If strong type is desired, OpenTelemetry API authors MAY decide the language idiomatic name(s), for example <code>CreateUInt64ObservableUpDownCounter</code>, <code>CreateDoubleObservableUpDownCounter</code>, <code>CreateObservableUpDownCounter&lt;UInt64&gt;</code>, <code>CreateObservableUpDownCounter&lt;double&gt;</code>.</p> <p>It is highly recommended that implementations use the name <code>ObservableUpDownCounter</code> (or any language idiomatic variation, e.g. <code>observable_updowncounter</code>) unless there is a strong reason not to do so. Please note that the name has nothing to do with asynchronous pattern and observer pattern.</p> <p>See the general requirements for asynchronous instruments.</p> <p>Note: Unlike UpDownCounter.Add() which takes the increment/delta value, the callback function reports the absolute value of the Asynchronous UpDownCounter. To determine the reported rate the Asynchronous UpDownCounter is changing, the difference between successive measurements is used.</p> <p>Here are some examples that OpenTelemetry API authors might consider:</p> <pre><code># Python\ndef ws_callback():\n# Note: in the real world these would be retrieved from the operating system\nreturn (\n(8,      (\"pid\", 0),   (\"bitness\", 64)),\n(20,     (\"pid\", 4),   (\"bitness\", 64)),\n(126032, (\"pid\", 880), (\"bitness\", 32)),\n)\nmeter.create_observable_updowncounter(\nname=\"process.workingset\",\ndescription=\"process working set\",\ncallback=ws_callback,\nunit=\"kB\",\nvalue_type=int)\n</code></pre> <pre><code># Python\ndef ws_callback(result):\n# Note: in the real world these would be retrieved from the operating system\nresult.Observe(8,      (\"pid\", 0),   (\"bitness\", 64))\nresult.Observe(20,     (\"pid\", 4),   (\"bitness\", 64))\nresult.Observe(126032, (\"pid\", 880), (\"bitness\", 32))\nmeter.create_observable_updowncounter(\nname=\"process.workingset\",\ndescription=\"process working set\",\ncallback=ws_callback,\nunit=\"kB\",\nvalue_type=int)\n</code></pre> <pre><code>// C#\n// A simple scenario where only one value is reported\nmeter.CreateObservableUpDownCounter&lt;UInt64&gt;(\"memory.physical.free\", () =&gt; WMI.Query(\"FreePhysicalMemory\"));\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#asynchronous-updowncounter-operations","title":"Asynchronous UpDownCounter operations","text":"<p>Asynchronous UpDownCounter uses an idiomatic interface for reporting measurements through a <code>callback</code>, which is registered during Asynchronous Updowncounter creation.</p> <p>For callback functions registered after an asynchronous instrument is created, the API is required to support a mechanism for unregistration. For example, the object returned from <code>register_callback</code> can support an <code>unregister()</code> method directly.</p> <pre><code># Python\nclass Device:\n\"\"\"A device with one updowncounter\"\"\"\ndef __init__(self, meter, x):\nself.x = x\nupdowncounter = meter.create_observable_updowncounter(name=\"queue_size\", description=\"items in process\")\nself.cb = updowncounter.register_callback(self.updowncounter_callback)\ndef updowncounter_callback(self, result):\nresult.Observe(self.read_updowncounter(), {'x', self.x})\ndef read_updowncounter(self):\nreturn 100  # ...\ndef stop(self):\nself.cb.unregister()\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#measurement","title":"Measurement","text":"<p>A <code>Measurement</code> represents a data point reported via the metrics API to the SDK. Please refer to the Metrics Programming Model for the interaction between the API and SDK.</p> <p><code>Measurement</code>s encapsulate:</p> <ul> <li>A value</li> <li><code>Attributes</code></li> </ul>"},{"location":"docs/specs/otel/metrics/api/#multiple-instrument-callbacks","title":"Multiple-instrument callbacks","text":"<p>The Metrics API MAY support an interface allowing the use of multiple instruments from a single registered Callback. The API to register a new Callback SHOULD accept:</p> <ul> <li>A <code>callback</code> function</li> <li>A list (or tuple, etc.) of Instruments used in the <code>callback</code> function.</li> </ul> <p>It is RECOMMENDED that the API authors use one of the following forms for the <code>callback</code> function:</p> <ul> <li>The list (or tuple, etc.) returned by the <code>callback</code> function contains   <code>(Instrument, Measurement)</code> pairs.</li> <li>the Observable Result parameter receives an additional   <code>(Instrument, Measurement)</code> pairs</li> </ul> <p>This interface is typically a more performant way to report multiple measurements when they are obtained through an expensive process, such as reading <code>/proc</code> files or probing the garbage collection subsystem.</p> <p>For example,</p> <pre><code># Python\nclass Device:\n\"\"\"A device with two instruments\"\"\"\ndef __init__(self, meter, property):\nself.property = property\nself.usage = meter.create_observable_counter(name=\"usage\", description=\"count of items used\")\nself.pressure = meter.create_observable_gauge(name=\"pressure\", description=\"force per unit area\")\n# Note the two associated instruments are passed to the callback.\nmeter.register_callback([self.usage, self.pressure], self.observe)\ndef observe(self, result):\nusage, pressure = expensive_system_call()\nresult.observe(self.usage, usage, {'property', self.property})\nresult.observe(self.pressure, pressure, {'property', self.property})\n</code></pre>"},{"location":"docs/specs/otel/metrics/api/#compatibility-requirements","title":"Compatibility requirements","text":"<p>All the metrics components SHOULD allow new APIs to be added to existing components without introducing breaking changes.</p> <p>All the metrics APIs SHOULD allow optional parameter(s) to be added to existing APIs without introducing breaking changes, if possible.</p>"},{"location":"docs/specs/otel/metrics/api/#concurrency-requirements","title":"Concurrency requirements","text":"<p>For languages which support concurrent execution the Metrics APIs provide specific guarantees and safeties.</p> <p>MeterProvider - all methods are safe to be called concurrently.</p> <p>Meter - all methods are safe to be called concurrently.</p> <p>Instrument - All methods of any Instrument are safe to be called concurrently.</p>"},{"location":"docs/specs/otel/metrics/data-model/","title":"Data model","text":""},{"location":"docs/specs/otel/metrics/data-model/#_1","title":"\u6570\u636e\u6a21\u578b","text":"<p>Status: Mixed</p>"},{"location":"docs/specs/otel/metrics/data-model/#overview","title":"Overview","text":"<p>Status: Stable</p> <p>The OpenTelemetry data model for metrics consists of a protocol specification and semantic conventions for delivery of pre-aggregated metric timeseries data. The data model is designed for importing data from existing systems and exporting data into existing systems, as well as to support internal OpenTelemetry use-cases for generating Metrics from streams of Spans or Logs.</p> <p>Popular existing metrics data formats can be unambiguously translated into the OpenTelemetry data model for metrics, without loss of semantics or fidelity. Translation from the Prometheus and Statsd exposition formats is explicitly specified.</p> <p>The data model specifies a number of semantics-preserving data transformations for use on the collection path, supporting flexible system configuration. The model supports reliability and statelessness controls, through the choice of cumulative and delta transport. The model supports cost controls, through spatial and temporal reaggregation.</p> <p>The OpenTelemetry collector is designed to accept metrics data in a number of formats, transport data using the OpenTelemetry data model, and then export into existing systems. The data model can be unambiguously translated into the Prometheus Remote Write protocol without loss of features or semantics, through well-defined translations of the data, including the ability to automatically remove attributes and lower histogram resolution.</p>"},{"location":"docs/specs/otel/metrics/data-model/#events-data-stream-timeseries","title":"Events =&gt; Data Stream =&gt; Timeseries","text":"<p>Status: Stable</p> <p>The OTLP Metrics protocol is designed as a standard for transporting metric data. To describe the intended use of this data and the associated semantic meaning, OpenTelemetry metric data stream types will be linked into a framework containing a higher-level model, about Metrics APIs and discrete input values, and a lower-level model, defining the Timeseries and discrete output values. The relationship between models is displayed in the diagram below.</p> <p></p> <p>This protocol was designed to meet the requirements of the OpenCensus Metrics system, particularly to meet its concept of Metrics Views. Views are accomplished in the OpenTelemetry Metrics data model through support for data transformation on the collection path.</p> <p>OpenTelemetry has identified three kinds of semantics-preserving Metric data transformation that are useful in building metrics collection systems as ways of controlling cost, reliability, and resource allocation. The OpenTelemetry Metrics data model is designed to support these transformations both inside an SDK as the data originates, or as a reprocessing stage inside the OpenTelemetry collector. These transformations are:</p> <ol> <li>Temporal reaggregation: Metrics that are collected at a high-frequency can be    re-aggregated into longer intervals, allowing low-resolution timeseries to be    pre-calculated or used in place of the original metric data.</li> <li>Spatial reaggregation: Metrics that are produced with unwanted attributes can    be re-aggregated into metrics having fewer attributes.</li> <li>Delta-to-Cumulative: Metrics that are input and output with Delta temporality    unburden the client from keeping high-cardinality state. The use of deltas    allows downstream services to bear the cost of conversion into cumulative    timeseries, or to forego the cost and calculate rates directly.</li> </ol> <p>OpenTelemetry Metrics data streams are designed so that these transformations can be applied automatically to streams of the same type, subject to conditions outlined below. Every OTLP data stream has an intrinsic decomposable aggregate function making it semantically well-defined to merge data points across both temporal and spatial attributes. Every OTLP data point also has two meaningful timestamps which, combined with intrinsic aggregation, make it possible to carry out the standard metric data transformations for each of the model\u2019s basic points while ensuring that the result carries the intended meaning.</p> <p>As in OpenCensus Metrics, metrics data can be transformed into one or more Views, just by selecting the aggregation interval and the desired attributes. One stream of OTLP data can be transformed into multiple timeseries outputs by configuring different Views, and the required Views processing may be applied inside the SDK or by an external collector.</p>"},{"location":"docs/specs/otel/metrics/data-model/#example-use-cases","title":"Example Use-cases","text":"<p>The metric data model is designed around a series of \"core\" use cases. While this list is not exhaustive, it is meant to be representative of the scope and breadth of OTel metrics usage.</p> <ol> <li>OTel SDK exports 10 second resolution to a single OTel collector, using    cumulative temporality for a stateful client, stateless server:</li> <li>Collector passes-through original data to an OTLP destination</li> <li>Collector re-aggregates into longer intervals without changing attributes</li> <li>Collector re-aggregates into several distinct views, each with a subset of      the available attributes, outputs to the same destination</li> <li>OTel SDK exports 10 second resolution to a single OTel collector, using delta    temporality for a stateless client, stateful server:</li> <li>Collector re-aggregates into 60 second resolution</li> <li>Collector converts delta to cumulative temporality</li> <li>OTel SDK exports both 10 seconds resolution (e.g. CPU, request latency) and    15 minutes resolution (e.g. room temperature) to a single OTel Collector. The    collector exports streams upstream with or without aggregation.</li> <li>A number of OTel SDKs running locally each exports 10 second resolution, each    reports to a single (local) OTel collector.</li> <li>Collector re-aggregates into 60 second resolution</li> <li>Collector re-aggregates to eliminate the identity of individual SDKs (e.g.,      distinct <code>service.instance.id</code> values)</li> <li>Collector outputs to an OTLP destination</li> <li>Pool of OTel collectors receive OTLP and export Prometheus Remote Write</li> <li>Collector joins service discovery with metric resources</li> <li>Collector computes \u201cup\u201d, staleness marker</li> <li>Collector applies a distinct external label</li> <li>OTel collector receives Statsd and exports OTLP</li> <li>With delta temporality: stateless collector</li> <li>With cumulative temporality: stateful collector</li> <li>OTel SDK exports directly to 3P backend</li> </ol> <p>These are considered the \"core\" use-cases used to analyze tradeoffs and design decisions within the metrics data model.</p>"},{"location":"docs/specs/otel/metrics/data-model/#out-of-scope-use-cases","title":"Out of Scope Use-cases","text":"<p>The metrics data model is NOT designed to be a perfect rosetta stone of metrics. Here are a set of use cases that, while won't be outright unsupported, are not in scope for key design decisions:</p> <ul> <li>Using OTLP as an intermediary format between two non-compatible formats</li> <li>Importing statsd =&gt; Prometheus PRW</li> <li>Importing collectd =&gt;     Prometheus PRW</li> <li>Importing Prometheus endpoint scrape =&gt; [statsd push | collectd |     opencensus]</li> <li>Importing OpenCensus \"oca\" =&gt; any non OC or OTel format</li> <li>TODO: define others.</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#model-details","title":"Model Details","text":"<p>Status: Stable</p> <p>OpenTelemetry fragments metrics into three interacting models:</p> <ul> <li>An Event model, representing how instrumentation reports metric data.</li> <li>A Timeseries model, representing how backends store metric data.</li> <li>A Metric Stream model, defining the *O*pen*T*e*L*emetry *P*rotocol (OTLP)   representing how metric data streams are manipulated and transmitted between   the Event model and the Timeseries storage. This is the model specified in   this document.</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#event-model","title":"Event Model","text":"<p>The event model is where recording of data happens. Its foundation is made of Instruments, which are used to record data observations via events. These raw events are then transformed in some fashion before being sent to some other system. OpenTelemetry metrics are designed such that the same instrument and events can be used in different ways to generate metric streams.</p> <p></p> <p>Even though observation events could be reported directly to a backend, in practice this would be infeasible due to the sheer volume of data used in observability systems, and the limited amount of network/CPU resources available for telemetry collection purposes. The best example of this is the Histogram metric where raw events are recorded in a compressed format rather than individual timeseries.</p> <p>Note: The above picture shows how one instrument can transform events into more than one type of metric stream. There are caveats and nuances for when and how to do this. Instrument and metric configuration are outlined in the metrics API specification.</p> <p>While OpenTelemetry provides flexibility in how instruments can be transformed into metric streams, the instruments are defined such that a reasonable default mapping can be provided. The exact OpenTelemetry instruments are detailed in the API specification.</p> <p>In the Event model, the primary data are (instrument, number) points, originally observed in real time or on demand (for the synchronous and asynchronous cases, respectively).</p>"},{"location":"docs/specs/otel/metrics/data-model/#timeseries-model","title":"Timeseries Model","text":"<p>In this low-level metrics data model, a Timeseries is defined by an entity consisting of several metadata properties:</p> <ul> <li>Metric name</li> <li>Attributes (dimensions)</li> <li>Value type of the point (integer, floating point, etc)</li> <li>Unit of measurement</li> </ul> <p>The primary data of each timeseries are ordered (timestamp, value) points, with one of the following value types:</p> <ol> <li>Counter (Monotonic, Cumulative)</li> <li>Gauge</li> <li>Histogram</li> <li>Exponential Histogram</li> </ol> <p>This model may be viewed as an idealization of Prometheus Remote Write. Like that protocol, we are additionally concerned with knowing when a point value is defined, as compared with being implicitly or explicitly absent. A metric stream of delta data points defines time-interval values, not point-in-time values. To precisely define presence and absence of data requires further development of the correspondence between these models.</p> <p>Note: Prometheus is not the only possible timeseries model for OpenTelemetry to map into, but is used as a reference throughout this document.</p>"},{"location":"docs/specs/otel/metrics/data-model/#opentelemetry-protocol-data-model","title":"OpenTelemetry Protocol data model","text":"<p>The OpenTelemetry protocol (OTLP) data model is composed of Metric data streams. These streams are in turn composed of metric data points. Metric data streams can be converted directly into Timeseries.</p> <p>Metric streams are grouped into individual <code>Metric</code> objects, identified by:</p> <ul> <li>The originating <code>Resource</code> attributes</li> <li>The instrumentation <code>Scope</code> (e.g., instrumentation library name, version)</li> <li>The metric stream's <code>name</code></li> </ul> <p>Including <code>name</code>, the <code>Metric</code> object is defined by the following properties:</p> <ul> <li>The data point type (e.g. <code>Sum</code>, <code>Gauge</code>, <code>Histogram</code> <code>ExponentialHistogram</code>,   <code>Summary</code>)</li> <li>The metric stream's <code>unit</code></li> <li>The metric stream's <code>description</code></li> <li>Intrinsic data point properties, where applicable: <code>AggregationTemporality</code>,   <code>Monotonic</code></li> </ul> <p>The data point type, <code>unit</code>, and intrinsic properties are considered identifying, whereas the <code>description</code> field is explicitly not identifying in nature.</p> <p>Extrinsic properties of specific points are not considered identifying; these include but are not limited to:</p> <ul> <li>Bucket boundaries of a <code>Histogram</code> data point</li> <li>Scale or bucket count of a <code>ExponentialHistogram</code> data point.</li> </ul> <p>The <code>Metric</code> object contains individual streams, identified by the set of <code>Attributes</code>. Within the individual streams, points are identified by one or two timestamps, details vary by data point type.</p> <p>Within certain data point types (e.g., <code>Sum</code> and <code>Gauge</code>) there is variation permitted in the numeric point value; in this case, the associated variation (i.e., floating-point vs. integer) is not considered identifying.</p>"},{"location":"docs/specs/otel/metrics/data-model/#opentelemetry-protocol-data-model-producer-recommendations","title":"OpenTelemetry Protocol data model: Producer recommendations","text":"<p>Producers SHOULD prevent the presence of multiple <code>Metric</code> identities for a given <code>name</code> with the same <code>Resource</code> and <code>Scope</code> attributes. Producers are expected to aggregate data for identical <code>Metric</code> objects as a basic feature, so the appearance of multiple <code>Metric</code>, considered a \"semantic error\", generally requires duplicate conflicting instrument registration to have occurred somewhere.</p> <p>Producers MAY be able to remediate the problem, depending on whether they are an SDK or a downstream processor:</p> <ol> <li>If the potential conflict involves a non-identifying property (i.e.,    <code>description</code>), the producer SHOULD choose the longer string.</li> <li>If the potential conflict involves similar but disagreeing units (e.g., \"ms\"    and \"s\"), an implementation MAY convert units to avoid semantic errors;    otherwise an implementation SHOULD inform the user of a semantic error and    pass through conflicting data.</li> <li>If the potential conflict involves an <code>AggregationTemporality</code> property, an    implementation MAY convert temporality using a Cumulative-to-Delta or a    Delta-to-Cumulative transformation; otherwise, an implementation SHOULD    inform the user of a semantic error and pass through conflicting data.</li> <li>Generally, for potential conflicts involving an identifying property (i.e.,    all properties except <code>description</code>), the producer SHOULD inform the user of    a semantic error and pass through conflicting data.</li> </ol> <p>When semantic errors such as these occur inside an implementation of the OpenTelemetry API, there is an presumption of a fixed <code>Resource</code> value. Consequently, SDKs implementing the OpenTelemetry API have complete information about the origin of duplicate instrument registration conflicts and are sometimes able to help users avoid semantic errors. See the SDK specification for specific details.</p>"},{"location":"docs/specs/otel/metrics/data-model/#opentelemetry-protocol-data-model-consumer-recommendations","title":"OpenTelemetry Protocol data model: Consumer recommendations","text":"<p>Consumers MAY reject OpenTelemetry Metrics data containing semantic errors (i.e., more than one <code>Metric</code> identity for a given <code>name</code>, <code>Resource</code>, and <code>Scope</code>).</p> <p>OpenTelemetry does not specify any means for conveying such an outcome to the end user, although this subject deserves attention.</p>"},{"location":"docs/specs/otel/metrics/data-model/#point-kinds","title":"Point kinds","text":"<p>A metric stream can use one of these basic point kinds, all of which satisfy the requirements above, meaning they define a decomposable aggregate function (also known as a \u201cnatural merge\u201d function) for points of the same kind. 1</p> <p>The basic point kinds are:</p> <ol> <li>Sum</li> <li>Gauge</li> <li>Histogram</li> <li>Exponential Histogram</li> </ol> <p>Comparing the OTLP Metric Data Stream and Timeseries data models, OTLP does not map 1:1 from its point types into timeseries points. In OTLP, a Sum point can represent a monotonic count or a non-monotonic count. This means an OTLP Sum is either translated into a Timeseries Counter, when the sum is monotonic, or a Gauge when the sum is not monotonic.</p> <p></p> <p>Specifically, in OpenTelemetry Sums always have an aggregate function where you can combine via addition. So, for non-monotonic sums in OpenTelemetry we can aggregate (naturally) via addition. In the timeseries model, you cannot assume that any particular Gauge is a sum, so the default aggregation would not be addition.</p> <p>In addition to the core point kinds used in OTLP, there are also data types designed for compatibility with existing metric formats.</p> <ul> <li>Summary</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#metric-points","title":"Metric points","text":"<p>Status: Stable</p>"},{"location":"docs/specs/otel/metrics/data-model/#sums","title":"Sums","text":"<p>Sums in OTLP consist of the following:</p> <ul> <li>An Aggregation Temporality of delta or cumulative.</li> <li>A flag denoting whether the Sum is   monotonic. In this case of   metrics, this means the sum is nominally increasing, which we assume without   loss of generality.</li> <li>For delta monotonic sums, this means the reader SHOULD expect non-negative     values.</li> <li>For cumulative monotonic sums, this means the reader SHOULD expect values     that are not less than the previous value.</li> <li>A set of data points, each containing:</li> <li>An independent set of Attribute name-value pairs.</li> <li>A time window (of <code>(start, end]</code>) time for which the Sum was calculated.<ul> <li>The time interval is inclusive of the end time.</li> <li>Times are specified in Value is UNIX Epoch time in nanoseconds since   <code>00:00:00 UTC on 1 January 1970</code></li> </ul> </li> <li>(optional) a set of examplars (see Exemplars).</li> </ul> <p>The aggregation temporality is used to understand the context in which the sum was calculated. When the aggregation temporality is \"delta\", we expect to have no overlap in time windows for metric streams, e.g.</p> <p></p> <p>Contrast with cumulative aggregation temporality where we expect to report the full sum since 'start' (where usually start means a process/application start):</p> <p></p> <p>There are various tradeoffs between using Delta vs. Cumulative aggregation, in various use cases, e.g.:</p> <ul> <li>Detecting process restarts</li> <li>Calculating rates</li> <li>Push vs. Pull based metric reporting</li> </ul> <p>OTLP supports both models, and allows APIs, SDKs and users to determine the best tradeoff for their use case.</p>"},{"location":"docs/specs/otel/metrics/data-model/#gauge","title":"Gauge","text":"<p>A Gauge in OTLP represents a sampled value at a given time. A Gauge stream consists of:</p> <ul> <li>A set of data points, each containing:</li> <li>An independent set of Attribute name-value pairs.</li> <li>A sampled value (e.g. current CPU temperature)</li> <li>A timestamp when the value was sampled (<code>time_unix_nano</code>)</li> <li>(optional) A timestamp (<code>start_time_unix_nano</code>) which best represents the     first possible moment a measurement could be recorded. This is commonly set     to the timestamp when a metric collection system started.</li> <li>(optional) a set of examplars (see Exemplars).</li> </ul> <p>In OTLP, a point within a Gauge stream represents the last-sampled event for a given time window.</p> <p></p> <p>In this example, we can see an underlying timeseries we are sampling with our Gauge. While the event model can sample more than once for a given metric reporting interval, only the last value is reported in the metric stream via OTLP.</p> <p>Gauges do not provide an aggregation semantic, instead \"last sample value\" is used when performing operations like temporal alignment or adjusting resolution.</p> <p>Gauges can be aggregated through transformation into histograms, or other metric types. These operations are not done by default, and require direct user configuration.</p>"},{"location":"docs/specs/otel/metrics/data-model/#histogram","title":"Histogram","text":"<p>Histogram metric data points convey a population of recorded measurements in a compressed format. A histogram bundles a set of events into divided populations with an overall event count and aggregate sum for all events.</p> <p></p> <p>Histograms consist of the following:</p> <ul> <li>An Aggregation Temporality of delta or cumulative.</li> <li>A set of data points, each containing:</li> <li>An independent set of Attribute name-value pairs.</li> <li>A time window (of <code>(start, end]</code>) time for which the Histogram was bundled.<ul> <li>The time interval is inclusive of the end time.</li> <li>Time values are specified as nanoseconds since the UNIX Epoch (00:00:00   UTC on 1 January 1970).</li> </ul> </li> <li>A count (<code>count</code>) of the total population of points in the histogram.</li> <li>A sum (<code>sum</code>) of all the values in the histogram.</li> <li>(optional) The min (<code>min</code>) of all values in the histogram.</li> <li>(optional) The max (<code>max</code>) of all values in the histogram.</li> <li>(optional) A series of buckets with:<ul> <li>Explicit boundary values. These values denote the lower and upper bounds   for buckets and whether not a given observation would be recorded in this   bucket.</li> <li>A count of the number of observations that fell within this bucket.</li> </ul> </li> <li>(optional) a set of examplars (see Exemplars).</li> </ul> <p>Like Sums, Histograms also define an aggregation temporality. The picture above denotes Delta temporality where accumulated event counts are reset to zero after reporting and a new aggregation occurs. Cumulative, on the other hand, continues to aggregate events, resetting with the use of a new start time.</p> <p>The aggregation temporality also has implications on the min and max fields. Min and max are more useful for Delta temporality, since the values represented by Cumulative min and max will stabilize as more events are recorded. Additionally, it is possible to convert min and max from Delta to Cumulative, but not from Cumulative to Delta. When converting from Cumulative to Delta, min and max can be dropped, or captured in an alternative representation such as a gauge.</p> <p>Bucket counts are optional. A Histogram without buckets conveys a population in terms of only the sum and count, and may be interpreted as a histogram with single bucket covering <code>(-Inf, +Inf)</code>.</p>"},{"location":"docs/specs/otel/metrics/data-model/#histogram-bucket-inclusivity","title":"Histogram: Bucket inclusivity","text":"<p>Bucket upper-bounds are inclusive (except for the case where the upper-bound is +Inf) while bucket lower-bounds are exclusive. That is, buckets express the number of values that are greater than their lower bound and less than or equal to their upper bound. Importers and exporters working with OpenTelemetry Metrics data are meant to disregard this specification when translating to and from histogram formats that use inclusive lower bounds and exclusive upper bounds. Changing the inclusivity and exclusivity of bounds is an example of worst-case Histogram error; users should choose Histogram boundaries so that worst-case error is within their error tolerance.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponentialhistogram","title":"ExponentialHistogram","text":"<p>Status: Stable</p> <p>ExponentialHistogram data points are an alternate representation to the Histogram data point, used to convey a population of recorded measurements in a compressed format. ExponentialHistogram compresses bucket boundaries using an exponential formula, making it suitable for conveying high dynamic range data with small relative error, compared with alternative representations of similar size.</p> <p>Statements about <code>Histogram</code> that refer to aggregation temporality, attributes, and timestamps, as well as the <code>sum</code>, <code>count</code>, <code>min</code>, <code>max</code> and <code>exemplars</code> fields, are the same for <code>ExponentialHistogram</code>. These fields all share identical interpretation as for <code>Histogram</code>, only the bucket structure differs between these two types.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponential-scale","title":"Exponential Scale","text":"<p>The resolution of the ExponentialHistogram is characterized by a parameter known as <code>scale</code>, with larger values of <code>scale</code> offering greater precision. Bucket boundaries of the ExponentialHistogram are located at integer powers of the <code>base</code>, also known as the \"growth factor\", where:</p> <pre><code>base = 2**(2**(-scale))\n</code></pre> <p>The symbol <code>**</code> in these formulas represents exponentiation, thus <code>2**x</code> is read \"Two to the power of <code>x</code>\", typically computed by an expression like <code>math.Pow(2.0, x)</code>. Calculated <code>base</code> values for selected scales are shown below:</p> Scale Base Expression 10 1.00068 2**(1/1024) 9 1.00135 2**(1/512) 8 1.00271 2**(1/256) 7 1.00543 2**(1/128) 6 1.01089 2**(1/64) 5 1.02190 2**(1/32) 4 1.04427 2**(1/16) 3 1.09051 2**(\u215b) 2 1.18921 2**(\u00bc) 1 1.41421 2**(\u00bd) 0 2 2**1 -1 4 2**2 -2 16 2**4 -3 256 2**8 -4 65536 2**16 <p>An important property of this design is described as \"perfect subsetting\". Buckets of an exponential Histogram with a given scale map exactly into buckets of exponential Histograms with lesser scales, which allows consumers to lower the resolution of a histogram (i.e., downscale) without introducing error.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponential-buckets","title":"Exponential Buckets","text":"<p>The ExponentialHistogram bucket identified by <code>index</code>, a signed integer, represents values in the population that are greater than <code>base**index</code> and less than or equal to <code>base**(index+1)</code>.</p> <p>The positive and negative ranges of the histogram are expressed separately. Negative values are mapped by their absolute value into the negative range using the same scale as the positive range. Note that in the negative range, therefore, histogram buckets use lower-inclusive boundaries.</p> <p>Each range of the ExponentialHistogram data point uses a dense representation of the buckets, where a range of buckets is expressed as a single <code>offset</code> value, a signed integer, and an array of count values, where array element <code>i</code> represents the bucket count for bucket index <code>offset+i</code>.</p> <p>For a given range, positive or negative:</p> <ul> <li>Bucket index <code>0</code> counts measurements in the range <code>(1, base]</code></li> <li>Positive indexes correspond with absolute values greater than <code>base</code></li> <li>Negative indexes correspond with absolute values less than or equal to 1</li> <li>There are <code>2**scale</code> buckets between successive powers of 2.</li> </ul> <p>For example, with <code>scale=3</code> there are <code>2**3</code> buckets between 1 and 2. Note that the lower boundary for bucket index 4 in a <code>scale=3</code> histogram maps into the lower boundary for bucket index 2 in a <code>scale=2</code> histogram and maps into the lower boundary for bucket index 1 (i.e., the <code>base</code>) in a <code>scale=1</code> histogram\u2014these are examples of perfect subsetting.</p> <code>scale=3</code> bucket index lower boundary equation 0 1 2**(0/8) 1 1.09051 2**(\u215b) 2 1.18921 2**(2/8), 2**(\u00bc) 3 1.29684 2**(\u215c) 4 1.41421 2**(4/8), 2**(2/4), 2**(\u00bd) 5 1.54221 2**(\u215d) 6 1.68179 2**(6/8) 7 1.83401 2**(\u215e)"},{"location":"docs/specs/otel/metrics/data-model/#zero-count-and-zero-threshold","title":"Zero Count and Zero Threshold","text":"<p>The ExponentialHistogram contains a special <code>zero_count</code> bucket and an optional <code>zero_threshold</code> field where <code>zero_count</code> contains the count of values whose absolute value is less than or equal to <code>zero_threshold</code>. The precise value for the <code>zero_threshold</code> is arbitrary and not related to the scale.</p> <p>When <code>zero_threshold</code> is unset or <code>0</code>, this bucket stores values that cannot be expressed using the standard exponential formula as well as values that have been rounded to zero.</p> <p>Histograms with different <code>zero_threshold</code> can still be merged easily by taking the largest <code>zero_threshold</code> of all involved Histograms and merge the lower buckets of Histograms with a smaller <code>zero_threshold</code> into the common wider zero bucket. If a merged <code>zero_threshold</code> is in the middle of a populated bucket, it needs to be increased to match the upper boundary of the bucket.</p> <p>In special cases, a wider zero bucket could be used to limit the total number of populated buckets.</p>"},{"location":"docs/specs/otel/metrics/data-model/#producer-expectations","title":"Producer Expectations","text":"<p>Producers MAY use an inexact mapping function because in the general case:</p> <ol> <li>Exact mapping functions are substantially more complex to implement.</li> <li>Boundaries cannot be exactly represented as floating point numbers for all    scales.</li> </ol> <p>Generally, producers SHOULD use a mapping function with an expected difference of at most 1 from the correct result for all inputs.</p> <p>The ExponentialHistogram design makes it possible to express values that are too large or small to be represented in the 64 bit \"double\" floating point format. Certain values for <code>scale</code>, while meaningful, are not necessarily useful.</p> <p>The range of data represented by an ExponentialHistogram determines which scales can be usefully applied. Regardless of scale, producers SHOULD ensure that the index of any encoded bucket falls within the range of a signed 32-bit integer. This recommendation is applied to limit the width of integers used in standard processing pipelines such as the OpenTelemetry collector. The wire-level protocol could be extended for 64-bit bucket indices in a future release.</p> <p>Producers use a mapping function to compute bucket indices. Producers are presumed to support IEEE double-width floating-point numbers with 11-bit exponent and 52-bit significand. The pseudo-code below for mapping values to exponents refers to the following constants:</p> <pre><code>const (\n// SignificandWidth is the size of an IEEE 754 double-precision\n// floating-point significand.\nSignificandWidth = 52\n// ExponentWidth is the size of an IEEE 754 double-precision\n// floating-point exponent.\nExponentWidth = 11\n// SignificandMask is the mask for the significand of an IEEE 754\n// double-precision floating-point value: 0xFFFFFFFFFFFFF.\nSignificandMask = 1 &lt;&lt; SignificandWidth - 1\n// ExponentBias is the exponent bias specified for encoding\n// the IEEE 754 double-precision floating point exponent: 1023.\nExponentBias = 1 &lt;&lt; (ExponentWidth-1) - 1\n// ExponentMask are set to 1 for the bits of an IEEE 754\n// floating point exponent: 0x7FF0000000000000.\nExponentMask = ((1 &lt;&lt; ExponentWidth) - 1) &lt;&lt; SignificandWidth\n)\n</code></pre> <p>The following choices of mapping function have been validated through reference implementations.</p>"},{"location":"docs/specs/otel/metrics/data-model/#scale-zero-extract-the-exponent","title":"Scale Zero: Extract the Exponent","text":"<p>For scale zero, the index of a value equals its normalized base-2 exponent, meaning the value of exponent in the base-2 fractional representation <code>1._significand_ * 2**_exponent_</code>. Normal IEEE 754 double-width floating point values have indices in the range <code>[-1022, +1023]</code> and subnormal values have indices in the range <code>[-1074, -1023]</code>. This may be written as:</p> <pre><code>// MapToIndexScale0 computes a bucket index at scale 0.\nfunc MapToIndexScale0(value float64) int32 {\nrawBits := math.Float64bits(value)\n// rawExponent is an 11-bit biased representation of the base-2\n// exponent:\n// - value 0 indicates a subnormal representation or a zero value\n// - value 2047 indicates an Inf or NaN value\n// - value [1, 2046] are offset by ExponentBias (1023)\nrawExponent := (int64(rawBits) &amp; ExponentMask) &gt;&gt; SignificandWidth\n// rawFragment represents (significand-1) for normal numbers,\n// where significand is in the range [1, 2).\nrawFragment := rawBits &amp; SignificandMask\n// Check for subnormal values:\nif rawExponent == 0 {\n// Handle subnormal values: rawFragment cannot be zero\n// unless value is zero.  Subnormal values have up to 52 bits\n// set, so for example greatest subnormal power of two, 0x1p-1023, has\n// rawFragment = 0x8000000000000.  Expressed in 64 bits, the value\n// (rawFragment-1) = 0x0007ffffffffffff has 13 leading zeros.\nrawExponent -= int64(bits.LeadingZeros64(rawFragment - 1) - 12)\n// In the example with 0x1p-1023, the preceding expression subtracts\n// (13-12)=1, leaving the rawExponent equal to -1.  The next statement\n// below subtracts `ExponentBias` (1023), leaving `ieeeExponent` equal\n// to -1024, which is the correct upper-inclusive bucket index for\n// the value 0x1p-1023.\n}\nieeeExponent := int32(rawExponent - ExponentBias)\n// Note that rawFragment and rawExponent cannot both be zero,\n// or else the value is exactly zero, in which case the the ZeroCount\n// bucket is used.\nif rawFragment == 0 {\n// Special case for normal power-of-two values: subtract one.\nreturn ieeeExponent - 1\n}\nreturn ieeeExponent\n}\n</code></pre> <p>Implementations are permitted to round subnormal values up to the smallest normal value, which may permit the use of a built-in function:</p> <pre><code>// MapToIndexScale0 computes a bucket index at scale 0.\nfunc MapToIndexScale0(value float64) int {\n// Note: Frexp() rounds submnormal values to the smallest normal\n// value and returns an exponent corresponding to fractions in the\n// range [0.5, 1), whereas an exponent for the range [1, 2), so\n// subtract 1 from the exponent immediately.\nfrac, exp := math.Frexp(value)\nexp--\nif frac == 0.5 {\n// Special case for powers of two: they fall into the bucket\n// numbered one less.\nexp--\n}\nreturn exp\n}\n</code></pre>"},{"location":"docs/specs/otel/metrics/data-model/#negative-scale-extract-and-shift-the-exponent","title":"Negative Scale: Extract and Shift the Exponent","text":"<p>For negative scales, the index of a value equals the normalized base-2 exponent (as by <code>MapToIndexScale0()</code> above) shifted to the right by <code>-scale</code>. Note that because of sign extension, this shift performs correct rounding for the negative indices. This may be written as:</p> <pre><code>// MapToIndexNegativeScale computes a bucket index for scales &lt;= 0.\nfunc MapToIndexNegativeScale(value float64) int {\nreturn MapToIndexScale0(value) &gt;&gt; -scale\n}\n</code></pre> <p>The reverse mapping function is:</p> <pre><code>// LowerBoundaryNegativeScale computes the lower boundary for index\n// with scales &lt;= 0.\nfunc LowerBoundaryNegativeScale(index int) {\nreturn math.Ldexp(1, index &lt;&lt; -scale)\n}\n</code></pre> <p>Note that the reverse mapping function is expected to produce subnormal values even when the mapping function rounds them into normal values, since the lower boundary of the bucket containing the smallest normal value may be subnormal. For example, at scale -4 the smallest normal value <code>0x1p-1022</code> falls into a bucket with lower boundary <code>0x1p-1024</code>.</p>"},{"location":"docs/specs/otel/metrics/data-model/#all-scales-use-the-logarithm-function","title":"All Scales: Use the Logarithm Function","text":"<p>The mapping and reverse-mapping functions for scale zero and negative scales above are recommended because they are exact. At these scales, <code>math.Log()</code> could be inaccurate and more expensive than directly calculating the bucket index. The methods in this section MAY be used at all scales, although they are definitely useful for positive scales.</p> <p>The built-in natural logarithm function can be used to compute the bucket index by applying a scaling factor, derived as follows.</p> <ol> <li>The exponential base is defined as <code>base == 2**(2**(-scale))</code></li> <li>We want <code>index</code> where <code>base**index &lt; value &lt;= base**(index+1)</code>.</li> <li>Apply the base-<code>base</code> logarithm, i.e.,    <code>log_base(base**index) &lt; log_base(value) &lt;= log_base(base**(index+1))</code> (where    <code>log_X(Y)</code> indicates the base-<code>X</code> logarithm of <code>Y</code>)</li> <li>Rewrite using <code>log_X(X**Y) == Y</code>:</li> <li>Thus, <code>index &lt; log_base(value) &lt;= index+1</code></li> <li>Using the <code>Ceiling()</code> function to simplify the equation:    <code>Ceiling(log_base(value)) == index+1</code></li> <li>Subtract one from each side: <code>index == Ceiling(log_base(value)) - 1</code></li> <li>Rewrite using <code>log_X(Y) == log_N(Y) / log_N(X)</code> to allow use of the natural    logarithm</li> <li>Thus, <code>index == Ceiling(log(value)/log(base)) - 1</code></li> <li>The scaling factor <code>1/log(base)</code> can be derived using the formulas in (1),     (4), and (8).</li> </ol> <p>The scaling factor equals <code>2**scale / log(2)</code> can be written as <code>math.Ldexp(math.Log2E, scale)</code> since the constant <code>math.Log2E</code> is defined as <code>1/log(2)</code>. Putting this together:</p> <pre><code>// MapToIndex for any scale.\nfunc MapToIndex(value float64) int {\nscaleFactor := math.Ldexp(math.Log2E, scale)\nreturn math.Ceil(math.Log(value) * scaleFactor) - 1\n}\n</code></pre> <p>The use of <code>math.Log()</code> to calculate the bucket index is not guaranteed to be exactly correct near powers of two. Values near a boundary could be mapped into the incorrect bucket due to inaccuracy. Defining an exact mapping function is out of scope for this document.</p> <p>However, when inputs are an exact power of two, it is possible to calculate the exactly correct bucket index. Since it is relatively simple to check for exact powers of two, implementations SHOULD apply such a special case:</p> <pre><code>// MapToIndex for any scale, exact for powers of two.\nfunc MapToIndex(value float64) int {\n// Special case for power-of-two values.\nif frac, exp := math.Frexp(value); frac == 0.5 {\nreturn ((exp - 1) &lt;&lt; scale) - 1\n}\nscaleFactor := math.Ldexp(math.Log2E, scale)\n// Note: math.Floor(value) equals math.Ceil(value)-1 when value\n// is not a power of two, which is checked above.\nreturn math.Floor(math.Log(value) * scaleFactor)\n}\n</code></pre> <p>The reverse mapping function for positive scales is:</p> <pre><code>// LowerBoundary computes the bucket boundary for positive scales.\nfunc LowerBoundary(index int) float64 {\ninverseFactor := math.Ldexp(math.Ln2, -scale)\nreturn math.Exp(index * inverseFactor), nil\n}\n</code></pre> <p>Implementations are expected to verify that their mapping function and inverse mapping function are correct near the lowest and highest IEEE floating point values. A mathematically correct formula may produce the wrong result, because of accumulated floating point calculation error or underflow/overflow of intermediate results. In the Golang reference implementation, for example, the above formula computes <code>+Inf</code> for the maximum-index bucket. In this case, it is appropriate to subtract <code>1&lt;&lt;scale</code> from the index and multiply the result by <code>2</code>.</p> <pre><code>func LowerBoundary(index int) float64 {\n// Use this form in case the equation above computes +Inf\n// as the lower boundary of a valid bucket.\ninverseFactor := math.Ldexp(math.Ln2, -scale)\nreturn 2.0 * math.Exp((index - (1 &lt;&lt; scale)) * inverseFactor), nil\n}\n</code></pre> <p>In the Golang reference implementation, for example, the above formula does not accurately compute the lower boundary of the minimum-index bucket (it is a subnormal value). In this case, it is appropriate to add <code>1&lt;&lt;scale</code> to the index and divide the result by <code>2</code>.</p> <p>Note that floating-point to integer type conversions have been omitted from the code fragments above, to improve readability.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponentialhistogram-producer-recommendations","title":"ExponentialHistogram: Producer Recommendations","text":"<p>At the lowest or highest end of the 64 bit IEEE floating point, a bucket's range may only be partially representable by the floating point number format. When mapping a number in these buckets, a producer may correctly return the index of such a partially representable bucket. This is considered a normal condition.</p> <p>For positive scales, the logarithm method is preferred because it requires very little code, is easy to validate and is nearly as fast and accurate as the lookup table approach. For zero scale and negative scales, directly calculating the index from the floating-point representation is more efficient.</p> <p>The use of a built-in logarithm function could lead to results that differ from the bucket index that would be computed using arbitrary precision or a lookup table, however producers are not required to perform an exact computation. As a result, ExponentialHistogram exemplars could map into buckets with zero count. We expect to find such values counted in the adjacent buckets.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponentialhistogram-consumer-recommendations","title":"ExponentialHistogram: Consumer Recommendations","text":"<p>ExponentialHistogram bucket indices are expected to map into buckets where both the upper and lower boundaries can be represented using IEEE 754 double-width floating point values. Consumers MAY round the unrepresentable boundary of a partially representable bucket index to the nearest representable value.</p> <p>Consumers SHOULD reject ExponentialHistogram data with <code>scale</code> and bucket indices that overflow or underflow this representation. Consumers that reject such data SHOULD warn the user through error logging that out-of-range data was received.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exponentialhistogram-bucket-inclusivity","title":"ExponentialHistogram: Bucket inclusivity","text":"<p>The specification on bucket inclusivity made for explicit-boundary Histogram data applies equally to ExponentialHistogram data.</p>"},{"location":"docs/specs/otel/metrics/data-model/#summary-legacy","title":"Summary (Legacy)","text":"<p>Summary metric data points convey quantile summaries, e.g. What is the 99-th percentile latency of my HTTP server. Unlike other point types in OpenTelemetry, Summary points cannot always be merged in a meaningful way. This point type is not recommended for new applications and exists for compatibility with other formats.</p> <p>Summary consists of the following:</p> <ul> <li>A set of data points, each containing:</li> <li>An independent set of Attribute name-value pairs.</li> <li>A timestamp when the value was sampled (<code>time_unix_nano</code>)</li> <li>(optional) A timestamp (<code>start_time_unix_nano</code>) that denotes the start time     of observation collection for the summary.</li> <li>A count of the number of observations in the population of the data point.</li> <li>A sum of the values in the population.</li> <li>A set of quantile values (in strictly increasing order) consisting of:<ul> <li>The quantile of a distribution, within the interval <code>[0.0, 1.0]</code>. For   example, the value <code>0.9</code> would represent the 90th-percentile.</li> <li>The value of the quantile. This MUST be non-negative.</li> </ul> </li> </ul> <p>Quantile values 0.0 and 1.0 are defined to be equal to the minimum and maximum values, respectively.</p> <p>Quantile values do not need to represent values observed between <code>start_time_unix_nano</code> and <code>time_unix_nano</code> and are expected to be calculated against recent time windows, typically the last 5-10 minutes.</p>"},{"location":"docs/specs/otel/metrics/data-model/#exemplars","title":"Exemplars","text":"<p>Status: Stable</p> <p>An exemplar is a recorded value that associates OpenTelemetry context to a metric event within a Metric. One use case is to allow users to link Trace signals w/ Metrics.</p> <p>Exemplars consist of:</p> <ul> <li>(optional) The trace associated with a recording (<code>trace_id</code>, <code>span_id</code>)</li> <li>The time of the observation (<code>time_unix_nano</code>)</li> <li>The recorded value (<code>value</code>)</li> <li>A set of filtered attributes (<code>filtered_attributes</code>) which provide additional   insight into the Context when the observation was made.</li> </ul> <p>For Histograms, when an exemplar exists, its value already participates in <code>bucket_counts</code>, <code>count</code> and <code>sum</code> reported by the histogram point.</p> <p>For Sums, when an exemplar exists, its value is already included in the overall sum.</p> <p>For Gauges, when an exemplar exists, its value was seen at some point within the gauge interval for the same source.</p>"},{"location":"docs/specs/otel/metrics/data-model/#single-writer","title":"Single-Writer","text":"<p>Status: Stable</p> <p>All metric data streams within OTLP MUST have one logical writer. This means, conceptually, that any Timeseries created from the Protocol MUST have one originating source of truth. In practical terms, this implies the following:</p> <ul> <li>All metric data streams produced by OTel SDKs SHOULD have globally unique   identity at any given point in time.   <code>Metric</code> identity is defined above.</li> <li>Aggregations of metric streams MUST only be written from a single logical   source at any given point time. Note: This implies aggregated metric streams   must reach one destination.</li> </ul> <p>In systems, there is the possibility of multiple writers sending data for the same metric stream (duplication). For example, if an SDK implementation fails to find uniquely identifying Resource attributes for a component, then all instances of that component could be reporting metrics as if they are from the same resource. In this case, metrics will be reported at inconsistent time intervals. For metrics like cumulative sums, this could cause issues where pairs of points appear to reset the cumulative sum leading to unusable metrics.</p> <p>Multiple writers for a metric stream is considered an error state, or misbehaving system. Receivers SHOULD presume a single writer was intended and eliminate overlap / deduplicate.</p> <p>Note: Identity is an important concept in most metrics systems. For example, Prometheus directly calls out uniqueness:</p> <p>Take care with <code>labeldrop</code> and <code>labelkeep</code> to ensure that metrics are still uniquely labeled once the labels are removed.</p> <p>For OTLP, the Single-Writer principle grants a way to reason over error scenarios and take corrective actions. Additionally, it ensures that well-behaved systems can perform metric stream manipulation without undesired degradation or loss of visibility.</p> <p>Note that violations of the Single-Writer principle are not semantic errors, generally they result from misconfiguration. Whereas semantic errors can sometimes be corrected by configuring Views, violations of the Single-Writer principle can be corrected by differentiating the <code>Resource</code> used or by ensuring that streams for a given <code>Resource</code> and <code>Attribute</code> set do not overlap in time.</p>"},{"location":"docs/specs/otel/metrics/data-model/#temporality","title":"Temporality","text":"<p>Status: Stable</p> <p>The notion of temporality refers to the way additive quantities are expressed, in relation to time, indicating whether reported values incorporate previous measurements or not. Sum, Histogram, and ExponentialHistogram data points, in particular, support a choice of aggregation temporality.</p> <p>Every OTLP metric data point has two associated timestamps. The first, mandatory timestamp is the one associated with the observation, the moment when the measurement became current or took effect, and is referred to as <code>TimeUnixNano</code>. The second, optional timestamp is used to indicate when a sequence of points is unbroken, and is referred to as <code>StartTimeUnixNano</code>.</p> <p>The second timestamp is strongly recommended for Sum, Histogram, and ExponentialHistogram points, as it is necessary to correctly interpret the rate from an OTLP stream, in a manner that is aware of restarts. The use of <code>StartTimeUnixNano</code> to indicate the start of an unbroken sequence of points means it can also be used to encode implicit gaps in the stream.</p> <ul> <li>Cumulative temporality means that successive data points repeat the starting   timestamp. For example, from start time T0, cumulative data points   cover time ranges (T0, T1], (T0,   T2], (T0, T3], and so on.</li> <li>Delta temporality means that successive data points advance the starting   timestamp. For example, from start time T0, delta data points cover   time ranges (T0, T1], (T1, T2],   (T2, T3], and so on.</li> </ul> <p>The use of cumulative temporality for monotonic sums is common, exemplified by Prometheus. Systems based in cumulative monotonic sums are naturally simpler, in terms of the cost of adding reliability. When collection fails intermittently, gaps in the data are naturally averaged from cumulative measurements. Cumulative data requires the sender to remember all previous measurements, an \u201cup-front\u201d memory cost proportional to cardinality.</p> <p>The use of delta temporality for metric sums is also common, exemplified by Statsd. There is a connection between OpenTelemetry tracing, in which a Span event commonly is translated into two metric events (a 1-count and a timing measurement). Delta temporality enables sampling and supports shifting the cost of cardinality outside of the process.</p>"},{"location":"docs/specs/otel/metrics/data-model/#resets-and-gaps","title":"Resets and Gaps","text":"<p>Status: Experimental</p> <p>When the <code>StartTimeUnixNano</code> field is present, it allows the consumer to observe when there are gaps and overlapping writers in a stream. Correctly used, the consumer can observe both transient and ongoing violations of the single-writer principle as well as reset events. In an unbroken sequence of observations, the <code>StartTimeUnixNano</code> always matches either the <code>TimeUnixNano</code> or the <code>StartTimeUnixNano</code> of other points in the same sequence. For the initial points in an unbroken sequence:</p> <ul> <li>When <code>StartTimeUnixNano</code> is less than <code>TimeUnixNano</code>, a new unbroken sequence   of observations begins with a \"true\" reset at a known start time. The zero   value is implicit, it is not necessary to record the starting point.</li> <li>When <code>StartTimeUnixNano</code> equals <code>TimeUnixNano</code>, a new unbroken sequence of   observations begins with a reset at an unknown start time. The initial   observed value is recorded to indicate that an unbroken sequence of   observations resumes. These points have zero duration, and indicate that   nothing is known about previously-reported points and that data may have been   lost.</li> </ul> <p>For subsequent points in an unbroken sequence:</p> <ul> <li>For points with delta aggregation temporality, the <code>StartTimeUnixNano</code> of each   point matches the <code>TimeUnixNano</code> of the preceding point</li> <li>Otherwise, the <code>StartTimeUnixNano</code> of each point matches the   <code>StartTimeUnixNano</code> of the initial observation.</li> </ul> <p>A metric stream has a gap, where it is implicitly undefined, anywhere there is a range of time such that no point covers that range range with its <code>StartTimeUnixNano</code> and <code>TimeUnixNano</code> fields.</p>"},{"location":"docs/specs/otel/metrics/data-model/#cumulative-streams-handling-unknown-start-time","title":"Cumulative streams: handling unknown start time","text":"<p>An unbroken stream of observations is resumed with a zero-duration point and non-zero value, as described above. For points with cumulative aggregation temporality, the rate contributed to the timeseries by each point depends on the prior point value in the stream.</p> <p>To correctly compute the rate contribution of the first point in an unbroken sequence requires knowing whether it is the first point. Unknown start-time reset points appear with <code>TimeUnixNano</code> equal to the <code>StartTimeUnixNano</code> of a stream of points, in which case the rate contribution of the first point is considered zero. An earlier sequence of observations is expected to have reported the same cumulative state prior to a gap in observations.</p> <p>The presence or absence of a point with <code>TimeUnixNano</code> equal to the <code>StartTimeUnixNano</code> indicates how to count rate contribution from the first point in a sequence. If the first point in an unknown start-time reset sequence is lost, the consumer of this data might overcount the rate contribution of the second point, as it then appears like a \"true\" reset.</p> <p>Various approaches can be taken to avoid overcounting. A system could use state from earlier in the stream to resolve start-time ambiguity, for example.</p>"},{"location":"docs/specs/otel/metrics/data-model/#cumulative-streams-inserting-true-reset-points","title":"Cumulative streams: inserting true reset points","text":"<p>The absolute value of the cumulative counter is often considered meaningful, but when the cumulative value is only used to calculate a rate function, it is possible to drop the initial unknown start-time reset point, but remember the initially observed value in order to modify subsequent observations. Later in the cumulative sequence are output relative to the initial value, thus appears as a true reset offset by an unknown constant.</p> <p>This process is known as inserting true reset points, a special case of reaggregation for cumulative series.</p>"},{"location":"docs/specs/otel/metrics/data-model/#overlap","title":"Overlap","text":"<p>Status: Experimental</p> <p>Overlap occurs when more than one metric data point is defined for a metric stream within a time window. Overlap is usually caused through mis-configuration, and it can lead to serious mis-interpretation of the data. <code>StartTimeUnixNano</code> is recommended so that consumers can recognize and response to overlapping points.</p> <p>We define three principles for handling overlap:</p> <ul> <li>Resolution (correction via dropping points)</li> <li>Observability (allowing the data to flow to backends)</li> <li>Interpolation (correction via data manipulation)</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#overlap-resolution","title":"Overlap resolution","text":"<p>When more than one process writes the same metric data stream, OTLP data points may appear to overlap. This condition typically results from misconfiguration, but can also result from running identical processes (indicative of operating system or SDK bugs, like missing process attributes). When there are overlapping points, receivers SHOULD eliminate points so that there are no overlaps. Which data to select in overlapping cases is not specified.</p>"},{"location":"docs/specs/otel/metrics/data-model/#overlap-observability","title":"Overlap observability","text":"<p>OpenTelemetry collectors SHOULD export telemetry when they observe overlapping points in data streams, so that the user can monitor for erroneous configurations.</p>"},{"location":"docs/specs/otel/metrics/data-model/#overlap-interpolation","title":"Overlap interpolation","text":"<p>When one process starts just as another exits, the appearance of overlapping points may be expected. In this case, OpenTelemetry collectors SHOULD modify points at the change-over using interpolation for Sum data points, to reduce gaps to zero width in these cases, without any overlap.</p>"},{"location":"docs/specs/otel/metrics/data-model/#stream-manipulations","title":"Stream Manipulations","text":"<p>Status: Experimental</p> <p>Pending introduction.</p>"},{"location":"docs/specs/otel/metrics/data-model/#sums-delta-to-cumulative","title":"Sums: Delta-to-Cumulative","text":"<p>While OpenTelemetry (and some metric backends) allows both Delta and Cumulative sums to be reported, the timeseries model we target does not support delta counters. To this end, converting from delta to cumulative needs to be defined so that backends can use this mechanism.</p> <p>Note: This is not the only possible Delta to Cumulative algorithm. It is just one possible implementation that fits the OTel Data Model.</p> <p>Converting from delta points to cumulative point is inherently a stateful operation. To successfully translate, we need all incoming delta points to reach one destination which can keep the current counter state and generate a new cumulative stream of data (see single writer principle).</p> <p>The algorithm is scheduled out as follows:</p> <ul> <li>Upon receiving the first Delta point for a given counter we set up the   following:</li> <li>A new counter which stores the cumulative sum, set to the initial counter.</li> <li>A start time that aligns with the start time of the first point.</li> <li>A \"last seen\" time that aligns with the time of the first point.</li> <li>Upon receiving future Delta points, we do the following:</li> <li>If the next point aligns with the expected next-time window (see     detecting delta restarts)<ul> <li>Update the \"last seen\" time to align with the time of the current point.</li> <li>Add the current value to the cumulative counter</li> <li>Output a new cumulative point with the original start time and current   last seen time and count.</li> </ul> </li> <li>if the current point precedes the start time, then drop this point. Note:     there are algorithms which can deal with late arriving points.</li> <li>if the next point does NOT align with the expected next-time window, then     reset the counter following the same steps performed as if the current point     was the first point seen.</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#sums-detecting-alignment-issues","title":"Sums: detecting alignment issues","text":"<p>When the next delta sum reported for a given metric stream does not align with where we expect it, one of several things could have occurred:</p> <ul> <li>The process reporting metrics was rebooted, leading to a new reporting   interval for the metric.</li> <li>A Single-Writer principle violation where multiple processes are reporting the   same metric stream.</li> <li>There was a lost data point, or dropped information.</li> </ul> <p>In all of these scenarios we do our best to give any cumulative metric knowledge that some data was lost, and reset the counter.</p> <p>We detect alignment via two mechanisms:</p> <ul> <li>If the incoming delta time interval has significant overlap with the previous   time interval, we assume a violation of the single-writer principle and can be   handled with one of the following options:</li> <li>Simply report the inconsistencies in time intervals, as the error condition     could be caused by a misconfiguration.</li> <li>Eliminate the overlap / deduplicate on the receiver side.</li> <li>Correct the inconsistent time intervals by differentiating the given     <code>Resource</code> and <code>Attribute</code> set used from overlapping time.</li> <li>If the incoming delta time interval has a significant gap from the last seen   time, we assume some kind of reboot/restart and reset the cumulative counter.</li> </ul>"},{"location":"docs/specs/otel/metrics/data-model/#sums-missing-timestamps","title":"Sums: Missing Timestamps","text":"<p>One degenerate case for the delta-to-cumulative algorithm is when timestamps are missing from metric data points. While this shouldn't be the case when using OpenTelemetry generated metrics, it can occur when adapting other metric formats, e.g. StatsD counts.</p> <p>In this scenario, the algorithm listed above would reset the cumulative sum on every data point due to not being able to determine alignment or point overlap. For comparison, see the simple logic used in statsd sums where all points are added, and lost points are ignored.</p>"},{"location":"docs/specs/otel/metrics/data-model/#footnotes","title":"Footnotes","text":"<p>[1] OTLP supports data point kinds that do not satisfy these conditions; they are well-defined but do not support standard metric data transformations.</p>"},{"location":"docs/specs/otel/metrics/metric-requirement-level/","title":"\u8bed\u4e49\u7ea6\u5b9a\u7684\u5ea6\u91cf\u9700\u6c42\u7ea7\u522b","text":"<p>Status: Stable</p> Table of Contents   - [Required](#required) - [Recommended](#recommended) - [Opt-In](#opt-in)   <p>The following metric requirement levels are specified:</p> <ul> <li>Required</li> <li>Recommended</li> <li>Opt-In</li> </ul>"},{"location":"docs/specs/otel/metrics/metric-requirement-level/#required","title":"Required","text":"<p>All instrumentations MUST emit the metric. A semantic convention defining a Required metric expects that an absolute majority of instrumentation libraries and applications are able to efficiently emit it.</p>"},{"location":"docs/specs/otel/metrics/metric-requirement-level/#recommended","title":"Recommended","text":"<p>Instrumentations SHOULD emit the metric by default if it's readily available and can be efficiently emitted. Instrumentations MAY offer a configuration option to disable Recommended metrics.</p> <p>Instrumentations that decide not to emit <code>Recommended</code> metrics due to performance, security, privacy, or other consideration by default, SHOULD allow for opting in to emitting them as defined for the <code>Opt-In</code> requirement level if the metrics are logically applicable.</p>"},{"location":"docs/specs/otel/metrics/metric-requirement-level/#opt-in","title":"Opt-In","text":"<p>Instrumentations SHOULD emit the metric if and only if the user configures the instrumentation to do so. Instrumentation that doesn't support configuration MUST NOT emit <code>Opt-In</code> metrics.</p> <p>This attribute requirement level is recommended for metrics that are particularly expensive to retrieve or might pose a security or privacy risk. These should therefore only be enabled deliberately by a user making an informed decision.</p>"},{"location":"docs/specs/otel/metrics/noop/","title":"Noop","text":""},{"location":"docs/specs/otel/metrics/noop/#api","title":"\u65e0\u64cd\u4f5c API \u5b9e\u73b0","text":"<p>Status: Experimental</p>  Table of Contents    - [MeterProvider](#meterprovider)   - [Meter Creation](#meter-creation) - [Meter](#meter)   - [Counter Creation](#counter-creation)   - [UpDownCounter Creation](#updowncounter-creation)   - [Histogram Creation](#histogram-creation)   - [Asynchronous Counter Creation](#asynchronous-counter-creation)   - [Asynchronous UpDownCounter Creation](#asynchronous-updowncounter-creation)   - [Asynchronous Gauge Creation](#asynchronous-gauge-creation) - [Instruments](#instruments)   - [Counter](#counter)     - [Counter Add](#counter-add)   - [UpDownCounter](#updowncounter)     - [UpDownCounter Add](#updowncounter-add)   - [Histogram](#histogram)     - [Histogram Record](#histogram-record)   - [Asynchronous Counter](#asynchronous-counter)   - [Asynchronous Counter Observations](#asynchronous-counter-observations)   - [Asynchronous UpDownCounter](#asynchronous-updowncounter)   - [Asynchronous UpDownCounter Observations](#asynchronous-updowncounter-observations)   - [Asynchronous Gauge](#asynchronous-gauge)   - [Asynchronous Gauge Observations](#asynchronous-gauge-observations)   <p>Users of OpenTelemetry need a way to disable the API from actually performing any operations. The No-Op OpenTelemetry API implementation (henceforth referred to as the No-Op) provides users with this functionally. It implements the OpenTelemetry API so that no telemetry is produced and computation resources are minimized.</p> <p>All language implementations of OpenTelemetry MUST provide a No-Op.</p>"},{"location":"docs/specs/otel/metrics/noop/#meterprovider","title":"MeterProvider","text":"<p>The No-Op MUST allow the creation of multiple MeterProviders.</p> <p>The MeterProviders created by the No-Op needs to hold as small a memory footprint as possible. Therefore, all MeterProviders created MUST NOT hold configuration or operational state.</p> <p>Since all MeterProviders hold the same empty state, a No-Op MAY provide the same MeterProvider instances to all creation requests.</p> <p>The No-Op is used by OpenTelemetry users to disable OpenTelemetry computation overhead and eliminate OpenTelemetry related output. For this reason, the MeterProvider MUST NOT return a non-empty error or log any message for any operations it performs.</p> <p>All operations a MeterProvider provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#meter-creation","title":"Meter Creation","text":"<p>New Meter instances are always created with a MeterProvider. Therefore, MeterProviders MUST allow for the creation of Meters. All Meters created MUST be an instance of the No-Op Meter.</p> <p>Since all Meters will hold the same empty state, a MeterProvider MAY return the same Meter instances to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The MeterProvider MUST accept these parameters. However, the MeterProvider MUST NOT validate any argument it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#meter","title":"Meter","text":"<p>The Meters created by the No-Op need to hold as small a memory footprint as possible. Therefore, all Meters created MUST NOT hold configuration or operational state.</p> <p>The Meter MUST NOT return a non-empty error or log any message for any operations it performs.</p> <p>All operations a Meter provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#counter-creation","title":"Counter Creation","text":"<p>The No-Op Meter MUST allow for the creation of a Counter instrument.</p> <p>Since all Counters hold the same empty state, a Meter MAY return the same Counter instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#updowncounter-creation","title":"UpDownCounter Creation","text":"<p>The No-Op Meter MUST allow for the creation of a UpDownCounter instrument.</p> <p>Since all UpDownCounters hold the same empty state, a Meter MAY return the same UpDownCounter instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#histogram-creation","title":"Histogram Creation","text":"<p>The No-Op Meter MUST allow for the creation of a Histogram instrument.</p> <p>Since all Histograms hold the same empty state, a Meter MAY return the same Histogram instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-counter-creation","title":"Asynchronous Counter Creation","text":"<p>The No-Op Meter MUST allow for the creation of an Asynchronous Counter instrument.</p> <p>Since all Asynchronous Counters hold the same empty state, a Meter MAY return the same Asynchronous Counter instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives and it MUST NOT hold any reference to the passed callbacks.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-updowncounter-creation","title":"Asynchronous UpDownCounter Creation","text":"<p>The No-Op Meter MUST allow for the creation of an Asynchronous UpDownCounter instrument.</p> <p>Since all Asynchronous UpDownCounters hold the same empty state, a Meter MAY return the same Asynchronous UpDownCounter instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives and it MUST NOT hold any reference to the passed callbacks.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-gauge-creation","title":"Asynchronous Gauge Creation","text":"<p>The No-Op Meter MUST allow for the creation of an Asynchronous Gauge instrument.</p> <p>Since all Asynchronous Gauges hold the same empty state, a Meter MAY return the same Asynchronous UpDownCounter instance to all creation requests.</p> <p>The API specifies multiple parameters that need to be accepted by the creation operation. The Meter MUST accept these parameters. However, the Meter MUST NOT validate any argument it receives and it MUST NOT hold any reference to the passed callbacks.</p>"},{"location":"docs/specs/otel/metrics/noop/#instruments","title":"Instruments","text":"<p>Instruments are used to make measurements and report telemetry for a system. However, the No-Op is used to disable this production of telemetry. Because of this, all instruments the No-Op provides MUST NOT hold any configuration or operational state including the aggregation of telemetry.</p>"},{"location":"docs/specs/otel/metrics/noop/#counter","title":"Counter","text":"<p>Counters MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations a Counter provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#counter-add","title":"Counter Add","text":"<p>The No-Op Counter MUST provide the user an interface to Add that implements the API. It MUST NOT validate or retain any state about the arguments it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#updowncounter","title":"UpDownCounter","text":"<p>UpDownCounters MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations an UpDownCounter provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#updowncounter-add","title":"UpDownCounter Add","text":"<p>The No-Op UpDownCounter MUST provide the user an interface to Add that implements the API. It MUST NOT validate or retain any state about the arguments it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#histogram","title":"Histogram","text":"<p>Histograms MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations a Histogram provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#histogram-record","title":"Histogram Record","text":"<p>The No-Op Histogram MUST provide the user an interface to Record that implements the API. It MUST NOT validate or retain any state about the arguments it receives.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-counter","title":"Asynchronous Counter","text":"<p>Asynchronous Counters MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations an Asynchronous Counter provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-counter-observations","title":"Asynchronous Counter Observations","text":"<p>The No-Op Asynchronous Counter MUST NOT validate or retain any state about observations made for the instrument.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-updowncounter","title":"Asynchronous UpDownCounter","text":"<p>Asynchronous UpDownCounters MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations an Asynchronous UpDownCounter provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-updowncounter-observations","title":"Asynchronous UpDownCounter Observations","text":"<p>The No-Op Asynchronous UpDownCounter MUST NOT validate or retain any state about observations made for the instrument.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-gauge","title":"Asynchronous Gauge","text":"<p>Asynchronous Gauges MUST NOT return a non-empty error or log any message for any operations they perform.</p> <p>All operations an Asynchronous Gauge provides MUST be safe to be run concurrently.</p>"},{"location":"docs/specs/otel/metrics/noop/#asynchronous-gauge-observations","title":"Asynchronous Gauge Observations","text":"<p>The No-Op Asynchronous Gauge MUST NOT validate or retain any state about observations made for the instrument.</p>"},{"location":"docs/specs/otel/metrics/sdk/","title":"Sdk","text":""},{"location":"docs/specs/otel/metrics/sdk/#sdk","title":"\u6307\u6807 SDK","text":"<p>Status: Mixed</p> Table of Contents   - [MeterProvider](#meterprovider)   - [MeterProvider Creation](#meterprovider-creation)   - [Meter Creation](#meter-creation)   - [Shutdown](#shutdown)   - [ForceFlush](#forceflush)   - [View](#view)   - [Aggregation](#aggregation)     - [Drop Aggregation](#drop-aggregation)     - [Default Aggregation](#default-aggregation)     - [Sum Aggregation](#sum-aggregation)     - [Last Value Aggregation](#last-value-aggregation)     - [Histogram Aggregations](#histogram-aggregations)       - [Explicit Bucket Histogram Aggregation](#explicit-bucket-histogram-aggregation)       - [Base2 Exponential Bucket Histogram Aggregation](#base2-exponential-bucket-histogram-aggregation)         - [Handle all normal values](#handle-all-normal-values)         - [Support a minimum and maximum scale](#support-a-minimum-and-maximum-scale)         - [Use the maximum scale for single measurements](#use-the-maximum-scale-for-single-measurements)         - [Maintain the ideal scale](#maintain-the-ideal-scale)   - [Observations inside asynchronous callbacks](#observations-inside-asynchronous-callbacks)   - [Cardinality limits](#cardinality-limits)     - [Synchronous instrument cardinality limits](#synchronous-instrument-cardinality-limits)     - [Asynchronous instrument cardinality limits](#asynchronous-instrument-cardinality-limits) - [Meter](#meter)   - [Duplicate instrument registration](#duplicate-instrument-registration)   - [Instrument name](#instrument-name)   - [Instrument unit](#instrument-unit)   - [Instrument description](#instrument-description)   - [Instrument advice](#instrument-advice) - [Attribute limits](#attribute-limits) - [Exemplar](#exemplar)   - [ExemplarFilter](#exemplarfilter)   - [Built-in ExemplarFilters](#built-in-exemplarfilters)     - [AlwaysOn](#alwayson)     - [AlwaysOff](#alwaysoff)     - [TraceBased](#tracebased)   - [ExemplarReservoir](#exemplarreservoir)   - [Exemplar defaults](#exemplar-defaults) - [MetricReader](#metricreader)   - [MetricReader operations](#metricreader-operations)     - [RegisterProducer(metricProducer)](#registerproducermetricproducer)     - [Collect](#collect)     - [Shutdown](#shutdown-1)   - [Periodic exporting MetricReader](#periodic-exporting-metricreader) - [MetricExporter](#metricexporter)   - [Push Metric Exporter](#push-metric-exporter)     - [Interface Definition](#interface-definition)       - [Export(batch)](#exportbatch)       - [ForceFlush()](#forceflush)       - [Shutdown()](#shutdown)   - [Pull Metric Exporter](#pull-metric-exporter) - [MetricProducer](#metricproducer)   - [Interface Definition](#interface-definition-1)     - [Produce() batch](#produce-batch) - [Defaults and configuration](#defaults-and-configuration) - [Numerical limits handling](#numerical-limits-handling) - [Compatibility requirements](#compatibility-requirements) - [Concurrency requirements](#concurrency-requirements)   <p>Users of OpenTelemetry need a way for instrumentation interactions with the OpenTelemetry API to actually produce telemetry. The OpenTelemetry SDK (henceforth referred to as the SDK) is an implementation of the OpenTelemetry API that provides users with this functionally.</p> <p>All language implementations of OpenTelemetry MUST provide an SDK.</p>"},{"location":"docs/specs/otel/metrics/sdk/#meterprovider","title":"MeterProvider","text":"<p>Status: Stable</p> <p>A <code>MeterProvider</code> MUST provide a way to allow a Resource to be specified. If a <code>Resource</code> is specified, it SHOULD be associated with all the metrics produced by any <code>Meter</code> from the <code>MeterProvider</code>. The tracing SDK specification has provided some suggestions regarding how to implement this efficiently.</p>"},{"location":"docs/specs/otel/metrics/sdk/#meterprovider-creation","title":"MeterProvider Creation","text":"<p>The SDK SHOULD allow the creation of multiple independent <code>MeterProvider</code>s.</p>"},{"location":"docs/specs/otel/metrics/sdk/#meter-creation","title":"Meter Creation","text":"<p>New <code>Meter</code> instances are always created through a <code>MeterProvider</code> (see API). The <code>name</code>, <code>version</code> (optional), <code>schema_url</code> (optional), and <code>attributes</code> (optional) arguments supplied to the <code>MeterProvider</code> MUST be used to create an <code>InstrumentationScope</code> instance which is stored on the created <code>Meter</code>.</p> <p>In the case where an invalid <code>name</code> (null or empty string) is specified, a working Meter MUST be returned as a fallback rather than returning null or throwing an exception, its <code>name</code> SHOULD keep the original invalid value, and a message reporting that the specified value is invalid SHOULD be logged.</p> <p>When a Schema URL is passed as an argument when creating a <code>Meter</code> the emitted telemetry for that <code>Meter</code> MUST be associated with the Schema URL, provided that the emitted data format is capable of representing such association.</p> <p>Configuration (i.e., MetricExporters, MetricReaders and Views) MUST be managed solely by the <code>MeterProvider</code> and the SDK MUST provide a way to configure all options that are implemented by the SDK. This MAY be done at the time of MeterProvider creation if appropriate.</p> <p>The <code>MeterProvider</code> MAY provide methods to update the configuration. If configuration is updated (e.g., adding a <code>MetricReader</code>), the updated configuration MUST also apply to all already returned <code>Meters</code> (i.e. it MUST NOT matter whether a <code>Meter</code> was obtained from the <code>MeterProvider</code> before or after the configuration change). Note: Implementation-wise, this could mean that <code>Meter</code> instances have a reference to their <code>MeterProvider</code> and access configuration only via this reference.</p>"},{"location":"docs/specs/otel/metrics/sdk/#shutdown","title":"Shutdown","text":"<p>This method provides a way for provider to do any cleanup required.</p> <p><code>Shutdown</code> MUST be called only once for each <code>MeterProvider</code> instance. After the call to <code>Shutdown</code>, subsequent attempts to get a <code>Meter</code> are not allowed. SDKs SHOULD return a valid no-op Meter for these calls, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> MAY be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the shutdown timeout configurable.</p> <p><code>Shutdown</code> MUST be implemented at least by invoking <code>Shutdown</code> on all registered MetricReader and MetricExporter instances.</p>"},{"location":"docs/specs/otel/metrics/sdk/#forceflush","title":"ForceFlush","text":"<p>This method provides a way for provider to notify the registered MetricReader and MetricExporter instances, so they can do as much as they could to consume or send the metrics. Note: unlike Push Metric Exporter which can send data on its own schedule, Pull Metric Exporter can only send the data when it is being asked by the scraper, so <code>ForceFlush</code> would not make much sense.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out. <code>ForceFlush</code> SHOULD return some ERROR status if there is an error condition; and if there is no error condition, it should return some NO ERROR status, language implementations MAY decide how to model ERROR and NO ERROR.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> MAY be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the flush timeout configurable.</p> <p><code>ForceFlush</code> MUST invoke <code>ForceFlush</code> on all registered MetricReader and Push Metric Exporter instances.</p>"},{"location":"docs/specs/otel/metrics/sdk/#view","title":"View","text":"<p>A <code>View</code> provides SDK users with the flexibility to customize the metrics that are output by the SDK. Here are some examples when a <code>View</code> might be needed:</p> <ul> <li>Customize which Instruments are to be   processed/ignored. For example, an   instrumented library can provide both   temperature and humidity, but the application developer might only want   temperature.</li> <li>Customize the aggregation - if the default aggregation associated with the   Instrument does not meet the needs of the user. For example, an HTTP client   library might expose HTTP client request duration as   Histogram by default, but the application developer   might only want the total count of outgoing requests.</li> <li>Customize which attribute(s) are to be reported on metrics. For example, an   HTTP server library might expose HTTP verb (e.g. GET, POST) and HTTP status   code (e.g. 200, 301, 404). The application developer might only care about   HTTP status code (e.g. reporting the total count of HTTP requests for each   HTTP status code). There could also be extreme scenarios in which the   application developer does not need any attributes (e.g. just get the total   count of all incoming requests).</li> </ul> <p>The SDK MUST provide the means to register Views with a <code>MeterProvider</code>. Here are the inputs:</p> <ul> <li>The Instrument selection criteria (required), which covers:</li> <li>The <code>type</code> of the Instrument(s) (optional).</li> <li>The <code>name</code> of the Instrument(s). OpenTelemetry SDK     authors MAY choose to support wildcard characters, with the question mark     (<code>?</code>) matching exactly one character and the asterisk character (<code>*</code>)     matching zero or more characters. If wildcards are not supported in general,     OpenTelemetry SDKs MUST specifically recognize the single <code>*</code> wildcard as     matching all instruments.</li> <li>The <code>unit</code> of the Instrument(s) (optional).</li> <li>The <code>name</code> of the Meter (optional).</li> <li>The <code>version</code> of the Meter (optional).</li> <li>The <code>schema_url</code> of the Meter (optional).</li> <li>OpenTelemetry SDK authors MAY choose to support more     criteria. For example, a strong typed language MAY support point type (e.g.     allow the users to select Instruments based on whether the underlying type     is integer or double).</li> <li>The criteria SHOULD be treated as additive, which means the Instrument has     to meet all the provided criteria. For example, if the criteria are     instrument name == \"Foobar\" and instrument type is Histogram, it will be     treated as (instrument name == \"Foobar\") AND (instrument type is     Histogram).</li> <li>If no criteria is provided, the SDK SHOULD treat it as an error. It is     recommended that the SDK implementations fail fast. Please refer to     Error handling in OpenTelemetry for the general     guidance.</li> <li>The <code>name</code> of the View (optional). If not provided, the Instrument <code>name</code> MUST   be used by default. This will be used as the name of the   metrics stream.</li> <li>The configuration for the resulting   metrics stream:</li> <li>The <code>description</code>. If not provided, the Instrument <code>description</code> MUST be     used by default.</li> <li>A list of <code>attribute keys</code> (optional). If provided, the attributes that are     not in the list will be ignored. If not provided, all the attribute keys     will be used by default (TODO: once the Hint API is available, the default     behavior should respect the Hint if it is available).</li> <li>The <code>aggregation</code> (optional) to be used. If not provided, the SDK MUST apply     a default aggregation configurable on the basis of     instrument kind according to the MetricReader instance.</li> <li>Status: Feature-freeze - the     <code>exemplar_reservoir</code> (optional) to use for storing exemplars. This should be     a factory or callback similar to aggregation which allows different     reservoirs to be chosen by the aggregation.</li> <li>Status: Experimental - the     <code>aggregation_cardinality_limit</code> (optional) associated with the view. This     should be a positive integer to be taken as a hard limit on the number of     data points that will be emitted during a single collection by a single     instrument. See cardinality limits, below.</li> </ul> <p>In order to avoid conflicts, views which specify a name SHOULD have an instrument selector that selects at most one instrument. For the registration mechanism described above, where selection is provided via configuration, the SDK SHOULD NOT allow Views with a specified name to be declared with instrument selectors that may select more than one instrument (e.g. wild card instrument name) in the same Meter. For this and other cases where registering a view will cause a conflict, SDKs MAY fail fast in accordance with initialization error handling principles.</p> <p>The SDK SHOULD use the following logic to determine how to process Measurements made with an Instrument:</p> <ul> <li>Determine the <code>MeterProvider</code> which \"owns\" the Instrument.</li> <li>If the <code>MeterProvider</code> has no <code>View</code> registered, take the Instrument and apply   the default Aggregation on the basis of instrument kind according to the   MetricReader instance's <code>aggregation</code> property.</li> <li>If the <code>MeterProvider</code> has one or more <code>View</code>(s) registered:</li> <li>For each View, if the Instrument could match the instrument selection     criteria:<ul> <li>Try to apply the View configuration. If applying the View results in   conflicting metric identities   the implementation SHOULD apply the View and emit a warning. If it is not   possible to apply the View without producing semantic errors (e.g. the   View sets an asynchronous instrument to use the   Explicit bucket histogram aggregation)   the implementation SHOULD emit a warning and proceed as if the View did   not exist.</li> </ul> </li> <li>If the Instrument could not match with any of the registered <code>View</code>(s), the     SDK SHOULD enable the instrument using the default aggregation and     temporality. Users can configure match-all Views using     Drop aggregation to disable instruments by default.</li> </ul> <p>Here are some examples:</p> <pre><code># Python\n'''\n+------------------+\n| MeterProvider    |\n|   Meter A        |\n|     Counter X    |\n|     Histogram Y  |\n|   Meter B        |\n|     Gauge Z      |\n+------------------+\n'''\n# metrics from X and Y (reported as Foo and Bar) will be exported\nmeter_provider\n.add_view(\"X\")\n.add_view(\"Foo\", instrument_name=\"Y\")\n.add_view(\n\"Bar\",\ninstrument_name=\"Y\",\naggregation=HistogramAggregation(buckets=[5.0, 10.0, 25.0, 50.0, 100.0]))\n.add_metric_reader(PeriodicExportingMetricReader(ConsoleExporter()))\n</code></pre> <pre><code># all the metrics will be exported using the default configuration\nmeter_provider.add_metric_reader(PeriodicExportingMetricReader(ConsoleExporter()))\n</code></pre> <pre><code># all the metrics will be exported using the default configuration\nmeter_provider\n.add_view(\"*\") # a wildcard view that matches everything\n.add_metric_reader(PeriodicExportingMetricReader(ConsoleExporter()))\n</code></pre> <pre><code># Counter X will be exported as cumulative sum\nmeter_provider\n.add_view(\"X\", aggregation=SumAggregation())\n.add_metric_reader(PeriodicExportingMetricReader(ConsoleExporter()))\n</code></pre> <pre><code># Counter X will be exported as delta sum\n# Histogram Y and Gauge Z will be exported with 2 attributes (a and b)\nmeter_provider\n.add_view(\"X\", aggregation=SumAggregation())\n.add_view(\"*\", attribute_keys=[\"a\", \"b\"])\n.add_metric_reader(PeriodicExportingMetricReader(ConsoleExporter()),\ntemporality=lambda kind: Delta if kind in [Counter, AsyncCounter, Histogram] else Cumulative)\n</code></pre>"},{"location":"docs/specs/otel/metrics/sdk/#aggregation","title":"Aggregation","text":"<p>An <code>Aggregation</code>, as configured via the View, informs the SDK on the ways and means to compute Aggregated Metrics from incoming Instrument Measurements.</p> <p>Note: the term aggregation is used instead of aggregator. It is recommended that implementors reserve the \"aggregator\" term for the future when the SDK allows custom aggregation implementations.</p> <p>An <code>Aggregation</code> specifies an operation (i.e. decomposable aggregate function like Sum, Histogram, Min, Max, Count) and optional configuration parameter overrides. The operation's default configuration parameter values will be used unless overridden by optional configuration parameter overrides.</p> <p>Note: Implementors MAY choose the best idiomatic practice for their language to represent the semantic of an Aggregation and optional configuration parameters.</p> <p>e.g. The View specifies an Aggregation by string name (i.e. \"ExplicitBucketHistogram\").</p> <pre><code># Use Histogram with custom boundaries\nmeter_provider\n.add_view(\n\"X\",\naggregation=\"ExplicitBucketHistogram\",\naggregation_params={\"Boundaries\": [0, 10, 100]}\n)\n</code></pre> <p>e.g. The View specifies an Aggregation by class/type instance.</p> <pre><code>// Use Histogram with custom boundaries\nmeterProviderBuilder\n.AddView(\ninstrumentName: \"X\",\naggregation: new ExplicitBucketHistogramAggregation(\nboundaries: new double[] { 0.0, 10.0, 100.0 }\n)\n);\n</code></pre> <p>TODO: after we release the initial Stable version of Metrics SDK specification, we will explore how to allow configuring custom ExemplarReservoirs with the View API.</p> <p>The SDK MUST provide the following <code>Aggregation</code> to support the Metric Points in the Metrics Data Model.</p> <ul> <li>Drop</li> <li>Default</li> <li>Sum</li> <li>Last Value</li> <li>Explicit Bucket Histogram</li> </ul> <p>The SDK SHOULD provide the following <code>Aggregation</code>:</p> <ul> <li>Base2 Exponential Bucket Histogram</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#drop-aggregation","title":"Drop Aggregation","text":"<p>The Drop Aggregation informs the SDK to ignore/drop all Instrument Measurements for this Aggregation.</p> <p>This Aggregation does not have any configuration parameters.</p>"},{"location":"docs/specs/otel/metrics/sdk/#default-aggregation","title":"Default Aggregation","text":"<p>The Default Aggregation informs the SDK to use the Instrument <code>kind</code> to select an aggregation and <code>advice</code> to influence aggregation configuration parameters (as noted in the \"Selected Aggregation\" column).</p> Instrument Kind Selected Aggregation Counter Sum Aggregation Asynchronous Counter Sum Aggregation UpDownCounter Sum Aggregation Asynchronous UpDownCounter Sum Aggregation Asynchronous Gauge Last Value Aggregation Histogram Explicit Bucket Histogram Aggregation, with <code>ExplicitBucketBoundaries</code> from advice if provided <p>This Aggregation does not have any configuration parameters.</p>"},{"location":"docs/specs/otel/metrics/sdk/#sum-aggregation","title":"Sum Aggregation","text":"<p>The Sum Aggregation informs the SDK to collect data for the Sum Metric Point.</p> <p>The monotonicity of the aggregation is determined by the instrument type:</p> Instrument Kind <code>SumType</code> Counter Monotonic UpDownCounter Non-Monotonic Histogram Monotonic Asynchronous Gauge Non-Monotonic Asynchronous Counter Monotonic Asynchronous UpDownCounter Non-Monotonic <p>This Aggregation does not have any configuration parameters.</p> <p>This Aggregation informs the SDK to collect:</p> <ul> <li>The arithmetic sum of <code>Measurement</code> values.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#last-value-aggregation","title":"Last Value Aggregation","text":"<p>The Last Value Aggregation informs the SDK to collect data for the Gauge Metric Point.</p> <p>This Aggregation does not have any configuration parameters.</p> <p>This Aggregation informs the SDK to collect:</p> <ul> <li>The last <code>Measurement</code>.</li> <li>The timestamp of the last <code>Measurement</code>.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#histogram-aggregations","title":"Histogram Aggregations","text":"<p>All histogram Aggregations inform the SDK to collect:</p> <ul> <li>Count of <code>Measurement</code> values in population.</li> <li>Arithmetic sum of <code>Measurement</code> values in population. This SHOULD NOT be   collected when used with instruments that record negative measurements (e.g.   <code>UpDownCounter</code> or <code>ObservableGauge</code>).</li> <li>Min (optional) <code>Measurement</code> value in population.</li> <li>Max (optional) <code>Measurement</code> value in population.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#explicit-bucket-histogram-aggregation","title":"Explicit Bucket Histogram Aggregation","text":"<p>The Explicit Bucket Histogram Aggregation informs the SDK to collect data for the Histogram Metric Point using a set of explicit boundary values for histogram bucketing.</p> <p>This Aggregation honors the following configuration parameters:</p> Key Value Default Value Description Boundaries double[] [ 0, 5, 10, 25, 50, 75, 100, 250, 500, 750, 1000, 2500, 5000, 7500, 10000 ] Array of increasing values representing explicit bucket boundary values.The Default Value represents the following buckets (heavily influenced by the default buckets of Prometheus clients, e.g. Java and Go):(-\u221e, 0], (0, 5.0], (5.0, 10.0], (10.0, 25.0], (25.0, 50.0], (50.0, 75.0], (75.0, 100.0], (100.0, 250.0], (250.0, 500.0], (500.0, 750.0], (750.0, 1000.0], (1000.0, 2500.0], (2500.0, 5000.0], (5000.0, 7500.0], (7500.0, 10000.0], (10000.0, +\u221e). SDKs SHOULD use the default value when boundaries are not explicitly provided, unless they have good reasons to use something different (e.g. for backward compatibility reasons in a stable SDK release). RecordMinMax true, false true Whether to record min and max. <p>Explicit buckets are stated in terms of their upper boundary. Buckets are exclusive of their lower boundary and inclusive of their upper bound (except at positive infinity). A measurement is defined to fall into the greatest-numbered bucket with boundary that is greater than or equal to the measurement.</p>"},{"location":"docs/specs/otel/metrics/sdk/#base2-exponential-bucket-histogram-aggregation","title":"Base2 Exponential Bucket Histogram Aggregation","text":"<p>The Base2 Exponential Histogram Aggregation informs the SDK to collect data for the Exponential Histogram Metric Point, which uses a base-2 exponential formula to determine bucket boundaries and an integer <code>scale</code> parameter to control resolution. Implementations adjust scale as necessary given the data.</p> <p>This Aggregation honors the following configuration parameters:</p> Key Value Default Value Description MaxSize integer 160 Maximum number of buckets in each of the positive and negative ranges, not counting the special zero bucket. MaxScale integer 20 Maximum <code>scale</code> factor. RecordMinMax true, false true Whether to record min and max. <p>The default of 160 buckets is selected to establish default support for a high-resolution histogram able to cover a long-tail latency distribution from 1ms to 100s with less than 5% relative error. Because 160 can be factored into <code>10 * 2**K</code>, maximum contrast is relatively simple to derive for scale <code>K</code>:</p> Scale Maximum data contrast at 10 * 2**K buckets K+2 5.657 (2**(10/4)) K+1 32 (2**(10/2)) K 1024 (2**10) K-1 1048576 (2**20) <p>The following table shows how the ideal scale for 160 buckets is calculated as a function of the input range:</p> Input range Contrast Ideal Scale Base Relative error 1ms - 4ms 4 6 1.010889 0.542% 1ms - 20ms 20 5 1.021897 1.083% 1ms - 1s 10**3 4 1.044274 2.166% 1ms - 100s 10**5 3 1.090508 4.329% 1\u03bcs - 10s 10**7 2 1.189207 8.643% <p>Note that relative error is calculated as half of the bucket width divided by the bucket midpoint, which is the same in every bucket. Using the bucket from [1, base), we have <code>(bucketWidth / 2) / bucketMidpoint = ((base - 1) / 2) / ((base + 1) / 2) = (base - 1) / (base + 1)</code>.</p> <p>This Aggregation uses the notion of \"ideal\" scale. The ideal scale is either:</p> <ol> <li>The <code>MaxScale</code> (see configuration parameters), generally used for    single-value histogram Aggregations where scale is not otherwise constrained.</li> <li>The largest value of scale such that no more than the maximum number of    buckets are needed to represent the full range of input data in either of the    positive or negative ranges.</li> </ol>"},{"location":"docs/specs/otel/metrics/sdk/#handle-all-normal-values","title":"Handle all normal values","text":"<p>Implementations are REQUIRED to accept the entire normal range of IEEE floating point values (i.e., all values except for +Inf, -Inf and NaN values).</p> <p>Implementations SHOULD NOT incorporate non-normal values (i.e., +Inf, -Inf, and NaNs) into the <code>sum</code>, <code>min</code>, and <code>max</code> fields, because these values do not map into a valid bucket.</p> <p>Implementations MAY round subnormal values away from zero to the nearest normal value.</p>"},{"location":"docs/specs/otel/metrics/sdk/#support-a-minimum-and-maximum-scale","title":"Support a minimum and maximum scale","text":"<p>The implementation MUST maintain reasonable minimum and maximum scale parameters that the automatic scale parameter will not exceed. The maximum scale is defined by the <code>MaxScale</code> configuration parameter.</p>"},{"location":"docs/specs/otel/metrics/sdk/#use-the-maximum-scale-for-single-measurements","title":"Use the maximum scale for single measurements","text":"<p>When the histogram contains not more than one value in either of the positive or negative ranges, the implementation SHOULD use the maximum scale.</p>"},{"location":"docs/specs/otel/metrics/sdk/#maintain-the-ideal-scale","title":"Maintain the ideal scale","text":"<p>Implementations SHOULD adjust the histogram scale as necessary to maintain the best resolution possible, within the constraint of maximum size (max number of buckets). Best resolution (highest scale) is achieved when the number of positive or negative range buckets exceeds half the maximum size, such that increasing scale by one would not be possible given the size constraint.</p>"},{"location":"docs/specs/otel/metrics/sdk/#observations-inside-asynchronous-callbacks","title":"Observations inside asynchronous callbacks","text":"<p>Callback functions MUST be invoked for the specific <code>MetricReader</code> performing collection, such that observations made or produced by executing callbacks only apply to the intended <code>MetricReader</code> during collection.</p> <p>The implementation SHOULD disregard the accidental use of APIs appurtenant to asynchronous instruments outside of registered callbacks in the context of a single <code>MetricReader</code> collection.</p> <p>The implementation SHOULD use a timeout to prevent indefinite callback execution.</p> <p>The implementation MUST complete the execution of all callbacks for a given instrument before starting a subsequent round of collection.</p>"},{"location":"docs/specs/otel/metrics/sdk/#cardinality-limits","title":"Cardinality limits","text":"<p>Status: Experimental</p> <p>Views SHOULD support being configured with a cardinality limit to be applied to all aggregators not configured by a specific view, specified via <code>MetricReader</code> configuration.</p> <p>View configuration SHOULD support applying per-aggregation cardinality limits.</p> <p>The cardinality limit is taken as an exact, hard limit on the number of data points that can be written per collection, per aggregation. Each aggregation configured view MUST NOT output more than the configured <code>aggregation_cardinality_limit</code> number of data points per period.</p> <p>The RECOMMENDED default aggregation cardinality limit is 2000.</p> <p>An overflow attribute set is defined, containing a single attribute <code>otel.metric.overflow</code> having (boolean) value <code>true</code>, which is used to report a synthetic aggregation of the metric events that could not be independently aggregated because of the limit.</p> <p>The SDK MUST create an Aggregator with the overflow attribute set prior to reaching the cardinality limit and use it to aggregate events for which the correct Aggregator could not be created. The maximum number of distinct, non-overflow attributes is one less than the limit, as a result.</p>"},{"location":"docs/specs/otel/metrics/sdk/#synchronous-instrument-cardinality-limits","title":"Synchronous instrument cardinality limits","text":"<p>Views of synchronous instruments with cumulative aggregation temporality MUST continue to export the all attribute sets that were observed prior to the beginning of overflow. Metric events corresponding with attribute sets that were not observed prior to the overflow will be reflected in a single data point described by (only) the overflow attribute.</p> <p>Views of synchronous instruments with delta aggregation temporality MAY choose an arbitrary subset of attribute sets to output to maintain the stated cardinality limit.</p> <p>Regardless of aggregation temporality, the SDK MUST ensure that every metric event is reflected in exactly one Aggregator, which is either an Aggregator associated with the correct attribute set or an aggregator associated with the overflow attribute set.</p> <p>Events MUST NOT be double-counted or dropped during an overflow.</p>"},{"location":"docs/specs/otel/metrics/sdk/#asynchronous-instrument-cardinality-limits","title":"Asynchronous instrument cardinality limits","text":"<p>Views of asynchronous instruments SHOULD prefer the first-observed attributes in the callback when limiting cardinality, regardless of aggregation temporality.</p>"},{"location":"docs/specs/otel/metrics/sdk/#meter","title":"Meter","text":"<p>Distinct meters MUST be treated as separate namespaces for the purposes of detecting duplicate instrument registrations.</p>"},{"location":"docs/specs/otel/metrics/sdk/#duplicate-instrument-registration","title":"Duplicate instrument registration","text":"<p>When more than one Instrument of the same <code>name</code> is created for identical Meters, denoted duplicate instrument registration, the Meter MUST create a valid Instrument in every case. Here, \"valid\" means an instrument that is functional and can be expected to export data, despite potentially creating a semantic error in the data model.</p> <p>It is unspecified whether or under which conditions the same or different Instrument instance will be returned as a result of duplicate instrument registration. The term identical applied to Instruments describes instances where all identifying fields are equal. The term distinct applied to Instruments describes instances where at least one field value is different.</p> <p>Based on the recommendations from the data model, the SDK MUST aggregate data from identical Instruments together in its export pipeline.</p> <p>When a duplicate instrument registration occurs, and it is not corrected with a View, a warning SHOULD be emitted. The emitted warning SHOULD include information for the user on how to resolve the conflict, if possible.</p> <ol> <li>If the potential conflict involves multiple <code>description</code> properties, setting    the <code>description</code> through a configured View SHOULD avoid the warning.</li> <li>If the potential conflict involves instruments that can be distinguished by a    supported View selector (e.g., instrument type) a renaming View recipe SHOULD    be included in the warning.</li> <li>Otherwise (e.g., use of multiple units), the SDK SHOULD pass through the data    by reporting both <code>Metric</code> objects and emit a generic warning describing the    duplicate instrument registration.</li> </ol>"},{"location":"docs/specs/otel/metrics/sdk/#instrument-name","title":"Instrument name","text":"<p>When a Meter creates an instrument, it SHOULD validate the instrument name conforms to the instrument name syntax</p> <p>If the instrument name does not conform to this syntax, the Meter SHOULD emit an error notifying the user about the invalid name. It is left unspecified if a valid instrument is also returned.</p>"},{"location":"docs/specs/otel/metrics/sdk/#instrument-unit","title":"Instrument unit","text":"<p>When a Meter creates an instrument, it SHOULD NOT validate the instrument unit. If a unit is not provided or the unit is null, the Meter MUST treat it the same as an empty unit string.</p>"},{"location":"docs/specs/otel/metrics/sdk/#instrument-description","title":"Instrument description","text":"<p>When a Meter creates an instrument, it SHOULD NOT validate the instrument description. If a description is not provided or the description is null, the Meter MUST treat it the same as an empty description string.</p>"},{"location":"docs/specs/otel/metrics/sdk/#instrument-advice","title":"Instrument advice","text":"<p>Status: Experimental</p> <p>When a Meter creates an instrument, it SHOULD validate the instrument advice parameters. If an advice parameter is not valid, the Meter SHOULD emit an error notifying the user and proceed as if the parameter was not provided.</p>"},{"location":"docs/specs/otel/metrics/sdk/#attribute-limits","title":"Attribute limits","text":"<p>Status: Stable</p> <p>Attributes which belong to Metrics are exempt from the common rules of attribute limits at this time. Attribute truncation or deletion could affect identity of metric time series and the topic requires further analysis.</p>"},{"location":"docs/specs/otel/metrics/sdk/#exemplar","title":"Exemplar","text":"<p>Status: Feature-freeze</p> <p>Exemplars are example data points for aggregated data. They provide specific context to otherwise general aggregations. Exemplars allow correlation between aggregated metric data and the original API calls where measurements are recorded. Exemplars work for trace-metric correlation across any metric, not just those that can also be derived from <code>Span</code>s.</p> <p>An Exemplar is a recorded Measurement that exposes the following pieces of information:</p> <ul> <li>The <code>value</code> of the <code>Measurement</code> that was recorded by the API call.</li> <li>The <code>time</code> the API call was made to record a <code>Measurement</code>.</li> <li>The set of Attributes associated with the   <code>Measurement</code> not already included in a metric data point.</li> <li>The associated   trace id and span id of   the active   Span within Context   of the <code>Measurement</code> at API call time.</li> </ul> <p>For example, if a user has configured a <code>View</code> to preserve the attributes: <code>X</code> and <code>Y</code>, but the user records a measurement as follows:</p> <pre><code>const span = tracer.startSpan('makeRequest');\napi.context.with(api.trace.setSpan(api.context.active(), span), () =&gt; {\n// Record a measurement.\ncache_miss_counter.add(1, {\"X\": \"x-value\", \"Y\": \"y-value\", \"Z\": \"z-value\"});\n...\nspan.end();\n})\n</code></pre> <p>Then an exemplar output in OTLP would consist of:</p> <ul> <li>The <code>value</code> of 1.</li> <li>The <code>time</code> when the <code>add</code> method was called</li> <li>The <code>Attributes</code> of <code>{\"Z\": \"z-value\"}</code>, as these are not preserved in the   resulting metric point.</li> <li>The trace/span id for the <code>makeRequest</code> span.</li> </ul> <p>While the metric data point for the counter would carry the attributes <code>X</code> and <code>Y</code>.</p> <p>A Metric SDK MUST provide a mechanism to sample <code>Exemplar</code>s from measurements via the <code>ExemplarFilter</code> and <code>ExemplarReservoir</code> hooks.</p> <p><code>Exemplar</code> sampling SHOULD be turned off by default. If <code>Exemplar</code> sampling is off, the SDK MUST NOT have overhead related to exemplar sampling.</p> <p>A Metric SDK MUST allow exemplar sampling to leverage the configuration of metric aggregation. For example, Exemplar sampling of histograms should be able to leverage bucket boundaries.</p> <p>A Metric SDK SHOULD provide extensible hooks for Exemplar sampling, specifically:</p> <ul> <li><code>ExemplarFilter</code>: filter which measurements can become exemplars.</li> <li><code>ExemplarReservoir</code>: storage and sampling of exemplars.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#exemplarfilter","title":"ExemplarFilter","text":"<p>The <code>ExemplarFilter</code> interface MUST provide a method to determine if a measurement should be sampled. Sampled here simply makes the measurement eligible for being included as an exemplar. <code>ExemplarReservoir</code> makes the final decision if a measurement becomes an exemplar.</p> <p>This interface SHOULD have access to:</p> <ul> <li>The <code>value</code> of the measurement.</li> <li>The complete set of <code>Attributes</code> of the measurement.</li> <li>The Context of the measurement, which covers the   Baggage and the current active   Span.</li> <li>A <code>timestamp</code> that best represents when the measurement was taken.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#built-in-exemplarfilters","title":"Built-in ExemplarFilters","text":"<p>OpenTelemetry supports a number of built-in exemplar filters to choose from. The default is <code>TraceBased</code>.</p>"},{"location":"docs/specs/otel/metrics/sdk/#alwayson","title":"AlwaysOn","text":"<p>An ExemplarFilter which makes all measurements eligible for being an Exemplar.</p>"},{"location":"docs/specs/otel/metrics/sdk/#alwaysoff","title":"AlwaysOff","text":"<p>An ExemplarFilter which makes no measurements eligible for being an Exemplar. Using this ExemplarFilter is as good as disabling Exemplar feature.</p>"},{"location":"docs/specs/otel/metrics/sdk/#tracebased","title":"TraceBased","text":"<p>An ExemplarFilter which makes those measurements eligible for being an Exemplar, which are recorded in the context of a sampled parent span.</p>"},{"location":"docs/specs/otel/metrics/sdk/#exemplarreservoir","title":"ExemplarReservoir","text":"<p>The <code>ExemplarReservoir</code> interface MUST provide a method to offer measurements to the reservoir and another to collect accumulated Exemplars.</p> <p>The \"offer\" method SHOULD accept measurements, including:</p> <ul> <li>The <code>value</code> of the measurement.</li> <li>The complete set of <code>Attributes</code> of the measurement.</li> <li>The Context of the measurement, which covers the   Baggage and the current active   Span.</li> <li>A <code>timestamp</code> that best represents when the measurement was taken.</li> </ul> <p>The \"offer\" method SHOULD have the ability to pull associated trace and span information without needing to record full context. In other words, current span context and baggage can be inspected at this point.</p> <p>The \"offer\" method does not need to store all measurements it is given and MAY further sample beyond the <code>ExemplarFilter</code>.</p> <p>The \"collect\" method MUST return accumulated <code>Exemplar</code>s. Exemplars are expected to abide by the <code>AggregationTemporality</code> of any metric point they are recorded with. In other words, Exemplars reported against a metric data point SHOULD have occurred within the start/stop timestamps of that point. SDKs are free to decide whether \"collect\" should also reset internal storage for delta temporal aggregation collection, or use a more optimal implementation.</p> <p><code>Exemplar</code>s MUST retain any attributes available in the measurement that are not preserved by aggregation or view configuration. Specifically, at a minimum, joining together attributes on an <code>Exemplar</code> with those available on its associated metric data point should result in the full set of attributes from the original sample measurement.</p> <p>The <code>ExemplarReservoir</code> SHOULD avoid allocations when sampling exemplars.</p>"},{"location":"docs/specs/otel/metrics/sdk/#exemplar-defaults","title":"Exemplar defaults","text":"<p>The SDK will come with two types of built-in exemplar reservoirs:</p> <ol> <li>SimpleFixedSizeExemplarReservoir</li> <li>AlignedHistogramBucketExemplarReservoir</li> </ol> <p>By default, explicit bucket histogram aggregation with more than 1 bucket will use <code>AlignedHistogramBucketExemplarReservoir</code>. All other aggregations will use <code>SimpleFixedSizeExemplarReservoir</code>.</p> <p>SimpleExemplarReservoir This Exemplar reservoir MAY take a configuration parameter for the size of the reservoir pool. The reservoir will accept measurements using an equivalent of the naive reservoir sampling algorithm</p> <pre><code>bucket = random_integer(0, num_measurements_seen)\nif bucket &lt; num_buckets then\n  reservoir[bucket] = measurement\nend\n</code></pre> <p>Additionally, the <code>num_measurements_seen</code> count SHOULD be reset at every collection cycle.</p> <p>AlignedHistogramBucketExemplarReservoir This Exemplar reservoir MUST take a configuration parameter that is the configuration of a Histogram. This implementation MUST keep the last seen measurement that falls within a histogram bucket. The reservoir will accept measurements using the equivalent of the following naive algorithm:</p> <pre><code>bucket = find_histogram_bucket(measurement)\nif bucket &lt; num_buckets then\n  reservoir[bucket] = measurement\nend\n\ndef find_histogram_bucket(measurement):\n  for boundary, idx in bucket_boundaries do\n    if value &lt;= boundary then\n      return idx\n    end\n  end\n  return boundaries.length\n</code></pre>"},{"location":"docs/specs/otel/metrics/sdk/#metricreader","title":"MetricReader","text":"<p>Status: Stable</p> <p><code>MetricReader</code> is an SDK implementation object that provides the common configurable aspects of the OpenTelemetry Metrics SDK and determines the following capabilities:</p> <ul> <li>Registering MetricProducer(s)</li> <li>Collecting metrics from the SDK and any registered   MetricProducers on demand.</li> <li>Handling the ForceFlush and Shutdown signals from   the SDK.</li> </ul> <p>To construct a <code>MetricReader</code> when setting up an SDK, the caller SHOULD provide at least the following:</p> <ul> <li>The <code>exporter</code> to use, which is a <code>MetricExporter</code> instance.</li> <li>The default output <code>aggregation</code> (optional), a function of instrument kind. If   not configured, the default aggregation SHOULD be   used.</li> <li>The default output <code>temporality</code> (optional), a function of instrument kind. If   not configured, the Cumulative temporality SHOULD be used.</li> <li>The default aggregation cardinality limit to use, a function of instrument   kind. If not configured, a default value of 2000 SHOULD be used.</li> </ul> <p>The MetricReader.Collect method allows general-purpose <code>MetricExporter</code> instances to explicitly initiate collection, commonly used with pull-based metrics collection. A common sub-class of <code>MetricReader</code>, the periodic exporting <code>MetricReader</code> SHOULD be provided to be used typically with push-based metrics collection.</p> <p>The <code>MetricReader</code> MUST ensure that data points from OpenTelemetry instruments are output in the configured aggregation temporality for each instrument kind. For synchronous instruments being output with Cumulative temporality, this means converting Delta to Cumulative aggregation temporality. For asynchronous instruments being output with Delta temporality, this means converting Cumulative to Delta aggregation temporality.</p> <p>The <code>MetricReader</code> is not required to ensure data points from a non-SDK MetricProducer are output in the configured aggregation temporality, as these data points are not collected using OpenTelemetry instruments.</p> <p>The SDK MUST support multiple <code>MetricReader</code> instances to be registered on the same <code>MeterProvider</code>, and the MetricReader.Collect invocation on one <code>MetricReader</code> instance SHOULD NOT introduce side-effects to other <code>MetricReader</code> instances. For example, if a <code>MetricReader</code> instance is receiving metric data points that have delta temporality, it is expected that SDK will update the time range - e.g. from (Tn, Tn+1] to (Tn+1, Tn+2] - ONLY for this particular <code>MetricReader</code> instance.</p> <p>The SDK MUST NOT allow a <code>MetricReader</code> instance to be registered on more than one <code>MeterProvider</code> instance.</p> <pre><code>+-----------------+            +--------------+\n|                 | Metrics... |              |\n| In-memory state +------------&gt; MetricReader |\n|                 |            |              |\n+-----------------+            +--------------+\n\n+-----------------+            +--------------+\n|                 | Metrics... |              |\n| In-memory state +------------&gt; MetricReader |\n|                 |            |              |\n+-----------------+            +--------------+\n</code></pre> <p>The SDK SHOULD provide a way to allow <code>MetricReader</code> to respond to MeterProvider.ForceFlush and MeterProvider.Shutdown. OpenTelemetry SDK authors MAY decide the language idiomatic approach, for example, as <code>OnForceFlush</code> and <code>OnShutdown</code> callback functions.</p>"},{"location":"docs/specs/otel/metrics/sdk/#metricreader-operations","title":"MetricReader operations","text":""},{"location":"docs/specs/otel/metrics/sdk/#registerproducermetricproducer","title":"RegisterProducer(metricProducer)","text":"<p>Status: Experimental</p> <p>RegisterProducer causes the MetricReader to use the provided MetricProducer as a source of aggregated metric data in subsequent invocations of Collect. RegisterProducer is expected to be called during initialization, but MAY be invoked later. Multiple registrations of the same MetricProducer MAY result in duplicate metric data being collected.</p> <p>If the MeterProvider is an instance of MetricProducer, this MAY be used to register the MeterProvider, but MUST NOT allow multiple MeterProviders to be registered with the same MetricReader.</p>"},{"location":"docs/specs/otel/metrics/sdk/#collect","title":"Collect","text":"<p>Collects the metrics from the SDK and any registered MetricProducers. If there are asynchronous SDK Instruments involved, their callback functions will be triggered.</p> <p><code>Collect</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out. When the <code>Collect</code> operation fails or times out on some of the instruments, the SDK MAY return successfully collected results and a failed reasons list to the caller.</p> <p><code>Collect</code> does not have any required parameters, however, OpenTelemetry SDK authors MAY choose to add parameters (e.g. callback, filter, timeout). OpenTelemetry SDK authors MAY choose the return value type, or do not return anything.</p> <p>Note: it is expected that the <code>MetricReader.Collect</code> implementations will be provided by the SDK, so it is RECOMMENDED to prevent the user from accidentally overriding it, if possible (e.g. <code>final</code> in C++ and Java, <code>sealed</code> in C#).</p>"},{"location":"docs/specs/otel/metrics/sdk/#shutdown_1","title":"Shutdown","text":"<p>This method provides a way for the <code>MetricReader</code> to do any cleanup required.</p> <p><code>Shutdown</code> MUST be called only once for each <code>MetricReader</code> instance. After the call to <code>Shutdown</code>, subsequent invocations to <code>Collect</code> are not allowed. SDKs SHOULD return some failure for these calls, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> MAY be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/metrics/sdk/#periodic-exporting-metricreader","title":"Periodic exporting MetricReader","text":"<p>This is an implementation of the <code>MetricReader</code> which collects metrics based on a user-configurable time interval, and passes the metrics to the configured Push Metric Exporter.</p> <p>Configurable parameters:</p> <ul> <li><code>exportIntervalMillis</code> - the time interval in milliseconds between two   consecutive exports. The default value is 60000 (milliseconds).</li> <li><code>exportTimeoutMillis</code> - how long the export can run before it is cancelled.   The default value is 30000 (milliseconds).</li> </ul> <p>One possible implementation of periodic exporting MetricReader is to inherit from <code>MetricReader</code> and start a background task which calls the inherited <code>Collect()</code> method at the requested <code>exportIntervalMillis</code>. The reader's <code>Collect()</code> method may still be invoked by other callers. For example,</p> <ul> <li>A user configures periodic exporting MetricReader with a push exporter and a   30 second interval.</li> <li>At the first 30 second interval, the background task calls <code>Collect()</code> which   passes metrics to the push exporter.</li> <li>After 15 seconds, the user decides to flush metrics for just this reader. They   call <code>Collect()</code> which passes metrics to the push exporter.</li> <li>After another 15 seconds (at the end of the second 30 second interval), the   background task calls <code>Collect()</code> which passes metrics to the push exporter.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#metricexporter","title":"MetricExporter","text":"<p>Status: Stable</p> <p><code>MetricExporter</code> defines the interface that protocol-specific exporters MUST implement so that they can be plugged into OpenTelemetry SDK and support sending of telemetry data.</p> <p>Metric Exporters always have an associated MetricReader. The <code>aggregation</code> and <code>temporality</code> properties used by the OpenTelemetry Metric SDK are determined when registering Metric Exporters through their associated MetricReader. OpenTelemetry language implementations MAY support automatically configuring the MetricReader to use for an Exporter.</p> <p>The goal of the interface is to minimize burden of implementation for protocol-dependent telemetry exporters. The protocol exporter is expected to be primarily a simple telemetry data encoder and transmitter.</p> <p>Metric Exporter has access to the aggregated metrics data. Metric Exporters SHOULD report an error condition for data output by the <code>MetricReader</code> with unsupported Aggregation or Aggregation Temporality, as this condition can be corrected by a change of <code>MetricReader</code> configuration.</p> <p>There could be multiple Push Metric Exporters or Pull Metric Exporters or even a mixture of both configured at the same time on a given <code>MeterProvider</code> using one <code>MetricReader</code> for each exporter. Different exporters can run at different schedule, for example:</p> <ul> <li>Exporter A is a push exporter which sends data every 1 minute.</li> <li>Exporter B is a push exporter which sends data every 5 seconds.</li> <li>Exporter C is a pull exporter which reacts to a scraper over HTTP.</li> <li>Exporter D is a pull exporter which reacts to another scraper over a named   pipe.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk/#push-metric-exporter","title":"Push Metric Exporter","text":"<p>Push Metric Exporter sends metric data it receives from a paired MetricReader. Here are some examples:</p> <ul> <li>Sends the data based on a user configured schedule, e.g. every 1 minute. This   MAY be accomplished by pairing the exporter with a   periodic exporting MetricReader.</li> <li>Sends the data when there is a severe error.</li> </ul> <p>The following diagram shows <code>Push Metric Exporter</code>'s relationship to other components in the SDK:</p> <pre><code>+-----------------+            +---------------------------------+\n|                 | Metrics... |                                 |\n| In-memory state +------------&gt; Periodic exporting MetricReader |\n|                 |            |                                 |\n+-----------------+            |    +-----------------------+    |\n                               |    |                       |    |\n                               |    | MetricExporter (push) +-------&gt; Another process\n                               |    |                       |    |\n                               |    +-----------------------+    |\n                               |                                 |\n                               +---------------------------------+\n</code></pre>"},{"location":"docs/specs/otel/metrics/sdk/#interface-definition","title":"Interface Definition","text":"<p>A Push Metric Exporter MUST support the following functions:</p>"},{"location":"docs/specs/otel/metrics/sdk/#exportbatch","title":"Export(batch)","text":"<p>Exports a batch of Metric points. Protocol exporters that will implement this function are typically expected to serialize and transmit the data to the destination.</p> <p>The SDK MUST provide a way for the exporter to get the Meter information (e.g. name, version, etc.) associated with each <code>Metric point</code>.</p> <p><code>Export</code> will never be called concurrently for the same exporter instance. <code>Export</code> can be called again only after the current call returns.</p> <p><code>Export</code> MUST NOT block indefinitely, there MUST be a reasonable upper limit after which the call must time out with an error result (Failure).</p> <p>Any retry logic that is required by the exporter is the responsibility of the exporter. The default SDK SHOULD NOT implement retry logic, as the required logic is likely to depend heavily on the specific protocol and backend the metrics are being sent to.</p> <p>Parameters:</p> <p><code>batch</code> - a batch of <code>Metric point</code>s. The exact data type of the batch is language specific, typically it is some kind of list. The exact type of <code>Metric point</code> is language specific, and is typically optimized for high performance. Here are some examples:</p> <pre><code>       +--------+ +--------+     +--------+\nBatch: | Metric | | Metric | ... | Metric |\n       +---+----+ +--------+     +--------+\n           |\n           +--&gt; name, unit, description, meter information, ...\n           |\n           |                  +-------------+ +-------------+     +-------------+\n           +--&gt; MetricPoints: | MetricPoint | | MetricPoint | ... | MetricPoint |\n                              +-----+-------+ +-------------+     +-------------+\n                                    |\n                                    +--&gt; timestamps, attributes, value (or buckets), exemplars, ...\n</code></pre> <p>Refer to the Metric points section from the Metrics Data Model specification for more details.</p> <p>Note: it is highly recommended that implementors design the <code>Metric</code> data type based on the Data Model, rather than directly use the data types generated from the proto files (because the types generated from proto files are not guaranteed to be backward compatible).</p> <p>Returns: <code>ExportResult</code></p> <p><code>ExportResult</code> is one of:</p> <ul> <li><code>Success</code> - The batch has been successfully exported. For protocol exporters   this typically means that the data is sent over the wire and delivered to the   destination server.</li> <li><code>Failure</code> - exporting failed. The batch must be dropped. For example, this can   happen when the batch contains bad data and cannot be serialized.</li> </ul> <p>Note: this result may be returned via an async mechanism or a callback, if that is idiomatic for the language implementation.</p>"},{"location":"docs/specs/otel/metrics/sdk/#forceflush_1","title":"ForceFlush()","text":"<p>This is a hint to ensure that the export of any <code>Metrics</code> the exporter has received prior to the call to <code>ForceFlush</code> SHOULD be completed as soon as possible, preferably before returning from this method.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD only be called in cases where it is absolutely necessary, such as when using some FaaS providers that may suspend the process after an invocation, but before the exporter exports the completed metrics.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry SDK authors MAY decide if they want to make the flush timeout configurable.</p>"},{"location":"docs/specs/otel/metrics/sdk/#shutdown_2","title":"Shutdown()","text":"<p>Shuts down the exporter. Called when SDK is shut down. This is an opportunity for exporter to do any cleanup required.</p> <p>Shutdown SHOULD be called only once for each <code>MetricExporter</code> instance. After the call to <code>Shutdown</code> subsequent calls to <code>Export</code> are not allowed and should return a Failure result.</p> <p><code>Shutdown</code> SHOULD NOT block indefinitely (e.g. if it attempts to flush the data and the destination is unavailable). OpenTelemetry SDK authors MAY decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/metrics/sdk/#pull-metric-exporter","title":"Pull Metric Exporter","text":"<p>Pull Metric Exporter reacts to the metrics scrapers and reports the data passively. This pattern has been widely adopted by Prometheus.</p> <p>Unlike Push Metric Exporter which can send data on its own schedule, pull exporter can only send the data when it is being asked by the scraper, and <code>ForceFlush</code> would not make sense.</p> <p>Implementors MAY choose the best idiomatic design for their language. For example, they could generalize the Push Metric Exporter interface design and use that for consistency, they could model the pull exporter as MetricReader, or they could design a completely different pull exporter interface. If the pull exporter is modeled as MetricReader, implementors MAY name the MetricExporter interface as PushMetricExporter to prevent naming confusion.</p> <p>The following diagram gives some examples on how <code>Pull Metric Exporter</code> can be modeled to interact with other components in the SDK:</p> <ul> <li>Model the pull exporter as MetricReader</li> </ul> <pre><code>+-----------------+            +-----------------------------+\n|                 | Metrics... |                             |\n| In-memory state +------------&gt; PrometheusExporter (pull)   +---&gt; Another process (scraper)\n|                 |            | (modeled as a MetricReader) |\n+-----------------+            |                             |\n                               +-----------------------------+\n</code></pre> <ul> <li>Use the same MetricExporter design for both push and pull exporters</li> </ul> <pre><code>+-----------------+            +-----------------------------+\n|                 | Metrics... |                             |\n| In-memory state +------------&gt; Exporting MetricReader      |\n|                 |            |                             |\n+-----------------+            |  +-----------------------+  |\n                               |  |                       |  |\n                               |  | MetricExporter (pull) +------&gt; Another process (scraper)\n                               |  |                       |  |\n                               |  +-----------------------+  |\n                               |                             |\n                               +-----------------------------+\n</code></pre>"},{"location":"docs/specs/otel/metrics/sdk/#metricproducer","title":"MetricProducer","text":"<p>Status: Experimental</p> <p><code>MetricProducer</code> defines the interface which bridges to third-party metric sources MUST implement so they can be plugged into an OpenTelemetry MetricReader as a source of aggregated metric data. The SDK's in-memory state MAY implement the <code>MetricProducer</code> interface for convenience.</p> <p><code>MetricProducer</code> implementations SHOULD accept configuration for the <code>AggregationTemporality</code> of produced metrics. SDK authors MAY provide utility libraries to facilitate conversion between delta and cumulative temporalities.</p> <p>If the batch of Metric points returned by <code>Produce()</code> includes a Resource, the <code>MetricProducer</code> MUST accept configuration for the Resource.</p> <pre><code>+-----------------+            +--------------+\n|                 | Metrics... |              |\n| In-memory state +------------&gt; MetricReader |\n|                 |            |              |\n+-----------------+            |              |\n                               |              |\n+-----------------+            |              |\n|                 | Metrics... |              |\n| MetricProducer  +------------&gt;              |\n|                 |            |              |\n+-----------------+            +--------------+\n</code></pre>"},{"location":"docs/specs/otel/metrics/sdk/#interface-definition_1","title":"Interface Definition","text":"<p>A <code>MetricProducer</code> MUST support the following functions:</p>"},{"location":"docs/specs/otel/metrics/sdk/#produce-batch","title":"Produce() batch","text":"<p><code>Produce</code> provides metrics from the MetricProducer to the caller. <code>Produce</code> MUST return a batch of Metric points. <code>Produce</code> does not have any required parameters, however, OpenTelemetry SDK authors MAY choose to add parameters (e.g. timeout).</p> <p><code>Produce</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out. When the <code>Produce</code> operation fails, the <code>MetricProducer</code> MAY return successfully collected results and a failed reasons list to the caller.</p> <p>If a batch of Metric points can include <code>InstrumentationScope</code> information, <code>Produce</code> SHOULD include a single InstrumentationScope which identifies the <code>MetricProducer</code>.</p>"},{"location":"docs/specs/otel/metrics/sdk/#defaults-and-configuration","title":"Defaults and configuration","text":"<p>The SDK MUST provide configuration according to the SDK environment variables specification.</p>"},{"location":"docs/specs/otel/metrics/sdk/#numerical-limits-handling","title":"Numerical limits handling","text":"<p>The SDK MUST handle numerical limits in a graceful way according to Error handling in OpenTelemetry.</p> <p>If the SDK receives float/double values from Instruments, it MUST handle all the possible values. For example, if the language runtime supports IEEE 754, the SDK needs to handle NaNs and Infinites.</p> <p>It is unspecified how the SDK should handle the input limits. The SDK authors MAY leverage/follow the language runtime behavior for better performance, rather than perform a check on each value coming from the API.</p> <p>It is unspecified how the SDK should handle the output limits (e.g. integer overflow). The SDK authors MAY rely on the language runtime behavior as long as errors/exceptions are taken care of.</p>"},{"location":"docs/specs/otel/metrics/sdk/#compatibility-requirements","title":"Compatibility requirements","text":"<p>Status: Stable</p> <p>All the metrics components SHOULD allow new methods to be added to existing components without introducing breaking changes.</p> <p>All the metrics SDK methods SHOULD allow optional parameter(s) to be added to existing methods without introducing breaking changes, if possible.</p>"},{"location":"docs/specs/otel/metrics/sdk/#concurrency-requirements","title":"Concurrency requirements","text":"<p>Status: Stable</p> <p>For languages which support concurrent execution the Metrics SDKs provide specific guarantees and safeties.</p> <p>MeterProvider - Meter creation, <code>ForceFlush</code> and <code>Shutdown</code> are safe to be called concurrently.</p> <p>ExemplarFilter - all methods are safe to be called concurrently.</p> <p>ExemplarReservoir - all methods are safe to be called concurrently.</p> <p>MetricReader - <code>Collect</code> and <code>Shutdown</code> are safe to be called concurrently.</p> <p>MetricExporter - <code>ForceFlush</code> and <code>Shutdown</code> are safe to be called concurrently.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/","title":"\u8865\u5145\u6307\u5357","text":"<p>Note: this document is NOT a spec, it is provided to support the Metrics API and SDK specifications, it does NOT add any extra requirements to the existing specifications.</p> Table of Contents   - [\u8865\u5145\u6307\u5357](#\u8865\u5145\u6307\u5357)   - [Guidelines for instrumentation library authors](#guidelines-for-instrumentation-library-authors)     - [Instrument selection](#instrument-selection)     - [Additive property](#additive-property)       - [Numeric type selection](#numeric-type-selection)         - [Integer](#integer)         - [Float](#float)     - [Monotonicity property](#monotonicity-property)     - [Semantic convention](#semantic-convention)   - [Guidelines for SDK authors](#guidelines-for-sdk-authors)     - [Aggregation temporality](#aggregation-temporality)       - [Synchronous example](#synchronous-example)         - [Synchronous example: Delta aggregation temporality](#synchronous-example-delta-aggregation-temporality)         - [Synchronous example: Cumulative aggregation temporality](#synchronous-example-cumulative-aggregation-temporality)       - [Asynchronous example](#asynchronous-example)         - [Asynchronous example: Cumulative temporality](#asynchronous-example-cumulative-temporality)         - [Asynchronous example: Delta temporality](#asynchronous-example-delta-temporality)         - [Asynchronous example: attribute removal in a view](#asynchronous-example-attribute-removal-in-a-view)     - [Memory management](#memory-management)"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#guidelines-for-instrumentation-library-authors","title":"Guidelines for instrumentation library authors","text":""},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#instrument-selection","title":"Instrument selection","text":"<p>The Instruments are part of the Metrics API. They allow Measurements to be recorded synchronously or asynchronously.</p> <p>Choosing the correct instrument is important, because:</p> <ul> <li>It helps the library to achieve better efficiency. For example, if we want to   report room temperature to Prometheus, we want to   consider using an Asynchronous Gauge rather   than periodically poll the sensor, so that we only access the sensor when   scraping happened.</li> <li>It makes the consumption easier for the user of the library. For example, if   we want to report HTTP server request latency, we want to consider a   Histogram, so most of the users can get a reasonable   experience (e.g. default buckets, min/max) by simply enabling the metrics   stream, rather than doing extra configurations.</li> <li>It generates clarity to the semantic of the metrics stream, so the consumers   have better understanding of the results. For example, if we want to report   the process heap size, by using an Asynchronous   UpDownCounter rather than an   Asynchronous Gauge, we've made it explicit that   the consumer can add up the numbers across all processes to get the \"total   heap size\".</li> </ul> <p>Here is one way of choosing the correct instrument:</p> <ul> <li>I want to count something (by recording a delta value):</li> <li>If the value is monotonically increasing (the delta value is always     non-negative) - use a Counter.</li> <li>If the value is NOT monotonically increasing (the delta value can be     positive, negative or zero) - use an     UpDownCounter.</li> <li>I want to record or time something, and the statistics about this   thing are likely to be meaningful - use a Histogram.</li> <li>I want to measure something (by reporting an absolute value):</li> <li>If the measurement values are non-additive, use     an Asynchronous Gauge.</li> <li>If the measurement values are additive:<ul> <li>If the value is monotonically increasing - use an Asynchronous   Counter.</li> <li>If the value is NOT monotonically increasing - use an Asynchronous   UpDownCounter.</li> </ul> </li> </ul>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#additive-property","title":"Additive property","text":"<p>In OpenTelemetry a Measurement encapsulates a value and a set of <code>Attributes</code>. Depending on the nature of the measurements, they can be additive, non-additive or somewhere in the middle. Here are some examples:</p> <ul> <li>The server temperature is non-additive. The temperatures in the table below   add up to <code>226.2</code>, but this value has no practical meaning.</li> </ul> Host Name Temperature (F) MachineA 58.8 MachineB 86.1 MachineC 81.3 <ul> <li>The mass of planets is additive, the value <code>1.18e25</code> (<code>3.30e23 + 6.42e23 +   4.87e24 + 5.97e24</code>) means the combined mass of terrestrial planets in the   solar system.</li> </ul> Planet Name Mass (kg) Mercury 3.30e23 Mars 6.42e23 Venus 4.87e24 Earth 5.97e24 <ul> <li>The voltage of battery cells can be added up if the batteries are connected in   series. However, if the batteries are connected in parallel, it makes no sense   to add up the voltage values anymore.</li> </ul> <p>In OpenTelemetry, each Instrument implies whether it is additive or not.</p> Instrument Additive Counter additive UpDownCounter additive Histogram mixed1 Asynchronous Gauge non-additive Asynchronous Counter additive Asynchronous UpDownCounter additive <p>1: The Histogram bucket counts are additive if the buckets are the same, the sum is additive, but the min and max are non-additive.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#numeric-type-selection","title":"Numeric type selection","text":"<p>For Instruments which take increments and/or decrements as the input (e.g. Counter and UpDownCounter), the underlying numeric types (e.g., signed integer, unsigned integer, double) have direct impact on the dynamic range, precision, and how the data is interpreted. Typically, integers are precise but have limited dynamic range, and might see overflow/underflow. IEEE-754 double-precision floating-point format has a wide dynamic range of numeric values with the sacrifice on precision.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#integer","title":"Integer","text":"<p>Let's take an example: a 16-bit signed integer is used to count the committed transactions in a database, reported as cumulative sum every 15 seconds:</p> <ul> <li>During (T0, T1], we reported <code>70</code>.</li> <li>During (T0, T2], we reported <code>115</code>.</li> <li>During (T0, T3], we reported <code>116</code>.</li> <li>During (T0, T4], we reported <code>128</code>.</li> <li>During (T0, T5], we reported <code>128</code>.</li> <li>During (T0, T6], we reported <code>173</code>.</li> <li>...</li> <li>During (T0, Tn+1], we reported <code>1,872</code>.</li> <li>During (Tn+2, Tn+3], we reported <code>35</code>.</li> <li>During (Tn+2, Tn+4], we reported <code>76</code>.</li> </ul> <p>In the above case, a backend system could tell that there was likely a system restart (because the start time has changed from T0 to Tn+2) during (Tn+1, Tn+2], so it has chance to adjust the data to:</p> <ul> <li>(T0, Tn+3] : <code>1,907</code> (1,872 + 35).</li> <li>(T0, Tn+4] : <code>1,948</code> (1,872 + 76).</li> </ul> <p>Imagine we keep the database running:</p> <ul> <li>During (T0, Tm+1], we reported <code>32,758</code>.</li> <li>During (T0, Tm+2], we reported <code>32,762</code>.</li> <li>During (T0, Tm+3], we reported <code>-32,738</code>.</li> <li>During (T0, Tm+4], we reported <code>-32,712</code>.</li> </ul> <p>In the above case, the backend system could tell that there was an integer overflow during (Tm+2, Tm+3] (because the start time remains the same as before, and the value becomes negative), so it has chance to adjust the data to:</p> <ul> <li>(T0, Tm+3] : <code>32,798</code> (32,762 + 36).</li> <li>(T0, Tm+4] : <code>32,824</code> (32,762 + 62).</li> </ul> <p>As we can see in this example, even with the limitation of 16-bit integer, we can count the database transactions with high fidelity, without having to worry about information loss caused by integer overflows.</p> <p>It is important to understand that we are handling counter reset and integer overflow/underflow based on the assumption that we've picked the proper dynamic range and reporting frequency. Imagine if we use the same 16-bit signed integer to count the transactions in a data center (which could have thousands if not millions of transactions per second), we wouldn't be able to tell if <code>-32,738</code> was a result of <code>32,762 + 36</code> or <code>32,762 + 65,572</code> or even <code>32,762 + 131,108</code> if we report the data every 15 seconds. In this situation, either using a larger number (e.g. 32-bit integer) or increasing the reporting frequency (e.g. every microsecond, if we can afford the cost) would help.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#float","title":"Float","text":"<p>Let's take an example: an IEEE-754 double precision floating point is used to count the number of positrons detected by an alpha magnetic spectrometer. Each time a positron is detected, the spectrometer will invoke <code>counter.Add(1)</code>, and the result is reported as cumulative sum every 1 second:</p> <ul> <li>During (T0, T1], we reported <code>131,108</code>.</li> <li>During (T0, T2], we reported <code>375,463</code>.</li> <li>During (T0, T3], we reported <code>832,019</code>.</li> <li>During (T0, T4], we reported <code>1,257,308</code>.</li> <li>During (T0, T5], we reported <code>1,860,103</code>.</li> <li>...</li> <li>During (T0, Tn+1], we reported <code>9,007,199,254,325,789</code>.</li> <li>During (T0, Tn+2], we reported <code>9,007,199,254,740,992</code>.</li> <li>During (T0, Tn+3], we reported <code>9,007,199,254,740,992</code>.</li> </ul> <p>In the above case, the counter stopped increasing at some point between Tn+1 and Tn+2, because the IEEE-754 double counter is \"saturated\", <code>9,007,199,254,740,992 + 1</code> will result in <code>9,007,199,254,740,992</code> so the number stopped growing.</p> <p>Note: in ECMAScript 6 the number <code>9,007,199,254,740,991</code> (<code>2 ^ 53 - 1</code>) is known as <code>Number.MAX_SAFE_INTEGER</code>, which is the maximum integer that can be exactly represented as an IEEE-754 double precision number, and whose IEEE-754 representation cannot be the result of rounding any other integer to fit the IEEE-754 representation.</p> <p>In addition to the \"saturation\" issue, we should also understand that IEEE-754 double supports subnormal numbers. For example, <code>1.0E308 + 1.0E308</code> would result in <code>+Inf</code> (positive infinity). Certain metrics backend might have trouble handling subnormal numbers.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#monotonicity-property","title":"Monotonicity property","text":"<p>In the OpenTelemetry Metrics Data Model and API specifications, the word <code>monotonic</code> has been used frequently.</p> <p>It is important to understand that different Instruments handle monotonicity differently.</p> <p>Let's take an example with a network driver using a Counter to record the total number of bytes received:</p> <ul> <li>During the time range (T0, T1]:</li> <li>no network packet has been received</li> <li>During the time range (T1, T2]:</li> <li>received a packet with <code>30</code> bytes - <code>Counter.Add(30)</code></li> <li>received a packet with <code>200</code> bytes - <code>Counter.Add(200)</code></li> <li>received a packet with <code>50</code> bytes - <code>Counter.Add(50)</code></li> <li>During the time range (T2, T3]</li> <li>received a packet with <code>100</code> bytes - <code>Counter.Add(100)</code></li> </ul> <p>You can see that the total increment during (T0, T1] is <code>0</code>, the total increment during (T1, T2] is <code>280</code> (<code>30 + 200 + 50</code>), the total increment during (T2, T3] is <code>100</code>, and the total increment during (T0, T3] is <code>380</code> (<code>0 + 280 + 100</code>). All the increments are non-negative, in other words, the sum is monotonically increasing.</p> <p>Note that it is inaccurate to say \"the total bytes received by T3 is <code>380</code>\", because there might be network packets received by the driver before we started to observe it (e.g. before the last operating system reboot). The accurate way is to say \"the total bytes received during (T0, T3] is <code>380</code>\". In a nutshell, the count represents a rate which is associated with a time range.</p> <p>This monotonicity property is important because it gives the downstream systems additional hints so they can handle the data in a better way. Imagine we report the total number of bytes received in a cumulative sum data stream:</p> <ul> <li>At Tn, we reported <code>3,896,473,820</code>.</li> <li>At Tn+1, we reported <code>4,294,967,293</code>.</li> <li>At Tn+2, we reported <code>1,800,372</code>.</li> </ul> <p>The backend system could tell that there was integer overflow or system restart during (Tn+1, Tn+2], so it has chance to \"fix\" the data. Refer to additive property for more information about integer overflow.</p> <p>Let's take another example with a process using an Asynchronous Counter to report the total page faults of the process:</p> <p>The page faults are managed by the operating system, and the process could retrieve the number of page faults via some system APIs.</p> <ul> <li>At T0:</li> <li>the process started</li> <li>the process didn't ask the operating system to report the page faults</li> <li>At T1:</li> <li>the operating system reported with <code>1000</code> page faults for the process</li> <li>At T2:</li> <li>the process didn't ask the operating system to report the page faults</li> <li>At T3:</li> <li>the operating system reported with <code>1050</code> page faults for the process</li> <li>At T4:</li> <li>the operating system reported with <code>1200</code> page faults for the process</li> </ul> <p>You can see that the number being reported is the absolute value rather than increments, and the value is monotonically increasing.</p> <p>If we need to calculate \"how many page faults have been introduced during (T3, T4]\", we need to apply subtraction <code>1200 - 1050 = 150</code>.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#semantic-convention","title":"Semantic convention","text":"<p>Once you decided which instrument(s) to be used, you will need to decide the names for the instruments and attributes.</p> <p>It is highly recommended that you align with the <code>OpenTelemetry Semantic Conventions</code>, rather than inventing your own semantics.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#guidelines-for-sdk-authors","title":"Guidelines for SDK authors","text":""},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#aggregation-temporality","title":"Aggregation temporality","text":""},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#synchronous-example","title":"Synchronous example","text":"<p>The OpenTelemetry Metrics Data Model and SDK are designed to support both Cumulative and Delta Temporality. It is important to understand that temporality will impact how the SDK could manage memory usage. Let's take the following HTTP requests example:</p> <ul> <li>During the time range (T0, T1]:</li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>50 (ms)</code></li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>100 (ms)</code></li> <li>verb = <code>GET</code>, status = <code>500</code>, duration = <code>1 (ms)</code></li> <li>During the time range (T1, T2]:</li> <li>no HTTP request has been received</li> <li>During the time range (T2, T3]</li> <li>verb = <code>GET</code>, status = <code>500</code>, duration = <code>5 (ms)</code></li> <li>verb = <code>GET</code>, status = <code>500</code>, duration = <code>2 (ms)</code></li> <li>During the time range (T3, T4]:</li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>100 (ms)</code></li> <li>During the time range (T4, T5]:</li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>100 (ms)</code></li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>30 (ms)</code></li> <li>verb = <code>GET</code>, status = <code>200</code>, duration = <code>50 (ms)</code></li> </ul> <p>Note that in the following examples, Delta aggregation temporality is discussed before Cumulative aggregation temporality because synchronous Counter and UpDownCounter measurements are input to the API with specified Delta aggregation temporality.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#synchronous-example-delta-aggregation-temporality","title":"Synchronous example: Delta aggregation temporality","text":"<p>Let's imagine we export the metrics as Histogram, and to simplify the story we will only have one histogram bucket <code>(-Inf, +Inf)</code>:</p> <p>If we export the metrics using Delta Temporality:</p> <ul> <li>(T0, T1]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>2</code>, min: <code>50 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>1</code>, min: <code>1 (ms)</code>, max:     <code>1 (ms)</code></li> <li>(T1, T2]</li> <li>nothing since we don't have any Measurement received</li> <li>(T2, T3]</li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>2</code>, min: <code>2 (ms)</code>, max:     <code>5 (ms)</code></li> <li>(T3, T4]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>1</code>, min: <code>100 (ms)</code>,     max: <code>100 (ms)</code></li> <li>(T4, T5]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>3</code>, min: <code>30 (ms)</code>, max:     <code>100 (ms)</code></li> </ul> <p>You can see that the SDK only needs to track what has happened after the latest collection/export cycle. For example, when the SDK started to process measurements in (T1, T2], it can completely forget about what has happened during (T0, T1].</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#synchronous-example-cumulative-aggregation-temporality","title":"Synchronous example: Cumulative aggregation temporality","text":"<p>If we export the metrics using Cumulative Temporality:</p> <ul> <li>(T0, T1]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>2</code>, min: <code>50 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>1</code>, min: <code>1 (ms)</code>, max:     <code>1 (ms)</code></li> <li>(T0, T2]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>2</code>, min: <code>50 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>1</code>, min: <code>1 (ms)</code>, max:     <code>1 (ms)</code></li> <li>(T0, T3]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>2</code>, min: <code>50 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>3</code>, min: <code>1 (ms)</code>, max:     <code>5 (ms)</code></li> <li>(T0, T4]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>3</code>, min: <code>50 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>3</code>, min: <code>1 (ms)</code>, max:     <code>5 (ms)</code></li> <li>(T0, T5]</li> <li>attributes: {verb = <code>GET</code>, status = <code>200</code>}, count: <code>6</code>, min: <code>30 (ms)</code>, max:     <code>100 (ms)</code></li> <li>attributes: {verb = <code>GET</code>, status = <code>500</code>}, count: <code>3</code>, min: <code>1 (ms)</code>, max:     <code>5 (ms)</code></li> </ul> <p>You can see that we are performing Delta-&gt;Cumulative conversion, and the SDK has to track what has happened prior to the latest collection/export cycle, in the worst case, the SDK will have to remember what has happened since the beginning of the process.</p> <p>Imagine if we have a long running service and we collect metrics with 7 attributes and each attribute can have 30 different values. We might eventually end up having to remember the complete set of all <code>21,870,000,000</code> permutations! This cardinality explosion is a well-known challenge in the metrics space.</p> <p>Making it even worse, if we export the permutations even if there are no recent updates, the export batch could become huge and will be very costly. For example, do we really need/want to export the same thing for (T0, T2] in the above case?</p> <p>So here are some suggestions that we encourage SDK implementers to consider:</p> <ul> <li>You want to control the memory usage rather than allow it to grow indefinitely   / unbounded - regardless of what aggregation temporality is being used.</li> <li>You want to improve the memory efficiency by being able to forget about   things that are no longer needed.</li> <li>You probably don't want to keep exporting the same thing over and over again,   if there is no updates. You might want to consider Resets and   Gaps. For example, if a Cumulative metrics   stream hasn't received any updates for a long period of time, would it be okay   to reset the start time?</li> </ul>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#asynchronous-example","title":"Asynchronous example","text":"<p>In the above case, we have Measurements reported by a Histogram Instrument. What if we collect measurements from an Asynchronous Counter?</p> <p>The following example shows the number of page faults of each thread since the thread ever started:</p> <ul> <li>During the time range (T0, T1]:</li> <li>pid = <code>1001</code>, tid = <code>1</code>, #PF = <code>50</code></li> <li>pid = <code>1001</code>, tid = <code>2</code>, #PF = <code>30</code></li> <li>During the time range (T1, T2]:</li> <li>pid = <code>1001</code>, tid = <code>1</code>, #PF = <code>53</code></li> <li>pid = <code>1001</code>, tid = <code>2</code>, #PF = <code>38</code></li> <li>During the time range (T2, T3]</li> <li>pid = <code>1001</code>, tid = <code>1</code>, #PF = <code>56</code></li> <li>pid = <code>1001</code>, tid = <code>2</code>, #PF = <code>42</code></li> <li>During the time range (T3, T4]:</li> <li>pid = <code>1001</code>, tid = <code>1</code>, #PF = <code>60</code></li> <li>pid = <code>1001</code>, tid = <code>2</code>, #PF = <code>47</code></li> <li>During the time range (T4, T5]:</li> <li>thread 1 died, thread 3 started</li> <li>pid = <code>1001</code>, tid = <code>2</code>, #PF = <code>53</code></li> <li>pid = <code>1001</code>, tid = <code>3</code>, #PF = <code>5</code></li> </ul> <p>Note that in the following examples, Cumulative aggregation temporality is discussed before Delta aggregation temporality because asynchronous Counter and UpDownCounter measurements are input to the API with specified Cumulative aggregation temporality.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#asynchronous-example-cumulative-temporality","title":"Asynchronous example: Cumulative temporality","text":"<p>If we export the metrics using Cumulative Temporality:</p> <ul> <li>(T0, T1]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, sum: <code>50</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, sum: <code>30</code></li> <li>(T0, T2]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, sum: <code>53</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, sum: <code>38</code></li> <li>(T0, T3]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, sum: <code>56</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, sum: <code>42</code></li> <li>(T0, T4]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, sum: <code>60</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, sum: <code>47</code></li> <li>(T0, T5]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, sum: <code>53</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>3</code>}, sum: <code>5</code></li> </ul> <p>The behavior in the first four periods is quite straightforward - we just take the data being reported from the asynchronous instruments and send them.</p> <p>The data model prescribes several valid behaviors at T5 in this case, where one stream dies and another starts.  The Resets and Gaps section describes how start timestamps and staleness markers can be used to increase the receiver's understanding of these events.</p> <p>Consider whether the SDK maintains individual timestamps for the individual stream, or just one per process.  In this example, where a thread can die and start counting page faults from zero, the valid behaviors at T5 are:</p> <ol> <li>If all streams in the process share a start time, and the SDK is    not required to remember all past streams: the thread restarts with    zero sum.  Receivers with reset detection are able to calculate a    correct rate (except for frequent restarts relative to the    collection interval), however the precise time of a reset will be    unknown.</li> <li>If the SDK maintains per-stream start times, it signals to the    receiver precisely when a stream started, making the first    observation in a stream more useful for diagnostics.  Receivers can    perform overlap detection or duplicate suppression and do not    require reset detection, in this case.</li> <li>Independent of above treatments, the SDK can add a staleness marker    to indicate the start of a gap in the stream when one thread dies    by remembering which streams have previously reported but are not    currently reporting.  If per-stream start timestamps are used,    staleness markers can be issued to precisely start a gap in the    stream and permit forgetting streams that have stopped reporting.</li> </ol> <p>It's OK to ignore the options to use per-stream start timestamps and staleness markers. The first course of action above requires no additional memory or code to achieve and is correct in terms of the data model.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#asynchronous-example-delta-temporality","title":"Asynchronous example: Delta temporality","text":"<p>If we export the metrics using Delta Temporality:</p> <ul> <li>(T0, T1]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, delta: <code>50</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, delta: <code>30</code></li> <li>(T1, T2]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, delta: <code>3</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, delta: <code>8</code></li> <li>(T2, T3]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, delta: <code>3</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, delta: <code>4</code></li> <li>(T3, T4]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>1</code>}, delta: <code>4</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, delta: <code>5</code></li> <li>(T4, T5]</li> <li>attributes: {pid = <code>1001</code>, tid = <code>2</code>}, delta: <code>6</code></li> <li>attributes: {pid = <code>1001</code>, tid = <code>3</code>}, delta: <code>5</code></li> </ul> <p>You can see that we are performing Cumulative-&gt;Delta conversion, and it requires us to remember the last value of every single permutation we've encountered so far, because if we don't, we won't be able to calculate the delta value using <code>current value - last value</code>. And as you can tell, this is super expensive.</p> <p>Making it more interesting, if we have min/max value, it is mathematically impossible to reliably deduce the Delta temporality from Cumulative temporality. For example:</p> <ul> <li>If the maximum value is 10 during (T0, T2] and the   maximum value is 20 during (T0, T3], we know that the   maximum value during (T2, T3] must be 20.</li> <li>If the maximum value is 20 during (T0, T2] and the   maximum value is also 20 during (T0, T3], we wouldn't   know what the maximum value is during (T2, T3], unless   we know that there is no value (count = 0).</li> </ul> <p>So here are some suggestions that we encourage SDK implementers to consider:</p> <ul> <li>If you have to do Cumulative-&gt;Delta conversion, and you encountered min/max,   rather than drop the data on the floor, you might want to convert them to   something useful - e.g. Gauge.</li> </ul>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#asynchronous-example-attribute-removal-in-a-view","title":"Asynchronous example: attribute removal in a view","text":"<p>Suppose the metrics in the asynchronous example above are exported through a view configured to remove the <code>tid</code> attribute, leaving a single-dimensional count of page faults by <code>pid</code>.  For each metric stream, two measurements are produced covering the same interval of time, which the SDK is expected to aggregate before producing the output.</p> <p>The data model specifies to use the \"natural merge\" function, in this case meaning to add the current point values together because they are <code>Sum</code> data points.  The expected output is, still in Cumulative Temporality:</p> <ul> <li>(T0, T1]</li> <li>dimensions: {pid = <code>1001</code>}, sum: <code>80</code></li> <li>(T0, T2]</li> <li>dimensions: {pid = <code>1001</code>}, sum: <code>91</code></li> <li>(T0, T3]</li> <li>dimensions: {pid = <code>1001</code>}, sum: <code>98</code></li> <li>(T0, T4]</li> <li>dimensions: {pid = <code>1001</code>}, sum: <code>107</code></li> <li>(T0, T5]</li> <li>dimensions: {pid = <code>1001</code>}, sum: <code>58</code></li> </ul> <p>As discussed in the asynchronous cumulative temporality example above, there are various treatments available for detecting resets.  Even if the first course is taken, which means doing nothing, a receiver that follows the data model's rules for unknown start time and inserting true start times will calculate a correct rate in this case.  The \"58\" received at T5 resets the stream - the change from \"107\" to \"58\" will register as a gap and rate calculations will resume correctly at T6.  The rules for reset handling are provided so that the unknown portion of \"58\" that was counted reflected in the \"107\" at T4 is not double-counted at T5 in the reset.</p> <p>If the option to use per-stream start timestamps is taken above, it lightens the duties of the receiver, making it possible to monitor gaps precisely and detect overlapping streams.  When per-stream state is available, the SDK has several approaches for calculating Views available in the presence of attributes that stop reporting and then reset some time later:</p> <ol> <li>By remembering the cumulative value for all streams across the    lifetime of the process, the cumulative sum will be correct despite    <code>attributes</code> that come and go.  The SDK has to detect per-stream resets    itself in this case, otherwise the View will be calculated incorrectly.</li> <li>When the cost of remembering all streams <code>attributes</code> becomes too    high, reset the View and all its state, give it a new start    timestamp, and let the caller see a a gap in the stream.</li> </ol> <p>When considering this matter, note also that the metrics API has a recommendation for each asynchronous instrument: User code is recommended not to provide more than one <code>Measurement</code> with the same <code>attributes</code> in a single callback..  Consider whether the impact of user error in this regard will impact the correctness of the view.  When maintaining per-stream state for the purpose of View correctness, SDK authors may want to consider detecting when the user makes duplicate measurements.  Without checking for duplicate measurements, Views may be calculated incorrectly.</p>"},{"location":"docs/specs/otel/metrics/supplementary-guidelines/#memory-management","title":"Memory management","text":"<p>Memory management is a wide topic, here we will only cover some of the most important things for OpenTelemetry SDK.</p> <p>Choose a better design so the SDK has less things to be memorized, avoid keeping things in memory unless there is a must need. One good example is the aggregation temporality.</p> <p>Design a better memory layout, so the storage is efficient and accessing the storage can be fast. This is normally specific to the targeting programming language and platform. For example, aligning the memory to the CPU cache line, keeping the hot memories close to each other, keeping the memory close to the hardware (e.g. non-paged pool, NUMA).</p> <p>Pre-allocate and pool the memory, so the SDK doesn't have to allocate memory on-the-fly. This is especially useful to language runtimes that have garbage collectors, as it ensures the hot path in the code won't trigger garbage collection.</p> <p>Limit the memory usage, and handle critical memory condition. The general expectation is that a telemetry SDK should not fail the application. This can be done via some cardinality-capping algorithm - e.g. start to combine/drop some data points when the SDK hits the memory limit, and provide a mechanism to report the data loss.</p> <p>Provide configurations to the application owner. The answer to \"what is an efficient memory usage\" is ultimately depending on the goal of the application owner. For example, the application owners might want to spend more memory in order to keep more permutations of metrics attributes, or they might want to use memory aggressively for certain attributes that are important, and keep a conservative limit for attributes that are less important.</p>"},{"location":"docs/specs/otel/metrics/sdk_exporters/","title":"Index","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/#_1","title":"\u6307\u6807\u5bfc\u51fa\u5668","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/in-memory/","title":"In memory","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/in-memory/#opentelemetry-","title":"OpenTelemetry \u6307\u6807\u5bfc\u51fa\u5668-\u5728\u5185\u5b58\u4e2d","text":"<p>Status: Stable</p> <p>In-memory Metrics Exporter is a Push Metric Exporter which accumulates metrics data in the local memory and allows to inspect it (useful for e.g. unit tests).</p> <p>In-memory Metrics Exporter MUST support both Cumulative and Delta Temporality.</p> <p>If a language provides a mechanism to automatically configure a MetricReader to pair with the associated exporter (e.g., using the <code>OTEL_METRICS_EXPORTER</code> environment variable), by default the exporter MUST be paired with a periodic exporting MetricReader.</p>"},{"location":"docs/specs/otel/metrics/sdk_exporters/otlp/","title":"Otlp","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/otlp/#opentelemetry-otlp","title":"OpenTelemetry \u6307\u6807\u5bfc\u51fa\u5668 - OTLP","text":"<p>Status: Stable</p>"},{"location":"docs/specs/otel/metrics/sdk_exporters/otlp/#general","title":"General","text":"<p>OTLP Metrics Exporter is a Push Metric Exporter which sends metrics via the OpenTelemetry Protocol.</p> <p>OTLP Metrics Exporter MUST support both Cumulative and Delta Aggregation Temporality.</p> <p>The exporter MUST provide configuration according to the OpenTelemetry Protocol Exporter specification.</p> <p>If a language provides a mechanism to automatically configure a MetricReader to pair with the associated Exporter (e.g., using the <code>OTEL_METRICS_EXPORTER</code> environment variable), then by default:</p> <ul> <li>The exporter MUST be paired with a   periodic exporting MetricReader.</li> <li>The exporter MUST configure the default aggregation temporality on the basis   of instrument kind using the   <code>OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE</code> variable as described   below.</li> <li>The exporter MUST configure the default aggregation on the basis of instrument   kind using the <code>OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION</code>   variable as described below if it is implemented.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk_exporters/otlp/#additional-configuration","title":"Additional Configuration","text":"Name Description Default <code>OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE</code> The aggregation temporality to use on the basis of instrument kind. <code>cumulative</code> <code>OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION</code> The default aggregation to use for histogram instruments. <code>explicit_bucket_histogram</code> <p>The recognized (case-insensitive) values for <code>OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE</code> are:</p> <ul> <li><code>Cumulative</code>: Choose cumulative aggregation temporality for all instrument   kinds.</li> <li><code>Delta</code>: Choose Delta aggregation temporality for Counter, Asynchronous   Counter and Histogram instrument kinds, choose Cumulative aggregation for   UpDownCounter and Asynchronous UpDownCounter instrument kinds.</li> <li><code>LowMemory</code>: This configuration uses Delta aggregation temporality for   Synchronous Counter and Histogram and uses Cumulative aggregation temporality   for Synchronous UpDownCounter, Asynchronous Counter, and Asynchronous   UpDownCounter instrument kinds.</li> </ul> <p>The \"LowMemory\" choice is so-named because the SDK can under certain conditions use less memory in this configuration than the others. Comparatively, the \"cumulative\" choice forces the SDK to maintain a delta-to-cumulative conversion for Synchronous Counter and Histogram instruments, while the \"delta\" choice forces the SDK to maintain a cumulative-to-delta conversion for Asynchronous Counter instruments.</p> <p>The recognized (case-insensitive) values for <code>OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION</code> are:</p> <ul> <li><code>explicit_bucket_histogram</code>: Use   Explicit Bucket Histogram Aggregation.</li> <li><code>base2_exponential_bucket_histogram</code>: Use   Base2 Exponential Bucket Histogram Aggregation.</li> </ul>"},{"location":"docs/specs/otel/metrics/sdk_exporters/prometheus/","title":"Prometheus","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/prometheus/#opentelemetry-prometheus","title":"OpenTelemetry \u6307\u6807\u5bfc\u51fa\u5668 - Prometheus","text":"<p>Status: Experimental</p> <p>A Prometheus Exporter is a Pull Metric Exporter which reports metrics by responding to the Prometheus scraper requests.</p> <p>A Prometheus Exporter MUST support Pull mode.</p> <p>A Prometheus Exporter MUST NOT support Push mode.</p> <p>A Prometheus Exporter MUST only support Cumulative Temporality.</p> <p>A Prometheus Exporter MUST support version <code>0.0.4</code> of the Text-based format.</p> <p>A Prometheus Exporter MAY support OpenMetrics Text Format, including the Exemplars.</p>"},{"location":"docs/specs/otel/metrics/sdk_exporters/stdout/","title":"Stdout","text":""},{"location":"docs/specs/otel/metrics/sdk_exporters/stdout/#opentelemetry-","title":"OpenTelemetry \u6307\u6807\u5bfc\u51fa\u5668 - \u6807\u51c6\u8f93\u51fa","text":"<p>Status: Stable</p> <p>\"Standard output\" Metrics Exporter is a Push Metric Exporter which outputs the metrics to stdout/console.</p> <p>OpenTelemetry SDK authors MAY choose the best idiomatic name for their language. For example, ConsoleExporter, StdoutExporter, StreamExporter, etc.</p> <p>\"Standard output\" Metrics Exporter MUST support both Cumulative and Delta Temporality.</p> <p>If a language provides a mechanism to automatically configure a MetricReader to pair with the associated exporter (e.g., using the <code>OTEL_METRICS_EXPORTER</code> environment variable), by default the exporter MUST be paired with a periodic exporting MetricReader with a default <code>exportIntervalMilliseconds</code> of 10000.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/","title":"Index","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/#_1","title":"\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Mixed</p> <ul> <li>General Guidelines</li> <li>Name Reuse Prohibition</li> <li>Units</li> <li>Pluralization<ul> <li>Use <code>count</code> Instead of Pluralization</li> </ul> </li> <li>General Metric Semantic Conventions</li> <li>Instrument Naming</li> <li>Instrument Units</li> <li>Instrument Types</li> <li>Consistent UpDownCounter timeseries</li> </ul> <p>The following semantic conventions surrounding metrics are defined:</p> <ul> <li>HTTP: For HTTP client and server metrics.</li> <li>RPC: For RPC client and server metrics.</li> <li>Database: For SQL and NoSQL client metrics.</li> <li>System: For standard system metrics.</li> <li>Process: For standard process metrics.</li> <li>Runtime Environment: For runtime environment   metrics.</li> <li>FaaS: For   Function as a Service   metrics.</li> <li>Hardware: For hardware-related metrics.</li> </ul> <p>Apart from semantic conventions for metrics and traces, OpenTelemetry also defines the concept of overarching Resources with their own Resource Semantic Conventions.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#general-guidelines","title":"General Guidelines","text":"<p>Status: Experimental</p> <p>Metric names and attributes exist within a single universe and a single hierarchy. Metric names and attributes MUST be considered within the universe of all existing metric names. When defining new metric names and attributes, consider the prior art of existing standard metrics and metrics from frameworks/libraries.</p> <p>Associated metrics SHOULD be nested together in a hierarchy based on their usage. Define a top-level hierarchy for common metric categories: for OS metrics, like CPU and network; for app runtimes, like GC internals. Libraries and frameworks should nest their metrics into a hierarchy as well. This aids in discovery and adhoc comparison. This allows a user to find similar metrics given a certain metric.</p> <p>The hierarchical structure of metrics defines the namespacing. Supporting OpenTelemetry artifacts define the metric structures and hierarchies for some categories of metrics, and these can assist decisions when creating future metrics.</p> <p>Common attributes SHOULD be consistently named. This aids in discoverability and disambiguates similar attributes to metric names.</p> <p>\"As a rule of thumb, aggregations over all the attributes of a given metric SHOULD be meaningful,\" as Prometheus recommends.</p> <p>Semantic ambiguity SHOULD be avoided. Use prefixed metric names in cases where similar metrics have significantly different implementations across the breadth of all existing metrics. For example, every garbage collected runtime has slightly different strategies and measures. Using a single set of metric names for GC, not divided by the runtime, could create dissimilar comparisons and confusion for end users. (For example, prefer <code>process.runtime.java.gc*</code> over <code>process.runtime.gc.*</code>.) Measures of many operating system metrics are similarly ambiguous.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#name-reuse-prohibition","title":"Name Reuse Prohibition","text":"<p>A new metric MUST NOT be added with the same name as a metric that existed in the past but was renamed (with a corresponding schema file).</p> <p>When introducing a new metric name check all existing schema files to make sure the name does not appear as a key of any \"rename_metrics\" section (keys denote old metric names in rename operations).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#units","title":"Units","text":"<p>Conventional metrics or metrics that have their units included in OpenTelemetry metadata (e.g. <code>metric.WithUnit</code> in Go) SHOULD NOT include the units in the metric name. Units may be included when it provides additional meaning to the metric name. Metrics MUST, above all, be understandable and usable.</p> <p>When building components that interoperate between OpenTelemetry and a system using the OpenMetrics exposition format, use the OpenMetrics Guidelines.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#pluralization","title":"Pluralization","text":"<p>Metric names SHOULD NOT be pluralized, unless the value being recorded represents discrete instances of a countable quantity. Generally, the name SHOULD be pluralized only if the unit of the metric in question is a non-unit (like <code>{fault}</code> or <code>{operation}</code>).</p> <p>Examples:</p> <ul> <li><code>system.filesystem.utilization</code>, <code>http.server.duration</code>, and <code>system.cpu.time</code>   should not be pluralized, even if many data points are recorded.</li> <li><code>system.paging.faults</code>, <code>system.disk.operations</code>, and <code>system.network.packets</code>   should be pluralized, even if only a single data point is recorded.</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#use-count-instead-of-pluralization","title":"Use <code>count</code> Instead of Pluralization","text":"<p>If the value being recorded represents the count of concepts signified by the namespace then the metric should be named <code>count</code> (within its namespace). The pluralization rule does not apply in this case.</p> <p>For example if we have a namespace <code>system.processes</code> which contains all metrics related to the processes then to represent the count of the processes we can have a metric named <code>system.processes.count</code>. The suffix <code>count</code> here indicates that it is the count of <code>system.processes</code>.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#general-metric-semantic-conventions","title":"General Metric Semantic Conventions","text":"<p>Status: Mixed</p> <p>The following semantic conventions aim to keep naming consistent. They provide guidelines for most of the cases in this specification and should be followed for other instruments not explicitly defined in this document.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#instrument-naming","title":"Instrument Naming","text":"<p>Status: Experimental</p> <ul> <li> <p>limit - an instrument that measures the constant, known total amount of   something should be called <code>entity.limit</code>. For example, <code>system.memory.limit</code>   for the total amount of memory on a system.</p> </li> <li> <p>usage - an instrument that measures an amount used out of a known total   (limit) amount should be called <code>entity.usage</code>. For example,   <code>system.memory.usage</code> with attribute <code>state = used | cached | free | ...</code> for   the amount of memory in a each state. Where appropriate, the sum of usage   over all attribute values SHOULD be equal to the limit.</p> </li> </ul> <p>A measure of the amount consumed of an unlimited resource, or of a resource   whose limit is unknowable, is differentiated from usage. For example, the   maximum possible amount of virtual memory that a process may consume may   fluctuate over time and is not typically known.</p> <ul> <li> <p>utilization - an instrument that measures the fraction of usage out   of its limit should be called <code>entity.utilization</code>. For example,   <code>system.memory.utilization</code> for the fraction of memory in use. Utilization   values are in the range <code>[0, 1]</code>.</p> </li> <li> <p>time - an instrument that measures passage of time should be called   <code>entity.time</code>. For example, <code>system.cpu.time</code> with attribute   <code>state = idle | user | system | ...</code>. time measurements are not   necessarily wall time and can be less than or greater than the real wall time   between measurements.</p> </li> </ul> <p>time instruments are a special case of usage metrics, where the   limit can usually be calculated as the sum of time over all attribute   values. utilization for time instruments can be derived automatically   using metric event timestamps. For example, <code>system.cpu.utilization</code> is   defined as the difference in <code>system.cpu.time</code> measurements divided by the   elapsed time and number of CPUs.</p> <ul> <li> <p>io - an instrument that measures bidirectional data flow should be called   <code>entity.io</code> and have attributes for direction. For example,   <code>system.network.io</code>.</p> </li> <li> <p>Other instruments that do not fit the above descriptions may be named more   freely. For example, <code>system.paging.faults</code> and <code>system.network.packets</code>.   Units do not need to be specified in the names since they are included during   instrument creation, but can be added if there is ambiguity.</p> </li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#instrument-units","title":"Instrument Units","text":"<p>Status: Stable</p> <p>Units should follow the Unified Code for Units of Measure.</p> <ul> <li>Instruments for utilization metrics (that measure the fraction out of a   total) are dimensionless and SHOULD use the default unit <code>1</code> (the unity).</li> <li>All non-units that use curly braces to annotate a quantity need to match the   grammatical number of the quantity it represent. For example if measuring the   number of individual requests to a process the unit would be <code>{request}</code>, not   <code>{requests}</code>.</li> <li>Instruments that measure an integer count of something SHOULD only use   annotations with curly braces to give   additional meaning without the leading default unit (<code>1</code>). For example, use   <code>{packet}</code>, <code>{error}</code>, <code>{fault}</code>, etc.</li> <li>Instrument units other than <code>1</code> and those that use   annotations SHOULD be specified using   the UCUM case sensitive (\"c/s\") variant. For example, \"Cel\" for the unit with   full name \"degree Celsius\".</li> <li>Instruments SHOULD use non-prefixed units (i.e. <code>By</code> instead of <code>MiBy</code>) unless   there is good technical reason to not do so.</li> <li>When instruments are measuring durations, seconds (i.e. <code>s</code>) SHOULD be used.</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#instrument-types","title":"Instrument Types","text":"<p>Status: Stable</p> <p>The semantic metric conventions specification is written to use the names of the synchronous instrument types, like <code>Counter</code> or <code>UpDownCounter</code>. However, compliant implementations MAY use the asynchronous equivalent instead, like <code>Asynchronous Counter</code> or <code>Asynchronous UpDownCounter</code>. Whether implementations choose the synchronous type or the asynchronous equivalent is considered to be an implementation detail. Both choices are compliant with this specification.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/#consistent-updowncounter-timeseries","title":"Consistent UpDownCounter timeseries","text":"<p>Status: Experimental</p> <p>When recording <code>UpDownCounter</code> metrics, the same attribute values used to record an increment SHOULD be used to record any associated decrement, otherwise those increments and decrements will end up as different timeseries.</p> <p>For example, if you are tracking <code>active_requests</code> with an <code>UpDownCounter</code>, and you are incrementing it each time a request starts and decrementing it each time a request ends, then any attributes which are not yet available when incrementing the counter at request start should not be used when decrementing the counter at request end.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/database-metrics/","title":"Database metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/database-metrics/#_1","title":"\u6570\u636e\u5e93\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>The conventions described in this section are specific to SQL and NoSQL clients.</p> <p>Disclaimer: These are initial database client metric instruments and attributes but more may be added in the future.</p> <ul> <li>Metric Instruments</li> <li>Connection pools</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/database-metrics/#metric-instruments","title":"Metric Instruments","text":"<p>The following metric instruments MUST be used to describe database client operations. They MUST be of the specified type and units.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/database-metrics/#connection-pools","title":"Connection pools","text":"<p>Below is a table of database client connection pool metric instruments that MUST be used by connection pool instrumentations:</p> Name Instrument Unit Unit (UCUM) Description <code>db.client.connections.usage</code> UpDownCounter connections <code>{connection}</code> The number of connections that are currently in state described by the <code>state</code> attribute. <p>All <code>db.client.connections.usage</code> measurements MUST include the following attribute:</p> Name Type Description Examples Requirement Level <code>state</code> string The state of a connection in the pool. Valid values include: <code>idle</code>, <code>used</code>. <code>idle</code> Required <p>Instrumentation libraries for database client connection pools that collect data for the following data MUST use the following metric instruments. Otherwise, if the instrumentation library does not collect this data, these instruments MUST NOT be used.</p> Name Instrument (*) Unit Unit (UCUM) Description <code>db.client.connections.idle.max</code> UpDownCounter connections <code>{connection}</code> The maximum number of idle open connections allowed. <code>db.client.connections.idle.min</code> UpDownCounter connections <code>{connection}</code> The minimum number of idle open connections allowed. <code>db.client.connections.max</code> UpDownCounter connections <code>{connection}</code> The maximum number of open connections allowed. <code>db.client.connections.pending_requests</code> UpDownCounter requests <code>{request}</code> The number of pending requests for an open connection, cumulative for the entire pool. <code>db.client.connections.timeouts</code> Counter timeouts <code>{timeout}</code> The number of connection timeouts that have occurred trying to obtain a connection from the pool. <code>db.client.connections.create_time</code> Histogram milliseconds <code>ms</code> The time it took to create a new connection. <code>db.client.connections.wait_time</code> Histogram milliseconds <code>ms</code> The time it took to obtain an open connection from the pool. <code>db.client.connections.use_time</code> Histogram milliseconds <code>ms</code> The time between borrowing a connection and returning it to the pool. <p>Below is a table of the attributes that MUST be included on all connection pool measurements:</p> Name Type Description Examples Requirement Level <code>pool.name</code> string The name of the connection pool; unique within the instrumented application. In case the connection pool implementation does not provide a name, then the db.connection_string should be used. <code>myDataSource</code> Required"},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/","title":"Faas metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#faas","title":"FaaS \u6307\u6807\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines how to describe an instance of a function that runs without provisioning or managing of servers (also known as serverless functions or Function as a Service (FaaS)) with metrics.</p> <p>The conventions described in this section are FaaS (function as a service) specific. When FaaS operations occur, metric events about those operations will be generated and reported to provide insights into the operations. By adding FaaS attributes to metric events it allows for finely tuned filtering.</p> <p>Disclaimer: These are initial FaaS metric instruments and attributes but more may be added in the future.</p> <ul> <li>Metric Instruments</li> <li>FaaS Invocations</li> <li>Attributes</li> <li>References</li> <li>Metric References</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#metric-instruments","title":"Metric Instruments","text":"<p>The following metric instruments MUST be used to describe FaaS operations. They MUST be of the specified type and units.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#faas-invocations","title":"FaaS Invocations","text":"<p>Below is a table of FaaS invocation metric instruments.</p> Name Instrument Type (*) Unit Unit (UCUM) Description <code>faas.invoke_duration</code> Histogram milliseconds <code>ms</code> Measures the duration of the invocation <code>faas.init_duration</code> Histogram milliseconds <code>ms</code> Measures the duration of the function's initialization, such as a cold start <code>faas.coldstarts</code> Counter default unit <code>{coldstart}</code> Number of invocation cold starts. <code>faas.errors</code> Counter default unit <code>{error}</code> Number of invocation errors. <code>faas.invocations</code> Counter default unit <code>{invocation}</code> Number of successful invocations. <code>faas.timeouts</code> Counter default unit <code>{timeout}</code> Number of invocation timeouts. <p>Optionally, when applicable:</p> Name Instrument Type (*) Unit Unit (UCUM) Description <code>faas.mem_usage</code> Histogram Bytes <code>By</code> Distribution of max memory usage per invocation <code>faas.cpu_usage</code> Histogram milliseconds <code>ms</code> Distribution of CPU usage per invocation <code>faas.net_io</code> Histogram Bytes <code>By</code> Distribution of net I/O usage per invocation"},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#attributes","title":"Attributes","text":"<p>Below is a table of the attributes to be included on FaaS metric events.</p> Name Requirement Level Notes and examples <code>faas.trigger</code> Required Type of the trigger on which the function is invoked. SHOULD be one of: <code>datasource</code>, <code>http</code>, <code>pubsub</code>, <code>timer</code>, <code>other</code> <code>faas.invoked_name</code> Required Name of the invoked function. Example: <code>my-function</code> <code>faas.invoked_provider</code> Required Cloud provider of the invoked function. Corresponds to the resource <code>cloud.provider</code>. Example: <code>aws</code> <code>faas.invoked_region</code> Required Cloud provider region of invoked function. Corresponds to resource <code>cloud.region</code>. Example: <code>us-east-1</code> <p>More details on these attributes, the function name and the difference compared to the faas.invoked*name can be found at the related FaaS tracing specification. For incoming FaaS invocations, the function for which metrics are reported is already described by its FaaS resource attributes. Outgoing FaaS invocations are identified using the <code>faas.invoked*_</code>attributes above.<code>faas.trigger</code>SHOULD be included in all metric events while<code>faas.invoked\\__</code> attributes apply on outgoing FaaS invocation events only.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#references","title":"References","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/faas-metrics/#metric-references","title":"Metric References","text":"<p>Below are links to documentation regarding metrics that are available with different FaaS providers. This list is not exhaustive.</p> <ul> <li>AWS Lambda Metrics</li> <li>AWS Lambda Insight Metrics</li> <li>Azure Functions Metrics</li> <li>Google CloudFunctions Metrics</li> <li>OpenFaas Metrics</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/","title":"Hardware metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#_1","title":"\u786c\u4ef6\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document describes instruments and attributes for common hardware level metrics in OpenTelemetry. Consider the general metric semantic conventions when creating instruments not explicitly defined in the specification.</p> <ul> <li>Common hardware attributes</li> <li>Metric Instruments</li> <li><code>hw.</code> - Common hardware metrics</li> <li><code>hw.host.</code> - Physical host metrics</li> <li><code>hw.battery.</code> - Battery metrics</li> <li><code>hw.cpu.</code> - Physical processor metrics</li> <li><code>hw.disk_controller.</code> - Disk controller metrics</li> <li><code>hw.enclosure.</code> - Enclosure metrics</li> <li><code>hw.fan.</code> - Fan metrics</li> <li><code>hw.gpu.</code> - GPU metrics</li> <li><code>hw.logical_disk.</code>- Logical disk metrics</li> <li><code>hw.memory.</code> - Memory module metrics</li> <li><code>hw.network.</code> - Network adapter metrics</li> <li><code>hw.physical_disk.</code>- Physical disk metrics</li> <li><code>hw.power_supply.</code> - Power supply metrics</li> <li><code>hw.tape_drive.</code> - Tape drive metrics</li> <li><code>hw.temperature.</code> - Temperature sensor metrics</li> <li><code>hw.voltage.</code> - Voltage sensor metrics</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#common-hardware-attributes","title":"Common hardware attributes","text":"<p>All metrics in <code>hw.</code> instruments should be attached to a Host Resource and therefore inherit its attributes, like <code>host.id</code> and <code>host.name</code>.</p> <p>Additionally, all metrics in <code>hw.</code> instruments have the following attributes:</p> Attribute Key Description Example Requirement Level <code>id</code> An identifier for the hardware component, unique within the monitored host <code>win32battery_battery_testsysa33_1</code> Required <code>name</code> An easily-recognizable name for the hardware component <code>eth0</code> Recommended <code>parent</code> Unique identifier of the parent component (typically the <code>id</code> attribute of the enclosure, or disk controller) <code>dellStorage_perc_0</code> Recommended"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#metric-instruments","title":"Metric Instruments","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hw-common-hardware-metrics","title":"<code>hw.</code> - Common hardware metrics","text":"<p>The below metrics apply to any type of hardware component.</p> Name Description Units Instrument Type (*) Value Type Attribute Key(s) Attribute Values <code>hw.energy</code> Energy consumed by the component, in joules J Counter Int64 <code>hw.errors</code> Number of errors encountered by the component {error} Counter Int64 <code>hw.error.type</code> (Recommended) <code>hw.power</code> Instantaneous power consumed by the component, in Watts (<code>hw.energy</code> is preferred) W Gauge Double <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <p>These common <code>hw.</code> metrics must include the below attributes to describe the monitored component:</p> Attribute Key Description Example Requirement Level <code>hw.type</code> Type of the component <code>battery</code>, <code>cpu</code>, <code>disk_controller</code>, <code>enclosure</code>, <code>fan</code>, <code>gpu</code>, <code>logical_disk</code>, <code>memory</code>, <code>network</code>, <code>physical_disk</code>, <code>power_supply</code>, <code>tape_drive</code>, <code>temperature</code>, <code>voltage</code> Required <p>Warning</p> <p><code>hw.status</code> is currently specified as an UpDownCounter but would ideally be represented using a StateSet as defined in OpenMetrics. This semantic convention will be updated once StateSet is specified in OpenTelemetry. This planned change is not expected to have any consequence on the way users query their timeseries backend to retrieve the values of <code>hw.status</code> over time.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwhost-physical-host-metrics","title":"<code>hw.host.</code> - Physical host metrics","text":"<p>Description: Physical system as opposed to a virtual system or a container. Examples: physical server, switch or disk array.</p> Name Description Units Instrument Type (*) Value Type Attribute Key(s) Attribute Values <code>hw.host.ambient_temperature</code> Ambient (external) temperature of the physical host Cel Gauge Double <code>hw.host.energy</code> Total energy consumed by the entire physical host, in joules J Counter Int64 <code>hw.host.heating_margin</code> By how many degrees Celsius the temperature of the physical host can be increased, before reaching a warning threshold on one of the internal sensors Cel Gauge Double <code>hw.host.power</code> Instantaneous power consumed by the entire physical host in Watts (<code>hw.host.energy</code> is preferred) W Gauge Double <p>Note The overall energy usage of a host MUST be reported using the specific <code>hw.host.energy</code> and <code>hw.host.power</code> metrics only, instead of the generic <code>hw.energy</code> and <code>hw.power</code> described in the previous section, to prevent summing up overlapping values.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwbattery-battery-metrics","title":"<code>hw.battery.</code> - Battery metrics","text":"<p>Description: A battery in a computer system or an UPS.</p> Name Description Units Instrument Type (*) Value Type Attribute Key(s) Attribute Values <code>hw.battery.charge</code> Remaining fraction of battery charge 1 Gauge Double <code>hw.battery.charge.limit</code> Lower limit of battery charge fraction to ensure proper operation 1 Gauge Double <code>limit_type</code> (Recommended) <code>critical</code>, <code>throttled</code>, <code>degraded</code> <code>hw.battery.time_left</code> Time left before battery is completely charged or discharged s Gauge Int <code>state</code> (Conditionally Required, if the battery is charging or discharging) <code>charging</code>, <code>discharging</code> <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>charging</code>, <code>discharging</code> <code>hw.type</code> <code>battery</code> <p>All <code>hw.battery.</code> metrics may include the below Recommended attributes to describe the characteristics of the monitored battery:</p> Attribute Key Description Example <code>chemistry</code> Chemistry of the battery Nickel-Cadmium, Lithium-ion <code>capacity</code> Design capacity in Watts-hours or Amper-hours 9.3Ah <code>model</code> Descriptive model name <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwcpu-physical-processor-metrics","title":"<code>hw.cpu.</code> - Physical processor metrics","text":"<p>Description: Physical processor (as opposed to the logical processor seen by the operating system for multi-core systems). A physical processor may include many individual cores.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Total number of errors encountered and corrected by the CPU {error} Counter Int64 <code>hw.type</code> (Required) <code>cpu</code> <code>hw.cpu.speed</code> CPU current frequency Hz Gauge Int64 <code>hw.cpu.speed.limit</code> CPU maximum frequency Hz Gauge Int64 <code>limit_type</code> (Recommended) <code>throttled</code>, <code>max</code>, <code>turbo</code> <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>predicted_failure</code> <code>hw.type</code> (Required) <code>cpu</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>model</code> Descriptive model name <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwdisk_controller-disk-controller-metrics","title":"<code>hw.disk_controller.</code> - Disk controller metrics","text":"<p>Description: Controller that controls the physical disks and organize them in RAID sets and logical disks that are exposed to the operating system.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>disk_controller</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>bios_version</code> BIOS version <code>driver_version</code> Driver for the controller <code>firmware_version</code> Firmware version <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwenclosure-enclosure-metrics","title":"<code>hw.enclosure.</code> - Enclosure metrics","text":"<p>Description: Computer chassis (can be an expansion enclosure)</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>open</code> <code>hw.type</code> (Required) <code>enclosure</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>bios_version</code> BIOS version <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>type</code> Type of the enclosure (useful for modular systems) Computer, Storage, Switch <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwfan-fan-metrics","title":"<code>hw.fan.</code> - Fan metrics","text":"<p>Description: Fan that keeps the air flowing to maintain the internal temperature of a computer</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.fan.speed</code> Fan speed in revolutions per minute rpm Gauge Int <code>hw.fan.speed.limit</code> Speed limit in rpm rpm Gauge Int <code>limit_type</code> (Recommended) <code>low.critical</code>, <code>low.degraded</code>, <code>max</code> <code>hw.fan.speed_ratio</code> Fan speed expressed as a fraction of its maximum speed 1 Gauge Double <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>fan</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>sensor_location</code> Location of the fan in the computer enclosure cpu0, ps1, INLET"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwgpu-gpu-metrics","title":"<code>hw.gpu.</code> - GPU metrics","text":"<p>Description: Graphics Processing Unit (discrete)</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered by the GPU {error} Counter Int64 <code>hw.error.type</code> (Recommended) <code>corrected</code>, <code>uncorrected</code> <code>hw.type</code> (Required) <code>gpu</code> <code>hw.gpu.io</code> Received and transmitted bytes by the GPU By Counter Int64 <code>direction</code> (Required) <code>receive</code>, <code>transmit</code> <code>hw.gpu.memory.limit</code> Size of the GPU memory By UpDownCounter Int64 <code>hw.gpu.memory.utilization</code> Fraction of GPU memory used 1 Gauge Double <code>hw.gpu.memory.usage</code> GPU memory used By UpDownCounter Int64 <code>hw.gpu.power</code> GPU instantaneous power consumption in Watts W Gauge Double <code>hw.gpu.utilization</code> Fraction of time spent in a specific task 1 Gauge Double <code>task</code> (Recommended) <code>decoder</code>, <code>encoder</code>, <code>general</code> <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>predicted_failure</code> <code>hw.type</code> (Required) <code>gpu</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>driver_version</code> Driver for the controller <code>firmware_version</code> Firmware version <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwlogical_disk-logical-disk-metrics","title":"<code>hw.logical_disk.</code>- Logical disk metrics","text":"<p>Description: Storage extent presented as a physical disk by a disk controller to the operating system (e.g. a RAID 1 set made of 2 disks, and exposed as /dev/hdd0 by the controller).</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered on this logical disk {error} Counter Int64 <code>hw.type</code> (Required) <code>logical_disk</code> <code>hw.logical_disk.limit</code> Size of the logical disk By UpDownCounter Int64 <code>hw.logical_disk.usage</code> Logical disk space usage By UpDownCounter Int64 <code>state</code> (Required) <code>used</code>, <code>free</code> <code>hw.logical_disk.utilization</code> Logical disk space utilization as a fraction 1 Gauge Double <code>state</code> (Required) <code>used</code>, <code>free</code> <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>logical_disk</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>raid_level</code> RAID Level <code>RAID0+1</code>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwmemory-memory-module-metrics","title":"<code>hw.memory.</code> - Memory module metrics","text":"<p>Description: A memory module in a computer system.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered on this memory module {error} Counter Int64 <code>hw.type</code> (Required) <code>memory</code> <code>hw.memory.size</code> Size of the memory module By UpDownCounter Int64 <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>predicted_failure</code> <code>hw.type</code> (Required) <code>memory</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>type</code> Type of the memory module <code>DDR5</code> <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwnetwork-network-adapter-metrics","title":"<code>hw.network.</code> - Network adapter metrics","text":"<p>Description: A physical network interface, or a network interface controller (NIC), excluding software-based virtual adapters and loopbacks. For example, a physical network interface on a server, switch, router or firewall, an HBA, a fiber channel port or a Wi-Fi adapter.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered by the network adapter {error} Counter Int64 <code>hw.error.type</code> (Recommended) <code>zero_buffer_credit</code>, <code>crc</code>, etc. <code>hw.type</code> (Required) <code>network</code> <code>direction</code> (Recommended) <code>receive</code>, <code>transmit</code> <code>hw.network.bandwidth.limit</code> Link speed By UpDownCounter Int64 <code>hw.network.bandwidth.utilization</code> Utilization of the network bandwidth as a fraction 1 Gauge Double <code>hw.network.io</code> Received and transmitted network traffic in bytes By Counter Int64 <code>direction</code> (Required) <code>receive</code>, <code>transmit</code> <code>hw.network.packets</code> Received and transmitted network traffic in packets (or frames) {packet} Counter Int64 <code>direction</code> (Required) <code>receive</code>, <code>transmit</code> <code>hw.network.up</code> Link status: <code>1</code> (up) or <code>0</code> (down) UpDownCounter Int <code>hw.status</code> Operational status, regardless of the link status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>network</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>model</code> Descriptive model name <code>logical_addresses</code> Logical addresses of the adapter (e.g. IP address, or WWPN) <code>172.16.8.21, 57.11.193.42</code> <code>physical_address</code> Physical address of the adapter (e.g. MAC address, or WWNN) <code>00-90-F5-E9-7B-36</code> <code>serial_number</code> Serial number <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwphysical_disk-physical-disk-metrics","title":"<code>hw.physical_disk.</code>- Physical disk metrics","text":"<p>Description: Physical hard drive (HDD or SDD)</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered on this disk {error} Counter Int64 <code>hw.error.type</code> (Recommended) <code>bad_sector</code>, <code>write</code>, etc. <code>hw.type</code> (Required) <code>physical_disk</code> <code>hw.physical_disk.endurance_utilization</code> Endurance remaining for this SSD disk 1 Gauge Double <code>state</code> (Required) <code>remaining</code> <code>hw.physical_disk.size</code> Size of the disk By UpDownCounter Int64 <code>hw.physical_disk.smart</code> Value of the corresponding S.M.A.R.T. attribute 1 Gauge Int <code>smart_attribute</code> (Recommended) <code>Seek Error Rate</code>, <code>Spin Retry Count</code>, etc. <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>predicted_failure</code> <code>hw.type</code> (Required) <code>physical_disk</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>firmware_version</code> Firmware version <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>type</code> Type of the disk <code>HDD</code>, <code>SSD</code>, <code>10K</code> <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwpower_supply-power-supply-metrics","title":"<code>hw.power_supply.</code> - Power supply metrics","text":"<p>Description: Power supply converting AC current to DC used by the motherboard and the GPUs</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.power_supply.limit</code> Maximum power output of the power supply W UpDownCounter Int64 <code>limit_type</code> (Recommended) <code>max</code>, <code>critical</code>, <code>throttled</code> <code>hw.power_supply.utilization</code> Utilization of the power supply as a fraction of its maximum output 1 Gauge Double <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>power_supply</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwtape_drive-tape-drive-metrics","title":"<code>hw.tape_drive.</code> - Tape drive metrics","text":"<p>Description: A tape drive in a computer or in a tape library (excluding virtual tape libraries)</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.errors</code> Number of errors encountered by the tape drive {error} Counter Int64 <code>hw.error.type</code> <code>read</code>, <code>write</code>, <code>mount</code>, etc. <code>hw.type</code> (Required) <code>tape_drive</code> <code>hw.tape_drive.operations</code> Operations performed by the tape drive {operation} Counter Int64 <code>type</code> (Recommended) <code>mount</code>, <code>unmount</code>, <code>clean</code> <code>hw.status</code> Operational status: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code>, <code>needs_cleaning</code> <code>hw.type</code> (Required) <code>tape_drive</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>model</code> Descriptive model name <code>serial_number</code> Serial number <code>vendor</code> Vendor name"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwtemperature-temperature-sensor-metrics","title":"<code>hw.temperature.</code> - Temperature sensor metrics","text":"<p>Description: A temperature sensor, either numeric or discrete</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.temperature</code> Temperature in degrees Celsius Cel Gauge Double <code>hw.temperature.limit</code> Temperature limit in degrees Celsius Cel Gauge Double <code>limit_type</code> (Recommended) <code>low.critical</code>, <code>low.degraded</code>, <code>high.degraded</code>, <code>high.critical</code> <code>hw.status</code> Whether the temperature is within normal range: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>temperature</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>sensor_location</code> Location of the sensor <code>CPU0_DIE</code>"},{"location":"docs/specs/otel/metrics/semantic_conventions/hardware-metrics/#hwvoltage-voltage-sensor-metrics","title":"<code>hw.voltage.</code> - Voltage sensor metrics","text":"<p>Description: A voltage sensor, either numeric or discrete</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values <code>hw.voltage.limit</code> Voltage limit in Volts V Gauge Double <code>limit_type</code> (Recommended) <code>low.critical</code>, <code>low.degraded</code>, <code>high.degraded</code>, <code>high.critical</code> <code>hw.voltage.nominal</code> Nominal (expected) voltage V Gauge Double <code>hw.voltage</code> Voltage measured by the sensor V Gauge Double <code>hw.status</code> Whether the voltage is within normal range: <code>1</code> (true) or <code>0</code> (false) for each of the possible states UpDownCounter Int <code>state</code> (Required) <code>ok</code>, <code>degraded</code>, <code>failed</code> <code>hw.type</code> (Required) <code>voltage</code> <p>Additional Recommended attributes:</p> Attribute Key Description Example <code>sensor_location</code> Location of the sensor <code>PS0 V3_3</code>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/","title":"Http metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#http","title":"HTTP \u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>The conventions described in this section are HTTP specific. When HTTP operations occur, metric events about those operations will be generated and reported to provide insight into the operations. By adding HTTP attributes to metric events it allows for finely tuned filtering.</p> <p>Disclaimer: These are initial HTTP metric instruments and attributes but more may be added in the future.</p> <ul> <li>HTTP Server</li> <li>Metric: <code>http.server.duration</code></li> <li>Metric: <code>http.server.active_requests</code></li> <li>Metric: <code>http.server.request.size</code></li> <li>Metric: <code>http.server.response.size</code></li> <li>HTTP Client</li> <li>Metric: <code>http.client.duration</code></li> <li>Metric: <code>http.client.request.size</code></li> <li>Metric: <code>http.client.response.size</code></li> </ul> <p>Warning Existing HTTP instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the HTTP or networking attributes that they   emit until the HTTP semantic conventions are marked stable (HTTP   stabilization will include stabilization of a core set of networking   attributes which are also used in HTTP instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental HTTP     and networking attributes the instrumentation was emitting previously.     This is the default value.</li> <li><code>http</code> - emit the new, stable HTTP and networking attributes, and stop     emitting the old experimental HTTP and networking attributes that the     instrumentation emitted previously.</li> <li><code>http/dup</code> - emit both the old and the stable HTTP and networking     attributes, allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#http-server","title":"HTTP Server","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpserverduration","title":"Metric: <code>http.server.duration</code>","text":"<p>This metric is required.</p> <p>This metric SHOULD be specified with <code>ExplicitBucketBoundaries</code> of <code>[ 0, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10 ]</code>.</p> Name Instrument Type Unit (UCUM) Description <code>http.server.duration</code> Histogram <code>s</code> Measures the duration of inbound HTTP requests. Attribute Type Description Examples Requirement Level <code>http.route</code> string The matched route (path template in the format used by the respective server framework). See note below [1] <code>/users/:userID?</code>; <code>{controller}/{action}/{id?}</code> Conditionally Required: If and only if it's available <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [2] <code>3.1.1</code> Recommended <code>server.address</code> string Name of the local HTTP server that received the request. [3] <code>example.com</code> Required <code>server.port</code> int Port of the local HTTP server that received the request. [4] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [5] <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>http</code>; <code>https</code> Required <p>[1]: MUST NOT be populated when this is not supported by the HTTP server framework as the route attribute should have low-cardinality and the URI path can NOT substitute it. SHOULD include the application root if there is one.</p> <p>[2]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[3]: Determined by using the first of the following that applies</p> <ul> <li>The   primary server name   of the matched virtual host. MUST only include host identifier.</li> <li>Host identifier of the   request target   if it's sent in absolute-form.</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if only IP address is available and capturing name would require a reverse DNS lookup.</p> <p>[4]: Determined by using the first of the following that applies</p> <ul> <li>Port identifier of the   primary server host   of the matched virtual host.</li> <li>Port identifier of the   request target   if it's sent in absolute-form.</li> <li>Port identifier of the <code>Host</code> header</li> </ul> <p>[5]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpserveractive_requests","title":"Metric: <code>http.server.active_requests</code>","text":"<p>This metric is optional.</p> Name Instrument Type Unit (UCUM) Description <code>http.server.active_requests</code> UpDownCounter <code>{request}</code> Measures the number of concurrent HTTP requests that are currently in-flight. Attribute Type Description Examples Requirement Level <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>server.address</code> string Name of the local HTTP server that received the request. [1] <code>example.com</code> Required <code>server.port</code> int Port of the local HTTP server that received the request. [2] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [3] <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>http</code>; <code>https</code> Required <p>[1]: Determined by using the first of the following that applies</p> <ul> <li>The   primary server name   of the matched virtual host. MUST only include host identifier.</li> <li>Host identifier of the   request target   if it's sent in absolute-form.</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if only IP address is available and capturing name would require a reverse DNS lookup.</p> <p>[2]: Determined by using the first of the following that applies</p> <ul> <li>Port identifier of the   primary server host   of the matched virtual host.</li> <li>Port identifier of the   request target   if it's sent in absolute-form.</li> <li>Port identifier of the <code>Host</code> header</li> </ul> <p>[3]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpserverrequestsize","title":"Metric: <code>http.server.request.size</code>","text":"<p>This metric is optional.</p> Name Instrument Type Unit (UCUM) Description <code>http.server.request.size</code> Histogram <code>By</code> Measures the size of HTTP request messages (compressed). Attribute Type Description Examples Requirement Level <code>http.route</code> string The matched route (path template in the format used by the respective server framework). See note below [1] <code>/users/:userID?</code>; <code>{controller}/{action}/{id?}</code> Conditionally Required: If and only if it's available <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [2] <code>3.1.1</code> Recommended <code>server.address</code> string Name of the local HTTP server that received the request. [3] <code>example.com</code> Required <code>server.port</code> int Port of the local HTTP server that received the request. [4] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [5] <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>http</code>; <code>https</code> Required <p>[1]: MUST NOT be populated when this is not supported by the HTTP server framework as the route attribute should have low-cardinality and the URI path can NOT substitute it. SHOULD include the application root if there is one.</p> <p>[2]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[3]: Determined by using the first of the following that applies</p> <ul> <li>The   primary server name   of the matched virtual host. MUST only include host identifier.</li> <li>Host identifier of the   request target   if it's sent in absolute-form.</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if only IP address is available and capturing name would require a reverse DNS lookup.</p> <p>[4]: Determined by using the first of the following that applies</p> <ul> <li>Port identifier of the   primary server host   of the matched virtual host.</li> <li>Port identifier of the   request target   if it's sent in absolute-form.</li> <li>Port identifier of the <code>Host</code> header</li> </ul> <p>[5]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpserverresponsesize","title":"Metric: <code>http.server.response.size</code>","text":"<p>This metric is optional.</p> Name Instrument Type Unit (UCUM) Description <code>http.server.response.size</code> Histogram <code>By</code> Measures the size of HTTP response messages (compressed). Attribute Type Description Examples Requirement Level <code>http.route</code> string The matched route (path template in the format used by the respective server framework). See note below [1] <code>/users/:userID?</code>; <code>{controller}/{action}/{id?}</code> Conditionally Required: If and only if it's available <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [2] <code>3.1.1</code> Recommended <code>server.address</code> string Name of the local HTTP server that received the request. [3] <code>example.com</code> Required <code>server.port</code> int Port of the local HTTP server that received the request. [4] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [5] <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>http</code>; <code>https</code> Required <p>[1]: MUST NOT be populated when this is not supported by the HTTP server framework as the route attribute should have low-cardinality and the URI path can NOT substitute it. SHOULD include the application root if there is one.</p> <p>[2]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[3]: Determined by using the first of the following that applies</p> <ul> <li>The   primary server name   of the matched virtual host. MUST only include host identifier.</li> <li>Host identifier of the   request target   if it's sent in absolute-form.</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if only IP address is available and capturing name would require a reverse DNS lookup.</p> <p>[4]: Determined by using the first of the following that applies</p> <ul> <li>Port identifier of the   primary server host   of the matched virtual host.</li> <li>Port identifier of the   request target   if it's sent in absolute-form.</li> <li>Port identifier of the <code>Host</code> header</li> </ul> <p>[5]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#http-client","title":"HTTP Client","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpclientduration","title":"Metric: <code>http.client.duration</code>","text":"<p>This metric is required.</p> <p>This metric SHOULD be specified with <code>ExplicitBucketBoundaries</code> of <code>[ 0, 0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5, 10 ]</code>.</p> Name Instrument Type Unit (UCUM) Description <code>http.client.duration</code> Histogram <code>s</code> Measures the duration of outbound HTTP requests. Attribute Type Description Examples Requirement Level <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [1] <code>3.1.1</code> Recommended <code>server.address</code> string Host identifier of the \"URI origin\" HTTP request is sent to. [2] <code>example.com</code> Required <code>server.port</code> int Port identifier of the \"URI origin\" HTTP request is sent to. [3] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [4] <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <p>[1]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[2]: Determined by using the first of the following that applies</p> <ul> <li>Host identifier of the   request target   if it's sent in absolute-form</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if capturing it would require an extra DNS lookup.</p> <p>[3]: When request target is absolute URI, <code>server.port</code> MUST match URI port identifier, otherwise it MUST match <code>Host</code> header port identifier.</p> <p>[4]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpclientrequestsize","title":"Metric: <code>http.client.request.size</code>","text":"<p>This metric is optional.</p> Name Instrument Type Unit (UCUM) Description <code>http.client.request.size</code> Histogram <code>By</code> Measures the size of HTTP request messages (compressed). Attribute Type Description Examples Requirement Level <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [1] <code>3.1.1</code> Recommended <code>server.address</code> string Host identifier of the \"URI origin\" HTTP request is sent to. [2] <code>example.com</code> Required <code>server.port</code> int Port identifier of the \"URI origin\" HTTP request is sent to. [3] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [4] <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <p>[1]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[2]: Determined by using the first of the following that applies</p> <ul> <li>Host identifier of the   request target   if it's sent in absolute-form</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if capturing it would require an extra DNS lookup.</p> <p>[3]: When request target is absolute URI, <code>server.port</code> MUST match URI port identifier, otherwise it MUST match <code>Host</code> header port identifier.</p> <p>[4]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/http-metrics/#metric-httpclientresponsesize","title":"Metric: <code>http.client.response.size</code>","text":"<p>This metric is optional.</p> Name Instrument Type Unit (UCUM) Description <code>http.client.response.size</code> Histogram <code>By</code> Measures the size of HTTP response messages (compressed). Attribute Type Description Examples Requirement Level <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [1] <code>3.1.1</code> Recommended <code>server.address</code> string Host identifier of the \"URI origin\" HTTP request is sent to. [2] <code>example.com</code> Required <code>server.port</code> int Port identifier of the \"URI origin\" HTTP request is sent to. [3] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [4] <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <p>[1]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[2]: Determined by using the first of the following that applies</p> <ul> <li>Host identifier of the   request target   if it's sent in absolute-form</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if capturing it would require an extra DNS lookup.</p> <p>[3]: When request target is absolute URI, <code>server.port</code> MUST match URI port identifier, otherwise it MUST match <code>Host</code> header port identifier.</p> <p>[4]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/process-metrics/","title":"Process metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/process-metrics/#_1","title":"\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u7a0b\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document describes instruments and attributes for common OS process level metrics in OpenTelemetry. Also consider the general metric semantic conventions when creating instruments not explicitly defined in this document. OS process metrics are not related to the runtime environment of the program, and should take measurements from the operating system. For runtime environment metrics see semantic conventions for runtime environment metrics.</p> <ul> <li>Metric Instruments</li> <li>Process</li> <li>Attributes</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/process-metrics/#metric-instruments","title":"Metric Instruments","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/process-metrics/#process","title":"Process","text":"<p>Below is a table of Process metric instruments.</p> Name Instrument Type (*) Unit Description Labels <code>process.cpu.time</code> Counter s Total CPU seconds broken down by different states. <code>state</code>, if specified, SHOULD be one of: <code>system</code>, <code>user</code>, <code>wait</code>. A process SHOULD be characterized either by data points with no <code>state</code> labels, or only data points with <code>state</code> labels. <code>process.cpu.utilization</code> Gauge 1 Difference in process.cpu.time since the last measurement, divided by the elapsed time and number of CPUs available to the process. <code>state</code>, if specified, SHOULD be one of: <code>system</code>, <code>user</code>, <code>wait</code>. A process SHOULD be characterized either by data points with no <code>state</code> labels, or only data points with <code>state</code> labels. <code>process.memory.usage</code> UpDownCounter By The amount of physical memory in use. <code>process.memory.virtual</code> UpDownCounter By The amount of committed virtual memory. <code>process.disk.io</code> Counter By Disk bytes transferred. <code>direction</code> SHOULD be one of: <code>read</code>, <code>write</code> <code>process.network.io</code> Counter By Network bytes transferred. <code>direction</code> SHOULD be one of: <code>receive</code>, <code>transmit</code> <code>process.threads</code> UpDownCounter {thread} Process threads count. <code>process.open_file_descriptors</code> UpDownCounter {count} Number of file descriptors in use by the process. <code>process.context_switches</code> Counter {count} Number of times the process has been context switched. <code>type</code> SHOULD be one of: <code>involuntary</code>, <code>voluntary</code> <code>process.paging.faults</code> Counter {fault} Number of page faults the process has made. <code>type</code>, if specified, SHOULD be one of: <code>major</code> (for major, or hard, page faults), <code>minor</code> (for minor, or soft, page faults)."},{"location":"docs/specs/otel/metrics/semantic_conventions/process-metrics/#attributes","title":"Attributes","text":"<p>Process metrics SHOULD be associated with a <code>process</code> resource whose attributes provide additional context about the process.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/","title":"Rpc metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#rpc","title":"\u4e00\u822c RPC \u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>The conventions described in this section are RPC specific. When RPC operations occur, measurements about those operations are recorded to instruments. The measurements are aggregated and exported as metrics, which provide insight into those operations. By including RPC properties as attributes on measurements, the metrics can be filtered for finer grain analysis.</p> <ul> <li>Metric instruments</li> <li>RPC Server</li> <li>RPC Client</li> <li>Attributes</li> <li>Service name</li> <li>gRPC conventions</li> <li>gRPC Attributes</li> <li>Connect RPC conventions</li> <li>Connect RPC Attributes</li> </ul> <p>Warning Existing RPC instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the networking attributes that they emit   until the HTTP semantic conventions are marked stable (HTTP stabilization   will include stabilization of a core set of networking attributes which are   also used in RPC instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental     networking attributes the instrumentation was emitting previously. This is     the default value.</li> <li><code>http</code> - emit the new, stable networking attributes, and stop emitting the     old experimental networking attributes that the instrumentation emitted     previously.</li> <li><code>http/dup</code> - emit both the old and the stable networking attributes,     allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#metric-instruments","title":"Metric instruments","text":"<p>The following metric instruments MUST be used to describe RPC operations. They MUST be of the specified type and units.</p> <p>Note: RPC server and client metrics are split to allow correlation across client/server boundaries, e.g. Lining up an RPC method latency to determine if the server is responsible for latency the client is seeing.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#rpc-server","title":"RPC Server","text":"<p>Below is a table of RPC server metric instruments.</p> Name Instrument Type (*) Unit Unit (UCUM) Description Status Streaming <code>rpc.server.duration</code> Histogram milliseconds <code>ms</code> measures duration of inbound RPC Recommended N/A. While streaming RPCs may record this metric as start-of-batch to end-of-batch, it's hard to interpret in practice. <code>rpc.server.request.size</code> Histogram Bytes <code>By</code> measures size of RPC request messages (uncompressed) Optional Recorded per message in a streaming batch <code>rpc.server.response.size</code> Histogram Bytes <code>By</code> measures size of RPC response messages (uncompressed) Optional Recorded per response in a streaming batch <code>rpc.server.requests_per_rpc</code> Histogram count <code>{count}</code> measures the number of messages received per RPC. Should be 1 for all non-streaming RPCs Optional Required <code>rpc.server.responses_per_rpc</code> Histogram count <code>{count}</code> measures the number of messages sent per RPC. Should be 1 for all non-streaming RPCs Optional Required"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#rpc-client","title":"RPC Client","text":"<p>Below is a table of RPC client metric instruments. These apply to traditional RPC usage, not streaming RPCs.</p> Name Instrument Type (*) Unit Unit (UCUM) Description Status Streaming <code>rpc.client.duration</code> Histogram milliseconds <code>ms</code> measures duration of outbound RPC Recommended N/A. While streaming RPCs may record this metric as start-of-batch to end-of-batch, it's hard to interpret in practice. <code>rpc.client.request.size</code> Histogram Bytes <code>By</code> measures size of RPC request messages (uncompressed) Optional Recorded per message in a streaming batch <code>rpc.client.response.size</code> Histogram Bytes <code>By</code> measures size of RPC response messages (uncompressed) Optional Recorded per message in a streaming batch <code>rpc.client.requests_per_rpc</code> Histogram count <code>{count}</code> measures the number of messages received per RPC. Should be 1 for all non-streaming RPCs Optional Required <code>rpc.client.responses_per_rpc</code> Histogram count <code>{count}</code> measures the number of messages sent per RPC. Should be 1 for all non-streaming RPCs Optional Required"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#attributes","title":"Attributes","text":"<p>Below is a table of attributes that SHOULD be included on client and server RPC measurements.</p> Attribute Type Description Examples Requirement Level <code>rpc.system</code> string A string identifying the remoting system. See below for a list of well-known identifiers. <code>grpc</code> Required <code>rpc.service</code> string The full (logical) name of the service being called, including its package name, if applicable. [1] <code>myservice.EchoService</code> Recommended <code>rpc.method</code> string The name of the (logical) method being called, must be equal to the $method part in the span name. [2] <code>exampleMethod</code> Recommended <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>server.address</code> string RPC server host name. [3] <code>example.com</code> Required <code>server.port</code> int Logical server port number <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: See below <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> See below <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: [4] <p>[1]: This is the logical name of the service from the RPC interface perspective, which can be different from the name of any implementing class. The <code>code.namespace</code> attribute may be used to store the latter (despite the attribute name, it may include a class name; e.g., class with method actually executing the call on the server side, RPC client stub class on the client side).</p> <p>[2]: This is the logical name of the method from the RPC interface perspective, which can be different from the name of any implementing method/function. The <code>code.function</code> attribute may be used to store the latter (e.g., method actually executing the call on the server side, RPC client stub method on the client side).</p> <p>[3]: May contain server IP address, DNS name, or local socket name. When host component is an IP address, instrumentations SHOULD NOT do a reverse proxy lookup to obtain DNS name and SHOULD set <code>server.address</code> to the IP address provided in the host component.</p> <p>[4]: If different than <code>server.port</code> and if <code>server.socket.address</code> is set.</p> <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>server.socket.address</code></li> <li><code>server.address</code></li> </ul> <p><code>rpc.system</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>grpc</code> gRPC <code>java_rmi</code> Java RMI <code>dotnet_wcf</code> .NET WCF <code>apache_dubbo</code> Apache Dubbo <code>connect_rpc</code> Connect RPC <p>To avoid high cardinality, implementations should prefer the most stable of <code>server.address</code> or <code>server.socket.address</code>, depending on expected deployment profile. For many cloud applications, this is likely <code>server.address</code> as names can be recycled even across re-instantiation of a server with a different <code>ip</code>.</p> <p>For client-side metrics <code>server.port</code> is required if the connection is IP-based and the port is available (it describes the server port they are connecting to). For server-side spans <code>server.port</code> is optional (it describes the port the client is connecting from).</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#service-name","title":"Service name","text":"<p>On the server process receiving and handling the remote procedure call, the service name provided in <code>rpc.service</code> does not necessarily have to match the [<code>service.name</code>][] resource attribute. One process can expose multiple RPC endpoints and thus have multiple RPC service names. From a deployment perspective, as expressed by the <code>service.*</code> resource attributes, it will be treated as one deployed service with one <code>service.name</code>.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#grpc-conventions","title":"gRPC conventions","text":"<p>For remote procedure calls via gRPC, additional conventions are described in this section.</p> <p><code>rpc.system</code> MUST be set to <code>\"grpc\"</code>.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#grpc-attributes","title":"gRPC Attributes","text":"<p>Below is a table of attributes that SHOULD be included on client and server RPC measurements when <code>rpc.system</code> is <code>\"grpc\"</code>.</p> Attribute Type Description Examples Requirement Level <code>rpc.grpc.status_code</code> int The numeric status code of the gRPC request. <code>0</code> Required <p><code>rpc.grpc.status_code</code> MUST be one of the following:</p> Value Description <code>0</code> OK <code>1</code> CANCELLED <code>2</code> UNKNOWN <code>3</code> INVALID_ARGUMENT <code>4</code> DEADLINE_EXCEEDED <code>5</code> NOT_FOUND <code>6</code> ALREADY_EXISTS <code>7</code> PERMISSION_DENIED <code>8</code> RESOURCE_EXHAUSTED <code>9</code> FAILED_PRECONDITION <code>10</code> ABORTED <code>11</code> OUT_OF_RANGE <code>12</code> UNIMPLEMENTED <code>13</code> INTERNAL <code>14</code> UNAVAILABLE <code>15</code> DATA_LOSS <code>16</code> UNAUTHENTICATED"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#connect-rpc-conventions","title":"Connect RPC conventions","text":"<p>For remote procedure calls via connect, additional conventions are described in this section.</p> <p><code>rpc.system</code> MUST be set to <code>\"connect_rpc\"</code>.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/rpc-metrics/#connect-rpc-attributes","title":"Connect RPC Attributes","text":"<p>Below is a table of attributes that SHOULD be included on client and server RPC measurements when <code>rpc.system</code> is <code>\"connect_rpc\"</code>.</p> Attribute Type Description Examples Requirement Level <code>rpc.connect_rpc.error_code</code> string The error codes of the Connect request. Error codes are always string values. <code>cancelled</code> Conditionally Required: [1] <p>[1]: If response is not successful and if error code available.</p> <p><code>rpc.connect_rpc.error_code</code> MUST be one of the following:</p> Value Description <code>cancelled</code> cancelled <code>unknown</code> unknown <code>invalid_argument</code> invalid_argument <code>deadline_exceeded</code> deadline_exceeded <code>not_found</code> not_found <code>already_exists</code> already_exists <code>permission_denied</code> permission_denied <code>resource_exhausted</code> resource_exhausted <code>failed_precondition</code> failed_precondition <code>aborted</code> aborted <code>out_of_range</code> out_of_range <code>unimplemented</code> unimplemented <code>internal</code> internal <code>unavailable</code> unavailable <code>data_loss</code> data_loss <code>unauthenticated</code> unauthenticated"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/","title":"Runtime environment metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#_1","title":"\u8fd0\u884c\u65f6\u73af\u5883\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document includes semantic conventions for runtime environment level metrics in OpenTelemetry. Also consider the general metric, system metrics and OS Process metrics semantic conventions when instrumenting runtime environments.</p> <ul> <li>Metric Instruments</li> <li>Runtime Environment Specific Metrics - <code>process.runtime.{environment}.</code></li> <li>Attributes</li> <li>JVM Metrics</li> <li>Metric: <code>process.runtime.jvm.memory.usage</code></li> <li>Metric: <code>process.runtime.jvm.memory.init</code></li> <li>Metric: <code>process.runtime.jvm.memory.committed</code></li> <li>Metric: <code>process.runtime.jvm.memory.limit</code></li> <li>Metric: <code>process.runtime.jvm.memory.usage_after_last_gc</code></li> <li>Metric: <code>process.runtime.jvm.gc.duration</code></li> <li>Metric: <code>process.runtime.jvm.threads.count</code></li> <li>Metric: <code>process.runtime.jvm.classes.loaded</code></li> <li>Metric: <code>process.runtime.jvm.classes.unloaded</code></li> <li>Metric: <code>process.runtime.jvm.classes.current_loaded</code></li> <li>Metric: <code>process.runtime.jvm.cpu.utilization</code></li> <li>Metric: <code>process.runtime.jvm.system.cpu.utilization</code></li> <li>Metric: <code>process.runtime.jvm.system.cpu.load_1m</code></li> <li>Metric: <code>process.runtime.jvm.buffer.usage</code></li> <li>Metric: <code>process.runtime.jvm.buffer.limit</code></li> <li>Metric: <code>process.runtime.jvm.buffer.count</code></li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-instruments","title":"Metric Instruments","text":"<p>Runtime environments vary widely in their terminology, implementation, and relative values for a given metric. For example, Go and Python are both garbage collected languages, but comparing heap usage between the Go and CPython runtimes directly is not meaningful. For this reason, this document does not propose any standard top-level runtime metric instruments. See OTEP 108 for additional discussion.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#runtime-environment-specific-metrics-processruntimeenvironment","title":"Runtime Environment Specific Metrics - <code>process.runtime.{environment}.</code>","text":"<p>Metrics specific to a certain runtime environment should be prefixed with <code>process.runtime.{environment}.</code> and follow the semantic conventions outlined in general metric semantic conventions. Authors of runtime instrumentations are responsible for the choice of <code>{environment}</code> to avoid ambiguity when interpreting a metric's name or values.</p> <p>For example, some programming languages have multiple runtime environments that vary significantly in their implementation, like Python which has many implementations. For such languages, consider using specific <code>{environment}</code> prefixes to avoid ambiguity, like <code>process.runtime.cpython.</code> and <code>process.runtime.pypy.</code>.</p> <p>There are other dimensions even within a given runtime environment to consider, for example pthreads vs green thread implementations.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#attributes","title":"Attributes","text":"<p><code>process.runtime</code> resource attributes SHOULD be included on runtime metric events as appropriate.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#jvm-metrics","title":"JVM Metrics","text":"<p>Description: Java Virtual Machine (JVM) metrics captured under <code>process.runtime.jvm.</code></p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmmemoryusage","title":"Metric: <code>process.runtime.jvm.memory.usage</code>","text":"<p>This metric is recommended. This metric is obtained from <code>MemoryPoolMXBean#getUsage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.memory.usage</code> UpDownCounter <code>By</code> Measure of memory used. Attribute Type Description Examples Requirement Level <code>type</code> string The type of memory. <code>heap</code>; <code>non_heap</code> Recommended <code>pool</code> string Name of the memory pool. [1] <code>G1 Old Gen</code>; <code>G1 Eden space</code>; <code>G1 Survivor Space</code> Recommended <p>[1]: Pool names are generally obtained via MemoryPoolMXBean#getName().</p> <p><code>type</code> MUST be one of the following:</p> Value Description <code>heap</code> Heap memory. <code>non_heap</code> Non-heap memory"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmmemoryinit","title":"Metric: <code>process.runtime.jvm.memory.init</code>","text":"<p>This metric is recommended. This metric is obtained from <code>MemoryPoolMXBean#getUsage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.memory.init</code> UpDownCounter <code>By</code> Measure of initial memory requested. Attribute Type Description Examples Requirement Level <code>type</code> string The type of memory. <code>heap</code>; <code>non_heap</code> Recommended <code>pool</code> string Name of the memory pool. [1] <code>G1 Old Gen</code>; <code>G1 Eden space</code>; <code>G1 Survivor Space</code> Recommended <p>[1]: Pool names are generally obtained via MemoryPoolMXBean#getName().</p> <p><code>type</code> MUST be one of the following:</p> Value Description <code>heap</code> Heap memory. <code>non_heap</code> Non-heap memory"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmmemorycommitted","title":"Metric: <code>process.runtime.jvm.memory.committed</code>","text":"<p>This metric is recommended. This metric is obtained from <code>MemoryPoolMXBean#getUsage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.memory.committed</code> UpDownCounter <code>By</code> Measure of memory committed. Attribute Type Description Examples Requirement Level <code>type</code> string The type of memory. <code>heap</code>; <code>non_heap</code> Recommended <code>pool</code> string Name of the memory pool. [1] <code>G1 Old Gen</code>; <code>G1 Eden space</code>; <code>G1 Survivor Space</code> Recommended <p>[1]: Pool names are generally obtained via MemoryPoolMXBean#getName().</p> <p><code>type</code> MUST be one of the following:</p> Value Description <code>heap</code> Heap memory. <code>non_heap</code> Non-heap memory"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmmemorylimit","title":"Metric: <code>process.runtime.jvm.memory.limit</code>","text":"<p>This metric is recommended. This metric is obtained from <code>MemoryPoolMXBean#getUsage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.memory.limit</code> UpDownCounter <code>By</code> Measure of max obtainable memory. Attribute Type Description Examples Requirement Level <code>type</code> string The type of memory. <code>heap</code>; <code>non_heap</code> Recommended <code>pool</code> string Name of the memory pool. [1] <code>G1 Old Gen</code>; <code>G1 Eden space</code>; <code>G1 Survivor Space</code> Recommended <p>[1]: Pool names are generally obtained via MemoryPoolMXBean#getName().</p> <p><code>type</code> MUST be one of the following:</p> Value Description <code>heap</code> Heap memory. <code>non_heap</code> Non-heap memory"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmmemoryusage_after_last_gc","title":"Metric: <code>process.runtime.jvm.memory.usage_after_last_gc</code>","text":"<p>This metric is recommended. This metric is obtained from <code>MemoryPoolMXBean#getCollectionUsage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.memory.usage_after_last_gc</code> UpDownCounter <code>By</code> Measure of memory used, as measured after the most recent garbage collection event on this pool. Attribute Type Description Examples Requirement Level <code>type</code> string The type of memory. <code>heap</code>; <code>non_heap</code> Recommended <code>pool</code> string Name of the memory pool. [1] <code>G1 Old Gen</code>; <code>G1 Eden space</code>; <code>G1 Survivor Space</code> Recommended <p>[1]: Pool names are generally obtained via MemoryPoolMXBean#getName().</p> <p><code>type</code> MUST be one of the following:</p> Value Description <code>heap</code> Heap memory. <code>non_heap</code> Non-heap memory"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmgcduration","title":"Metric: <code>process.runtime.jvm.gc.duration</code>","text":"<p>This metric is recommended. This metric is obtained by subscribing to <code>GarbageCollectionNotificationInfo</code> events provided by <code>GarbageCollectorMXBean</code>. The duration value is obtained from <code>GcInfo</code></p> <p>This metric SHOULD be specified with <code>ExplicitBucketBoundaries</code> of <code>[]</code> (single bucket histogram capturing count, sum, min, max).</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.gc.duration</code> Histogram <code>s</code> Duration of JVM garbage collection actions. Attribute Type Description Examples Requirement Level <code>gc</code> string Name of the garbage collector. [1] <code>G1 Young Generation</code>; <code>G1 Old Generation</code> Recommended <code>action</code> string Name of the garbage collector action. [2] <code>end of minor GC</code>; <code>end of major GC</code> Recommended <p>[1]: Garbage collector name is generally obtained via GarbageCollectionNotificationInfo#getGcName().</p> <p>[2]: Garbage collector action is generally obtained via GarbageCollectionNotificationInfo#getGcAction().</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmthreadscount","title":"Metric: <code>process.runtime.jvm.threads.count</code>","text":"<p>This metric is recommended. This metric is obtained from <code>ThreadMXBean#getDaemonThreadCount()</code> and <code>ThreadMXBean#getThreadCount()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.threads.count</code> UpDownCounter <code>{thread}</code> Number of executing threads. Attribute Type Description Examples Requirement Level <code>daemon</code> boolean Whether the thread is daemon or not. Recommended"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmclassesloaded","title":"Metric: <code>process.runtime.jvm.classes.loaded</code>","text":"<p>This metric is recommended. This metric is obtained from <code>ClassLoadingMXBean#getTotalLoadedClassCount()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.classes.loaded</code> Counter <code>{class}</code> Number of classes loaded since JVM start."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmclassesunloaded","title":"Metric: <code>process.runtime.jvm.classes.unloaded</code>","text":"<p>This metric is recommended. This metric is obtained from <code>ClassLoadingMXBean#getUnloadedClassCount()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.classes.unloaded</code> Counter <code>{class}</code> Number of classes unloaded since JVM start."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmclassescurrent_loaded","title":"Metric: <code>process.runtime.jvm.classes.current_loaded</code>","text":"<p>This metric is recommended. This metric is obtained from <code>ClassLoadingMXBean#getLoadedClassCount()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.classes.current_loaded</code> UpDownCounter <code>{class}</code> Number of classes currently loaded."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmcpuutilization","title":"Metric: <code>process.runtime.jvm.cpu.utilization</code>","text":"<p>This metric is recommended. This metric is obtained from <code>com.sun.management.OperatingSystemMXBean#getProcessCpuLoad()</code> on HotSpot and <code>com.ibm.lang.management.OperatingSystemMXBean#getProcessCpuLoad()</code> on J9.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.cpu.utilization</code> Gauge <code>1</code> Recent CPU utilization for the process."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmsystemcpuutilization","title":"Metric: <code>process.runtime.jvm.system.cpu.utilization</code>","text":"<p>This metric is recommended. This metric is obtained from <code>com.sun.management.OperatingSystemMXBean#getSystemCpuLoad()</code> on Java version 8..13, <code>com.sun.management.OperatingSystemMXBean#getCpuLoad()</code> on Java version 14+, and <code>com.ibm.lang.management.OperatingSystemMXBean#getSystemCpuLoad()</code> on J9.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.system.cpu.utilization</code> Gauge <code>1</code> Recent CPU utilization for the whole system."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmsystemcpuload_1m","title":"Metric: <code>process.runtime.jvm.system.cpu.load_1m</code>","text":"<p>This metric is recommended. This metric is obtained from <code>OperatingSystemMXBean#getSystemLoadAverage()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.system.cpu.load_1m</code> Gauge <code>1</code> Average CPU load of the whole system for the last minute."},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmbufferusage","title":"Metric: <code>process.runtime.jvm.buffer.usage</code>","text":"<p>This metric is recommended. This metric is obtained from <code>BufferPoolMXBean#getMemoryUsed()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.buffer.usage</code> UpDownCounter <code>By</code> Measure of memory used by buffers. Attribute Type Description Examples Requirement Level <code>pool</code> string Name of the buffer pool. [1] <code>mapped</code>; <code>direct</code> Recommended <p>[1]: Pool names are generally obtained via BufferPoolMXBean#getName().</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmbufferlimit","title":"Metric: <code>process.runtime.jvm.buffer.limit</code>","text":"<p>This metric is recommended. This metric is obtained from <code>BufferPoolMXBean#getTotalCapacity()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.buffer.limit</code> UpDownCounter <code>By</code> Measure of total memory capacity of buffers. Attribute Type Description Examples Requirement Level <code>pool</code> string Name of the buffer pool. [1] <code>mapped</code>; <code>direct</code> Recommended <p>[1]: Pool names are generally obtained via BufferPoolMXBean#getName().</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/runtime-environment-metrics/#metric-processruntimejvmbuffercount","title":"Metric: <code>process.runtime.jvm.buffer.count</code>","text":"<p>This metric is recommended. This metric is obtained from <code>BufferPoolMXBean#getCount()</code>.</p> Name Instrument Type Unit (UCUM) Description <code>process.runtime.jvm.buffer.count</code> UpDownCounter <code>{buffer}</code> Number of buffers in the pool. Attribute Type Description Examples Requirement Level <code>pool</code> string Name of the buffer pool. [1] <code>mapped</code>; <code>direct</code> Recommended <p>[1]: Pool names are generally obtained via BufferPoolMXBean#getName().</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/","title":"System metrics","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#_1","title":"\u7cfb\u7edf\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document describes instruments and attributes for common system level metrics in OpenTelemetry. Consider the general metric semantic conventions when creating instruments not explicitly defined in the specification.</p> <ul> <li>\u7cfb\u7edf\u5ea6\u91cf\u7684\u8bed\u4e49\u7ea6\u5b9a</li> <li>Metric Instruments<ul> <li><code>system.cpu.</code> - Processor metrics</li> <li><code>system.memory.</code> - Memory metrics</li> <li><code>system.paging.</code> - Paging/swap metrics</li> <li><code>system.disk.</code> - Disk controller metrics</li> <li><code>system.filesystem.</code> - Filesystem metrics</li> <li><code>system.network.</code> - Network metrics</li> <li><code>system.processes.</code> - Aggregate system process metrics</li> <li><code>system.{os}.</code> - OS Specific System Metrics</li> </ul> </li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#metric-instruments","title":"Metric Instruments","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemcpu-processor-metrics","title":"<code>system.cpu.</code> - Processor metrics","text":"<p>Description: System level processor metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key(s) Attribute Values system.cpu.time s Counter Double state idle, user, system, interrupt, etc. cpu CPU number [0..n-1] system.cpu.utilization Difference in system.cpu.time since the last measurement, divided by the elapsed time and number of CPUs 1 Gauge Double state idle, user, system, interrupt, etc. cpu CPU number (0..n)"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemmemory-memory-metrics","title":"<code>system.memory.</code> - Memory metrics","text":"<p>Description: System level memory metrics. This does not include paging/swap memory.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.memory.usage By UpDownCounter Int64 state used, free, cached, etc. system.memory.utilization 1 Gauge Double state used, free, cached, etc."},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systempaging-pagingswap-metrics","title":"<code>system.paging.</code> - Paging/swap metrics","text":"<p>Description: System level paging/swap memory metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.paging.usage Unix swap or windows pagefile usage By UpDownCounter Int64 state used, free system.paging.utilization 1 Gauge Double state used, free system.paging.faults {fault} Counter Int64 type major, minor system.paging.operations {operation} Counter Int64 type major, minor direction in, out"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemdisk-disk-controller-metrics","title":"<code>system.disk.</code> - Disk controller metrics","text":"<p>Description: System level disk performance metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.disk.io By Counter Int64 device (identifier) direction read, write system.disk.operations {operation} Counter Int64 device (identifier) direction read, write system.disk.io_time[1] Time disk spent activated s Counter Double device (identifier) system.disk.operation_time[2] Sum of the time each operation took to complete s Counter Double device (identifier) direction read, write system.disk.merged {operation} Counter Int64 device (identifier) direction read, write <p>1 The real elapsed time (\"wall clock\") used in the I/O path (time from operations running in parallel are not counted). Measured as:</p> <ul> <li>Linux: Field 13 from   procfs-diskstats</li> <li>Windows: The complement of   \"Disk\\% Idle Time\"   performance counter: <code>uptime * (100 - \"Disk\\% Idle Time\") / 100</code></li> </ul> <p>2 Because it is the sum of time each request took, parallel-issued requests each contribute to make the count grow. Measured as:</p> <ul> <li>Linux: Fields 7 &amp; 11 from   procfs-diskstats</li> <li>Windows: \"Avg. Disk sec/Read\" perf counter multiplied by \"Disk Reads/sec\" perf   counter (similar for Writes)</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemfilesystem-filesystem-metrics","title":"<code>system.filesystem.</code> - Filesystem metrics","text":"<p>Description: System level filesystem metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.filesystem.usage By UpDownCounter Int64 device (identifier) state used, free, reserved type ext4, tmpfs, etc. mode rw, ro, etc. mountpoint (path) system.filesystem.utilization 1 Gauge Double device (identifier) state used, free, reserved type ext4, tmpfs, etc. mode rw, ro, etc. mountpoint (path)"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemnetwork-network-metrics","title":"<code>system.network.</code> - Network metrics","text":"<p>Description: System level network metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.network.dropped[1] Count of packets that are dropped or discarded even though there was no error {packet} Counter Int64 device (identifier) direction transmit, receive system.network.packets {packet} Counter Int64 device (identifier) direction transmit, receive system.network.errors[2] Count of network errors detected {error} Counter Int64 device (identifier) direction transmit, receive system.network.io By Counter Int64 device (identifier) direction transmit, receive system.network.connections {connection} UpDownCounter Int64 device (identifier) protocol tcp, udp, etc. state If specified, SHOULD be one of: close, close_wait, closing, delete, established, fin_wait_1, fin_wait_2, last_ack, listen, syn_recv, syn_sent, time_wait. A stateless protocol MUST NOT set this attribute. <p>1 Measured as:</p> <ul> <li>Linux: the <code>drop</code> column in <code>/proc/dev/net</code>   (source).</li> <li>Windows:   <code>InDiscards</code>/<code>OutDiscards</code>   from   <code>GetIfEntry2</code>.</li> </ul> <p>2 Measured as:</p> <ul> <li>Linux: the <code>errs</code> column in <code>/proc/dev/net</code>   (source).</li> <li>Windows:   <code>InErrors</code>/<code>OutErrors</code>   from   <code>GetIfEntry2</code>.</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemprocesses-aggregate-system-process-metrics","title":"<code>system.processes.</code> - Aggregate system process metrics","text":"<p>Description: System level aggregate process metrics. For metrics at the individual process level, see process metrics.</p> Name Description Units Instrument Type (*) Value Type Attribute Key Attribute Values system.processes.count Total number of processes in each state {process} UpDownCounter Int64 status running, sleeping, etc. system.processes.created Total number of processes created over uptime of the host {process} Counter Int64 - -"},{"location":"docs/specs/otel/metrics/semantic_conventions/system-metrics/#systemos-os-specific-system-metrics","title":"<code>system.{os}.</code> - OS Specific System Metrics","text":"<p>Instrument names for system level metrics that have different and conflicting meaning across multiple OSes should be prefixed with <code>system.{os}.</code> and follow the hierarchies listed above for different entities like CPU, memory, and network.</p> <p>For example, UNIX load average over a given interval is not well standardized and its value across different UNIX like OSes may vary despite being under similar load:</p> <p>Without getting into the vagaries of every Unix-like operating system in existence, the load average more or less represents the average number of processes that are in the running (using the CPU) or runnable (waiting for the CPU) states. One notable exception exists: Linux includes processes in uninterruptible sleep states, typically waiting for some I/O activity to complete. This can markedly increase the load average on Linux systems.</p> <p>(source of quote, linux source code)</p> <p>An instrument for load average over 1 minute on Linux could be named <code>system.linux.cpu.load_1m</code>, reusing the <code>cpu</code> name proposed above and having an <code>{os}</code> prefix to split this metric across OSes.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/","title":"Instrumentation","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p>"},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/kafka/","title":"Kafka","text":""},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/kafka/#instrumenting-kafka","title":"Instrumenting Kafka","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines how to apply semantic conventions when instrumenting Kafka.</p> <ul> <li>Kafka Metrics</li> <li>Kafka Producer Metrics</li> <li>Kafka Consumer Metrics</li> </ul>"},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/kafka/#kafka-metrics","title":"Kafka Metrics","text":"<p>Description: General Kafka metrics.</p> Name Instrument Value type Unit Unit (UCUM) Description Attribute Key Attribute Values messaging.kafka.messages Counter Int64 messages <code>{message}</code> The number of messages received by the broker. messaging.kafka.requests.failed Counter Int64 requests <code>{request}</code> The number of requests to the broker resulting in a failure. <code>type</code> <code>produce</code>, <code>fetch</code> messaging.kafka.requests.queue UpDownCounter Int64 requests <code>{request}</code> The number of requests in the request queue. messaging.kafka.network.io Counter Int64 bytes <code>By</code> The bytes received or sent by the broker. <code>state</code> <code>in</code>, <code>out</code> messaging.kafka.purgatory.size UpDownCounter Int64 requests <code>{request}</code> The number of requests waiting in the purgatory. <code>type</code> <code>produce</code>, <code>fetch</code> messaging.kafka.partitions.all UpDownCounter Int64 partitions <code>{partition}</code> The number of partitions in the broker. messaging.kafka.partitions.offline UpDownCounter Int64 partitions <code>{partition}</code> The number of offline partitions. messaging.kafka.partitions.under-replicated UpDownCounter Int64 partition <code>{partition}</code> The number of under replicated partitions. messaging.kafka.isr.operations Counter Int64 operations <code>{operation}</code> The number of in-sync replica shrink and expand operations. <code>operation</code> <code>shrink</code>, <code>expand</code> messaging.kafka.lag_max Gauge Int64 lag max <code>{message}</code> Max lag in messages between follower and leader replicas. messaging.kafka.controllers.active UpDownCounter Int64 controllers <code>{controller}</code> The number of active controllers in the broker. messaging.kafka.leader.elections Counter Int64 elections <code>{election}</code> Leader election rate (increasing values indicates broker failures). messaging.kafka.leader.unclean-elections Counter Int64 elections <code>{election}</code> Unclean leader election rate (increasing values indicates broker failures). messaging.kafka.brokers UpDownCounter Int64 brokers <code>{broker}</code> Number of brokers in the cluster. messaging.kafka.topic.partitions UpDownCounter Int64 partitions <code>{partition}</code> Number of partitions in topic. <code>topic</code> The ID (integer) of a topic messaging.kafka.partition.current_offset Gauge Int64 partition offset <code>{partition offset}</code> Current offset of partition of topic. <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition messaging.kafka.partition.oldest_offset Gauge Int64 partition offset <code>{partition offset}</code> Oldest offset of partition of topic <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition messaging.kafka.partition.replicas.all UpDownCounter Int64 replicas <code>{replica}</code> Number of replicas for partition of topic <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition messaging.kafka.partition.replicas.in_sync UpDownCounter Int64 replicas <code>{replica}</code> Number of synchronized replicas of partition <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition"},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/kafka/#kafka-producer-metrics","title":"Kafka Producer Metrics","text":"<p>Description: Kafka Producer level metrics.</p> Name Instrument Value type Unit Unit (UCUM) Description Attribute Key Attribute Values messaging.kafka.producer.outgoing-bytes.rate Gauge Double bytes per second <code>By/s</code> The average number of outgoing bytes sent per second to all servers. <code>client-id</code> <code>client-id</code> value messaging.kafka.producer.responses.rate Gauge Double responses per second <code>{response}/s</code> The average number of responses received per second. <code>client-id</code> <code>client-id</code> value messaging.kafka.producer.bytes.rate Gauge Double bytes per second <code>By/s</code> The average number of bytes sent per second for a specific topic. <code>client-id</code> <code>client-id</code> value <code>topic</code> topic name messaging.kafka.producer.compression-ratio Gauge Double compression ratio <code>{compression}</code> The average compression ratio of record batches for a specific topic. <code>client-id</code> <code>client-id</code> value <code>topic</code> topic name messaging.kafka.producer.record-error.rate Gauge Double error rate <code>{error}/s</code> The average per-second number of record sends that resulted in errors for a specific topic. <code>client-id</code> <code>client-id</code> value <code>topic</code> topic name messaging.kafka.producer.record-retry.rate Gauge Double retry rate <code>{retry}/s</code> The average per-second number of retried record sends for a specific topic. <code>client-id</code> <code>client-id</code> value <code>topic</code> topic name messaging.kafka.producer.record-sent.rate Gauge Double records sent rate <code>{record_sent}/s</code> The average number of records sent per second for a specific topic. <code>client-id</code> <code>client-id</code> value <code>topic</code> topic name"},{"location":"docs/specs/otel/metrics/semantic_conventions/instrumentation/kafka/#kafka-consumer-metrics","title":"Kafka Consumer Metrics","text":"<p>Description: Kafka Consumer level metrics.</p> Name Instrument Value type Unit Unit (UCUM) Description Attribute Key Attribute Values messaging.kafka.consumer.members UpDownCounter Int64 members <code>{member}</code> Count of members in the consumer group <code>group</code> The ID (string) of a consumer group messaging.kafka.consumer.offset Gauge Int64 offset <code>{offset}</code> Current offset of the consumer group at partition of topic <code>group</code> The ID (string) of a consumer group <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition messaging.kafka.consumer.offset_sum Gauge Int64 offset sum <code>{offset sum}</code> Sum of consumer group offset across partitions of topic <code>group</code> The ID (string) of a consumer group <code>topic</code> The ID (integer) of a topic messaging.kafka.consumer.lag Gauge Int64 lag <code>{lag}</code> Current approximate lag of consumer group at partition of topic <code>group</code> The ID (string) of a consumer group <code>topic</code> The ID (integer) of a topic <code>partition</code> The number (integer) of the partition messaging.kafka.consumer.lag_sum Gauge Int64 lag sum <code>{lag sum}</code> Current approximate sum of consumer group lag across all partitions of topic <code>group</code> The ID (string) of a consumer group <code>topic</code> The ID (integer) of a topic"},{"location":"docs/specs/otel/protocol/","title":"OpenTelemetry \u534f\u8bae","text":"<p>\u8fd9\u662f\u65b0\u7684 OpenTelemetry \u534f\u8bae(OTLP)\u7684\u89c4\u8303\u3002</p> <ul> <li>\u8bbe\u8ba1\u76ee\u6807.</li> <li>\u9700\u6c42.</li> <li>\u89c4\u8303.</li> <li>SDK \u5bfc\u51fa\u5668.</li> </ul>"},{"location":"docs/specs/otel/protocol/design-goals/","title":"OpenTelemetry \u6709\u7ebf\u534f\u8bae\u7684\u8bbe\u8ba1\u76ee\u6807","text":"<p>We want to design a telemetry data exchange protocol that has the following characteristics:</p> <ul> <li> <p>Be suitable for use between all of the following node types: instrumented   applications, telemetry backends, local agents, stand-alone   collectors/forwarders.</p> </li> <li> <p>Have high reliability of data delivery and clear visibility when the data   cannot be delivered.</p> </li> <li> <p>Have low CPU usage for serialization and deserialization.</p> </li> <li> <p>Impose minimal pressure on memory manager, including pass-through scenarios,   where deserialized data is short-lived and must be serialized as-is shortly   after and where such short-lived data is created and discarded at high   frequency (think telemetry data forwarders).</p> </li> <li> <p>Support ability to efficiently modify deserialized data and serialize again to   pass further. This is related but slightly different from the previous   requirement.</p> </li> <li> <p>Ensure high throughput (within the available bandwidth) in high latency   networks (e.g. scenarios where telemetry source and the backend are separated   by high latency network).</p> </li> <li> <p>Allow backpressure signalling.</p> </li> <li> <p>Be load-balancer friendly (do not hinder re-balancing).</p> </li> </ul>"},{"location":"docs/specs/otel/protocol/exporter/","title":"OpenTelemetry \u534f\u8bae\u5bfc\u51fa\u5668","text":"<p>Status: Stable</p> <p>This document specifies the configuration options available to the OpenTelemetry Protocol (OTLP) Exporter as well as the retry behavior.</p>"},{"location":"docs/specs/otel/protocol/exporter/#configuration-options","title":"Configuration Options","text":"<p>The following configuration options MUST be available to configure the OTLP exporter. Each configuration option MUST be overridable by a signal specific option.</p> <ul> <li> <p>Endpoint (OTLP/HTTP): Target URL to which the exporter is going to send   spans or metrics. The endpoint MUST be a valid URL with scheme (http or https)   and host, MAY contain a port, SHOULD contain a path and MUST NOT contain other   parts (such as query string or fragment). A scheme of https indicates a secure   connection. When using <code>OTEL_EXPORTER_OTLP_ENDPOINT</code>, exporters MUST construct   per-signal URLs as described below. The   per-signal endpoint configuration options take precedence and can be used to   override this behavior (the URL is used as-is for them, without any   modifications). See the OTLP Specification for more details.</p> </li> <li> <p>Default: <code>http://localhost:4318</code> [1]</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_METRICS_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_LOGS_ENDPOINT</code></p> </li> <li> <p>Endpoint (OTLP/gRPC): Target to which the exporter is going to send spans   or metrics. The endpoint SHOULD accept any form allowed by the underlying gRPC   client implementation. Additionally, the endpoint MUST accept a URL with a   scheme of either <code>http</code> or <code>https</code>. A scheme of <code>https</code> indicates a secure   connection and takes precedence over the <code>insecure</code> configuration setting. A   scheme of <code>http</code> indicates an insecure connection and takes precedence over   the <code>insecure</code> configuration setting. If the gRPC client implementation does   not support an endpoint with a scheme of <code>http</code> or <code>https</code> then the endpoint   SHOULD be transformed to the most sensible format for that implementation.</p> </li> <li> <p>Default: <code>http://localhost:4317</code> [1]</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_TRACES_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_METRICS_ENDPOINT</code> <code>OTEL_EXPORTER_OTLP_LOGS_ENDPOINT</code></p> </li> <li> <p>Insecure: Whether to enable client transport security for the exporter's   gRPC connection. This option only applies to OTLP/gRPC when an endpoint is   provided without the <code>http</code> or <code>https</code> scheme - OTLP/HTTP always uses the   scheme provided for the <code>endpoint</code>. Implementations MAY choose to not   implement the <code>insecure</code> option if it is not required or supported by the   underlying gRPC client implementation.</p> </li> <li> <p>Default: <code>false</code></p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_INSECURE</code> <code>OTEL_EXPORTER_OTLP_TRACES_INSECURE</code> <code>OTEL_EXPORTER_OTLP_METRICS_INSECURE</code> <code>OTEL_EXPORTER_OTLP_LOGS_INSECURE</code> <code>OTEL_EXPORTER_OTLP_SPAN_INSECURE</code> <code>OTEL_EXPORTER_OTLP_METRIC_INSECURE</code> [2]</p> </li> <li> <p>Certificate File: The trusted certificate to use when verifying a server's   TLS credentials. Should only be used for a secure connection.</p> </li> <li> <p>Default: n/a</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_TRACES_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_METRICS_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_LOGS_CERTIFICATE</code></p> </li> <li> <p>Client key file: Clients private key to use in mTLS communication in PEM   format.</p> </li> <li> <p>Default: n/a</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_CLIENT_KEY</code> <code>OTEL_EXPORTER_OTLP_TRACES_CLIENT_KEY</code> <code>OTEL_EXPORTER_OTLP_METRICS_CLIENT_KEY</code> <code>OTEL_EXPORTER_OTLP_LOGS_CLIENT_KEY</code></p> </li> <li> <p>Client certificate file: Client certificate/chain trust for clients   private key to use in mTLS communication in PEM format.</p> </li> <li> <p>Default: n/a</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_CLIENT_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_TRACES_CLIENT_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_METRICS_CLIENT_CERTIFICATE</code> <code>OTEL_EXPORTER_OTLP_LOGS_CLIENT_CERTIFICATE</code></p> </li> <li> <p>Headers: Key-value pairs to be used as headers associated with gRPC or   HTTP requests. See   Specifying headers   for more details.</p> </li> <li> <p>Default: n/a</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_HEADERS</code> <code>OTEL_EXPORTER_OTLP_TRACES_HEADERS</code> <code>OTEL_EXPORTER_OTLP_METRICS_HEADERS</code> <code>OTEL_EXPORTER_OTLP_LOGS_HEADERS</code></p> </li> <li> <p>Compression: Compression key for supported compression types. Supported   compression: <code>gzip</code>.</p> </li> <li> <p>Default: No value [2]</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_COMPRESSION</code> <code>OTEL_EXPORTER_OTLP_TRACES_COMPRESSION</code> <code>OTEL_EXPORTER_OTLP_METRICS_COMPRESSION</code> <code>OTEL_EXPORTER_OTLP_LOGS_COMPRESSION</code></p> </li> <li> <p>Timeout: Maximum time the OTLP exporter will wait for each batch export.</p> </li> <li> <p>Default: 10s</p> </li> <li> <p>Env vars: <code>OTEL_EXPORTER_OTLP_TIMEOUT</code> <code>OTEL_EXPORTER_OTLP_TRACES_TIMEOUT</code> <code>OTEL_EXPORTER_OTLP_METRICS_TIMEOUT</code> <code>OTEL_EXPORTER_OTLP_LOGS_TIMEOUT</code></p> </li> <li> <p>Protocol: The transport protocol. Options MUST be one of: <code>grpc</code>,   <code>http/protobuf</code>, <code>http/json</code>. See   Specify Protocol for more details.</p> </li> <li>Default: <code>http/protobuf</code> [3]</li> <li>Env vars: <code>OTEL_EXPORTER_OTLP_PROTOCOL</code> <code>OTEL_EXPORTER_OTLP_TRACES_PROTOCOL</code> <code>OTEL_EXPORTER_OTLP_METRICS_PROTOCOL</code> <code>OTEL_EXPORTER_OTLP_LOGS_PROTOCOL</code></li> </ul> <p>[1]: SDKs SHOULD default endpoint variables to use <code>http</code> scheme unless they have good reasons to choose <code>https</code> scheme for the default (e.g., for backward compatibility reasons in a stable SDK release).</p> <p>[2]: The environment variables <code>OTEL_EXPORTER_OTLP_SPAN_INSECURE</code> and <code>OTEL_EXPORTER_OTLP_METRIC_INSECURE</code> are obsolete because they do not follow the common naming scheme of the other environment variables. They are still supported because they were part of a stable release of the specification.</p> <p>Use <code>OTEL_EXPORTER_OTLP_TRACES_INSECURE</code> instead of <code>OTEL_EXPORTER_OTLP_SPAN_INSECURE</code> and <code>OTEL_EXPORTER_OTLP_METRICS_INSECURE</code> instead of <code>OTEL_EXPORTER_OTLP_METRIC_INSECURE</code>.</p> <p>[3]: If no compression value is explicitly specified, SIGs can default to the value they deem most useful among the supported options. This is especially important in the presence of technical constraints, e.g. directly sending telemetry data from mobile devices to backend servers.</p> <p>Supported values for <code>OTEL_EXPORTER_OTLP_*COMPRESSION</code> options:</p> <ul> <li><code>none</code> if compression is disabled.</li> <li><code>gzip</code> is the only specified compression method for now.</li> </ul>"},{"location":"docs/specs/otel/protocol/exporter/#endpoint-urls-for-otlphttp","title":"Endpoint URLs for OTLP/HTTP","text":"<p>Based on the environment variables above, the OTLP/HTTP exporter MUST construct URLs for each signal as follow:</p> <ol> <li>For the per-signal variables (<code>OTEL_EXPORTER_OTLP_&lt;signal&gt;_ENDPOINT</code>), the    URL MUST be used as-is without any modification. The only exception is that    if an URL contains no path part, the root path <code>/</code> MUST be used (see    Example 2).</li> <li> <p>If signals are sent that have no per-signal configuration from the previous    point, <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> is used as a base URL and the signals    are sent to these paths relative to that:</p> </li> <li> <p>Traces: <code>v1/traces</code></p> </li> <li>Metrics: <code>v1/metrics</code>.</li> <li>Logs: <code>v1/logs</code>.</li> </ol> <p>Non-normatively, this could be implemented by ensuring that the base URL ends    with a slash and then appending the relative URLs as strings.</p> <p>An SDK MUST NOT modify the URL in ways other than specified above. That also means, if the port is empty or not given, TCP port 80 is the default for the <code>http</code> scheme and TCP port 443 is the default for the <code>https</code> scheme, as per the usual rules for these schemes (RFC 7230).</p>"},{"location":"docs/specs/otel/protocol/exporter/#example-1","title":"Example 1","text":"<p>The following configuration sends all signals to the same collector:</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318\n</code></pre> <p>Traces are sent to <code>http://collector:4318/v1/traces</code>, metrics to <code>http://collector:4318/v1/metrics</code> and logs to <code>http://collector:4318/v1/logs</code>.</p>"},{"location":"docs/specs/otel/protocol/exporter/#example-2","title":"Example 2","text":"<p>Traces and metrics are sent to different collectors and paths:</p> <pre><code>export OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://collector:4318\nexport OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=https://collector.example.com/v1/metrics\n</code></pre> <p>This will send traces directly to the root path <code>http://collector:4318/</code> (<code>/v1/traces</code> is only automatically added when using the non-signal-specific environment variable) and metrics to <code>https://collector.example.com/v1/metrics</code>, using the default https port (443).</p>"},{"location":"docs/specs/otel/protocol/exporter/#example-3","title":"Example 3","text":"<p>The following configuration sends all signals except for metrics to the same collector:</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4318/mycollector/\nexport OTEL_EXPORTER_OTLP_METRICS_ENDPOINT=https://collector.example.com/v1/metrics/\n</code></pre> <p>Traces are sent to <code>http://collector:4318/mycollector/v1/traces</code>, logs to <code>http://collector:4318/mycollector/v1/logs</code> and metrics to <code>https://collector.example.com/v1/metrics/</code>, using the default https port (443). Other signals, (if there were any) would be sent to their specific paths relative to <code>http://collector:4318/mycollector/</code>.</p>"},{"location":"docs/specs/otel/protocol/exporter/#specify-protocol","title":"Specify Protocol","text":"<p>The <code>OTEL_EXPORTER_OTLP_PROTOCOL</code>, <code>OTEL_EXPORTER_OTLP_TRACES_PROTOCOL</code>, and <code>OTEL_EXPORTER_OTLP_METRICS_PROTOCOL</code> environment variables specify the OTLP transport protocol. Supported values:</p> <ul> <li><code>grpc</code> for protobuf-encoded data using gRPC wire format over HTTP/2 connection</li> <li><code>http/protobuf</code> for protobuf-encoded data over HTTP connection</li> <li><code>http/json</code> for JSON-encoded data over HTTP connection</li> </ul> <p>[2]: SDKs SHOULD support both <code>grpc</code> and <code>http/protobuf</code> transports and MUST support at least one of them. If they support only one, it SHOULD be <code>http/protobuf</code>. They also MAY support <code>http/json</code>.</p> <p>If no configuration is provided the default transport SHOULD be <code>http/protobuf</code> unless SDKs have good reasons to choose <code>grpc</code> as the default (e.g. for backward compatibility reasons when <code>grpc</code> was already the default in a stable SDK release).</p>"},{"location":"docs/specs/otel/protocol/exporter/#specifying-headers-via-environment-variables","title":"Specifying headers via environment variables","text":"<p>The <code>OTEL_EXPORTER_OTLP_HEADERS</code>, <code>OTEL_EXPORTER_OTLP_TRACES_HEADERS</code>, <code>OTEL_EXPORTER_OTLP_METRICS_HEADERS</code> environment variables will contain a list of key value pairs, and these are expected to be represented in a format matching to the W3C Correlation-Context, except that additional semi-colon delimited metadata is not supported, i.e.: key1=value1,key2=value2. All attribute values MUST be considered strings.</p>"},{"location":"docs/specs/otel/protocol/exporter/#retry","title":"Retry","text":"<p>Transient errors MUST be handled with a retry strategy. This retry strategy MUST implement an exponential back-off with jitter to avoid overwhelming the destination until the network is restored or the destination has recovered.</p> <p>For OTLP/HTTP, the errors <code>408 (Request Timeout)</code> and <code>5xx (Server Errors)</code> are defined as transient, detailed information about errors can be found in the HTTP failures section. For the OTLP/gRPC, the full list of the gRPC retryable status codes can be found in the gRPC response section.</p>"},{"location":"docs/specs/otel/protocol/exporter/#user-agent","title":"User Agent","text":"<p>OpenTelemetry protocol exporters SHOULD emit a User-Agent header to at a minimum identify the exporter, the language of its implementation, and the version of the exporter. For example, the Python OTLP exporter version 1.2.3 would report the following:</p> <pre><code>OTel-OTLP-Exporter-Python/1.2.3\n</code></pre> <p>The format of the header SHOULD follow RFC 7231. The conventions used for specifying the OpenTelemetry SDK language and version are available in the Resource semantic conventions.</p>"},{"location":"docs/specs/otel/protocol/file-exporter/","title":"OpenTelemetry \u534f\u8bae\u6587\u4ef6\u5bfc\u51fa\u5668","text":"<p>Status: Experimental</p> <p>This document provides a placeholder for specifying an OTLP exporter capable of exporting to either a file or stdout.</p> <p>Currently, it only describes the serialization of OpenTelemetry data to the OTLP JSON format.</p>"},{"location":"docs/specs/otel/protocol/file-exporter/#table-of-contents","title":"Table of Contents","text":"<ul> <li>JSON File serialization</li> <li>File storage requirements<ul> <li>JSON lines file</li> <li>Streaming appending</li> </ul> </li> <li>Telemetry data requirements</li> <li>Examples</li> </ul>"},{"location":"docs/specs/otel/protocol/file-exporter/#json-file-serialization","title":"JSON File serialization","text":"<p>This document describes the serialization of OpenTelemetry data as JSON objects that can be stored in files.</p>"},{"location":"docs/specs/otel/protocol/file-exporter/#file-storage-requirements","title":"File storage requirements","text":""},{"location":"docs/specs/otel/protocol/file-exporter/#json-lines-file","title":"JSON lines file","text":"<p>This file is a JSON lines file (jsonlines.org), and therefore follows those requirements:</p> <ul> <li>UTF-8 encoding</li> <li>Each line is a valid JSON value</li> <li>The line separator is <code>\\n</code></li> <li>The preferred file extension is <code>jsonl</code>.</li> </ul>"},{"location":"docs/specs/otel/protocol/file-exporter/#streaming-appending","title":"Streaming appending","text":"<p>There is no guarantee that the data in the file is ordered.</p> <p>There is no guarantee in particular that timestamps will be monotonically increasing.</p>"},{"location":"docs/specs/otel/protocol/file-exporter/#telemetry-data-requirements","title":"Telemetry data requirements","text":"<p>This defines the first version of the serialization scheme.</p> <p>The data must be encoded according to the format specified in the OTLP JSON Encoding.</p> <p>Only top-level objects, <code>ExportTraceServiceRequest</code>, <code>ExportMetricsServiceRequest</code>, and <code>ExportLogsServiceRequest</code> are supported.</p> <p>Files must contain exactly one type of data: traces, metrics, or logs.</p>"},{"location":"docs/specs/otel/protocol/file-exporter/#examples","title":"Examples","text":"<p>This is an example showing traces:</p> <p>```json lines {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeSpans\":[{\"scope\":{},\"spans\":[{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationA\",\"startTimeUnixNano\":\"1581452772000000321\",\"endTimeUnixNano\":\"1581452773000000789\",\"droppedAttributesCount\":1,\"events\":[{\"timeUnixNano\":\"1581452773000000123\",\"name\":\"event-with-attr\",\"attributes\":[{\"key\":\"span-event-attr\",\"value\":{\"stringValue\":\"span-event-attr-val\"}}],\"droppedAttributesCount\":2},{\"timeUnixNano\":\"1581452773000000123\",\"name\":\"event\",\"droppedAttributesCount\":2}],\"droppedEventsCount\":1,\"status\":{\"message\":\"status-cancelled\",\"code\":2}},{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationB\",\"startTimeUnixNano\":\"1581452772000000321\",\"endTimeUnixNano\":\"1581452773000000789\",\"links\":[{\"traceId\":\"\",\"spanId\":\"\",\"attributes\":[{\"key\":\"span-link-attr\",\"value\":{\"stringValue\":\"span-link-attr-val\"}}],\"droppedAttributesCount\":4},{\"traceId\":\"\",\"spanId\":\"\",\"droppedAttributesCount\":1}],\"droppedLinksCount\":3,\"status\":{}}]}]}]} {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeSpans\":[{\"scope\":{},\"spans\":[{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationA\",\"startTimeUnixNano\":\"1581452772000000321\",\"endTimeUnixNano\":\"1581452773000000789\",\"droppedAttributesCount\":1,\"events\":[{\"timeUnixNano\":\"1581452773000000424\",\"name\":\"event-with-attr\",\"attributes\":[{\"key\":\"span-event-attr\",\"value\":{\"stringValue\":\"span-event-attr-val\"}}],\"droppedAttributesCount\":2},{\"timeUnixNano\":\"1581452773000000424\",\"name\":\"event\",\"droppedAttributesCount\":2}],\"droppedEventsCount\":1,\"status\":{\"message\":\"status-cancelled\",\"code\":2}},{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationB\",\"startTimeUnixNano\":\"1581452772000000343\",\"endTimeUnixNano\":\"1581452773000001089\",\"links\":[{\"traceId\":\"\",\"spanId\":\"\",\"attributes\":[{\"key\":\"span-link-attr\",\"value\":{\"stringValue\":\"span-link-attr-val\"}}],\"droppedAttributesCount\":3},{\"traceId\":\"\",\"spanId\":\"\",\"droppedAttributesCount\":4}],\"droppedLinksCount\":2,\"status\":{}}]}]}]} {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeSpans\":[{\"scope\":{},\"spans\":[{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationA\",\"startTimeUnixNano\":\"1581452772000000321\",\"endTimeUnixNano\":\"1581452773000000789\",\"droppedAttributesCount\":1,\"events\":[{\"timeUnixNano\":\"1581452773000000826\",\"name\":\"event-with-attr\",\"attributes\":[{\"key\":\"span-event-attr\",\"value\":{\"stringValue\":\"span-event-attr-val\"}}],\"droppedAttributesCount\":2},{\"timeUnixNano\":\"1581452773000000826\",\"name\":\"event\",\"droppedAttributesCount\":2}],\"droppedEventsCount\":1,\"status\":{\"message\":\"status-cancelled\",\"code\":2}},{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationB\",\"startTimeUnixNano\":\"1581452772000200521\",\"endTimeUnixNano\":\"1581452773000004789\",\"links\":[{\"traceId\":\"\",\"spanId\":\"\",\"attributes\":[{\"key\":\"span-link-attr\",\"value\":{\"stringValue\":\"span-link-attr-val\"}}],\"droppedAttributesCount\":5},{\"traceId\":\"\",\"spanId\":\"\",\"droppedAttributesCount\":2}],\"droppedLinksCount\":3,\"status\":{}}]}]}]} {\"resourceSpans\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeSpans\":[{\"scope\":{},\"spans\":[{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationA\",\"startTimeUnixNano\":\"1581452772000000321\",\"endTimeUnixNano\":\"1581452773000000789\",\"droppedAttributesCount\":1,\"events\":[{\"timeUnixNano\":\"1581452773000010925\",\"name\":\"event-with-attr\",\"attributes\":[{\"key\":\"span-event-attr\",\"value\":{\"stringValue\":\"span-event-attr-val\"}}],\"droppedAttributesCount\":2},{\"timeUnixNano\":\"1581452773000010925\",\"name\":\"event\",\"droppedAttributesCount\":2}],\"droppedEventsCount\":1,\"status\":{\"message\":\"status-cancelled\",\"code\":2}},{\"traceId\":\"\",\"spanId\":\"\",\"parentSpanId\":\"\",\"name\":\"operationB\",\"startTimeUnixNano\":\"1581452772000011821\",\"endTimeUnixNano\":\"1581452772000012924\",\"links\":[{\"traceId\":\"\",\"spanId\":\"\",\"attributes\":[{\"key\":\"span-link-attr\",\"value\":{\"stringValue\":\"span-link-attr-val\"}}],\"droppedAttributesCount\":2},{\"traceId\":\"\",\"spanId\":\"\",\"droppedAttributesCount\":2}],\"droppedLinksCount\":5,\"status\":{}}]}]}]} <pre><code>This is an example showing metrics:\n\n```json lines\n{\"resourceMetrics\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeMetrics\":[{\"scope\":{},\"metrics\":[{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000000789\",\"timeUnixNano\":\"1581452773000000789\",\"asInt\":\"123\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000000789\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}},{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000000789\",\"asInt\":\"123\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000000789\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}}]}]}]}\n{\"resourceMetrics\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeMetrics\":[{\"scope\":{},\"metrics\":[{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000001459\",\"timeUnixNano\":\"1581452773000001459\",\"asInt\":\"120\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452773000001459\",\"timeUnixNano\":\"1581452773000001459\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}},{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000001459\",\"timeUnixNano\":\"1581452773000001459\",\"asInt\":\"123\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452773000001459\",\"timeUnixNano\":\"1581452773000001459\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}}]}]}]}\n{\"resourceMetrics\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeMetrics\":[{\"scope\":{},\"metrics\":[{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000002346\",\"timeUnixNano\":\"1581452773000002346\",\"asInt\":\"121\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452773000002346\",\"timeUnixNano\":\"1581452773000002346\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}},{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000002346\",\"timeUnixNano\":\"1581452773000002346\",\"asInt\":\"123\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000000789\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}}]}]}]}\n{\"resourceMetrics\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeMetrics\":[{\"scope\":{},\"metrics\":[{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000007891\",\"timeUnixNano\":\"1581452773000007891\",\"asInt\":\"125\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000007891\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}},{\"name\":\"counter-int\",\"unit\":\"1\",\"sum\":{\"dataPoints\":[{\"attributes\":[{\"key\":\"label-1\",\"value\":{\"stringValue\":\"label-value-1\"}}],\"startTimeUnixNano\":\"1581452773000007891\",\"timeUnixNano\":\"1581452773000007891\",\"asInt\":\"123\"},{\"attributes\":[{\"key\":\"label-2\",\"value\":{\"stringValue\":\"label-value-2\"}}],\"startTimeUnixNano\":\"1581452772000000321\",\"timeUnixNano\":\"1581452773000007891\",\"asInt\":\"456\"}],\"aggregationTemporality\":2,\"isMonotonic\":true}}]}]}]}\n</code></pre></p> <p>This is an example showing logs:</p> <p><code>json lines {\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"timeUnixNano\":\"1581452773000000789\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logA\",\"body\":{\"stringValue\":\"This is a log message\"},\"attributes\":[{\"key\":\"app\",\"value\":{\"stringValue\":\"server\"}},{\"key\":\"instance_num\",\"value\":{\"intValue\":\"1\"}}],\"droppedAttributesCount\":1,\"traceId\":\"08040201000000000000000000000000\",\"spanId\":\"0102040800000000\"},{\"timeUnixNano\":\"1581452773000000789\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logB\",\"body\":{\"stringValue\":\"something happened\"},\"attributes\":[{\"key\":\"customer\",\"value\":{\"stringValue\":\"acme\"}},{\"key\":\"env\",\"value\":{\"stringValue\":\"dev\"}}],\"droppedAttributesCount\":1,\"traceId\":\"\",\"spanId\":\"\"}]}]}]} {\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"timeUnixNano\":\"1581452773000001233\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logA\",\"body\":{\"stringValue\":\"This is a log message\"},\"attributes\":[{\"key\":\"app\",\"value\":{\"stringValue\":\"server\"}},{\"key\":\"instance_num\",\"value\":{\"intValue\":\"1\"}}],\"droppedAttributesCount\":1,\"traceId\":\"08040201000000000000000000000000\",\"spanId\":\"0102040800000000\"},{\"timeUnixNano\":\"1581452773000000789\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logB\",\"body\":{\"stringValue\":\"something happened\"},\"attributes\":[{\"key\":\"customer\",\"value\":{\"stringValue\":\"acme\"}},{\"key\":\"env\",\"value\":{\"stringValue\":\"dev\"}}],\"droppedAttributesCount\":1,\"traceId\":\"\",\"spanId\":\"\"}]}]}]} {\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"timeUnixNano\":\"1581452773000005443\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logA\",\"body\":{\"stringValue\":\"This is a log message\"},\"attributes\":[{\"key\":\"app\",\"value\":{\"stringValue\":\"server\"}},{\"key\":\"instance_num\",\"value\":{\"intValue\":\"1\"}}],\"droppedAttributesCount\":1,\"traceId\":\"08040201000000000000000000000000\",\"spanId\":\"0102040800000000\"},{\"timeUnixNano\":\"1581452773000000789\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logB\",\"body\":{\"stringValue\":\"something happened\"},\"attributes\":[{\"key\":\"customer\",\"value\":{\"stringValue\":\"acme\"}},{\"key\":\"env\",\"value\":{\"stringValue\":\"dev\"}}],\"droppedAttributesCount\":1,\"traceId\":\"\",\"spanId\":\"\"}]}]}]} {\"resourceLogs\":[{\"resource\":{\"attributes\":[{\"key\":\"resource-attr\",\"value\":{\"stringValue\":\"resource-attr-val-1\"}}]},\"scopeLogs\":[{\"scope\":{},\"logRecords\":[{\"timeUnixNano\":\"1581452773000009875\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logA\",\"body\":{\"stringValue\":\"This is a log message\"},\"attributes\":[{\"key\":\"app\",\"value\":{\"stringValue\":\"server\"}},{\"key\":\"instance_num\",\"value\":{\"intValue\":\"1\"}}],\"droppedAttributesCount\":1,\"traceId\":\"08040201000000000000000000000000\",\"spanId\":\"0102040800000000\"},{\"timeUnixNano\":\"1581452773000000789\",\"severityNumber\":9,\"severityText\":\"Info\",\"name\":\"logB\",\"body\":{\"stringValue\":\"something happened\"},\"attributes\":[{\"key\":\"customer\",\"value\":{\"stringValue\":\"acme\"}},{\"key\":\"env\",\"value\":{\"stringValue\":\"dev\"}}],\"droppedAttributesCount\":1,\"traceId\":\"\",\"spanId\":\"\"}]}]}]}</code></p>"},{"location":"docs/specs/otel/protocol/otlp/","title":"OpenTelemetry \u534f\u8bae\u89c4\u8303","text":"<p>Status: Stable</p> <p>The OpenTelemetry Protocol (OTLP) specification describes the encoding, transport, and delivery mechanism of telemetry data between telemetry sources, intermediate nodes such as collectors and telemetry backends.</p> Table of Contents   - [Protocol Details](#protocol-details)   - [OTLP/gRPC](#otlpgrpc)     - [OTLP/gRPC Concurrent Requests](#otlpgrpc-concurrent-requests)     - [OTLP/gRPC Response](#otlpgrpc-response)       - [Full Success](#full-success)       - [Partial Success](#partial-success)       - [Failures](#failures)     - [OTLP/gRPC Throttling](#otlpgrpc-throttling)     - [OTLP/gRPC Service and Protobuf Definitions](#otlpgrpc-service-and-protobuf-definitions)     - [OTLP/gRPC Default Port](#otlpgrpc-default-port)   - [OTLP/HTTP](#otlphttp)     - [Binary Protobuf Encoding](#binary-protobuf-encoding)     - [JSON Protobuf Encoding](#json-protobuf-encoding)     - [OTLP/HTTP Request](#otlphttp-request)     - [OTLP/HTTP Response](#otlphttp-response)       - [Full Success](#full-success-1)       - [Partial Success](#partial-success-1)       - [Failures](#failures-1)       - [Bad Data](#bad-data)       - [OTLP/HTTP Throttling](#otlphttp-throttling)       - [All Other Responses](#all-other-responses)     - [OTLP/HTTP Connection](#otlphttp-connection)     - [OTLP/HTTP Concurrent Requests](#otlphttp-concurrent-requests)     - [OTLP/HTTP Default Port](#otlphttp-default-port) - [Implementation Recommendations](#implementation-recommendations)   - [Multi-Destination Exporting](#multi-destination-exporting) - [Known Limitations](#known-limitations)   - [Request Acknowledgements](#request-acknowledgements)     - [Duplicate Data](#duplicate-data) - [Future Versions and Interoperability](#future-versions-and-interoperability) - [Glossary](#glossary) - [References](#references)   <p>OTLP is a general-purpose telemetry data delivery protocol designed in the scope of the OpenTelemetry project.</p>"},{"location":"docs/specs/otel/protocol/otlp/#protocol-details","title":"Protocol Details","text":"<p>OTLP defines the encoding of telemetry data and the protocol used to exchange data between the client and the server.</p> <p>This specification defines how OTLP is implemented over gRPC and HTTP 1.1 transports and specifies Protocol Buffers schema that is used for the payloads.</p> <p>OTLP is a request/response style protocol: the clients send requests, and the server replies with corresponding responses. This document defines one request and response type: <code>Export</code>.</p> <p>All server components MUST support the following transport compression options:</p> <ul> <li>No compression, denoted by <code>none</code>.</li> <li>Gzip compression, denoted by <code>gzip</code>.</li> </ul>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc","title":"OTLP/gRPC","text":"<p>After establishing the underlying gRPC transport, the client starts sending telemetry data using unary requests using Export*ServiceRequest messages (ExportLogsServiceRequest for logs, ExportMetricsServiceRequest for metrics, ExportTraceServiceRequest for traces). The client continuously sends a sequence of requests to the server and expects to receive a response to each request:</p> <p></p> <p>Note: this protocol is concerned with the reliability of delivery between one pair of client/server nodes and aims to ensure that no data is lost in transit between the client and the server. Many telemetry collection systems have intermediary nodes that the data must travel across until reaching the final destination (e.g. application -&gt; agent -&gt; collector -&gt; backend). End-to-end delivery guarantees in such systems is outside of the scope of OTLP. The acknowledgements described in this protocol happen between a single client/server pair and do not span intermediary nodes in multi-hop delivery paths.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc-concurrent-requests","title":"OTLP/gRPC Concurrent Requests","text":"<p>After sending the request the client MAY wait until the response is received from the server. In that case there will be at most only one request in flight that is not yet acknowledged by the server.</p> <p></p> <p>Sequential operation is recommended when simplicity of implementation is desirable, and when the client and the server are connected via very low-latency network, such as when the client is an instrumented application and the server is an OpenTelemetry Collector running as a local daemon (agent).</p> <p>The implementations that need to achieve high throughput SHOULD support concurrent Unary calls to achieve higher throughput. The client SHOULD send new requests without waiting for the response to the earlier sent requests, essentially creating a pipeline of requests that are currently in flight that are not acknowledged.</p> <p></p> <p>The number of concurrent requests SHOULD be configurable.</p> <p>The maximum achievable throughput is <code>max_concurrent_requests * max_request_size / (network_latency + server_response_time)</code>. For example, if the request can contain at most 100 spans, network roundtrip latency is 200ms, and server response time is 300 ms, then the maximum achievable throughput with one concurrent request is <code>100 spans / (200ms+300ms)</code> or 200 spans per second. It is easy to see that in high latency networks or when the server response time is high to achieve good throughput, the requests need to be very big or a lot concurrent requests must be done.</p> <p>If the client is shutting down (e.g. when the containing process wants to exit) the client will optionally wait until all pending acknowledgements are received or until an implementation-specific timeout expires. This ensures the reliable delivery of telemetry data. The client implementation SHOULD expose an option to turn on and off the waiting during a shutdown.</p> <p>If the client is unable to deliver a certain request (e.g. a timer expired while waiting for acknowledgements) the client SHOULD record the fact that the data was not delivered.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc-response","title":"OTLP/gRPC Response","text":"<p>The response MUST be the appropriate message (see below for the specific message to use in the Full Success, Partial Success and Failure cases).</p>"},{"location":"docs/specs/otel/protocol/otlp/#full-success","title":"Full Success","text":"<p>The success response indicates telemetry data is successfully accepted by the server.</p> <p>If the server receives an empty request (a request that does not carry any telemetry data) the server SHOULD respond with success.</p> <p>On success, the server response MUST be a ExportServiceResponse message (<code>ExportTraceServiceResponse</code> for traces, <code>ExportMetricsServiceResponse</code> for metrics and <code>ExportLogsServiceResponse</code> for logs). <p>The server MUST leave the <code>partial_success</code> field unset in case of a successful response.</p>"},{"location":"docs/specs/otel/protocol/otlp/#partial-success","title":"Partial Success","text":"<p>If the request is only partially accepted (i.e. when the server accepts only parts of the data and rejects the rest), the server response MUST be the same ExportServiceResponse message as in the Full Success case. <p>Additionally, the server MUST initialize the <code>partial_success</code> field (<code>ExportTracePartialSuccess</code> message for traces, <code>ExportMetricsPartialSuccess</code> message for metrics and <code>ExportLogsPartialSuccess</code> message for logs), and it MUST set the respective <code>rejected_spans</code>, <code>rejected_data_points</code> or <code>rejected_log_records</code> field with the number of spans/data points/log records it rejected.</p> <p>The server SHOULD populate the <code>error_message</code> field with a human-readable error message in English. The message should explain why the server rejected parts of the data and might offer guidance on how users can address the issues. The protocol does not attempt to define the structure of the error message.</p> <p>Servers MAY also use the <code>partial_success</code> field to convey warnings/suggestions to clients even when the server fully accepts the request. In such cases, the <code>rejected_&lt;signal&gt;</code> field MUST have a value of <code>0</code>, and the <code>error_message</code> field MUST be non-empty.</p> <p>The client MUST NOT retry the request when it receives a partial success response where the <code>partial_success</code> is populated.</p>"},{"location":"docs/specs/otel/protocol/otlp/#failures","title":"Failures","text":"<p>When the server returns an error, it falls into 2 broad categories: retryable and not-retryable:</p> <ul> <li> <p>Retryable errors indicate that telemetry data processing failed, and the   client SHOULD record the error and may retry exporting the same data. For   example, this can happen when the server is temporarily unable to process the   data.</p> </li> <li> <p>Not-retryable errors indicate that telemetry data processing failed, and the   client MUST NOT retry sending the same telemetry data. The client MUST drop   the telemetry data. For example, this can happen, when the request contains   bad data and cannot be deserialized or processed by the server. The client   SHOULD maintain a counter of such dropped data.</p> </li> </ul> <p>The server MUST indicate retryable errors using code Unavailable and MAY supply additional details via status using RetryInfo containing 0 value of RetryDelay. Here is a sample Go code to illustrate:</p> <pre><code>  // Do this on server side.\nst, err := status.New(codes.Unavailable, \"Server is unavailable\").\nWithDetails(&amp;errdetails.RetryInfo{RetryDelay: &amp;duration.Duration{Seconds: 0}})\nif err != nil {\nlog.Fatal(err)\n}\nreturn st.Err()\n</code></pre> <p>To indicate not-retryable errors, the server is recommended to use code InvalidArgument and MAY supply additional details via status using BadRequest. If more appropriate, another gRPC status code may be used. Here is a snippet of sample Go code to illustrate:</p> <pre><code>  // Do this on the server side.\nst, err := status.New(codes.InvalidArgument, \"Invalid Argument\").\nWithDetails(&amp;errdetails.BadRequest{})\nif err != nil {\nlog.Fatal(err)\n}\nreturn st.Err()\n</code></pre> <p>The server MAY use other gRPC codes to indicate retryable and not-retryable errors if those other gRPC codes are more appropriate for a particular erroneous situation. The client SHOULD interpret gRPC status codes as retryable or not-retryable according to the following table:</p> gRPC Code Retryable? CANCELLED Yes UNKNOWN No INVALID_ARGUMENT No DEADLINE_EXCEEDED Yes NOT_FOUND No ALREADY_EXISTS No PERMISSION_DENIED No UNAUTHENTICATED No RESOURCE_EXHAUSTED Only if the server can recover (see below) FAILED_PRECONDITION No ABORTED Yes OUT_OF_RANGE Yes UNIMPLEMENTED No INTERNAL No UNAVAILABLE Yes DATA_LOSS Yes <p>When retrying, the client SHOULD implement an exponential backoff strategy. An exception to this is the Throttling case explained below, which provides explicit instructions about retrying interval.</p> <p>The client SHOULD interpret <code>RESOURCE_EXHAUSTED</code> code as retryable only if the server signals that the recovery from resource exhaustion is possible. This is signaled by the server by returning a status containing RetryInfo. In this case the behavior of the server and the client is exactly as described in OTLP/gRPC Throttling section. If no such status is returned, then the <code>RESOURCE_EXHAUSTED</code> code SHOULD be treated as non-retryable.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc-throttling","title":"OTLP/gRPC Throttling","text":"<p>OTLP allows backpressure signaling.</p> <p>If the server is unable to keep up with the pace of data it receives from the client then it SHOULD signal that fact to the client. The client MUST then throttle itself to avoid overwhelming the server.</p> <p>To signal backpressure when using gRPC transport, the server MUST return an error with code Unavailable and MAY supply additional details via status using RetryInfo. Here is a snippet of sample Go code to illustrate:</p> <pre><code>  // Do this on the server side.\nst, err := status.New(codes.Unavailable, \"Server is unavailable\").\nWithDetails(&amp;errdetails.RetryInfo{RetryDelay: &amp;duration.Duration{Seconds: 30}})\nif err != nil {\nlog.Fatal(err)\n}\nreturn st.Err()\n...\n// Do this on the client side.\nst := status.Convert(err)\nfor _, detail := range st.Details() {\nswitch t := detail.(type) {\ncase *errdetails.RetryInfo:\nif t.RetryDelay.Seconds &gt; 0 || t.RetryDelay.Nanos &gt; 0 {\n// Wait before retrying.\n}\n}\n}\n</code></pre> <p>When the client receives this signal, it SHOULD follow the recommendations outlined in documentation for RetryInfo:</p> <pre><code>// Describes when the clients can retry a failed request. Clients could ignore\n// the recommendation here or retry when this information is missing from the error\n// responses.\n//\n// It's always recommended that clients should use exponential backoff when\n// retrying.\n//\n// Clients should wait until `retry_delay` amount of time has passed since\n// receiving the error response before retrying.  If retrying requests also\n// fail, clients should use an exponential backoff scheme to increase gradually\n// the delay between retries based on `retry_delay` until either a maximum\n// number of retries has been reached, or a maximum retry delay cap has been\n// reached.\n</code></pre> <p>The value of <code>retry_delay</code> is determined by the server and is implementation dependant. The server SHOULD choose a <code>retry_delay</code> value that is big enough to give the server time to recover yet is not too big to cause the client to drop data while being throttled.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc-service-and-protobuf-definitions","title":"OTLP/gRPC Service and Protobuf Definitions","text":"<p>gRPC service definitions are here.</p> <p>Protobuf definitions for requests and responses are here.</p> <p>Please make sure to check the proto version and maturity level. Schemas for different signals may be at different maturity level - some stable, some in beta.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlpgrpc-default-port","title":"OTLP/gRPC Default Port","text":"<p>The default network port for OTLP/gRPC is 4317.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp","title":"OTLP/HTTP","text":"<p>OTLP/HTTP uses Protobuf payloads encoded either in binary format or in JSON format. Regardless of the encoding the Protobuf schema of the messages is the same for OTLP/HTTP and OTLP/gRPC as defined here.</p> <p>OTLP/HTTP uses HTTP POST requests to send telemetry data from clients to servers. Implementations MAY use HTTP/1.1 or HTTP/2 transports. Implementations that use HTTP/2 transport SHOULD fallback to HTTP/1.1 transport if HTTP/2 connection cannot be established.</p>"},{"location":"docs/specs/otel/protocol/otlp/#binary-protobuf-encoding","title":"Binary Protobuf Encoding","text":"<p>Binary Protobuf encoded payloads use proto3 encoding standard.</p> <p>The client and the server MUST set \"Content-Type: application/x-protobuf\" request and response headers when sending binary Protobuf encoded payload.</p>"},{"location":"docs/specs/otel/protocol/otlp/#json-protobuf-encoding","title":"JSON Protobuf Encoding","text":"<p>JSON Protobuf encoded payloads use proto3 standard defined JSON Mapping for mapping between Protobuf and JSON, with the following deviations from that mapping:</p> <ul> <li> <p>The <code>trace_id</code> and <code>span_id</code> byte arrays are represented as   case-insensitive hex-encoded strings;   they are not base64-encoded as is defined in the standard   Protobuf JSON Mapping.   Hex encoding is used for <code>trace_id</code> and <code>span_id</code> fields in all OTLP Protobuf   messages, e.g., the <code>Span</code>, <code>Link</code>, <code>LogRecord</code>, etc. messages. For example,   the <code>trace_id</code> field in a Span can be represented like this: { \"trace_id\":   \"5B8EFFF798038103D269B633813FC60C\", ... }</p> </li> <li> <p>Values of enum fields MUST be encoded as integer values. Unlike the standard   Protobuf JSON Mapping,   which allows values of enum fields to be encoded as either integer values or   as enum name strings, only integer enum values are allowed in OTLP JSON   Protobuf Encoding; the enum name strings MUST NOT be used. For example, the   <code>kind</code> field with a value of SPAN_KIND_SERVER in a Span can be represented   like this: { \"kind\": 2, ... }</p> </li> <li> <p>OTLP/JSON receivers MUST ignore message fields with unknown names and MUST   unmarshal the message as if the unknown field was not present in the payload.   This aligns with the behavior of the Binary Protobuf unmarshaler and ensures   that adding new fields to OTLP messages does not break existing receivers.</p> </li> <li> <p>The keys of JSON objects are field names converted to lowerCamelCase. Original   field names are not valid to use as keys for JSON objects. For example, this   is a valid JSON representation of a Resource:   <code>{ \"attributes\": {...}, \"droppedAttributesCount\": 123 }</code>, and this is NOT a   valid representation:   <code>{ \"attributes\": {...}, \"dropped_attributes_count\": 123 }</code>.</p> </li> </ul> <p>Note that according to Protobuf specs 64-bit integer numbers in JSON-encoded payloads are encoded as decimal strings, and either numbers or strings are accepted when decoding.</p> <p>The client and the server MUST set \"Content-Type: application/json\" request and response headers when sending JSON Protobuf encoded payload.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-request","title":"OTLP/HTTP Request","text":"<p>Telemetry data is sent via HTTP POST request. The body of the POST request is a payload either in binary-encoded Protobuf format or in JSON-encoded Protobuf format.</p> <p>The default URL path for requests that carry trace data is <code>/v1/traces</code> (for example the full URL when connecting to \"example.com\" server will be <code>https://example.com/v1/traces</code>). The request body is a Protobuf-encoded <code>ExportTraceServiceRequest</code> message.</p> <p>The default URL path for requests that carry metric data is <code>/v1/metrics</code> and the request body is a Protobuf-encoded <code>ExportMetricsServiceRequest</code> message.</p> <p>The default URL path for requests that carry log data is <code>/v1/logs</code> and the request body is a Protobuf-encoded <code>ExportLogsServiceRequest</code> message.</p> <p>The client MAY gzip the content and in that case MUST include \"Content-Encoding: gzip\" request header. The client MAY include \"Accept-Encoding: gzip\" request header if it can receive gzip-encoded responses.</p> <p>Non-default URL paths for requests MAY be configured on the client and server sides.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-response","title":"OTLP/HTTP Response","text":"<p>The response body MUST be the appropriate serialized Protobuf message (see below for the specific message to use in the Full Success, Partial Success and Failure cases).</p> <p>The server MUST set \"Content-Type: application/x-protobuf\" header if the response body is binary-encoded Protobuf payload. The server MUST set \"Content-Type: application/json\" if the response is JSON-encoded Protobuf payload. The server MUST use the same \"Content-Type\" in the response as it received in the request.</p> <p>If the request header \"Accept-Encoding: gzip\" is present in the request the server MAY gzip-encode the response and set \"Content-Encoding: gzip\" response header.</p>"},{"location":"docs/specs/otel/protocol/otlp/#full-success_1","title":"Full Success","text":"<p>The success response indicates telemetry data is successfully accepted by the server.</p> <p>If the server receives an empty request (a request that does not carry any telemetry data) the server SHOULD respond with success.</p> <p>On success, the server MUST respond with <code>HTTP 200 OK</code>. The response body MUST be a Protobuf-encoded ExportServiceResponse message (<code>ExportTraceServiceResponse</code> for traces, <code>ExportMetricsServiceResponse</code> for metrics and <code>ExportLogsServiceResponse</code> for logs). <p>The server MUST leave the <code>partial_success</code> field unset in case of a successful response.</p>"},{"location":"docs/specs/otel/protocol/otlp/#partial-success_1","title":"Partial Success","text":"<p>If the request is only partially accepted (i.e. when the server accepts only parts of the data and rejects the rest), the server MUST respond with <code>HTTP 200 OK</code>. The response body MUST be the same ExportServiceResponse message as in the Full Success case. <p>Additionally, the server MUST initialize the <code>partial_success</code> field (<code>ExportTracePartialSuccess</code> message for traces, <code>ExportMetricsPartialSuccess</code> message for metrics and <code>ExportLogsPartialSuccess</code> message for logs), and it MUST set the respective <code>rejected_spans</code>, <code>rejected_data_points</code> or <code>rejected_log_records</code> field with the number of spans/data points/log records it rejected.</p> <p>The server SHOULD populate the <code>error_message</code> field with a human-readable error message in English. The message should explain why the server rejected parts of the data and might offer guidance on how users can address the issues. The protocol does not attempt to define the structure of the error message.</p> <p>Servers MAY also use the <code>partial_success</code> field to convey warnings/suggestions to clients even when it fully accepts the request. In such cases, the <code>rejected_&lt;signal&gt;</code> field MUST have a value of <code>0</code>, and the <code>error_message</code> field MUST be non-empty.</p> <p>The client MUST NOT retry the request when it receives a partial success response where the <code>partial_success</code> is populated.</p>"},{"location":"docs/specs/otel/protocol/otlp/#failures_1","title":"Failures","text":"<p>If the processing of the request fails, the server MUST respond with appropriate <code>HTTP 4xx</code> or <code>HTTP 5xx</code> status code. See the sections below for more details about specific failure cases and HTTP status codes that should be used.</p> <p>The response body for all <code>HTTP 4xx</code> and <code>HTTP 5xx</code> responses MUST be a Protobuf-encoded Status message that describes the problem.</p> <p>This specification does not use <code>Status.code</code> field and the server MAY omit <code>Status.code</code> field. The clients are not expected to alter their behavior based on <code>Status.code</code> field but MAY record it for troubleshooting purposes.</p> <p>The <code>Status.message</code> field SHOULD contain a developer-facing error message as defined in <code>Status</code> message schema.</p> <p>The server MAY include <code>Status.details</code> field with additional details. Read below about what this field can contain in each specific failure case.</p> <p>The server SHOULD use HTTP response status codes to indicate retryable and not-retryable errors for a particular erroneous situation. The client SHOULD honour HTTP response status codes as retryable or not-retryable. The requests that receive a response status code listed in following table SHOULD be retried. All other <code>4xx</code> or <code>5xx</code> response status codes MUST NOT be retried.</p> HTTP response status code 429 Too Many Requests 502 Bad Gateway 503 Service Unavailable 504 Gateway Timeout"},{"location":"docs/specs/otel/protocol/otlp/#bad-data","title":"Bad Data","text":"<p>If the processing of the request fails because the request contains data that cannot be decoded or is otherwise invalid and such failure is permanent, then the server MUST respond with <code>HTTP 400 Bad Request</code>. The <code>Status.details</code> field in the response SHOULD contain a BadRequest that describes the bad data.</p> <p>The client MUST NOT retry the request when it receives <code>HTTP 400 Bad Request</code> response.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-throttling","title":"OTLP/HTTP Throttling","text":"<p>If the server receives more requests than the client is allowed or the server is overloaded, the server SHOULD respond with <code>HTTP 429 Too Many Requests</code> or <code>HTTP 503 Service Unavailable</code> and MAY include \"Retry-After\" header with a recommended time interval in seconds to wait before retrying.</p> <p>The client SHOULD honour the waiting interval specified in the \"Retry-After\" header if it is present. If the client receives an <code>HTTP 429</code> or an <code>HTTP 503</code> response and the \"Retry-After\" header is not present in the response, then the client SHOULD implement an exponential backoff strategy between retries.</p>"},{"location":"docs/specs/otel/protocol/otlp/#all-other-responses","title":"All Other Responses","text":"<p>All other HTTP responses that are not explicitly listed in this document should be treated according to HTTP specifications.</p> <p>If the server disconnects without returning a response, the client SHOULD retry and send the same request. The client SHOULD implement an exponential backoff strategy between retries to avoid overwhelming the server.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-connection","title":"OTLP/HTTP Connection","text":"<p>If the client cannot connect to the server, the client SHOULD retry the connection using an exponential backoff strategy between retries. The interval between retries must have a random jitter.</p> <p>The client SHOULD keep the connection alive between requests.</p> <p>Server implementations SHOULD accept OTLP/HTTP with binary-encoded Protobuf payload and OTLP/HTTP with JSON-encoded Protobuf payload requests on the same port and multiplex the requests to the corresponding payload decoder based on the \"Content-Type\" request header.</p> <p>Server implementations MAY accept OTLP/gRPC and OTLP/HTTP requests on the same port and multiplex the connections to the corresponding transport handler based on the \"Content-Type\" request header.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-concurrent-requests","title":"OTLP/HTTP Concurrent Requests","text":"<p>To achieve higher total throughput, the client MAY send requests using several parallel HTTP connections. In that case, the maximum number of parallel connections SHOULD be configurable.</p>"},{"location":"docs/specs/otel/protocol/otlp/#otlphttp-default-port","title":"OTLP/HTTP Default Port","text":"<p>The default network port for OTLP/HTTP is 4318.</p>"},{"location":"docs/specs/otel/protocol/otlp/#implementation-recommendations","title":"Implementation Recommendations","text":""},{"location":"docs/specs/otel/protocol/otlp/#multi-destination-exporting","title":"Multi-Destination Exporting","text":"<p>An additional complication must be accounted for when one client must send telemetry data to more than one destination server. When one of the servers acknowledges the data and the other server does not (yet), the client needs to decide how to move forward.</p> <p>In such a situation, the client SHOULD implement queuing, acknowledgment handling, and retrying logic per destination. This ensures that servers do not block each other. The queues SHOULD reference shared, immutable data to be sent, thus minimizing the memory overhead caused by having multiple queues.</p> <p></p> <p>This ensures that all destination servers receive the data regardless of their speed of reception (within the available limits imposed by the size of the client-side queue).</p>"},{"location":"docs/specs/otel/protocol/otlp/#known-limitations","title":"Known Limitations","text":""},{"location":"docs/specs/otel/protocol/otlp/#request-acknowledgements","title":"Request Acknowledgements","text":""},{"location":"docs/specs/otel/protocol/otlp/#duplicate-data","title":"Duplicate Data","text":"<p>In edge cases (e.g. on reconnections, network interruptions, etc) the client has no way of knowing if recently sent data was delivered if no acknowledgement was received yet. The client will typically choose to re-send such data to guarantee delivery, which may result in duplicate data on the server side. This is a deliberate choice and is considered to be the right tradeoff for telemetry data.</p>"},{"location":"docs/specs/otel/protocol/otlp/#future-versions-and-interoperability","title":"Future Versions and Interoperability","text":"<p>OTLP will evolve and change over time. Future versions of OTLP must be designed and implemented in a way that ensures that clients and servers that implement different versions of OTLP can interoperate and exchange telemetry data. Old clients must be able to talk to new servers and vice versa. Suppose new versions of OTLP introduce new functionality that cannot be understood and supported by nodes implementing the old versions of OTLP. In that case, the protocol must regress to the lowest common denominator from a functional perspective.</p> <p>When possible, the interoperability MUST be ensured between all versions of OTLP that are not declared obsolete.</p> <p>OTLP does not use explicit protocol version numbering. OTLP's interoperability of clients and servers of different versions is based on the following concepts:</p> <ol> <li> <p>OTLP (current and future versions) defines a set of capabilities, some of    which are mandatory, while others are optional. Clients and servers must    implement mandatory capabilities and can choose to implement only a subset of    optional capabilities.</p> </li> <li> <p>For minor changes to the protocol, future versions and extensions of OTLP are    encouraged to use the Protobuf's ability to evolve the message schema in a    backward-compatible manner. Newer versions of OTLP may add new fields to    messages that will be ignored by clients and servers that do not understand    these fields. In many cases, careful design of such schema changes and    correct choice of default values for new fields is enough to ensure    interoperability of different versions without nodes explicitly detecting    that their peer node has different capabilities.</p> </li> <li> <p>More significant changes must be explicitly defined as new optional    capabilities in future OTEPs. Such capabilities SHOULD be discovered by    client and server implementations after establishing the underlying    transport. The exact discovery mechanism SHOULD be described in future OTEPs,    which define the new capabilities and typically can be implemented by making    a discovery request/response message exchange from the client to server. The    mandatory capabilities defined by this specification are implied and do not    require discovery. The implementation which supports a new, optional    capability MUST adjust its behavior to match the expectation of a peer that    does not support a particular capability.</p> </li> </ol>"},{"location":"docs/specs/otel/protocol/otlp/#glossary","title":"Glossary","text":"<p>There are 2 parties involved in telemetry data exchange. In this document the party that is the source of telemetry data is called the <code>Client</code>, the party that is the destination of telemetry data is called the <code>Server</code>.</p> <p></p> <p>Examples of a Client are instrumented applications or sending side of telemetry collectors, examples of Servers are telemetry backends or receiving side of telemetry collectors (so a Collector is typically both a Client and a Server depending on which side you look from).</p> <p>Both the Client and the Server are also a <code>Node</code>. This term is used in the document when referring to either one.</p>"},{"location":"docs/specs/otel/protocol/otlp/#references","title":"References","text":"<ul> <li>OTEP 0035   OpenTelemetry Protocol Specification</li> <li>OTEP 0099   OTLP/HTTP: HTTP Transport Extension for OTLP</li> <li>OTEP 0122   OTLP: JSON Encoding for OTLP/HTTP</li> </ul>"},{"location":"docs/specs/otel/protocol/requirements/","title":"OpenTelemetry \u534f\u8bae\u8981\u6c42","text":"<p>This document will drive OpenTelemetry Protocol design and RFC.</p>"},{"location":"docs/specs/otel/protocol/requirements/#goals","title":"Goals","text":"<p>See the goals of OpenTelemetry Protocol design here.</p>"},{"location":"docs/specs/otel/protocol/requirements/#vocabulary","title":"Vocabulary","text":"<p>There are 2 parties involved in telemetry data exchange. In this document the party that is the source of telemetry data is called the Client, the party that is the destination of telemetry data is called the Server.</p> <p>Examples of a Client are instrumented applications or sending side of telemetry collectors, examples of Servers are telemetry backends or receiving side of telemetry collectors (so a Collector is typically both a Client and a Server depending on which side you look from).</p>"},{"location":"docs/specs/otel/protocol/requirements/#known-issues-with-existing-protocols","title":"Known Issues with Existing Protocols","text":"<p>Our experience with OpenCensus and other protocols has been that many of them have one or more of the following drawbacks:</p> <ul> <li>High CPU consumption for serialization and especially deserialization of   received telemetry data.</li> <li>High and frequent CPU consumption by Garbage Collector.</li> <li>Lack of delivery guarantees for certain protocols (e.g. stream-based gRPC   OpenCensus protocol) which makes troubleshooting of telemetry pipelines   difficult.</li> <li>Not aware / not cooperating with load balancers resulting in potentially large   imbalances in horizontally scaled backends.</li> <li>Support either traces or metrics but not both.</li> </ul> <p>Our goal is to avoid or mitigate these known issues in the new protocol.</p>"},{"location":"docs/specs/otel/protocol/requirements/#requirements","title":"Requirements","text":"<p>The following are OpenTelemetry protocol requirements.</p>"},{"location":"docs/specs/otel/protocol/requirements/#supported-node-types","title":"Supported Node Types","text":"<p>The protocol must be suitable for use between all of the following node types: instrumented applications, telemetry backends, telemetry agents running as local daemons, stand-alone collector/forwarder services.</p>"},{"location":"docs/specs/otel/protocol/requirements/#supported-data-types","title":"Supported Data Types","text":"<p>The protocol must support traces and metrics as data types.</p>"},{"location":"docs/specs/otel/protocol/requirements/#reliability-of-delivery","title":"Reliability of Delivery","text":"<p>The protocol must ensure reliable data delivery and clear visibility when the data cannot be delivered. This should be achieved by sending data acknowledgements from the Server to the Client.</p> <p>Note that acknowledgements alone are not sufficient to guarantee that: a) no data will be lost and b) no data will be duplicated. Acknowledgements can help to guarantee a) but not b). Guaranteeing both at the same is difficult. Because it is usually preferable for telemetry data to be duplicated than to lose it, we choose to guarantee that there are no data losses while potentially allowing duplicate data.</p> <p>Duplicates can typically happen in edge cases (e.g. on reconnections, network interruptions, etc) when the client has no way of knowing if last sent data was delivered. In these cases the client will usually choose to re-send the data to guarantee the delivery which in turn may result in duplicate data on the server side.</p> <p>To avoid having duplicates the client and the server could track sent and delivered items using uniquely identifying ids. The exact mechanism for tracking the ids and performing data de-duplication may be defined at the layer above the protocol layer and is outside the scope of this document.</p> <p>For this reason we have slightly relaxed requirements and consider duplicate data acceptable in rare cases.</p> <p>Note: this protocol is concerned with reliability of delivery between one pair of client/server nodes and aims to ensure that no data is lost in-transit between the client and the server. Many telemetry collection systems have multiple nodes that the data must travel across until reaching the final destination (e.g. application -&gt; agent -&gt; collector -&gt; backend). End-to-end delivery guarantees in such systems is outside of the scope for this document. The acknowledgements described in this protocol happen between a single client/server pair and do not span multiple nodes in multi-hop delivery paths.</p>"},{"location":"docs/specs/otel/protocol/requirements/#throughput","title":"Throughput","text":"<p>The protocol must ensure high throughput in high latency networks when the client and the server are not in the same data center.</p> <p>This requirement may rule out half-duplex protocols. The throughput of half-duplex protocols is highly dependent on network roundtrip time and request size. To achieve good throughput request sizes may be too large to be practical.</p>"},{"location":"docs/specs/otel/protocol/requirements/#compression","title":"Compression","text":"<p>The protocol must achieve high compression ratios for telemetry data. The protocol design must consider batching of telemetry data and grouping of similar data (both can help to achieve better compression using common compression algorithms).</p>"},{"location":"docs/specs/otel/protocol/requirements/#encryption","title":"Encryption","text":"<p>Industry standard encryption (e.g. TLS/HTTPS) must be supported.</p>"},{"location":"docs/specs/otel/protocol/requirements/#backpressure-signalling-and-throttling","title":"Backpressure Signalling and Throttling","text":"<p>The protocol must allow backpressure signalling.</p> <p>If the server is unable to keep up with the pace of data it receives from the client then it must be able to signal that fact to the client. The client may then throttle itself to avoid overwhelming the server.</p> <p>If the underlying transport is a stream that has its own flow control mechanism then the backpressure could be applied by delaying the reading of data from the server\u2019s endpoint which could then be signalled to the client via underlying flow-control. However this approach makes it difficult for the client to distinguish server overloading from network delays (due to e.g. network losses). Such distinction is important for observability reasons. Because of this it is required for the protocol to allow to explicitly and clearly signal backpressure from the server to the client without relying on implicit signalling using underlying flow-control mechanisms.</p> <p>The backpressure signal should include a hint to the client about desirable reduced rate of data.</p>"},{"location":"docs/specs/otel/protocol/requirements/#serialization-performance","title":"Serialization Performance","text":"<p>The protocol must have fast data serialization and deserialization characteristics.</p> <p>Ideally it must also support very fast pass-through mode (when no modifications to the data are needed), fast \u201caugmenting\u201d or \u201ctagging\u201d of data and partial inspection of data (e.g. check for presence of specific tag). These requirements help to create fast Agents and Collectors.</p>"},{"location":"docs/specs/otel/protocol/requirements/#memory-usage-profile","title":"Memory Usage Profile","text":"<p>The protocol must impose minimal pressure on memory manager, including pass-through scenarios, when deserialized data is short-lived and must be serialized as-is shortly after and when such short-lived data is created and discarded at high frequency (think telemetry data forwarders).</p> <p>The implementation of telemetry protocol must aim to minimize the number of memory allocations and dealocations performed during serialization and deserialization and aim to minimize the pressure on Garbage Collection (for GC languages).</p>"},{"location":"docs/specs/otel/protocol/requirements/#level-7-load-balancer-friendly","title":"Level 7 Load Balancer Friendly","text":"<p>The protocol must allow Level 7 load balancers such as Envoy to re-balance the traffic for each batch of telemetry data. The traffic should not get pinned by a load balancer to one server for the entire duration of telemetry data sending, thus potentially leading to imbalanced load of servers located behind the load balancer.</p>"},{"location":"docs/specs/otel/protocol/requirements/#backwards-compatibility","title":"Backwards Compatibility","text":"<p>The protocol should be possible to evolve over time. It should be possible for nodes that implement different versions of OpenTelemetry protocol to interoperate (while possibly regressing to the lowest common denominator from functional perspective).</p>"},{"location":"docs/specs/otel/protocol/requirements/#general-requirements","title":"General Requirements","text":"<p>The protocol must use well-known, mature encoding and transport mechanisms with ubiquitous availability of implementations in wide selection of languages that are supported by OpenTelemetry.</p>"},{"location":"docs/specs/otel/resource/","title":"\u8d44\u6e90","text":""},{"location":"docs/specs/otel/resource/sdk/","title":"Resource SDK","text":"<p>Status: Stable</p> <p>A Resource is an immutable representation of the entity producing telemetry as Attributes. For example, a process producing telemetry that is running in a container on Kubernetes has a Pod name, it is in a namespace and possibly is part of a Deployment which also has a name. All three of these attributes can be included in the <code>Resource</code>. Note that there are certain \"standard attributes\" that have prescribed meanings.</p> <p>The primary purpose of resources as a first-class concept in the SDK is decoupling of discovery of resource information from exporters. This allows for independent development and easy customization for users that need to integrate with closed source environments. The SDK MUST allow for creation of <code>Resources</code> and for associating them with telemetry.</p> <p>When used with distributed tracing, a resource can be associated with the TracerProvider when the TracerProvider is created. That association cannot be changed later. When associated with a <code>TracerProvider</code>, all <code>Span</code>s produced by any <code>Tracer</code> from the provider MUST be associated with this <code>Resource</code>.</p> <p>Analogous to distributed tracing, when used with metrics, a resource can be associated with a <code>MeterProvider</code>. When associated with a <code>MeterProvider</code>, all metrics produced by any <code>Meter</code> from the provider will be associated with this <code>Resource</code>.</p>"},{"location":"docs/specs/otel/resource/sdk/#sdk-provided-resource-attributes","title":"SDK-provided resource attributes","text":"<p>The SDK MUST provide access to a Resource with at least the attributes listed at Semantic Attributes with SDK-provided Default Value. This resource MUST be associated with a <code>TracerProvider</code> or <code>MeterProvider</code> if another resource was not explicitly specified.</p> <p>Note: This means that it is possible to create and associate a resource that does not have all or any of the SDK-provided attributes present. However, that does not happen by default. If a user wants to combine custom attributes with the default resource, they can use <code>Merge</code> with their custom resource or specify their attributes by implementing Custom resource detectors instead of explicitly associating a resource.</p>"},{"location":"docs/specs/otel/resource/sdk/#resource-creation","title":"Resource creation","text":"<p>The SDK must support two ways to instantiate new resources. Those are:</p>"},{"location":"docs/specs/otel/resource/sdk/#create","title":"Create","text":"<p>The interface MUST provide a way to create a new resource, from <code>Attributes</code>. Examples include a factory method or a constructor for a resource object. A factory method is recommended to enable support for cached objects.</p> <p>Required parameters:</p> <ul> <li><code>Attributes</code></li> <li>[since 1.4.0] <code>schema_url</code> (optional): Specifies the Schema URL that should be   recorded in the emitted resource. If the <code>schema_url</code> parameter is unspecified   then the created resource will have an empty Schema URL.</li> </ul>"},{"location":"docs/specs/otel/resource/sdk/#merge","title":"Merge","text":"<p>The interface MUST provide a way for an old resource and an updating resource to be merged into a new resource.</p> <p>Note: This is intended to be utilized for merging of resources whose attributes come from different sources, such as environment variables, or metadata extracted from the host or container.</p> <p>The resulting resource MUST have all attributes that are on any of the two input resources. If a key exists on both the old and updating resource, the value of the updating resource MUST be picked (even if the updated value is empty).</p> <p>The resulting resource will have the Schema URL calculated as follows:</p> <ul> <li>If the old resource's Schema URL is empty then the resulting resource's Schema   URL will be set to the Schema URL of the updating resource,</li> <li>Else if the updating resource's Schema URL is empty then the resulting   resource's Schema URL will be set to the Schema URL of the old resource,</li> <li>Else if the Schema URLs of the old and updating resources are the same then   that will be the Schema URL of the resulting resource,</li> <li>Else this is a merging error (this is the case when the Schema URL of the old   and updating resources are not empty and are different). The resulting resource is   undefined, and its contents are implementation-specific.</li> </ul> <p>Required parameters:</p> <ul> <li>the old resource</li> <li>the updating resource whose attributes take precedence</li> </ul>"},{"location":"docs/specs/otel/resource/sdk/#the-empty-resource","title":"The empty resource","text":"<p>It is recommended, but not required, to provide a way to quickly create an empty resource.</p>"},{"location":"docs/specs/otel/resource/sdk/#detecting-resource-information-from-the-environment","title":"Detecting resource information from the environment","text":"<p>Custom resource detectors related to generic platforms (e.g. Docker, Kubernetes) or vendor specific environments (e.g. EKS, AKS, GKE) MUST be implemented as packages separate from the SDK.</p> <p>Resource detector packages MUST provide a method that returns a resource. This can then be associated with <code>TracerProvider</code> or <code>MeterProvider</code> instances as described above.</p> <p>Resource detector packages MAY detect resource information from multiple possible sources and merge the result using the <code>Merge</code> operation described above.</p> <p>Resource detection logic is expected to complete quickly since this code will be run during application initialization. Errors should be handled as specified in the Error Handling principles. Note the failure to detect any resource information MUST NOT be considered an error, whereas an error that occurs during an attempt to detect resource information SHOULD be considered an error.</p> <p>Resource detectors that populate resource attributes according to OpenTelemetry semantic conventions MUST ensure that the resource has a Schema URL set to a value that matches the semantic conventions. Empty Schema URL SHOULD be used if the detector does not populate the resource with any known attributes that have a semantic convention or if the detector does not know what attributes it will populate (e.g. the detector that reads the attributes from environment values will not know what Schema URL to use). If multiple detectors are combined and the detectors use different non-empty Schema URL it MUST be an error since it is impossible to merge such resources. The resulting resource is undefined, and its contents are implementation specific.</p>"},{"location":"docs/specs/otel/resource/sdk/#specifying-resource-information-via-an-environment-variable","title":"Specifying resource information via an environment variable","text":"<p>The SDK MUST extract information from the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable and merge this, as the secondary resource, with any resource information provided by the user, i.e. the user provided resource information has higher priority.</p> <p>The <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable will contain of a list of key value pairs, and these are expected to be represented in a format matching to the W3C Baggage, except that additional semi-colon delimited metadata is not supported, i.e.: <code>key1=value1,key2=value2</code>. All attribute values MUST be considered strings and characters outside the <code>baggage-octet</code> range MUST be percent-encoded.</p>"},{"location":"docs/specs/otel/resource/sdk/#resource-operations","title":"Resource operations","text":"<p>Resources are immutable. Thus, in addition to resource creation, only the following operations should be provided:</p>"},{"location":"docs/specs/otel/resource/sdk/#retrieve-attributes","title":"Retrieve attributes","text":"<p>The SDK should provide a way to retrieve a read only collection of attributes associated with a resource.</p> <p>There is no need to guarantee the order of the attributes.</p> <p>The most common operation when retrieving attributes is to enumerate over them. As such, it is recommended to optimize the resulting collection for fast enumeration over other considerations such as a way to quickly retrieve a value for a attribute with a specific key.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/","title":"\u8d44\u6e90\u8bed\u4e49\u7ea6\u5b9a","text":"<p>Note</p> <p>\u8bed\u4e49\u7ea6\u5b9a\u6b63\u5728\u8f6c\u79fb\u5230\u4e00\u4e2a\u65b0\u7684\u4f4d\u7f6e.</p> <p>\u4e0d\u5141\u8bb8\u5bf9\u672c\u6587\u6863\u8fdb\u884c\u4efb\u4f55\u4fee\u6539\u3002</p> <p>Status: Mixed</p> <p>\u672c\u6587\u6863\u5b9a\u4e49\u4e86\u8d44\u6e90\u7684\u6807\u51c6\u5c5e\u6027\u3002\u8fd9\u4e9b\u5c5e\u6027\u901a\u5e38\u5728\u8d44\u6e90\u4e2d\u4f7f\u7528\uff0c\u4e5f\u5efa\u8bae\u5728\u9700\u8981 \u4ee5\u4e00\u81f4\u7684\u65b9\u5f0f\u63cf\u8ff0\u8d44\u6e90\u7684\u4efb\u4f55\u5176\u4ed6\u5730\u65b9\u4f7f\u7528\u3002\u8fd9\u4e9b\u5c5e\u6027\u7684\u5927\u90e8\u5206\u90fd\u7ee7\u627f \u81eaOpenCensus \u8d44\u6e90\u6807\u51c6.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/#_2","title":"\u5f85\u529e\u4e8b\u9879","text":"<ul> <li>\u589e\u52a0\u66f4\u591a\u8ba1\u7b97\u5355\u5143:AppEngine \u5355\u5143\u7b49\u3002</li> <li>\u6dfb\u52a0 Web \u6d4f\u89c8\u5668\u3002</li> <li>\u51b3\u5b9a\u662f\u5426\u53ea\u4f7f\u7528\u5c0f\u5199\u5b57\u7b26\u4e32\u3002</li> <li>\u8003\u8651\u4e3a\u6bcf\u4e2a\u5c5e\u6027\u548c\u5c5e\u6027\u7ec4\u5408\u6dfb\u52a0\u53ef\u9009/\u5fc5\u9700\u5c5e\u6027(\u4f8b\u5982\uff0c\u5f53\u63d0\u4f9b k8 \u8d44\u6e90\u65f6\uff0c\u53ef\u80fd\u9700\u8981\u6240\u6709   k8)\u3002</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#document-conventions","title":"Document Conventions","text":"<p>Status: Stable</p> <p>Attributes are grouped logically by the type of the concept that they described. Attributes in the same group have a common prefix that ends with a dot. For example all attributes that describe Kubernetes properties start with \"k8s.\"</p> <p>See Attribute Requirement Levels for details on when attributes should be included.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/#attributes-with-special-handling","title":"Attributes with Special Handling","text":"<p>Status: Stable</p> <p>Given their significance some resource attributes are treated specifically as described below.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/#semantic-attributes-with-dedicated-environment-variable","title":"Semantic Attributes with Dedicated Environment Variable","text":"<p>These are the attributes which MAY be configurable via a dedicated environment variable as specified in OpenTelemetry Environment Variable Specification:</p> <ul> <li><code>service.name</code></li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#semantic-attributes-with-sdk-provided-default-value","title":"Semantic Attributes with SDK-provided Default Value","text":"<p>These are the attributes which MUST be provided by the SDK as specified in the Resource SDK specification:</p> <ul> <li><code>service.name</code></li> <li><code>telemetry.sdk</code> group</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#service","title":"Service","text":"<p>Status: Stable</p> <p>type: <code>service</code></p> <p>Description: A service instance.</p> Attribute Type Description Examples Requirement Level <code>service.name</code> string Logical name of the service. [1] <code>shoppingcart</code> Required <p>[1]: MUST be the same for all instances of horizontally scaled services. If the value was not specified, SDKs MUST fallback to <code>unknown_service:</code> concatenated with <code>process.executable.name</code>, e.g. <code>unknown_service:bash</code>. If <code>process.executable.name</code> is not available, the value MUST be set to <code>unknown_service</code>.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/#service-experimental","title":"Service (Experimental)","text":"<p>Status: Experimental</p> <p>type: <code>service</code></p> <p>Description: Additions to service instance.</p> Attribute Type Description Examples Requirement Level <code>service.namespace</code> string A namespace for <code>service.name</code>. [1] <code>Shop</code> Recommended <code>service.instance.id</code> string The string ID of the service instance. [2] <code>my-k8s-pod-deployment-1</code>; <code>627cc493-f310-47de-96bd-71410b7dec09</code> Recommended <code>service.version</code> string The version string of the service API or implementation. <code>2.0.0</code> Recommended <p>[1]: A string value having a meaning that helps to distinguish a group of services, for example the team name that owns a group of services. <code>service.name</code> is expected to be unique within the same namespace. If <code>service.namespace</code> is not specified in the Resource then <code>service.name</code> is expected to be unique for all services that have no explicit namespace defined (so the empty/unspecified namespace is simply one more valid namespace). Zero-length namespace string is assumed equal to unspecified namespace.</p> <p>[2]: MUST be unique for each instance of the same <code>service.namespace,service.name</code> pair (in other words <code>service.namespace,service.name,service.instance.id</code> triplet MUST be globally unique). The ID helps to distinguish instances of the same service that exist at the same time (e.g. instances of a horizontally scaled service). It is preferable for the ID to be persistent and stay the same for the lifetime of the service instance, however it is acceptable that the ID is ephemeral and changes during important lifetime events for the service (e.g. service restarts). If the service has no inherent unique ID that can be used as the value of this attribute it is recommended to generate a random Version 1 or Version 4 RFC 4122 UUID (services aiming for reproducible UUIDs may also use Version 5, see RFC 4122 for more recommendations).</p> <p>Note: <code>service.namespace</code> and <code>service.name</code> are not intended to be concatenated for the purpose of forming a single globally unique name for the service. For example the following 2 sets of attributes actually describe 2 different services (despite the fact that the concatenation would result in the same string):</p> <pre><code># Resource attributes that describes a service.\nnamespace = Company.Shop\nservice.name = shoppingcart\n</code></pre> <pre><code># Another set of resource attributes that describe a different service.\nnamespace = Company\nservice.name = Shop.shoppingcart\n</code></pre>"},{"location":"docs/specs/otel/resource/semantic_conventions/#telemetry-sdk","title":"Telemetry SDK","text":"<p>Status: Stable</p> <p>type: <code>telemetry.sdk</code></p> <p>Description: The telemetry SDK used to capture data recorded by the instrumentation libraries.</p> Attribute Type Description Examples Requirement Level <code>telemetry.sdk.name</code> string The name of the telemetry SDK as defined above. [1] <code>opentelemetry</code> Required <code>telemetry.sdk.language</code> string The language of the telemetry SDK. <code>cpp</code> Required <code>telemetry.sdk.version</code> string The version string of the telemetry SDK. <code>1.2.3</code> Required <p>[1]: The OpenTelemetry SDK MUST set the <code>telemetry.sdk.name</code> attribute to <code>opentelemetry</code>. If another SDK, like a fork or a vendor-provided implementation, is used, this SDK MUST set the <code>telemetry.sdk.name</code> attribute to the fully-qualified class or module name of this SDK's main entry point or another suitable identifier depending on the language. The identifier <code>opentelemetry</code> is reserved and MUST NOT be used in this case. All custom identifiers SHOULD be stable across different versions of an implementation.</p> <p><code>telemetry.sdk.language</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>cpp</code> cpp <code>dotnet</code> dotnet <code>erlang</code> erlang <code>go</code> go <code>java</code> java <code>nodejs</code> nodejs <code>php</code> php <code>python</code> python <code>ruby</code> ruby <code>rust</code> rust <code>swift</code> swift <code>webjs</code> webjs"},{"location":"docs/specs/otel/resource/semantic_conventions/#telemetry-sdk-experimental","title":"Telemetry SDK (Experimental)","text":"<p>Status: Experimental</p> <p>type: <code>telemetry.sdk</code></p> <p>Description: Additions to the telemetry SDK.</p> Attribute Type Description Examples Requirement Level <code>telemetry.auto.version</code> string The version string of the auto instrumentation agent, if used. <code>1.2.3</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/#compute-unit","title":"Compute Unit","text":"<p>Status: Experimental</p> <p>Attributes defining a compute unit (e.g. Container, Process, Function as a Service):</p> <ul> <li>Container</li> <li>Function as a Service</li> <li>Process</li> <li>Web engine</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#compute-instance","title":"Compute Instance","text":"<p>Status: Experimental</p> <p>Attributes defining a computing instance (e.g. host):</p> <ul> <li>Host</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#environment","title":"Environment","text":"<p>Status: Experimental</p> <p>Attributes defining a running environment (e.g. Operating System, Cloud, Data Center, Deployment Service):</p> <ul> <li>Operating System</li> <li>Device</li> <li>Cloud</li> <li>Deployment:</li> <li>Deployment Environment</li> <li>Kubernetes</li> <li>Browser</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/#version-attributes","title":"Version attributes","text":"<p>Status: Stable</p> <p>Version attributes, such as <code>service.version</code>, are values of type <code>string</code>. They are the exact version used to identify an artifact. This may be a semantic version, e.g., <code>1.2.3</code>, git hash, e.g., <code>8ae73a</code>, or an arbitrary version string, e.g., <code>0.1.2.20210101</code>, whatever was used when building the artifact.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/#cloud-provider-specific-attributes","title":"Cloud-Provider-Specific Attributes","text":"<p>Status: Experimental</p> <p>Attributes that are only applicable to resources from a specific cloud provider. Currently, these resources can only be defined for providers listed as a valid <code>cloud.provider</code> in Cloud and below. Provider-specific attributes all reside in the <code>cloud_provider</code> directory. Valid cloud providers are:</p> <ul> <li>Alibaba Cloud (<code>alibaba_cloud</code>)</li> <li>Amazon Web Services   (<code>aws</code>)</li> <li>Google Cloud Platform   (<code>gcp</code>)</li> <li>Microsoft Azure (<code>azure</code>)</li> <li>Tencent Cloud (<code>tencent_cloud</code>)</li> <li>Heroku dyno</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/browser/","title":"\u6d4f\u89c8\u5668","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>browser</code></p> <p>Description: The web browser in which the application represented by the resource is running. The <code>browser.*</code> attributes MUST be used only for resources that represent applications running in a web browser (regardless of whether running on a mobile or desktop device).</p> <p>All of these attributes can be provided by the user agent itself in the form of an HTTP header (e.g. Sec-CH-UA, Sec-CH-Platform, User-Agent). However, the headers could be removed by proxy servers, and are tied to calls from individual clients. In order to support batching through services like the Collector and to prevent loss of data (e.g. due to proxy servers removing headers), these attributes should be used when possible.</p> Attribute Type Description Examples Requirement Level <code>browser.brands</code> string[] Array of brand name and version separated by a space [1] <code>[ Not A;Brand 99, Chromium 99, Chrome 99]</code> Recommended <code>browser.platform</code> string The platform on which the browser is running [2] <code>Windows</code>; <code>macOS</code>; <code>Android</code> Recommended <code>browser.mobile</code> boolean A boolean that is true if the browser is running on a mobile device [3] Recommended <code>browser.language</code> string Preferred language of the user using the browser [4] <code>en</code>; <code>en-US</code>; <code>fr</code>; <code>fr-FR</code> Recommended <code>user_agent.original</code> string Full user-agent string provided by the browser [5] <code>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36</code> Recommended <p>[1]: This value is intended to be taken from the UA client hints API (<code>navigator.userAgentData.brands</code>).</p> <p>[2]: This value is intended to be taken from the UA client hints API (<code>navigator.userAgentData.platform</code>). If unavailable, the legacy <code>navigator.platform</code> API SHOULD NOT be used instead and this attribute SHOULD be left unset in order for the values to be consistent. The list of possible values is defined in the W3C User-Agent Client Hints specification. Note that some (but not all) of these values can overlap with values in the <code>os.type</code> and <code>os.name</code> attributes. However, for consistency, the values in the <code>browser.platform</code> attribute should capture the exact value that the user agent provides.</p> <p>[3]: This value is intended to be taken from the UA client hints API (<code>navigator.userAgentData.mobile</code>). If unavailable, this attribute SHOULD be left unset.</p> <p>[4]: This value is intended to be taken from the Navigator API <code>navigator.language</code>.</p> <p>[5]: The user-agent value SHOULD be provided only from browsers that do not have a mechanism to retrieve brands and platform individually from the User-Agent Client Hints API. To retrieve the value, the legacy <code>navigator.userAgent</code> API can be used.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud/","title":"\u4e91","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>cloud</code></p> <p>Description: A cloud infrastructure (e.g. GCP, Azure, AWS).</p> Attribute Type Description Examples Requirement Level <code>cloud.provider</code> string Name of the cloud provider. <code>alibaba_cloud</code> Recommended <code>cloud.account.id</code> string The cloud account ID the resource is assigned to. <code>111111111111</code>; <code>opentelemetry</code> Recommended <code>cloud.region</code> string The geographical region the resource is running. [1] <code>us-central1</code>; <code>us-east-1</code> Recommended <code>cloud.resource_id</code> string Cloud provider-specific native identifier of the monitored cloud resource (e.g. an ARN on AWS, a fully qualified resource ID on Azure, a full resource name on GCP) [2] <code>arn:aws:lambda:REGION:ACCOUNT_ID:function:my-function</code>; <code>//run.googleapis.com/projects/PROJECT_ID/locations/LOCATION_ID/services/SERVICE_ID</code>; <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code> Recommended <code>cloud.availability_zone</code> string Cloud regions often have multiple, isolated locations known as zones to increase availability. Availability zone represents the zone where the resource is running. [3] <code>us-east-1c</code> Recommended <code>cloud.platform</code> string The cloud platform in use. [4] <code>alibaba_cloud_ecs</code> Recommended <p>[1]: Refer to your provider's docs to see the available regions, for example Alibaba Cloud regions, AWS regions, Azure regions, Google Cloud regions, or Tencent Cloud regions.</p> <p>[2]: On some cloud providers, it may not be possible to determine the full ID at startup, so it may be necessary to set <code>cloud.resource_id</code> as a span attribute instead.</p> <p>The exact value to use for <code>cloud.resource_id</code> depends on the cloud provider. The following well-known definitions MUST be used if you set this attribute and they apply:</p> <ul> <li>AWS Lambda: The function   ARN.   Take care not to use the \"invoked ARN\" directly but replace any   alias suffix   with the resolved function version, as the same runtime instance may be   invokable with multiple different aliases.</li> <li>GCP: The   URI of the resource</li> <li>Azure: The   Fully Qualified Resource ID   of the invoked function, not the function app, having the form   <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code>.   This means that a span attribute MUST be used, as an Azure function app can   host multiple functions that would usually share a TracerProvider.</li> </ul> <p>[3]: Availability zones are called \"zones\" on Alibaba Cloud and Google Cloud.</p> <p>[4]: The prefix of the service SHOULD match the one specified in <code>cloud.provider</code>.</p> <p><code>cloud.provider</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>alibaba_cloud</code> Alibaba Cloud <code>aws</code> Amazon Web Services <code>azure</code> Microsoft Azure <code>gcp</code> Google Cloud Platform <code>heroku</code> Heroku Platform as a Service <code>ibm_cloud</code> IBM Cloud <code>tencent_cloud</code> Tencent Cloud <p><code>cloud.platform</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>alibaba_cloud_ecs</code> Alibaba Cloud Elastic Compute Service <code>alibaba_cloud_fc</code> Alibaba Cloud Function Compute <code>alibaba_cloud_openshift</code> Red Hat OpenShift on Alibaba Cloud <code>aws_ec2</code> AWS Elastic Compute Cloud <code>aws_ecs</code> AWS Elastic Container Service <code>aws_eks</code> AWS Elastic Kubernetes Service <code>aws_lambda</code> AWS Lambda <code>aws_elastic_beanstalk</code> AWS Elastic Beanstalk <code>aws_app_runner</code> AWS App Runner <code>aws_openshift</code> Red Hat OpenShift on AWS (ROSA) <code>azure_vm</code> Azure Virtual Machines <code>azure_container_instances</code> Azure Container Instances <code>azure_aks</code> Azure Kubernetes Service <code>azure_functions</code> Azure Functions <code>azure_app_service</code> Azure App Service <code>azure_openshift</code> Azure Red Hat OpenShift <code>gcp_compute_engine</code> Google Cloud Compute Engine (GCE) <code>gcp_cloud_run</code> Google Cloud Run <code>gcp_kubernetes_engine</code> Google Cloud Kubernetes Engine (GKE) <code>gcp_cloud_functions</code> Google Cloud Functions (GCF) <code>gcp_app_engine</code> Google Cloud App Engine (GAE) <code>gcp_openshift</code> Red Hat OpenShift on Google Cloud <code>ibm_cloud_openshift</code> Red Hat OpenShift on IBM Cloud <code>tencent_cloud_cvm</code> Tencent Cloud Cloud Virtual Machine (CVM) <code>tencent_cloud_eks</code> Tencent Cloud Elastic Kubernetes Service (EKS) <code>tencent_cloud_scf</code> Tencent Cloud Serverless Cloud Function (SCF)"},{"location":"docs/specs/otel/resource/semantic_conventions/container/","title":"\u5bb9\u5668","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>container</code></p> <p>Description: A container instance.</p> Attribute Type Description Examples Requirement Level <code>container.name</code> string Container name used by container runtime. <code>opentelemetry-autoconf</code> Recommended <code>container.id</code> string Container ID. Usually a UUID, as for example used to identify Docker containers. The UUID might be abbreviated. <code>a3bf90e006b2</code> Recommended <code>container.runtime</code> string The container runtime managing this container. <code>docker</code>; <code>containerd</code>; <code>rkt</code> Recommended <code>container.image.name</code> string Name of the image the container was built on. <code>gcr.io/opentelemetry/operator</code> Recommended <code>container.image.tag</code> string Container image tag. <code>0.1</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/deployment_environment/","title":"\u5f00\u53d1\u73af\u5883","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>deployment</code></p> <p>Description: The software deployment.</p> Attribute Type Description Examples Requirement Level <code>deployment.environment</code> string Name of the deployment environment (aka deployment tier). <code>staging</code>; <code>production</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/device/","title":"\u786c\u4ef6","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>device</code></p> <p>Description: The device on which the process represented by this resource is running.</p> Attribute Type Description Examples Requirement Level <code>device.id</code> string A unique identifier representing the device [1] <code>2ab2916d-a51f-4ac8-80ee-45ac31a28092</code> Recommended <code>device.model.identifier</code> string The model identifier for the device [2] <code>iPhone3,4</code>; <code>SM-G920F</code> Recommended <code>device.model.name</code> string The marketing name for the device model [3] <code>iPhone 6s Plus</code>; <code>Samsung Galaxy S6</code> Recommended <code>device.manufacturer</code> string The name of the device manufacturer [4] <code>Apple</code>; <code>Samsung</code> Recommended <p>[1]: The device identifier MUST only be defined using the values outlined below. This value is not an advertising identifier and MUST NOT be used as such. On iOS (Swift or Objective-C), this value MUST be equal to the vendor identifier. On Android (Java or Kotlin), this value MUST be equal to the Firebase Installation ID or a globally unique UUID which is persisted across sessions in your application. More information can be found here on best practices and exact implementation details. Caution should be taken when storing personal data or anything which can identify a user. GDPR and data protection laws may apply, ensure you do your own due diligence.</p> <p>[2]: It's recommended this value represents a machine readable version of the model identifier rather than the market or consumer-friendly name of the device.</p> <p>[3]: It's recommended this value represents a human readable version of the device model rather than a machine readable alternative.</p> <p>[4]: The Android OS provides this field via Build. iOS apps SHOULD hardcode the value <code>Apple</code>.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/faas/","title":"\u51fd\u6570\u5373\u670d\u52a1","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>faas</code></p> <p>Description: A \"function as a service\" aka \"serverless function\" instance.</p> <p>See also:</p> <ul> <li>The   Trace semantic conventions for FaaS</li> <li>The Cloud resource conventions</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/faas/#faas-resource-attributes","title":"FaaS resource attributes","text":"Attribute Type Description Examples Requirement Level <code>faas.name</code> string The name of the single function that this runtime instance executes. [1] <code>my-function</code>; <code>myazurefunctionapp/some-function-name</code> Required <code>faas.version</code> string The immutable version of the function being executed. [2] <code>26</code>; <code>pinkfroid-00002</code> Recommended <code>faas.instance</code> string The execution environment ID as a string, that will be potentially reused for other invocations to the same function/function version. [3] <code>2021/06/28/[$LATEST]2f399eb14537447da05ab2a2e39309de</code> Recommended <code>faas.max_memory</code> int The amount of memory available to the serverless function converted to Bytes. [4] <code>134217728</code> Recommended <code>cloud.resource_id</code> string Cloud provider-specific native identifier of the monitored cloud resource (e.g. an ARN on AWS, a fully qualified resource ID on Azure, a full resource name on GCP) [5] <code>arn:aws:lambda:REGION:ACCOUNT_ID:function:my-function</code>; <code>//run.googleapis.com/projects/PROJECT_ID/locations/LOCATION_ID/services/SERVICE_ID</code>; <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code> Recommended <p>[1]: This is the name of the function as configured/deployed on the FaaS platform and is usually different from the name of the callback function (which may be stored in the <code>code.namespace</code>/<code>code.function</code> span attributes).</p> <p>For some cloud providers, the above definition is ambiguous. The following definition of function name MUST be used for this attribute (and consequently the span name) for the listed cloud providers/products:</p> <ul> <li>Azure: The full name <code>&lt;FUNCAPP&gt;/&lt;FUNC&gt;</code>, i.e., function app name followed   by a forward slash followed by the function name (this form can also be seen   in the resource JSON for the function). This means that a span attribute MUST   be used, as an Azure function app can host multiple functions that would   usually share a TracerProvider (see also the <code>cloud.resource_id</code> attribute).</li> </ul> <p>[2]: Depending on the cloud provider and platform, use:</p> <ul> <li>AWS Lambda: The   function version   (an integer represented as a decimal string).</li> <li>Google Cloud Run (Services): The   revision (i.e., the   function name plus the revision suffix).</li> <li>Google Cloud Functions: The value of the   <code>K_REVISION</code> environment variable.</li> <li>Azure Functions: Not applicable. Do not set this attribute.</li> </ul> <p>[3]: * AWS Lambda: Use the (full) log stream name.</p> <p>[4]: It's recommended to set this attribute since e.g. too little memory can easily stop a Java AWS Lambda function from working correctly. On AWS Lambda, the environment variable <code>AWS_LAMBDA_FUNCTION_MEMORY_SIZE</code> provides this information (which must be multiplied by 1,048,576).</p> <p>[5]: On some cloud providers, it may not be possible to determine the full ID at startup, so it may be necessary to set <code>cloud.resource_id</code> as a span attribute instead.</p> <p>The exact value to use for <code>cloud.resource_id</code> depends on the cloud provider. The following well-known definitions MUST be used if you set this attribute and they apply:</p> <ul> <li>AWS Lambda: The function   ARN.   Take care not to use the \"invoked ARN\" directly but replace any   alias suffix   with the resolved function version, as the same runtime instance may be   invokable with multiple different aliases.</li> <li>GCP: The   URI of the resource</li> <li>Azure: The Fully Qualified Resource ID of the invoked function, not the function app, having the form <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code>. This means that a span attribute MUST be used, as an Azure function app can host multiple functions that would usually share a TracerProvider.</li> </ul> <p>Note: The resource attribute <code>faas.instance</code> differs from the span attribute <code>faas.invocation_id</code>. For more information see the Semantic conventions for FaaS spans.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/faas/#using-span-attributes-instead-of-resource-attributes","title":"Using span attributes instead of resource attributes","text":"<p>There are cases where a FaaS resource attribute is better applied as a span attribute instead. See the FaaS trace conventions for more.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/host/","title":"\u4e3b\u673a","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>host</code></p> <p>Description: A host is defined as a computing instance. For example, physical servers, virtual machines, switches or disk array.</p> Attribute Type Description Examples Requirement Level <code>host.id</code> string Unique host ID. For Cloud, this must be the instance_id assigned by the cloud provider. For non-containerized systems, this should be the <code>machine-id</code>. See the table below for the sources to use to determine the <code>machine-id</code> based on operating system. <code>fdbf79e8af94cb7f9e8df36789187052</code> Recommended <code>host.name</code> string Name of the host. On Unix systems, it may contain what the hostname command returns, or the fully qualified hostname, or another name specified by the user. <code>opentelemetry-test</code> Recommended <code>host.type</code> string Type of host. For Cloud, this must be the machine type. <code>n1-standard-1</code> Recommended <code>host.arch</code> string The CPU architecture the host system is running on. <code>amd64</code> Recommended <code>host.image.name</code> string Name of the VM image or OS install the host was instantiated from. <code>infra-ami-eks-worker-node-7d4ec78312</code>; <code>CentOS-8-x86_64-1905</code> Recommended <code>host.image.id</code> string VM image ID or host OS image ID. For Cloud, this value is from the provider. <code>ami-07b06b442921831e5</code> Recommended <code>host.image.version</code> string The version string of the VM image or host OS as defined in Version Attributes. <code>0.1</code> Recommended <p><code>host.arch</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>amd64</code> AMD64 <code>arm32</code> ARM32 <code>arm64</code> ARM64 <code>ia64</code> Itanium <code>ppc32</code> 32-bit PowerPC <code>ppc64</code> 64-bit PowerPC <code>s390x</code> IBM z/Architecture <code>x86</code> 32-bit x86"},{"location":"docs/specs/otel/resource/semantic_conventions/host/#collecting-hostid-from-non-containerized-systems","title":"Collecting host.id from non-containerized systems","text":""},{"location":"docs/specs/otel/resource/semantic_conventions/host/#non-privileged-machine-id-lookup","title":"Non-privileged Machine ID Lookup","text":"<p>When collecting <code>host.id</code> for non-containerized systems non-privileged lookups of the machine id are preferred. SDK detector implementations MUST use the sources listed below to obtain the machine id.</p> OS Primary Fallback Linux contents of <code>/etc/machine-id</code> contents of <code>/var/lib/dbus/machine-id</code> BSD contents of <code>/etc/hostid</code> output of <code>kenv -q smbios.system.uuid</code> MacOS <code>IOPlatformUUID</code> line from the output of <code>ioreg -rd1 -c \"IOPlatformExpertDevice\"</code> - Windows <code>MachineGuid</code> from registry <code>HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Microsoft\\\\Cryptography</code> -"},{"location":"docs/specs/otel/resource/semantic_conventions/host/#privileged-machine-id-lookup","title":"Privileged Machine ID Lookup","text":"<p>The <code>host.id</code> can be looked up using privileged sources. For example, Linux systems can use the output of <code>dmidecode -t system</code>, <code>dmidecode -t baseboard</code>, <code>dmidecode -t chassis</code>, or read the corresponding data from the filesystem (e.g. <code>cat /sys/devices/virtual/dmi/id/product_id</code>, <code>cat /sys/devices/virtual/dmi/id/product_uuid</code>, etc), however, SDK resource detector implementations MUST not collect <code>host.id</code> from privileged sources. If privileged lookup of <code>host.id</code> is required, the value should be injected via the <code>OTEL_RESOURCE_ATTRIBUTES</code> environment variable.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/","title":"Kubernetes","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Useful resources to understand Kubernetes objects and metadata:</p> <ul> <li>Namespace</li> <li>Names and UIDs.</li> <li>Pods</li> <li>Controllers</li> </ul> <p>The \"name\" of a Kubernetes object is unique for that type of object within a \"namespace\" and only at a specific moment of time (names can be reused over time). The \"uid\" is unique across your whole cluster, and very likely across time. Because of this it is recommended to always set the UID for every Kubernetes object, but \"name\" is usually more user friendly so can be also set.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#cluster","title":"Cluster","text":"<p>type: <code>k8s.cluster</code></p> <p>Description: A Kubernetes Cluster.</p> Attribute Type Description Examples Requirement Level <code>k8s.cluster.name</code> string The name of the cluster. <code>opentelemetry-cluster</code> Recommended <code>k8s.cluster.uid</code> string A pseudo-ID for the cluster, set to the UID of the <code>kube-system</code> namespace. [1] <code>218fc5a9-a5f1-4b54-aa05-46717d0ab26d</code> Recommended <p>[1]: K8s does not have support for obtaining a cluster ID. If this is ever added, we will recommend collecting the <code>k8s.cluster.uid</code> through the official APIs. In the meantime, we are able to use the <code>uid</code> of the <code>kube-system</code> namespace as a proxy for cluster ID. Read on for the rationale.</p> <p>Every object created in a K8s cluster is assigned a distinct UID. The <code>kube-system</code> namespace is used by Kubernetes itself and will exist for the lifetime of the cluster. Using the <code>uid</code> of the <code>kube-system</code> namespace is a reasonable proxy for the K8s ClusterID as it will only change if the cluster is rebuilt. Furthermore, Kubernetes UIDs are UUIDs as standardized by ISO/IEC 9834-8 and ITU-T X.667. Which states:</p> <p>If generated according to one of the mechanisms defined in Rec.   ITU-T X.667 | ISO/IEC 9834-8, a UUID is either guaranteed to be   different from all other UUIDs generated before 3603 A.D., or is   extremely likely to be different (depending on the mechanism chosen).</p> <p>Therefore, UIDs between clusters should be extremely unlikely to conflict.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#node","title":"Node","text":"<p>type: <code>k8s.node</code></p> <p>Description: A Kubernetes Node.</p> Attribute Type Description Examples Requirement Level <code>k8s.node.name</code> string The name of the Node. <code>node-1</code> Recommended <code>k8s.node.uid</code> string The UID of the Node. <code>1eb3a0c6-0477-4080-a9cb-0cb7db65c6a2</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#namespace","title":"Namespace","text":"<p>Namespaces provide a scope for names. Names of objects need to be unique within a namespace, but not across namespaces.</p> <p>type: <code>k8s.namespace</code></p> <p>Description: A Kubernetes Namespace.</p> Attribute Type Description Examples Requirement Level <code>k8s.namespace.name</code> string The name of the namespace that the pod is running in. <code>default</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#pod","title":"Pod","text":"<p>The smallest and simplest Kubernetes object. A Pod represents a set of running containers on your cluster.</p> <p>type: <code>k8s.pod</code></p> <p>Description: A Kubernetes Pod object.</p> Attribute Type Description Examples Requirement Level <code>k8s.pod.uid</code> string The UID of the Pod. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.pod.name</code> string The name of the Pod. <code>opentelemetry-pod-autoconf</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#container","title":"Container","text":"<p>A container specification in a Pod template. This type is intended to be used to capture information such as name of a container in a Pod template which is different from the name of the running container.</p> <p>Note: This type is different from container, which corresponds to a running container.</p> <p>type: <code>k8s.container</code></p> <p>Description: A container in a PodTemplate.</p> Attribute Type Description Examples Requirement Level <code>k8s.container.name</code> string The name of the Container from Pod specification, must be unique within a Pod. Container runtime usually uses different globally unique name (<code>container.name</code>). <code>redis</code> Recommended <code>k8s.container.restart_count</code> int Number of times the container was restarted. This attribute can be used to identify a particular container (running or stopped) within a container spec. <code>0</code>; <code>2</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#replicaset","title":"ReplicaSet","text":"<p>A ReplicaSet\u2019s purpose is to maintain a stable set of replica Pods running at any given time.</p> <p>type: <code>k8s.replicaset</code></p> <p>Description: A Kubernetes ReplicaSet object.</p> Attribute Type Description Examples Requirement Level <code>k8s.replicaset.uid</code> string The UID of the ReplicaSet. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.replicaset.name</code> string The name of the ReplicaSet. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#deployment","title":"Deployment","text":"<p>An API object that manages a replicated application, typically by running Pods with no local state. Each replica is represented by a Pod, and the Pods are distributed among the nodes of a cluster.</p> <p>type: <code>k8s.deployment</code></p> <p>Description: A Kubernetes Deployment object.</p> Attribute Type Description Examples Requirement Level <code>k8s.deployment.uid</code> string The UID of the Deployment. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.deployment.name</code> string The name of the Deployment. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#statefulset","title":"StatefulSet","text":"<p>Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.</p> <p>type: <code>k8s.statefulset</code></p> <p>Description: A Kubernetes StatefulSet object.</p> Attribute Type Description Examples Requirement Level <code>k8s.statefulset.uid</code> string The UID of the StatefulSet. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.statefulset.name</code> string The name of the StatefulSet. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#daemonset","title":"DaemonSet","text":"<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod.</p> <p>type: <code>k8s.daemonset</code></p> <p>Description: A Kubernetes DaemonSet object.</p> Attribute Type Description Examples Requirement Level <code>k8s.daemonset.uid</code> string The UID of the DaemonSet. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.daemonset.name</code> string The name of the DaemonSet. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#job","title":"Job","text":"<p>A Job creates one or more Pods and ensures that a specified number of them successfully terminate.</p> <p>type: <code>k8s.job</code></p> <p>Description: A Kubernetes Job object.</p> Attribute Type Description Examples Requirement Level <code>k8s.job.uid</code> string The UID of the Job. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.job.name</code> string The name of the Job. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/k8s/#cronjob","title":"CronJob","text":"<p>A CronJob creates Jobs on a repeating schedule.</p> <p>type: <code>k8s.cronjob</code></p> <p>Description: A Kubernetes CronJob object.</p> Attribute Type Description Examples Requirement Level <code>k8s.cronjob.uid</code> string The UID of the CronJob. <code>275ecb36-5aa8-4c2a-9c47-d8bb681b9aff</code> Recommended <code>k8s.cronjob.name</code> string The name of the CronJob. <code>opentelemetry</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/os/","title":"\u64cd\u4f5c\u7cfb\u7edf","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>type: <code>os</code></p> <p>Description: The operating system (OS) on which the process represented by this resource is running.</p> <p>In case of virtualized environments, this is the operating system as it is observed by the process, i.e., the virtualized guest rather than the underlying host.</p> Attribute Type Description Examples Requirement Level <code>os.type</code> string The operating system type. <code>windows</code> Required <code>os.description</code> string Human readable (not intended to be parsed) OS version information, like e.g. reported by <code>ver</code> or <code>lsb_release -a</code> commands. <code>Microsoft Windows [Version 10.0.18363.778]</code>; <code>Ubuntu 18.04.1 LTS</code> Recommended <code>os.name</code> string Human readable operating system name. <code>iOS</code>; <code>Android</code>; <code>Ubuntu</code> Recommended <code>os.version</code> string The version string of the operating system as defined in Version Attributes. <code>14.2.1</code>; <code>18.04.1</code> Recommended <p><code>os.type</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>windows</code> Microsoft Windows <code>linux</code> Linux <code>darwin</code> Apple Darwin <code>freebsd</code> FreeBSD <code>netbsd</code> NetBSD <code>openbsd</code> OpenBSD <code>dragonflybsd</code> DragonFly BSD <code>hpux</code> HP-UX (Hewlett Packard Unix) <code>aix</code> AIX (Advanced Interactive eXecutive) <code>solaris</code> SunOS, Oracle Solaris <code>z_os</code> IBM z/OS"},{"location":"docs/specs/otel/resource/semantic_conventions/process/","title":"\u8fdb\u7a0b\u548c\u8fdb\u7a0b\u8fd0\u884c\u65f6\u8d44\u6e90","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <ul> <li>Process</li> <li>Process runtimes</li> <li>Erlang Runtimes</li> <li>Go Runtimes</li> <li>Java runtimes</li> <li>JavaScript runtimes</li> <li>.NET Runtimes</li> <li>Python Runtimes</li> <li>Ruby Runtimes</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#process","title":"Process","text":"<p>type: <code>process</code></p> <p>Description: An operating system process.</p> Attribute Type Description Examples Requirement Level <code>process.pid</code> int Process identifier (PID). <code>1234</code> Recommended <code>process.parent_pid</code> int Parent Process identifier (PID). <code>111</code> Recommended <code>process.executable.name</code> string The name of the process executable. On Linux based systems, can be set to the <code>Name</code> in <code>proc/[pid]/status</code>. On Windows, can be set to the base name of <code>GetProcessImageFileNameW</code>. <code>otelcol</code> Conditionally Required: See alternative attributes below. <code>process.executable.path</code> string The full path to the process executable. On Linux based systems, can be set to the target of <code>proc/[pid]/exe</code>. On Windows, can be set to the result of <code>GetProcessImageFileNameW</code>. <code>/usr/bin/cmd/otelcol</code> Conditionally Required: See alternative attributes below. <code>process.command</code> string The command used to launch the process (i.e. the command name). On Linux based systems, can be set to the zeroth string in <code>proc/[pid]/cmdline</code>. On Windows, can be set to the first parameter extracted from <code>GetCommandLineW</code>. <code>cmd/otelcol</code> Conditionally Required: See alternative attributes below. <code>process.command_line</code> string The full command used to launch the process as a single string representing the full command. On Windows, can be set to the result of <code>GetCommandLineW</code>. Do not set this if you have to assemble it just for monitoring; use <code>process.command_args</code> instead. <code>C:\\cmd\\otecol --config=\"my directory\\config.yaml\"</code> Conditionally Required: See alternative attributes below. <code>process.command_args</code> string[] All the command arguments (including the command/executable itself) as received by the process. On Linux-based systems (and some other Unixoid systems supporting procfs), can be set according to the list of null-delimited strings extracted from <code>proc/[pid]/cmdline</code>. For libc-based executables, this would be the full argv vector passed to <code>main</code>. <code>[cmd/otecol, --config=config.yaml]</code> Conditionally Required: See alternative attributes below. <code>process.owner</code> string The username of the user that owns the process. <code>root</code> Recommended <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>process.executable.name</code></li> <li><code>process.executable.path</code></li> <li><code>process.command</code></li> <li><code>process.command_line</code></li> <li><code>process.command_args</code></li> </ul> <p>Between <code>process.command_args</code> and <code>process.command_line</code>, usually <code>process.command_args</code> should be preferred. On Windows and other systems where the native format of process commands is a single string, <code>process.command_line</code> can additionally (or instead) be used.</p> <p>For backwards compatibility with older versions of this semantic convention, it is possible but deprecated to use an array as type for <code>process.command_line</code>. In that case it MUST be interpreted as if it was <code>process.command_args</code>.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#process-runtimes","title":"Process runtimes","text":"<p>type: <code>process.runtime</code></p> <p>Description: The single (language) runtime instance which is monitored.</p> Attribute Type Description Examples Requirement Level <code>process.runtime.name</code> string The name of the runtime of this process. For compiled native binaries, this SHOULD be the name of the compiler. <code>OpenJDK Runtime Environment</code> Recommended <code>process.runtime.version</code> string The version of the runtime of this process, as returned by the runtime without modification. <code>14.0.2</code> Recommended <code>process.runtime.description</code> string An additional description about the runtime of the process, for example a specific vendor customization of the runtime environment. <code>Eclipse OpenJ9 Eclipse OpenJ9 VM openj9-0.21.0</code> Recommended <p>How to set these attributes for particular runtime kinds is described in the following subsections.</p> <p>In addition to these attributes, <code>telemetry.sdk.language</code> can be used to determine the general kind of runtime used.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#erlang-runtimes","title":"Erlang Runtimes","text":"<ul> <li><code>process.runtime.name</code> - The name of the Erlang VM being used, i.e.,   <code>erlang:system_info(machine)</code>.</li> <li><code>process.runtime.version</code> - The version of the runtime (ERTS - Erlang Runtime   System), i.e., <code>erlang:system_info(version)</code>.</li> <li><code>process.runtime.description</code> - string | An additional description about the   runtime made by combining the OTP version, i.e.,   <code>erlang:system_info(otp_release)</code>, and ERTS version.</li> </ul> <p>Example:</p> <code>process.runtime.name</code> <code>process.runtime.version</code> <code>process.runtime.description</code> BEAM 11.1 Erlang/OTP 23 erts-11.1"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#go-runtimes","title":"Go Runtimes","text":"<p>Go Runtimes should fill in the as follows:</p> <ul> <li><code>process.runtime.name</code> - Fill in an interpretation of Go's   <code>runtime.Compiler</code> constant, according   to the following rule: If the value is <code>gc</code>, fill in <code>go</code>. Otherwise, fill in   the exact value of <code>runtime.Compiler</code>.</li> </ul> <p>This can be implemented with the following Go snippet:</p> <pre><code>import \"runtime\"\nfunc getRuntimeName() string {\nif runtime.Compiler == \"gc\" {\nreturn \"go\"\n}\nreturn runtime.Compiler\n}\n</code></pre> <ul> <li><code>process.runtime.version</code> - Fill in the exact value returned by   <code>runtime.Version()</code>, i.e. <code>go1.17</code>.</li> <li><code>process.runtime.description</code> - Use of this field is not recommended.</li> </ul> <p>Examples for some Go compilers/runtimes:</p> <code>process.runtime.name</code> Description <code>go</code> Official Go compiler. Also known as <code>cmd/compile</code>. <code>gccgo</code> gccgo is a Go front end for GCC. <code>tinygo</code> TinyGo compiler."},{"location":"docs/specs/otel/resource/semantic_conventions/process/#java-runtimes","title":"Java runtimes","text":"<p>Java instrumentation should fill in the values by copying from system properties.</p> <ul> <li><code>process.runtime.name</code> - Fill in the value of <code>java.runtime.name</code> as is</li> <li><code>process.runtime.version</code> - Fill in the value of <code>java.runtime.version</code> as is</li> <li><code>process.runtime.description</code> - Fill in the values of <code>java.vm.vendor</code>,   <code>java.vm.name</code>, <code>java.vm.version</code> in that order, separated by spaces.</li> </ul> <p>Examples for some Java runtimes</p> Name <code>process.runtime.name</code> <code>process.runtime.version</code> <code>process.runtime.description</code> OpenJDK OpenJDK Runtime Environment 11.0.8+10 Oracle Corporation OpenJDK 64-Bit Server VM 11.0.8+10 AdoptOpenJDK Eclipse J9 OpenJDK Runtime Environment 11.0.8+10 Eclipse OpenJ9 Eclipse OpenJ9 VM openj9-0.21.0 AdoptOpenJDK Hotspot OpenJDK Runtime Environment 11.0.8+10 AdoptOpenJDK OpenJDK 64-Bit Server VM 11.0.8+10 SapMachine OpenJDK Runtime Environment 11.0.8+10-LTS-sapmachine SAP SE OpenJDK 64-Bit Server VM 11.0.8+10-LTS-sapmachine Zulu OpenJDK OpenJDK Runtime Environment 11.0.8+10-LTS Azul Systems, Inc OpenJDK 64-Bit Server VM Zulu11.41+23-CA Oracle Hotspot 8 (32 bit) Java(TM) SE Runtime Environment 1.8.0_221-b11 Oracle Corporation Java HotSpot(TM) Client VM 25.221-b11 IBM J9 8 Java(TM) SE Runtime Environment 8.0.5.25 - pwa6480sr5fp25-20181030_01(SR5 FP25) IBM Corporation IBM J9 VM 2.9 Android 11 Android Runtime 0.9 The Android Project Dalvik 2.1.0"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#javascript-runtimes","title":"JavaScript runtimes","text":"<p>JavaScript instrumentation should fill in the values by copying from built-in runtime constants.</p> <ul> <li><code>process.runtime.name</code>:</li> <li>When the runtime is Node.js, fill in the constant value <code>nodejs</code>.</li> <li>When the runtime is Web Browser, fill in the constant value <code>browser</code>.</li> <li><code>process.runtime.version</code>:</li> <li>When the runtime is Node.js, fill in the value of <code>process.versions.node</code>.</li> <li>When the runtime is Web Browser, fill in the value of <code>navigator.userAgent</code>.</li> </ul> <p>Examples for some JavaScript runtimes</p> Name <code>process.runtime.name</code> <code>process.runtime.version</code> Node.js nodejs 14.15.4 Web Browser browser Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#net-runtimes","title":".NET Runtimes","text":"<p>TODO(https://github.com/open-telemetry/opentelemetry-dotnet/issues/1281): Confirm the contents here</p> Value Description <code>dotnet-core</code> .NET Core, .NET 5+ <code>dotnet-framework</code> .NET Framework <code>mono</code> Mono"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#python-runtimes","title":"Python Runtimes","text":"<p>Python instrumentation should fill in the values as follows:</p> <ul> <li><code>process.runtime.name</code> - Fill in the value of   <code>sys.implementation.name</code></li> <li><code>process.runtime.version</code> - Fill in the   <code>sys.implementation.version</code> values separated by dots. Leave out   the release level and serial if the release level equals <code>final</code> and the   serial equals zero (leave out either both or none).</li> </ul> <p>This can be implemented with the following Python snippet:</p> <pre><code>vinfo = sys.implementation.version\nresult =  \".\".join(map(\nstr,\nvinfo[:3]\nif vinfo.releaselevel == \"final\" and not vinfo.serial\nelse vinfo\n))\n</code></pre> <ul> <li><code>process.runtime.description</code> - Fill in the value of   <code>sys.version</code> as-is.</li> </ul> <p>Examples for some Python runtimes:</p> Name <code>process.runtime.name</code> <code>process.runtime.version</code> <code>process.runtime.description</code> CPython 3.7.3 on Windows cpython 3.7.3 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)] CPython 3.8.6 on Linux cpython 3.8.6 3.8.6 (default, Sep 30 2020, 04:00:38) [GCC 10.2.0] PyPy 3 7.3.2 on Linux pypy 3.7.4 3.7.4 (?, Sep 27 2020, 15:12:26)[PyPy 7.3.2-alpha0 with GCC 10.2.0] <p>Note that on Linux, there is an actual newline in the <code>sys.version</code> string, and the CPython string had a trailing space in the first line.</p> <p>Pypy provided a CPython-compatible version in <code>sys.implementation.version</code> instead of the actual implementation version which is available in <code>sys.version</code>.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/process/#ruby-runtimes","title":"Ruby Runtimes","text":"<p>Ruby instrumentation should fill in the values by copying from built-in runtime constants.</p> <ul> <li><code>process.runtime.name</code> - Fill in the value of <code>RUBY_ENGINE</code> as is</li> <li><code>process.runtime.version</code> - Fill in the value of <code>RUBY_VERSION</code> as is</li> <li><code>process.runtime.description</code> - Fill in the value of <code>RUBY_DESCRIPTION</code> as is</li> </ul> <p>Examples for some Ruby runtimes</p> Name <code>process.runtime.name</code> <code>process.runtime.version</code> <code>process.runtime.description</code> MRI ruby 2.7.1 ruby 2.7.1p83 (2020-03-31 revision a0c7c23c9c) [x86_64-darwin19] TruffleRuby truffleruby 2.6.2 truffleruby (Shopify) 20.0.0-dev-92ed3059, like ruby 2.6.2, GraalVM CE Native [x86_64-darwin]"},{"location":"docs/specs/otel/resource/semantic_conventions/webengine/","title":"Web \u5f15\u64ce","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>type: <code>webengine</code></p> <p>Description: Resource describing the packaged software running the application code. Web engines are typically executed using process.runtime.</p> Attribute Type Description Examples Requirement Level <code>webengine.name</code> string The name of the web engine. <code>WildFly</code> Required <code>webengine.version</code> string The version of the web engine. <code>21.0.0</code> Recommended <code>webengine.description</code> string Additional description of the web engine (e.g. detailed version and edition information). <code>WildFly Full 21.0.0.Final (WildFly Core 13.0.1.Final) - 2.2.2.Final</code> Recommended <p>Information describing the web engine SHOULD be captured using the values acquired from the API provided by the web engine, preferably during runtime, to avoid maintenance burden on engine version upgrades. As an example - Java engines are often but not always packaged as application servers. For Java application servers supporting Servlet API the required information MAY be captured by invoking <code>ServletContext.getServerInfo()</code> during runtime and parsing the result.</p> <p>A resource can be attributed to at most one web engine.</p> <p>The situations where there are multiple candidates, it is up to instrumentation library authors to choose the web engine. To illustrate, let's look at a Python application using Apache HTTP Server with mod_wsgi as the server and Django as the web framework. In this situation:</p> <ul> <li>Either Apache HTTP Server or <code>mod_wsgi</code> MAY be chosen as <code>webengine</code>,   depending on the decision made by the instrumentation authors.</li> <li>Django SHOULD NOT be set as an <code>webengine</code> as the required information is   already available in instrumentation library and setting this into <code>webengine</code>   would duplicate the information.</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/heroku/","title":"Heroku","text":"<p>Status: Experimental</p> <p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>type: <code>heroku</code></p> <p>Description: Heroku dyno metadata</p> Attribute Type Description Examples Requirement Level <code>heroku.release.creation_timestamp</code> string Time and date the release was created <code>2022-10-23T18:00:42Z</code> Opt-In <code>heroku.release.commit</code> string Commit hash for the current release <code>e6134959463efd8966b20e75b913cafe3f5ec</code> Opt-In <code>heroku.app.id</code> string Unique identifier for the application <code>2daa2797-e42b-4624-9322-ec3f968df4da</code> Opt-In <p>Mapping:</p> Dyno metadata environment variable Resource attribute <code>HEROKU_APP_ID</code> <code>heroku.app.id</code> <code>HEROKU_APP_NAME</code> <code>service.name</code> <code>HEROKU_DYNO_ID</code> <code>service.instance.id</code> <code>HEROKU_RELEASE_CREATED_AT</code> <code>heroku.release.creation_timestamp</code> <code>HEROKU_RELEASE_VERSION</code> <code>service.version</code> <code>HEROKU_SLUG_COMMIT</code> <code>heroku.release.commit</code> <p>Additionally, the <code>cloud.provider</code> resource attribute MUST be set to <code>heroku</code>.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/","title":"AWS Semantic Conventions","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This directory defines standards for resource attributes that only apply to Amazon Web Services (AWS) resources. If an attribute could apply to resources from more than one cloud provider (like account ID, operating system, etc), it belongs in the parent <code>semantic_conventions</code> directory.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/#generic-aws-attributes","title":"Generic AWS Attributes","text":"<p>Attributes that relate to AWS or use AWS-specific terminology, but are used by several services within AWS or are abstracted away from any particular service:</p> <ul> <li>AWS Logs</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/#services","title":"Services","text":"<p>Attributes that relate to an individual AWS service:</p> <ul> <li>Elastic Container Service (ECS)</li> <li>Elastic Kubernetes Service (EKS)</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/ecs/","title":"AWS ECS","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>type: <code>aws.ecs</code></p> <p>Description: Resources used by AWS Elastic Container Service (ECS).</p> Attribute Type Description Examples Requirement Level <code>aws.ecs.container.arn</code> string The Amazon Resource Name (ARN) of an ECS container instance. <code>arn:aws:ecs:us-west-1:123456789123:container/32624152-9086-4f0e-acae-1a75b14fe4d9</code> Recommended <code>aws.ecs.cluster.arn</code> string The ARN of an ECS cluster. <code>arn:aws:ecs:us-west-2:123456789123:cluster/my-cluster</code> Recommended <code>aws.ecs.launchtype</code> string The launch type for an ECS task. <code>ec2</code> Recommended <code>aws.ecs.task.arn</code> string The ARN of an ECS task definition. <code>arn:aws:ecs:us-west-1:123456789123:task/10838bed-421f-43ef-870a-f43feacbbb5b</code> Recommended <code>aws.ecs.task.family</code> string The task definition family this task definition is a member of. <code>opentelemetry-family</code> Recommended <code>aws.ecs.task.revision</code> string The revision for this task definition. <code>8</code>; <code>26</code> Recommended <p><code>aws.ecs.launchtype</code> MUST be one of the following:</p> Value Description <code>ec2</code> ec2 <code>fargate</code> fargate"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/eks/","title":"AWS EKS","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>type: <code>aws.eks</code></p> <p>Description: Resources used by AWS Elastic Kubernetes Service (EKS).</p> Attribute Type Description Examples Requirement Level <code>aws.eks.cluster.arn</code> string The ARN of an EKS cluster. <code>arn:aws:ecs:us-west-2:123456789123:cluster/my-cluster</code> Recommended"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/aws/logs/","title":"AWS Logs","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>Type: <code>aws.log</code></p> <p>Description: Log attributes for Amazon Web Services.</p> Attribute Type Description Examples Requirement Level <code>aws.log.group.names</code> string[] The name(s) of the AWS log group(s) an application is writing to. [1] <code>[/aws/lambda/my-function, opentelemetry-service]</code> Recommended <code>aws.log.group.arns</code> string[] The Amazon Resource Name(s) (ARN) of the AWS log group(s). [2] <code>[arn:aws:logs:us-west-1:123456789012:log-group:/aws/my/group:*]</code> Recommended <code>aws.log.stream.names</code> string[] The name(s) of the AWS log stream(s) an application is writing to. <code>[logs/main/10838bed-421f-43ef-870a-f43feacbbb5b]</code> Recommended <code>aws.log.stream.arns</code> string[] The ARN(s) of the AWS log stream(s). [3] <code>[arn:aws:logs:us-west-1:123456789012:log-group:/aws/my/group:log-stream:logs/main/10838bed-421f-43ef-870a-f43feacbbb5b]</code> Recommended <p>[1]: Multiple log groups must be supported for cases like multi-container applications, where a single application has sidecar containers, and each write to their own log group.</p> <p>[2]: See the log group ARN format documentation.</p> <p>[3]: See the log stream ARN format documentation. One log group can contain several log streams, so these ARNs necessarily identify both a log group and a log stream.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/gcp/","title":"GCP Semantic Conventions","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>This directory defines standards for resource attributes that only apply to Google Cloud Platform (GCP). If an attribute could apply to resources from more than one cloud provider (like account ID, operating system, etc), it belongs in the parent <code>semantic_conventions</code> directory.</p>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/gcp/#services","title":"Services","text":"<ul> <li>Cloud Run</li> </ul>"},{"location":"docs/specs/otel/resource/semantic_conventions/cloud_provider/gcp/cloud_run/","title":"Google Cloud Run","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>These conventions are recommended for resources running on Cloud Run.</p> <p>Type: <code>gcp.cloud_run</code></p> <p>Description: Resource attributes for GCE instances.</p> Attribute Type Description Examples Requirement Level <code>gcp.cloud_run.job.execution</code> string The name of the Cloud Run execution being run for the Job, as set by the <code>CLOUD_RUN_EXECUTION</code> environment variable. <code>job-name-xxxx</code>; <code>sample-job-mdw84</code> Recommended <code>gcp.cloud_run.job.task_index</code> int The index for a task within an execution as provided by the <code>CLOUD_RUN_TASK_INDEX</code> environment variable. <code>0</code>; <code>1</code> Recommended"},{"location":"docs/specs/otel/schemas/","title":"Index","text":""},{"location":"docs/specs/otel/schemas/#_1","title":"\u9065\u6d4b\u6280\u672f\u6a21\u5f0f","text":"<p>Status: Stable</p> Table of Contents   - [Motivation](#motivation) - [How Schemas Work](#how-schemas-work) - [What is Out of Scope](#what-is-out-of-scope) - [Use Cases](#use-cases)   - [Full Schema-Aware](#full-schema-aware)   - [Collector-Assisted Schema Transformation](#collector-assisted-schema-transformation) - [Schema URL](#schema-url) - [Schema Version Number](#schema-version-number) - [OTLP Support](#otlp-support) - [API Support](#api-support) - [OpenTelemetry Schema](#opentelemetry-schema)"},{"location":"docs/specs/otel/schemas/#motivation","title":"Motivation","text":"<p>Telemetry sources such as instrumented applications and consumers of telemetry such as observability backends sometimes make implicit assumptions about the emitted telemetry. They assume that the telemetry will contain certain attributes or otherwise have a certain shape and composition of data (this is referred to as \"telemetry schema\" throughout this document).</p> <p>This makes it difficult or impossible to change the composition of the emitted telemetry data without breaking the consumers. For example changing the name of an attribute of a span created by an instrumentation library can break the backend if the backend expects to find that attribute by its name.</p> <p>Semantic conventions are an important part of this problem. These conventions define what names and values to use for span attributes, metric names and other fields. If semantic conventions are changed the existing implementations (telemetry source or consumers) need to be also changed correspondingly. Furthermore, to make things worse, the implementations of telemetry sources and implementations of telemetry consumers that work together and that depend on the changed semantic convention need to be changed simultaneously, otherwise such implementations will no longer work correctly together.</p> <p>Essentially there is a coupling between 3 parties: 1) OpenTelemetry semantic conventions, 2) telemetry sources and 3) telemetry consumers. The coupling complicates the independent evolution of these 3 parties.</p> <p>We recognize the following needs:</p> <ul> <li> <p>OpenTelemetry semantic conventions need to evolve over time. When conventions   are first defined, mistakes are possible and we may want to fix the mistakes   over time. We may also want to change conventions to re-group the attributes   into different namespaces as our understanding of the attribute taxonomy   improves.</p> </li> <li> <p>Telemetry sources over time may want to change the schema of the telemetry   they emit. This may be because for example the semantic conventions evolved   and we want to make our telemetry match the newly introduced conventions.</p> </li> <li> <p>In an observability system there may simultaneously exist telemetry sources   that produce data that conforms to different telemetry schemas because   different sources evolve at a different pace and are implemented and   controlled by different entities.</p> </li> <li> <p>Telemetry consumers have a need to understand what schema a particular piece   of received telemetry confirms to. The consumers also need a way to be able to   interpret the telemetry data that uses different telemetry schemas.</p> </li> </ul> <p>Telemetry Schemas that were proposed and accepted in OTEP0152 address these needs.</p>"},{"location":"docs/specs/otel/schemas/#how-schemas-work","title":"How Schemas Work","text":"<p>We believe that the 3 parties described above are able to evolve independently over time, while continuously retaining the ability to correctly work together.</p> <p>Telemetry Schemas are central to how we make this possible. Here is a summary of how the schemas work:</p> <ul> <li> <p>OpenTelemetry defines a file format for defining   telemetry schemas.</p> </li> <li> <p>Telemetry schemas are versioned. Over time the schema may evolve and telemetry   sources may emit data conforming to newer versions of the schema.</p> </li> <li> <p>Telemetry schemas explicitly define transformations that are necessary to   convert telemetry data between different versions of the schema, provided that   such conversions are possible. When conversions are not possible it   constitutes a breaking change between versions.</p> </li> <li> <p>Telemetry schemas are identified by Schema URLs, that are unique for each   schema version.</p> </li> <li> <p>Telemetry sources (e.g. instrumentation libraries) will include a schema URL   in the emitted telemetry.</p> </li> <li> <p>Telemetry consumers will pay attention to the schema of the received   telemetry. If necessary, telemetry consumers may transform the telemetry data   from the received schema version to the target schema version as expected at   the point of use (e.g. a dashboard may define which schema version it   expects).</p> </li> <li> <p>OpenTelemetry publishes a telemetry schema as part of the specification. The   schema contains the list of transformations that semantic conventions undergo.   The schema is to be available, to be referred and downloaded at a well known   URL: <code>https://opentelemetry.io/schemas/&lt;version&gt;</code> (where <code>&lt;version&gt;</code> matches   the specification version number).</p> </li> <li> <p>OpenTelemetry instrumentation libraries include the OpenTelemetry Schema URL   in all emitted telemetry. This is currently work-in-progress,   here is an example   of how it is done in Go SDK's Resource detectors.</p> </li> <li> <p>OTLP allows inclusion of a schema URL in the emitted telemetry.</p> </li> <li> <p>Third-party libraries, instrumentation or applications are advised to define   and publish their own telemetry schema if it is completely different from   OpenTelemetry schema (or use OpenTelemetry schema) and include the schema URL   in the emitted telemetry.</p> </li> </ul>"},{"location":"docs/specs/otel/schemas/#what-is-out-of-scope","title":"What is Out of Scope","text":"<ul> <li> <p>The concept of schema does not attempt to fully describe the shape of   telemetry. The schema for example does not define all possible valid values   for attributes or expected data types for metrics, etc. It is not a goal. Our   goal is narrowly defined to solve the following problem only: to allow   OpenTelemetry Semantic Conventions to evolve over time. For that reason this   document is concerned with changes to the schema as opposed to the full   state of the schema. We do not preclude this though: the schema file format   is extensible and in the future may allow defining the full state of the   schema.</p> </li> <li> <p>We intentionally limit the types of transformations of schemas to the bare   minimum that is necessary to handle the most common changes that we believe   OpenTelemetry Semantic Conventions will require in the near future. More types   of transformations may be proposed in the future. This proposal does not   attempt to support a comprehensive set of possible transformation types that   can handle all possible changes to schemas that we can imagine. That would be   too complicated and very likely superfluous. Any new transformation types MUST   be proposed and added in the future to the schema file format when there is an   evidence that they are necessary for the evolution of OpenTelemetry.</p> </li> </ul>"},{"location":"docs/specs/otel/schemas/#use-cases","title":"Use Cases","text":"<p>This section shows a couple interesting use-cases for the telemetry schemas (other uses-cases are also possible, this is not an exhaustive list).</p>"},{"location":"docs/specs/otel/schemas/#full-schema-aware","title":"Full Schema-Aware","text":"<p>Here is an example on a schema-aware observability system:</p> <p></p> <p>Let's have a closer look at what happens with the Telemetry Source and Backend pair as the telemetry data is emitted, delivered and stored:</p> <p></p> <p>In this example the telemetry source produces spans that comply with version 1.2.0 of OpenTelemetry schema, where the \"deployment.environment\" attribute is used to record that the span is coming from production.</p> <p>The telemetry consumer desires to store the telemetry in version 1.1.0 of OpenTelemetry schema. The schema translator compares the schema_url in the received span with the desired schema and sees that a version conversion is needed. It then applies the change that is described in the schema file and renames the attribute from \"deployment.environment\" to \"environment\" before storing the span.</p> <p>And here is for example how the schemas can be used to query stored data:</p> <p></p>"},{"location":"docs/specs/otel/schemas/#collector-assisted-schema-transformation","title":"Collector-Assisted Schema Transformation","text":"<p>Here is a somewhat different use case, where the backend is not aware of schemas and we rely on OpenTelemetry Collector to translate the telemetry to a schema that the backend expects to receive. The \"Schema Translate Processor\" is configured, the target schema_url is specified and all telemetry data that passes through the Collector is converted to that target schema:</p> <p></p>"},{"location":"docs/specs/otel/schemas/#schema-url","title":"Schema URL","text":"<p>Schema URL is an identifier of a Schema. The URL specifies a location of a Schema File that can be retrieved (so it is a URL and not just a URI) using HTTP or HTTPS protocol.</p> <p>Fetching the specified URL may return an HTTP redirect status code. The fetcher MUST follow the HTTP standard and honour the redirect response and fetch the file from the redirected URL.</p> <p>The last part of the URL path is the version number of the schema.</p> <pre><code>http[s]://server[:port]/path/&lt;version&gt;\n</code></pre> <p>The part of the URL preceding the <code>&lt;version&gt;</code> is called Schema Family identifier. All schemas in one Schema Family have identical Schema Family identifiers.</p> <p>To create a new version of the schema copy the schema file for the last version in the schema family and add the definition of the new version. The schema file that corresponds to the new version MUST be retrievable at a new URL.</p> <p>Important: schema files are immutable once they are published. Once the schema file is retrieved it is recommended to be cached permanently. Schema files may be also packaged at build time with the software that anticipates it may need the schema (e.g. the latest OpenTelemetry schema file can be packaged at build time with OpenTelemetry Collector's schema translation processor).</p>"},{"location":"docs/specs/otel/schemas/#schema-version-number","title":"Schema Version Number","text":"<p>Version number follows the MAJOR.MINOR.PATCH format, similar to semver 2.0.</p> <p>Version numbers use the ordering rules defined by semver 2.0 specification. See how ordering is used in the Order of Transformations. Other than the ordering rules the schema version numbers do not carry any other semantic meaning.</p> <p>OpenTelemetry schema version numbers match OpenTelemetry specification version numbers, see more details here.</p>"},{"location":"docs/specs/otel/schemas/#otlp-support","title":"OTLP Support","text":"<p>To allow carrying the Schema URL in emitted telemetry OTLP includes a schema_url field in the messages:</p> <ul> <li> <p>The schema_url field in the ResourceSpans, ResourceMetrics, ResourceLogs   messages applies to the contained Resource, Span, SpanEvent, Metric, LogRecord   messages.</p> </li> <li> <p>The schema_url field in the InstrumentationLibrarySpans message applies to the   contained Span and SpanEvent messages.</p> </li> <li> <p>The schema_url field in the InstrumentationLibraryMetrics message applies to   the contained Metric messages.</p> </li> <li> <p>The schema_url field in the InstrumentationLibraryLogs message applies to the   contained LogRecord messages.</p> </li> <li> <p>If schema_url field is non-empty both in Resource* message and in the   contained InstrumentationLibrary* message then the value in   InstrumentationLibrary* message takes the precedence.</p> </li> </ul>"},{"location":"docs/specs/otel/schemas/#api-support","title":"API Support","text":"<p>The OpenTelemetry API allows getting a Tracer/Meter that is associated with a Schema URL.</p>"},{"location":"docs/specs/otel/schemas/#opentelemetry-schema","title":"OpenTelemetry Schema","text":"<p>OpenTelemetry publishes it own schema at opentelemetry.io/schemas/&lt;version&gt;. The version number of the schema is the same as the specification version number which publishes the schema. Every time a new specification version is released a corresponding new schema MUST be released simultaneously. If the specification release did not introduce any change the \"changes\" section of the corresponding version in the schema file will be empty.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/","title":"File format v1.0.0","text":""},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#100","title":"\u67b6\u6784\u6587\u4ef6\u683c\u5f0f 1.0.0","text":"<p>Status: Experimental</p> <p>A Schema File is a YAML file that describes the schema of a particular version. It defines the transformations that can be used to convert the telemetry data represented in any other older compatible version of the same schema family to this schema version.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#file-structure","title":"File Structure","text":"<p>Here is the structure of the Schema File:</p> <pre><code># Defines the file format. MUST be set to 1.0.0.\nfile_format: 1.0.0\n# The Schema URL that this file is published at. The version number in the URL\n# MUST match the highest version number in the \"versions\" section below.\n# Note: the schema version number in the URL is not related in any way to\n# the file_format setting above.\nschema_url: https://opentelemetry.io/schemas/1.2.0\n# Definitions for each schema version in this family.\n# Note: the ordering of versions is defined according to semver\n# version number ordering rules.\nversions:\n&lt;version_number_last&gt;:\n# definitions for this version. See details below.\n&lt;version_number_previous&gt;:\n# definitions for previous version\n...\n&lt;version_number_first&gt;:\n# Defines the first version.\n</code></pre> <p>Each <code>&lt;version_number&gt;</code> section has the following structure:</p> <pre><code>&lt;version_number&gt;:\nall:\nchanges:\n# sequence of transformations.\nresources:\nchanges:\n# sequence of transformations.\nspans:\nchanges:\n# sequence of transformations.\nspan_events:\nchanges:\n# sequence of transformations.\nmetrics:\nchanges:\n# sequence of transformations.\nlogs:\nchanges:\n# sequence of transformations.\n</code></pre> <p>There are 6 sub-sections under each version definition: \"all\", \"resources\", \"spans\", \"span_events\", \"metrics\", \"logs\". The last 5 sub-sections in this list contain definitions that apply only to the corresponding telemetry data type. Section \"all\" contains definitions that apply to all types of telemetry data.</p> <p>Below we describe each section in detail.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#all-section","title":"all Section","text":"<p>\"all\" section in the schema file defines transformations. It must contain a sub-section named \"changes\" that defines how attributes were renamed from the previous version to this version.</p> <p>The \"changes\" section is a sequence of transformations. Only one transformation is supported for section \"all\": \"rename_attributes\" transformation.</p> <p>\"rename_attributes\" transformation requires a map of key/value pairs, where the key is the old name of the attribute used in the previous version, the value is the new name of the attribute starting from this version. Here is the structure:</p> <pre><code>all:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values.\n</code></pre> <p>The transformations in section \"all\" apply to the following telemetry data: resource attributes, span attributes, span event attributes, log attributes, metric attributes.</p> <p>Important: when converting from the previous version to the current version the transformation sequence in section \"all\" is performed first. After that the transformations in the specific section (\"resources\", \"spans\", \"span_events\", \"metrics\" or \"logs\") that correspond to the data type that is being converted are applied.</p> <p>Note that \"rename_attributes\" transformation in most cases is reversible. It is possible to apply it backwards, so that telemetry data is converted from this version to the previous version. The only exception is when 2 or more different attributes in the previous version are renamed to the same attribute in the new version. In that case the reverse transformation is not possible since it would be ambiguous. When the reverse transformation is not possible it is considered an incompatible change. In this case the MAJOR version number of the schema SHOULD be increased in the new version.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#resources-section","title":"resources Section","text":"<p>\"resources\" section is very similar in its structure to \"all\". Like section \"all\" the transformations in section \"resources\" may contain only \"rename_attributes\" transformation.</p> <p>The only difference from section \"all\" is that this transformation is only applicable to Resource data type.</p> <p>Here is the structure:</p> <pre><code>resources:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#spans-section","title":"spans Section","text":"<p>\"spans\" section in the schema file defines transformations that are applicable only to Span data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert Spans from the previous version to this version.</p> <p>One transformation is supported for section \"span\": \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_attributes-transformation","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"all\" and \"resource\" sections. In addition it is also possible to optionally specify spans that the transformation should apply to. Here is the structure:</p> <pre><code>spans:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_spans:\n# Optional. If it is missing the transformation is applied\n# to all spans. If it is present the transformation is applied\n# only to the spans with the name that is found in the sequence\n# specified below.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#span_events-section","title":"span_events Section","text":"<p>\"spans_events\" section in the schema file defines transformations that are applicable only to Span's Event data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert events from the previous version to this version.</p> <p>Two transformations are supported for section \"spans_events\": \"rename_events\" and \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_events-transformation","title":"rename_events Transformation","text":"<p>This transformation allows to change event names. It is applied to all events or only to events of spans that have the specified name. Here is the structure:</p> <pre><code>span_events:\nchanges:\n- rename_events:\nname_map:\n# The keys are old event name used in the previous version, the\n# values are the new event name starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_attributes-transformation_1","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"all\" and \"resource\" sections. In addition it is also possible to optionally specify spans and events that the transformation should apply to (both optional conditions must match, if specified, for transformation to be applicable). Here is the structure:</p> <pre><code>span_events:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_spans:\n# Optional span names to apply to. If empty applies to all spans.\napply_to_events:\n# Optional event names to apply to. If empty applies to all events.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#metrics-section","title":"metrics Section","text":"<p>\"metrics\" section in the schema file defines transformations that are applicable only to Metric data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert metrics from the previous version to this version.</p> <p>Two transformations are supported for section \"metrics\": \"rename_metrics\" and \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_metrics-transformation","title":"rename_metrics Transformation","text":"<p>This transformation allows to change metric names. It is applied to all metrics. Here is the structure:</p> <pre><code>metrics:\nchanges:\n- rename_metrics:\n# map of key/values. The keys are the old metric name used\n# in the previous version, the values are the new metric name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_attributes-transformation_2","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"span\" sections. Here is the structure:</p> <pre><code>metrics:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_metrics:\n# Optional. If it is missing the transformation is applied\n# to all metrics. If it is present the transformation is applied\n# only to the metrics with the name that is found in the sequence\n# specified below.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#logs-section","title":"logs Section","text":"<p>\"logs\" section in the schema file defines transformations that are applicable only to the Log Record data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert logs from the previous version to this version.</p> <p>One transformation is supported for section \"logs\": \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#rename_attributes-transformation_3","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"spans\" section. Here is the structure:</p> <pre><code>logs:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#order-of-transformations","title":"Order of Transformations","text":"<p>When converting from older version X to newer version Y of the schema (both belonging to the same schema family) the transformations specified in each version in the range [X..Y] are applied one by one, i.e. first we convert from X to X+1, then from X+1 to X+2, ..., Y-2 to Y-1, Y-1 to Y. (Note, version numbers are not a continuum of integer numbers. The notion of adding a natural number 1 to the version number is a placeholder for the phrase \"next newer version number that is defined for this schema family\".)</p> <p>The transformations that are listed for a particular version X describe changes that happened since the schema version that precedes version X and belongs to the same schema family. These transformations are listed in 6 sections: \"all\", \"resources\", \"spans\", \"span_events\", \"metrics\", \"logs\". Here is the order in which the transformations are applied:</p> <ul> <li> <p>Transformations in section \"all\" always are applied first, before any of the   transformations in the other 5 sections.</p> </li> <li> <p>Transformations in section \"spans\" are applied before transformations in   section \"span_events\".</p> </li> <li> <p>The order in which the transformations in remaining sections (\"resources\",   \"metrics\", logs\") are applied relative to each other or relative to \"spans\"   section is undefined (since there are not-interdependencies, the order does   not matter).</p> </li> </ul> <p>In the \"changes\" subsection of each particular one of these 6 sections the sequence of transformations is applied in the order it is listed in the schema file, from top to bottom.</p> <p>When converting in the opposite direction, from newer version Y to older version X the order of transformation listed above is exactly the reverse, with each individual transformation also performing the reverse conversion.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#schema-file-format-number","title":"Schema File Format Number","text":"<p>The \"file_format\" setting in the schema file specifies the format version of the file. The format version follows the MAJOR.MINOR.PATCH format, similar to semver 2.0.</p> <p>The \"file_format\" setting is used by consumers of the file to know if they are capable of interpreting the content of the file.</p> <p>The current value for this setting is \"1.0.0\". Any change to this number MUST follow the OTEP process and be published in the specification.</p> <p>The current schema file format allows representing a limited set of transformations of telemetry data. We anticipate that in the future more types of transformations may be desirable to support or other, additional information may be desirable to record in the schema file.</p> <p>As the schema file format evolves over time the format version number SHOULD change according to the following rules:</p> <ul> <li>PATCH number SHOULD be increased when the file format changes in a way that   does not affect the existing consumers of the file. For example addition of a   completely new section in the schema file that has no effect on existing   sections and has no effect on any existing schema functionality may be done   via incrementing the PATCH number only. This approach is only valid if the new   setting in the file is completely and safely ignorable by all existing   processing logic.</li> </ul> <p>For example adding a completely new section that describes the full state of   the schema has no effect on existing consumers which only care about \"changes\"   section (unless we explicitly define the semantics of the new section such   that it needs to be taken into account when processing schema changes). So,   adding such a new section can be done using a PATCH number increase.</p> <ul> <li>MINOR number SHOULD be increased if a new setting is added to the file format   in a backward compatible manner. \"Backward compatible\" in this context means   that consumers that are aware of the new MINOR number can consume the file of   a particular MINOR version number or of any MINOR version number lower than   that, provided that MAJOR version numbers are the same. Typically, this means   that the added setting in file format is optional and the default value of the   setting matches the behavior of the previous file format version.</li> </ul> <p>Note: there is no \"forward compatibility\" based on MINOR version number.   Consumers which support reading up to a particular MINOR version number SHOULD   NOT attempt to consume files with higher MINOR version numbers.</p> <ul> <li>MAJOR number SHOULD be increased if the file format is changed in an   incompatible way. For example adding a new transformation type in the   \"changes\" section is an incompatible change because it cannot be ignored by   existing schema conversion logic, so such a change will require a new MAJOR   number.</li> </ul> <p>Correspondingly:</p> <ul> <li> <p>Consumers of the schema file SHOULD NOT attempt to interpret the schema file   if the MAJOR version number is different (higher or lower) than what the   consumer supports.</p> </li> <li> <p>Consumers of the schema file SHOULD NOT attempt to interpret the schema file   if the MINOR version number is higher than what the consumer supports.</p> </li> <li> <p>Consumers MAY ignore the PATCH number.</p> </li> </ul> <p>To illustrate this with some examples:</p> File Format Version Consumer's Expected Version Consumer Can Read? 1.0.0     1.0.0     yes     1.0.x     1.0.y     yes, for any x and y.     1.a.x     1.b.x     if a&lt;b then yes, otherwise no.     2.0.0     1.x.y     no"},{"location":"docs/specs/otel/schemas/file_format_v1.0.0/#appendix-a-example-schema-file","title":"Appendix A. Example Schema File","text":"<pre><code># Defines the file format. MUST be set to 1.0.0.\nfile_format: 1.0.0\n# The Schema URL that this file is published at. The version number in the URL\n# MUST match the highest version number in the \"versions\" section below.\n# Note: the schema version number in the URL is not related in any way to\n# the file_format setting above.\nschema_url: https://opentelemetry.io/schemas/1.1.0\n# Definitions for each schema version in this family.\n# Note: the ordering of versions is defined according to semver\n# version number ordering rules.\nversions:\n1.1.0:\n# Definitions for version 1.1.0.\nall:\n# Definitions that apply to all data types.\nchanges:\n# Transformations to apply when converting from version 1.0.0 to 1.1.0.\n- rename_attributes:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n# Rename k8s.* to kubernetes.*\nk8s.cluster.name: kubernetes.cluster.name\nk8s.namespace.name: kubernetes.namespace.name\nk8s.node.name: kubernetes.node.name\nk8s.node.uid: kubernetes.node.uid\nk8s.pod.name: kubernetes.pod.name\nk8s.pod.uid: kubernetes.pod.uid\nk8s.container.name: kubernetes.container.name\nk8s.replicaset.name: kubernetes.replicaset.name\nk8s.replicaset.uid: kubernetes.replicaset.uid\nk8s.cronjob.name: kubernetes.cronjob.name\nk8s.cronjob.uid: kubernetes.cronjob.uid\nk8s.job.name: kubernetes.job.name\nk8s.job.uid: kubernetes.job.uid\nk8s.statefulset.name: kubernetes.statefulset.name\nk8s.statefulset.uid: kubernetes.statefulset.uid\nk8s.daemonset.name: kubernetes.daemonset.name\nk8s.daemonset.uid: kubernetes.daemonset.uid\nk8s.deployment.name: kubernetes.deployment.name\nk8s.deployment.uid: kubernetes.deployment.uid\nresources:\n# Definitions that apply to Resource data type.\nchanges:\n- rename_attributes:\ntelemetry.auto.version: telemetry.auto_instr.version\nspans:\n# Definitions that apply to Span data type.\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\npeer.service: peer.service.name\napply_to_spans:\n# apply only to spans named \"HTTP GET\"\n- 'HTTP GET'\nspan_events:\n# Definitions that apply to Span Event data type.\nchanges:\n- rename_events:\n# The keys are old event name used in the previous version, the\n# values are the new event name starting from this version.\nname_map: { stacktrace: stack_trace }\n- rename_attributes:\nattribute_map:\npeer.service: peer.service.name\napply_to_events:\n# Optional event names to apply to. If empty applies to all events.\n- exception.stack_trace\nmetrics:\n# Definitions that apply to Metric data type.\nchanges:\n- rename_metrics:\n# map of key/values. The keys are the old metric name used\n# in the previous version, the values are the new metric name\n# starting from this version.\ncontainer.cpu.usage.total: cpu.usage.total\ncontainer.memory.usage.max: memory.usage.max\n- rename_attributes:\nattribute_map:\nstatus: state\napply_to_metrics:\n# Optional. If it is missing the transformation is applied\n# to all metrics. If it is present the transformation is applied\n# only to the metrics with the name that is found in the sequence\n# specified below.\n- system.cpu.utilization\n- system.memory.usage\n- system.memory.utilization\n- system.paging.usage\nlogs:\n# Definitions that apply to LogRecord data type.\nchanges:\n- rename_attributes:\nattribute_map:\nprocess.executable_name: process.executable.name\n1.0.0:\n# First version of this schema family.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/","title":"File format v1.1.0","text":""},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#110","title":"\u67b6\u6784\u6587\u4ef6\u683c\u5f0f 1.1.0","text":"<p>Status: Experimental</p> <p>A Schema File is a YAML file that describes the schema of a particular version. It defines the transformations that can be used to convert the telemetry data represented in any other older compatible version of the same schema family to this schema version.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#file-structure","title":"File Structure","text":"<p>Here is the structure of the Schema File:</p> <pre><code># Defines the file format. MUST be set to 1.1.0.\nfile_format: 1.1.0\n# The Schema URL that this file is published at. The version number in the URL\n# MUST match the highest version number in the \"versions\" section below.\n# Note: the schema version number in the URL is not related in any way to\n# the file_format setting above.\nschema_url: https://opentelemetry.io/schemas/1.2.0\n# Definitions for each schema version in this family.\n# Note: the ordering of versions is defined according to semver\n# version number ordering rules.\nversions:\n&lt;version_number_last&gt;:\n# definitions for this version. See details below.\n&lt;version_number_previous&gt;:\n# definitions for previous version\n...\n&lt;version_number_first&gt;:\n# Defines the first version.\n</code></pre> <p>Each <code>&lt;version_number&gt;</code> section has the following structure:</p> <pre><code>&lt;version_number&gt;:\nall:\nchanges:\n# sequence of transformations.\nresources:\nchanges:\n# sequence of transformations.\nspans:\nchanges:\n# sequence of transformations.\nspan_events:\nchanges:\n# sequence of transformations.\nmetrics:\nchanges:\n# sequence of transformations.\nlogs:\nchanges:\n# sequence of transformations.\n</code></pre> <p>There are 6 sub-sections under each version definition: \"all\", \"resources\", \"spans\", \"span_events\", \"metrics\", \"logs\". The last 5 sub-sections in this list contain definitions that apply only to the corresponding telemetry data type. Section \"all\" contains definitions that apply to all types of telemetry data.</p> <p>Below we describe each section in detail.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#all-section","title":"all Section","text":"<p>\"all\" section in the schema file defines transformations. It must contain a sub-section named \"changes\" that defines how attributes were renamed from the previous version to this version.</p> <p>The \"changes\" section is a sequence of transformations. Only one transformation is supported for section \"all\": \"rename_attributes\" transformation.</p> <p>\"rename_attributes\" transformation requires a map of key/value pairs, where the key is the old name of the attribute used in the previous version, the value is the new name of the attribute starting from this version. Here is the structure:</p> <pre><code>all:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values.\n</code></pre> <p>The transformations in section \"all\" apply to the following telemetry data: resource attributes, span attributes, span event attributes, log attributes, metric attributes.</p> <p>Important: when converting from the previous version to the current version the transformation sequence in section \"all\" is performed first. After that the transformations in the specific section (\"resources\", \"spans\", \"span_events\", \"metrics\" or \"logs\") that correspond to the data type that is being converted are applied.</p> <p>Note that \"rename_attributes\" transformation in most cases is reversible. It is possible to apply it backwards, so that telemetry data is converted from this version to the previous version. The only exception is when 2 or more different attributes in the previous version are renamed to the same attribute in the new version. In that case the reverse transformation is not possible since it would be ambiguous. When the reverse transformation is not possible it is considered an incompatible change. In this case the MAJOR version number of the schema SHOULD be increased in the new version.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#resources-section","title":"resources Section","text":"<p>\"resources\" section is very similar in its structure to \"all\". Like section \"all\" the transformations in section \"resources\" may contain only \"rename_attributes\" transformation.</p> <p>The only difference from section \"all\" is that this transformation is only applicable to Resource data type.</p> <p>Here is the structure:</p> <pre><code>resources:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#spans-section","title":"spans Section","text":"<p>\"spans\" section in the schema file defines transformations that are applicable only to Span data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert Spans from the previous version to this version.</p> <p>One transformation is supported for section \"span\": \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_attributes-transformation","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"all\" and \"resource\" sections. In addition it is also possible to optionally specify spans that the transformation should apply to. Here is the structure:</p> <pre><code>spans:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_spans:\n# Optional. If it is missing the transformation is applied\n# to all spans. If it is present the transformation is applied\n# only to the spans with the name that is found in the sequence\n# specified below.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#span_events-section","title":"span_events Section","text":"<p>\"spans_events\" section in the schema file defines transformations that are applicable only to Span's Event data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert events from the previous version to this version.</p> <p>Two transformations are supported for section \"spans_events\": \"rename_events\" and \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_events-transformation","title":"rename_events Transformation","text":"<p>This transformation allows to change event names. It is applied to all events or only to events of spans that have the specified name. Here is the structure:</p> <pre><code>span_events:\nchanges:\n- rename_events:\nname_map:\n# The keys are old event name used in the previous version, the\n# values are the new event name starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_attributes-transformation_1","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"all\" and \"resource\" sections. In addition it is also possible to optionally specify spans and events that the transformation should apply to (both optional conditions must match, if specified, for transformation to be applicable). Here is the structure:</p> <pre><code>span_events:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_spans:\n# Optional span names to apply to. If empty applies to all spans.\napply_to_events:\n# Optional event names to apply to. If empty applies to all events.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#metrics-section","title":"metrics Section","text":"<p>\"metrics\" section in the schema file defines transformations that are applicable only to Metric data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert metrics from the previous version to this version.</p> <p>Three transformations are supported for section \"metrics\": \"rename_metrics\", \"rename_attributes\" and \"split\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_metrics-transformation","title":"rename_metrics Transformation","text":"<p>This transformation allows to change metric names. It is applied to all metrics. Here is the structure:</p> <pre><code>metrics:\nchanges:\n- rename_metrics:\n# map of key/values. The keys are the old metric name used\n# in the previous version, the values are the new metric name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_attributes-transformation_2","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"span\" sections. Here is the structure:</p> <pre><code>metrics:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\napply_to_metrics:\n# Optional. If it is missing the transformation is applied\n# to all metrics. If it is present the transformation is applied\n# only to the metrics with the name that is found in the sequence\n# specified below.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#split-transformation","title":"split Transformation","text":"<p>This transformation splits a metric into several metrics and eliminates an attribute. Here is the structure:</p> <pre><code>metrics:\nchanges:\n- split:\n# Name of old metric to split.\napply_to_metric:\n# Name of attribute in the old metric to use for splitting. The attribute will be\n# eliminated, the new metric will not have it.\nby_attribute:\n# Names of new metrics to create, one for each possible value of attribute.\nmetrics_from_attributes:\n# map of key/values. The keys are the new metric name starting from this\n# version, the values are the old attribute value used in the previous version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#logs-section","title":"logs Section","text":"<p>\"logs\" section in the schema file defines transformations that are applicable only to the Log Record data type. It must contain a sub-section named \"changes\" that defines a sequence of actions to be applied to convert logs from the previous version to this version.</p> <p>One transformation is supported for section \"logs\": \"rename_attributes\".</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#rename_attributes-transformation_3","title":"rename_attributes Transformation","text":"<p>This is similar to the \"rename_attributes\" transformation supported in \"spans\" section. Here is the structure:</p> <pre><code>logs:\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n</code></pre>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#order-of-transformations","title":"Order of Transformations","text":"<p>When converting from older version X to newer version Y of the schema (both belonging to the same schema family) the transformations specified in each version in the range [X..Y] are applied one by one, i.e. first we convert from X to X+1, then from X+1 to X+2, ..., Y-2 to Y-1, Y-1 to Y. (Note, version numbers are not a continuum of integer numbers. The notion of adding a natural number 1 to the version number is a placeholder for the phrase \"next newer version number that is defined for this schema family\".)</p> <p>The transformations that are listed for a particular version X describe changes that happened since the schema version that precedes version X and belongs to the same schema family. These transformations are listed in 6 sections: \"all\", \"resources\", \"spans\", \"span_events\", \"metrics\", \"logs\". Here is the order in which the transformations are applied:</p> <ul> <li> <p>Transformations in section \"all\" always are applied first, before any of the   transformations in the other 5 sections.</p> </li> <li> <p>Transformations in section \"spans\" are applied before transformations in   section \"span_events\".</p> </li> <li> <p>The order in which the transformations in remaining sections (\"resources\",   \"metrics\", logs\") are applied relative to each other or relative to \"spans\"   section is undefined (since there are not-interdependencies, the order does   not matter).</p> </li> </ul> <p>In the \"changes\" subsection of each particular one of these 6 sections the sequence of transformations is applied in the order it is listed in the schema file, from top to bottom.</p> <p>When converting in the opposite direction, from newer version Y to older version X the order of transformation listed above is exactly the reverse, with each individual transformation also performing the reverse conversion.</p>"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#schema-file-format-number","title":"Schema File Format Number","text":"<p>The \"file_format\" setting in the schema file specifies the format version of the file. The format version follows the MAJOR.MINOR.PATCH format, similar to semver 2.0.</p> <p>The \"file_format\" setting is used by consumers of the file to know if they are capable of interpreting the content of the file.</p> <p>The current value for this setting is \"1.1.0\". Any change to this number MUST be published in the specification.</p> <p>The current schema file format allows representing a limited set of transformations of telemetry data. We anticipate that in the future more types of transformations may be desirable to support or other, additional information may be desirable to record in the schema file.</p> <p>As the schema file format evolves over time the format version number SHOULD change according to the following rules:</p> <ul> <li>PATCH number SHOULD be increased when the file format changes in a way that   does not affect the existing consumers of the file. For example addition of a   completely new section in the schema file that has no effect on existing   sections and has no effect on any existing schema functionality may be done   via incrementing the PATCH number only. This approach is only valid if the new   setting in the file is completely and safely ignorable by all existing   processing logic.</li> </ul> <p>For example adding a completely new section that describes the full state of   the schema has no effect on existing consumers which only care about \"changes\"   section (unless we explicitly define the semantics of the new section such   that it needs to be taken into account when processing schema changes). So,   adding such a new section can be done using a PATCH number increase.</p> <ul> <li> <p>MINOR number SHOULD be increased if a new setting is added to the file format   in a backward compatible manner. \"Backward compatible\" in this context means   that consumers that are aware of the new MINOR number can consume the file of   a particular MINOR version number or of any MINOR version number lower than   that, provided that MAJOR version numbers are the same. This can happen for   example when:</p> </li> <li> <p>A new transformation type is added.</p> </li> <li> <p>A new setting is added to an existing transformation. The new setting is     optional and the default value of the setting matches the behavior of the     previous file format version.</p> </li> </ul> <p>Note: there is no \"forward compatibility\" based on MINOR version number.   Consumers which support reading up to a particular MINOR version number SHOULD   NOT attempt to consume files with higher MINOR version numbers.</p> <ul> <li>MAJOR number SHOULD be increased if the file format is changed in an   incompatible way. This means the consumers of the file need to parse or   interpret the file differently compared to previous MAJOR version.</li> </ul> <p>Correspondingly:</p> <ul> <li> <p>Consumers of the schema file SHOULD NOT attempt to interpret the schema file   if the MAJOR version number is different (higher or lower) than what the   consumer supports.</p> </li> <li> <p>Consumers of the schema file SHOULD NOT attempt to interpret the schema file   if the MINOR version number is higher than what the consumer supports.</p> </li> <li> <p>Consumers MAY ignore the PATCH number.</p> </li> </ul> <p>To illustrate this with some examples:</p> File Format Version Consumer's Expected Version Consumer Can Read? 1.0.0     1.0.0     yes     1.0.x     1.0.y     yes, for any x and y.     1.a.x     1.b.x     if a&lt;b then yes, otherwise no.     2.0.0     1.x.y     no"},{"location":"docs/specs/otel/schemas/file_format_v1.1.0/#appendix-a-example-schema-file","title":"Appendix A. Example Schema File","text":"<pre><code># Defines the file format. MUST be set to 1.1.0.\nfile_format: 1.1.0\n# The Schema URL that this file is published at. The version number in the URL\n# MUST match the highest version number in the \"versions\" section below.\n# Note: the schema version number in the URL is not related in any way to\n# the file_format setting above.\nschema_url: https://opentelemetry.io/schemas/1.1.0\n# Definitions for each schema version in this family.\n# Note: the ordering of versions is defined according to semver\n# version number ordering rules.\nversions:\n1.1.0:\n# Definitions for version 1.1.0.\nall:\n# Definitions that apply to all data types.\nchanges:\n# Transformations to apply when converting from version 1.0.0 to 1.1.0.\n- rename_attributes:\n# map of key/values. The keys are the old attribute name used\n# the previous version, the values are the new attribute name\n# starting from this version.\n# Rename k8s.* to kubernetes.*\nk8s.cluster.name: kubernetes.cluster.name\nk8s.namespace.name: kubernetes.namespace.name\nk8s.node.name: kubernetes.node.name\nk8s.node.uid: kubernetes.node.uid\nk8s.pod.name: kubernetes.pod.name\nk8s.pod.uid: kubernetes.pod.uid\nk8s.container.name: kubernetes.container.name\nk8s.replicaset.name: kubernetes.replicaset.name\nk8s.replicaset.uid: kubernetes.replicaset.uid\nk8s.cronjob.name: kubernetes.cronjob.name\nk8s.cronjob.uid: kubernetes.cronjob.uid\nk8s.job.name: kubernetes.job.name\nk8s.job.uid: kubernetes.job.uid\nk8s.statefulset.name: kubernetes.statefulset.name\nk8s.statefulset.uid: kubernetes.statefulset.uid\nk8s.daemonset.name: kubernetes.daemonset.name\nk8s.daemonset.uid: kubernetes.daemonset.uid\nk8s.deployment.name: kubernetes.deployment.name\nk8s.deployment.uid: kubernetes.deployment.uid\nresources:\n# Definitions that apply to Resource data type.\nchanges:\n- rename_attributes:\ntelemetry.auto.version: telemetry.auto_instr.version\nspans:\n# Definitions that apply to Span data type.\nchanges:\n- rename_attributes:\nattribute_map:\n# map of key/values. The keys are the old attribute name used\n# in the previous version, the values are the new attribute name\n# starting from this version.\npeer.service: peer.service.name\napply_to_spans:\n# apply only to spans named \"HTTP GET\"\n- 'HTTP GET'\nspan_events:\n# Definitions that apply to Span Event data type.\nchanges:\n- rename_events:\n# The keys are old event name used in the previous version, the\n# values are the new event name starting from this version.\nname_map: { stacktrace: stack_trace }\n- rename_attributes:\nattribute_map:\npeer.service: peer.service.name\napply_to_events:\n# Optional event names to apply to. If empty applies to all events.\n- exception.stack_trace\nmetrics:\n# Definitions that apply to Metric data type.\nchanges:\n- rename_metrics:\n# map of key/values. The keys are the old metric name used\n# in the previous version, the values are the new metric name\n# starting from this version.\ncontainer.cpu.usage.total: cpu.usage.total\ncontainer.memory.usage.max: memory.usage.max\n- rename_attributes:\nattribute_map:\nstatus: state\napply_to_metrics:\n# Optional. If it is missing the transformation is applied\n# to all metrics. If it is present the transformation is applied\n# only to the metrics with the name that is found in the sequence\n# specified below.\n- system.cpu.utilization\n- system.memory.usage\n- system.memory.utilization\n- system.paging.usage\n- split:\n# Example from the change done by https://github.com/open-telemetry/opentelemetry-specification/pull/2617\n# Name of old metric to split.\napply_to_metric: system.paging.operations\n# Name of attribute in the old metric to use for splitting. The attribute will be\n# eliminated, the new metric will not have it.\nby_attribute: direction\n# Names of new metrics to create, one for each possible value of attribute.\nmetrics_from_attributes:\n# If \"direction\" attribute equals \"in\" create a new metric called \"system.paging.operations.in\".\nsystem.paging.operations.in: in\n# If \"direction\" attribute equals \"out\" create a new metric called \"system.paging.operations.out\".\nsystem.paging.operations.out: out\nlogs:\n# Definitions that apply to LogRecord data type.\nchanges:\n- rename_attributes:\nattribute_map:\nprocess.executable_name: process.executable.name\n1.0.0:\n# First version of this schema family.\n</code></pre>"},{"location":"docs/specs/otel/trace/","title":"\u8ffd\u8e2a","text":""},{"location":"docs/specs/otel/trace/api/","title":"\u8ffd\u8e2a API","text":"<p>Status: Stable, Feature-freeze</p> Table of Contents   - [Data types](#data-types)   - [Time](#time)     - [Timestamp](#timestamp)     - [Duration](#duration) - [TracerProvider](#tracerprovider)   - [TracerProvider operations](#tracerprovider-operations)     - [Get a Tracer](#get-a-tracer) - [Context Interaction](#context-interaction) - [Tracer](#tracer)   - [Tracer operations](#tracer-operations) - [SpanContext](#spancontext)   - [Retrieving the TraceId and SpanId](#retrieving-the-traceid-and-spanid)   - [IsValid](#isvalid)   - [IsRemote](#isremote)   - [TraceState](#tracestate) - [Span](#span)   - [Span Creation](#span-creation)     - [Determining the Parent Span from a Context](#determining-the-parent-span-from-a-context)     - [Specifying links](#specifying-links)   - [Span operations](#span-operations)     - [Get Context](#get-context)     - [IsRecording](#isrecording)     - [Set Attributes](#set-attributes)     - [Add Events](#add-events)     - [Set Status](#set-status)     - [UpdateName](#updatename)     - [End](#end)     - [Record Exception](#record-exception)   - [Span lifetime](#span-lifetime)   - [Wrapping a SpanContext in a Span](#wrapping-a-spancontext-in-a-span) - [SpanKind](#spankind) - [Concurrency](#concurrency) - [Included Propagators](#included-propagators) - [Behavior of the API in the absence of an installed SDK](#behavior-of-the-api-in-the-absence-of-an-installed-sdk)   <p>The Tracing API consist of these main classes:</p> <ul> <li><code>TracerProvider</code> is the entry point of the API. It provides   access to <code>Tracer</code>s.</li> <li><code>Tracer</code> is the class responsible for creating <code>Span</code>s.</li> <li><code>Span</code> is the API to trace an operation.</li> </ul>"},{"location":"docs/specs/otel/trace/api/#data-types","title":"Data types","text":"<p>While languages and platforms have different ways of representing data, this section defines some generic requirements for this API.</p>"},{"location":"docs/specs/otel/trace/api/#time","title":"Time","text":"<p>OpenTelemetry can operate on time values up to nanosecond (ns) precision. The representation of those values is language specific.</p>"},{"location":"docs/specs/otel/trace/api/#timestamp","title":"Timestamp","text":"<p>A timestamp is the time elapsed since the Unix epoch.</p> <ul> <li>The minimal precision is milliseconds.</li> <li>The maximal precision is nanoseconds.</li> </ul>"},{"location":"docs/specs/otel/trace/api/#duration","title":"Duration","text":"<p>A duration is the elapsed time between two events.</p> <ul> <li>The minimal precision is milliseconds.</li> <li>The maximal precision is nanoseconds.</li> </ul>"},{"location":"docs/specs/otel/trace/api/#tracerprovider","title":"TracerProvider","text":"<p><code>Tracer</code>s can be accessed with a <code>TracerProvider</code>.</p> <p>In implementations of the API, the <code>TracerProvider</code> is expected to be the stateful object that holds any configuration.</p> <p>Normally, the <code>TracerProvider</code> is expected to be accessed from a central place. Thus, the API SHOULD provide a way to set/register and access a global default <code>TracerProvider</code>.</p> <p>Notwithstanding any global <code>TracerProvider</code>, some applications may want to or have to use multiple <code>TracerProvider</code> instances, e.g. to have different configuration (like <code>SpanProcessor</code>s) for each (and consequently for the <code>Tracer</code>s obtained from them), or because its easier with dependency injection frameworks. Thus, implementations of <code>TracerProvider</code> SHOULD allow creating an arbitrary number of <code>TracerProvider</code> instances.</p>"},{"location":"docs/specs/otel/trace/api/#tracerprovider-operations","title":"TracerProvider operations","text":"<p>The <code>TracerProvider</code> MUST provide the following functions:</p> <ul> <li>Get a <code>Tracer</code></li> </ul>"},{"location":"docs/specs/otel/trace/api/#get-a-tracer","title":"Get a Tracer","text":"<p>This API MUST accept the following parameters:</p> <ul> <li><code>name</code> (required): This name SHOULD uniquely identify the   instrumentation scope, such as the   instrumentation library (e.g.   <code>io.opentelemetry.contrib.mongodb</code>), package, module or class name. If an   application or library has built-in OpenTelemetry instrumentation, both   Instrumented library and   Instrumentation library may refer to   the same library. In that scenario, the <code>name</code> denotes a module name or   component name within that library or application. In case an invalid name   (null or empty string) is specified, a working Tracer implementation MUST be   returned as a fallback rather than returning null or throwing an exception,   its <code>name</code> property SHOULD be set to an empty string, and a message   reporting that the specified value is invalid SHOULD be logged. A library,   implementing the OpenTelemetry API may also ignore this name and return a   default instance for all calls, if it does not support \"named\" functionality   (e.g. an implementation which is not even observability-related). A   TracerProvider could also return a no-op Tracer here if application owners   configure the SDK to suppress telemetry produced by this library.</li> <li><code>version</code> (optional): Specifies the version of the instrumentation scope if   the scope has a version (e.g. a library version). Example value: <code>1.0.0</code>.</li> <li>[since 1.4.0] <code>schema_url</code> (optional): Specifies the Schema URL that should be   recorded in the emitted telemetry.</li> <li>[since 1.13.0] <code>attributes</code> (optional): Specifies the instrumentation scope   attributes to associate with emitted telemetry.</li> </ul> <p>Tracers are identified by <code>name</code>, <code>version</code>, and <code>schema_url</code> fields. When more than one <code>Tracer</code> of the same <code>name</code>, <code>version</code>, and <code>schema_url</code> is created, it is unspecified whether or under which conditions the same or different <code>Tracer</code> instances are returned. It is a user error to create Tracers with different attributes but the same identity.</p> <p>The term identical applied to Tracers describes instances where all identifying fields are equal. The term distinct applied to Tracers describes instances where at least one identifying field has a different value.</p> <p>Implementations MUST NOT require users to repeatedly obtain a <code>Tracer</code> again with the same identity to pick up configuration changes. This can be achieved either by allowing to work with an outdated configuration or by ensuring that new configuration applies also to previously returned <code>Tracer</code>s.</p> <p>Note: This could, for example, be implemented by storing any mutable configuration in the <code>TracerProvider</code> and having <code>Tracer</code> implementation objects have a reference to the <code>TracerProvider</code> from which they were obtained. If configuration must be stored per-tracer (such as disabling a certain tracer), the tracer could, for example, do a look-up with its identity in a map in the <code>TracerProvider</code>, or the <code>TracerProvider</code> could maintain a registry of all returned <code>Tracer</code>s and actively update their configuration if it changes.</p> <p>The effect of associating a Schema URL with a <code>Tracer</code> MUST be that the telemetry emitted using the <code>Tracer</code> will be associated with the Schema URL, provided that the emitted data format is capable of representing such association.</p>"},{"location":"docs/specs/otel/trace/api/#context-interaction","title":"Context Interaction","text":"<p>This section defines all operations within the Tracing API that interact with the <code>Context</code>.</p> <p>The API MUST provide the following functionality to interact with a <code>Context</code> instance:</p> <ul> <li>Extract the <code>Span</code> from a <code>Context</code> instance</li> <li>Combine the <code>Span</code> with a <code>Context</code> instance, creating a new <code>Context</code>   instance</li> </ul> <p>The functionality listed above is necessary because API users SHOULD NOT have access to the Context Key used by the Tracing API implementation.</p> <p>If the language has support for implicitly propagated <code>Context</code> (see here), the API SHOULD also provide the following functionality:</p> <ul> <li>Get the currently active span from the implicit context. This is equivalent to   getting the implicit context, then extracting the <code>Span</code> from the context.</li> <li>Set the currently active span into a new context, and make that the implicit   context. This is equivalent to combining the current implicit context's values   with the <code>Span</code> to create a new context, which is then made the current   implicit context.</li> </ul> <p>All the above functionalities operate solely on the context API, and they MAY be exposed as either static methods on the trace module, or as static methods on a class inside the trace module. This functionality SHOULD be fully implemented in the API when possible.</p>"},{"location":"docs/specs/otel/trace/api/#tracer","title":"Tracer","text":"<p>The tracer is responsible for creating <code>Span</code>s.</p> <p>Note that <code>Tracer</code>s should usually not be responsible for configuration. This should be the responsibility of the <code>TracerProvider</code> instead.</p>"},{"location":"docs/specs/otel/trace/api/#tracer-operations","title":"Tracer operations","text":"<p>The <code>Tracer</code> MUST provide functions to:</p> <ul> <li>Create a new <code>Span</code> (see the section on <code>Span</code>)</li> </ul>"},{"location":"docs/specs/otel/trace/api/#spancontext","title":"SpanContext","text":"<p>A <code>SpanContext</code> represents the portion of a <code>Span</code> which must be serialized and propagated along side of a distributed context. <code>SpanContext</code>s are immutable.</p> <p>The OpenTelemetry <code>SpanContext</code> representation conforms to the W3C TraceContext specification. It contains two identifiers - a <code>TraceId</code> and a <code>SpanId</code> - along with a set of common <code>TraceFlags</code> and system-specific <code>TraceState</code> values.</p> <p><code>TraceId</code> A valid trace identifier is a 16-byte array with at least one non-zero byte.</p> <p><code>SpanId</code> A valid span identifier is an 8-byte array with at least one non-zero byte.</p> <p><code>TraceFlags</code> contain details about the trace. Unlike TraceState values, TraceFlags are present in all traces. The current version of the specification only supports a single flag called sampled.</p> <p><code>TraceState</code> carries vendor-specific trace identification data, represented as a list of key-value pairs. TraceState allows multiple tracing systems to participate in the same trace. It is fully described in the W3C Trace Context specification. For specific OTel values in <code>TraceState</code>, see the TraceState Handling document.</p> <p><code>IsRemote</code>, a boolean indicating whether the SpanContext was received from somewhere else or locally generated, see IsRemote.</p> <p>The API MUST implement methods to create a <code>SpanContext</code>. These methods SHOULD be the only way to create a <code>SpanContext</code>. This functionality MUST be fully implemented in the API, and SHOULD NOT be overridable.</p>"},{"location":"docs/specs/otel/trace/api/#retrieving-the-traceid-and-spanid","title":"Retrieving the TraceId and SpanId","text":"<p>The API MUST allow retrieving the <code>TraceId</code> and <code>SpanId</code> in the following forms:</p> <ul> <li>Hex - returns the lowercase   hex encoded <code>TraceId</code> (result   MUST be a 32-hex-character lowercase string) or <code>SpanId</code> (result MUST be a   16-hex-character lowercase string).</li> <li>Binary - returns the binary representation of the <code>TraceId</code> (result MUST be a   16-byte array) or <code>SpanId</code> (result MUST be an 8-byte array).</li> </ul> <p>The API SHOULD NOT expose details about how they are internally stored.</p>"},{"location":"docs/specs/otel/trace/api/#isvalid","title":"IsValid","text":"<p>An API called <code>IsValid</code>, that returns a boolean value, which is <code>true</code> if the SpanContext has a non-zero TraceID and a non-zero SpanID, MUST be provided.</p>"},{"location":"docs/specs/otel/trace/api/#isremote","title":"IsRemote","text":"<p>An API called <code>IsRemote</code>, that returns a boolean value, which is <code>true</code> if the SpanContext was propagated from a remote parent, MUST be provided. When extracting a <code>SpanContext</code> through the Propagators API, <code>IsRemote</code> MUST return true, whereas for the SpanContext of any child spans it MUST return false.</p>"},{"location":"docs/specs/otel/trace/api/#tracestate","title":"TraceState","text":"<p><code>TraceState</code> is a part of <code>SpanContext</code>, represented by an immutable list of string key/value pairs and formally defined by the W3C Trace Context specification. Tracing API MUST provide at least the following operations on <code>TraceState</code>:</p> <ul> <li>Get value for a given key</li> <li>Add a new key/value pair</li> <li>Update an existing value for a given key</li> <li>Delete a key/value pair</li> </ul> <p>These operations MUST follow the rules described in the W3C Trace Context specification. All mutating operations MUST return a new <code>TraceState</code> with the modifications applied. <code>TraceState</code> MUST at all times be valid according to rules specified in W3C Trace Context specification. Every mutating operations MUST validate input parameters. If invalid value is passed the operation MUST NOT return <code>TraceState</code> containing invalid data and MUST follow the general error handling guidelines.</p> <p>Please note, since <code>SpanContext</code> is immutable, it is not possible to update <code>SpanContext</code> with a new <code>TraceState</code>. Such changes then make sense only right before <code>SpanContext</code> propagation or telemetry data exporting. In both cases, <code>Propagator</code>s and <code>SpanExporter</code>s may create a modified <code>TraceState</code> copy before serializing it to the wire.</p>"},{"location":"docs/specs/otel/trace/api/#span","title":"Span","text":"<p>A <code>Span</code> represents a single operation within a trace. Spans can be nested to form a trace tree. Each trace contains a root span, which typically describes the entire operation and, optionally, one or more sub-spans for its sub-operations.</p> <p><code>Span</code>s encapsulate:</p> <ul> <li>The span name</li> <li>An immutable <code>SpanContext</code> that uniquely identifies the <code>Span</code></li> <li>A parent span in the form of a <code>Span</code>, <code>SpanContext</code>,   or null</li> <li>A <code>SpanKind</code></li> <li>A start timestamp</li> <li>An end timestamp</li> <li><code>Attributes</code></li> <li>A list of <code>Link</code>s to other <code>Span</code>s</li> <li>A list of timestamped <code>Event</code>s</li> <li>A <code>Status</code>.</li> </ul> <p>The span name concisely identifies the work represented by the Span, for example, an RPC method name, a function name, or the name of a subtask or stage within a larger computation. The span name SHOULD be the most general string that identifies a (statistically) interesting class of Spans, rather than individual Span instances while still being human-readable. That is, \"get_user\" is a reasonable name, while \"get_user/314159\", where \"314159\" is a user ID, is not a good name due to its high cardinality. Generality SHOULD be prioritized over human-readability.</p> <p>For example, here are potential span names for an endpoint that gets a hypothetical account information:</p> Span Name Guidance <code>get</code> Too general <code>get_account/42</code> Too specific <code>get_account</code> Good, and account_id=42 would make a nice Span attribute <code>get_account/{accountId}</code> Also good (using the \"HTTP route\") <p>The <code>Span</code>'s start and end timestamps reflect the elapsed real time of the operation.</p> <p>For example, if a span represents a request-response cycle (e.g. HTTP or an RPC), the span should have a start time that corresponds to the start time of the first sub-operation, and an end time of when the final sub-operation is complete. This includes:</p> <ul> <li>receiving the data from the request</li> <li>parsing of the data (e.g. from a binary or json format)</li> <li>any middleware or additional processing logic</li> <li>business logic</li> <li>construction of the response</li> <li>sending of the response</li> </ul> <p>Child spans (or in some cases events) may be created to represent sub-operations which require more detailed observability. Child spans should measure the timing of the respective sub-operation, and may add additional attributes.</p> <p>A <code>Span</code>'s start time SHOULD be set to the current time on span creation. After the <code>Span</code> is created, it SHOULD be possible to change its name, set its <code>Attribute</code>s, add <code>Event</code>s, and set the <code>Status</code>. These MUST NOT be changed after the <code>Span</code>'s end time has been set.</p> <p><code>Span</code>s are not meant to be used to propagate information within a process. To prevent misuse, implementations SHOULD NOT provide access to a <code>Span</code>'s attributes besides its <code>SpanContext</code>.</p> <p>Vendors may implement the <code>Span</code> interface to effect vendor-specific logic. However, alternative implementations MUST NOT allow callers to create <code>Span</code>s directly. All <code>Span</code>s MUST be created via a <code>Tracer</code>.</p>"},{"location":"docs/specs/otel/trace/api/#span-creation","title":"Span Creation","text":"<p>There MUST NOT be any API for creating a <code>Span</code> other than with a <code>Tracer</code>.</p> <p>In languages with implicit <code>Context</code> propagation, <code>Span</code> creation MUST NOT set the newly created <code>Span</code> as the active <code>Span</code> in the current <code>Context</code> by default, but this functionality MAY be offered additionally as a separate operation.</p> <p>The API MUST accept the following parameters:</p> <ul> <li>The span name. This is a required parameter.</li> <li>The parent <code>Context</code> or an indication that the new <code>Span</code> should be a root   <code>Span</code>. The API MAY also have an option for implicitly using the current   Context as parent as a default behavior. This API MUST NOT accept a <code>Span</code> or   <code>SpanContext</code> as parent, only a full <code>Context</code>.</li> </ul> <p>The semantic parent of the Span MUST be determined according to the rules   described in   Determining the Parent Span from a Context.</p> <ul> <li><code>SpanKind</code>, default to <code>SpanKind.Internal</code> if not specified.</li> <li><code>Attributes</code>. Additionally, these attributes   may be used to make a sampling decision as noted in   sampling description. An empty collection will be assumed   if not specified.</li> </ul> <p>The API documentation MUST state that adding attributes at span creation is   preferred to calling <code>SetAttribute</code> later, as samplers can only consider   information already present during span creation.</p> <ul> <li><code>Link</code>s - an ordered sequence of Links, see API definition   here.</li> <li><code>Start timestamp</code>, default to current time. This argument SHOULD only be set   when span creation time has already passed. If API is called at a moment of a   Span logical start, API user MUST NOT explicitly set this argument.</li> </ul> <p>Each span has zero or one parent span and zero or more child spans, which represent causally related operations. A tree of related spans comprises a trace. A span is said to be a root span if it does not have a parent. Each trace includes a single root span, which is the shared ancestor of all other spans in the trace. Implementations MUST provide an option to create a <code>Span</code> as a root span, and MUST generate a new <code>TraceId</code> for each root span created. For a Span with a parent, the <code>TraceId</code> MUST be the same as the parent. Also, the child span MUST inherit all <code>TraceState</code> values of its parent by default.</p> <p>A <code>Span</code> is said to have a remote parent if it is the child of a <code>Span</code> created in another process. Each propagators' deserialization must set <code>IsRemote</code> to true on a parent <code>SpanContext</code> so <code>Span</code> creation knows if the parent is remote.</p> <p>Any span that is created MUST also be ended. This is the responsibility of the user. API implementations MAY leak memory or other resources (including, for example, CPU time for periodic work that iterates all spans) if the user forgot to end the span.</p>"},{"location":"docs/specs/otel/trace/api/#determining-the-parent-span-from-a-context","title":"Determining the Parent Span from a Context","text":"<p>When a new <code>Span</code> is created from a <code>Context</code>, the <code>Context</code> may contain a <code>Span</code> representing the currently active instance, and will be used as parent. If there is no <code>Span</code> in the <code>Context</code>, the newly created <code>Span</code> will be a root span.</p> <p>A <code>SpanContext</code> cannot be set as active in a <code>Context</code> directly, but by wrapping it into a Span. For example, a <code>Propagator</code> performing context extraction may need this.</p>"},{"location":"docs/specs/otel/trace/api/#specifying-links","title":"Specifying links","text":"<p>During <code>Span</code> creation, a user MUST have the ability to record links to other <code>Span</code>s. Linked <code>Span</code>s can be from the same or a different trace -- see Links between spans. <code>Link</code>s cannot be added after Span creation.</p> <p>A <code>Link</code> is structurally defined by the following properties:</p> <ul> <li><code>SpanContext</code> of the <code>Span</code> to link to.</li> <li>Zero or more <code>Attributes</code> further describing   the link.</li> </ul> <p>The Span creation API MUST provide:</p> <ul> <li>An API to record a single <code>Link</code> where the <code>Link</code> properties are passed as   arguments. This MAY be called <code>AddLink</code>. This API takes the <code>SpanContext</code> of   the <code>Span</code> to link to and optional <code>Attributes</code>, either as individual   parameters or as an immutable object encapsulating them, whichever is most   appropriate for the language. Implementations MAY ignore links with an   invalid <code>SpanContext</code>.</li> </ul> <p>Links SHOULD preserve the order in which they're set.</p>"},{"location":"docs/specs/otel/trace/api/#span-operations","title":"Span operations","text":"<p>With the exception of the function to retrieve the <code>Span</code>'s <code>SpanContext</code> and <code>IsRecording</code>, none of the below may be called after the <code>Span</code> is finished.</p>"},{"location":"docs/specs/otel/trace/api/#get-context","title":"Get Context","text":"<p>The Span interface MUST provide:</p> <ul> <li>An API that returns the <code>SpanContext</code> for the given <code>Span</code>. The returned value   may be used even after the <code>Span</code> is finished. The returned value MUST be the   same for the entire Span lifetime. This MAY be called <code>GetContext</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/api/#isrecording","title":"IsRecording","text":"<p>A <code>Span</code> is recording (<code>IsRecording</code> returns <code>true</code>) when the data provided to it via functions like <code>SetAttributes</code>, <code>AddEvent</code>, <code>SetStatus</code> is captured in some form (e.g. in memory). When a <code>Span</code> is not recording (<code>IsRecording</code> returns <code>false</code>), all this data is discarded right away. Further attempts to set or add data will not record, making the span effectively a no-op.</p> <p>This flag may be <code>true</code> despite the entire trace not being sampled. This allows information about the individual Span to be recorded and processed without sending it to the backend. An example of this scenario may be recording and processing of all incoming requests for the processing and building of SLA/SLO latency charts while sending only a subset - sampled spans - to the backend. See also the sampling section of SDK design.</p> <p>After a <code>Span</code> is ended, it SHOULD become non-recording and <code>IsRecording</code> SHOULD always return <code>false</code>. The one known exception to this is streaming implementations of the API that do not keep local state and cannot change the value of <code>IsRecording</code> after ending the span.</p> <p><code>IsRecording</code> SHOULD NOT take any parameters.</p> <p>This flag SHOULD be used to avoid expensive computations of a Span attributes or events in case when a Span is definitely not recorded. Note that any child span's recording is determined independently from the value of this flag (typically based on the <code>sampled</code> flag of a <code>TraceFlags</code> on SpanContext).</p> <p>Users of the API should only access the <code>IsRecording</code> property when instrumenting code and never access <code>SampledFlag</code> unless used in context propagators.</p>"},{"location":"docs/specs/otel/trace/api/#set-attributes","title":"Set Attributes","text":"<p>A <code>Span</code> MUST have the ability to set <code>Attributes</code> associated with it.</p> <p>The Span interface MUST provide:</p> <ul> <li>An API to set a single <code>Attribute</code> where the attribute properties are passed   as arguments. This MAY be called <code>SetAttribute</code>. To avoid extra allocations   some implementations may offer a separate API for each of the possible value   types.</li> </ul> <p>The Span interface MAY provide:</p> <ul> <li>An API to set multiple <code>Attributes</code> at once, where the <code>Attributes</code> are passed   in a single method call.</li> </ul> <p>Setting an attribute with the same key as an existing attribute SHOULD overwrite the existing attribute's value.</p> <p>Note that the OpenTelemetry project documents certain \"standard attributes\" that have prescribed semantic meanings.</p> <p>Note that Samplers can only consider information already present during span creation. Any changes done later, including new or changed attributes, cannot change their decisions.</p>"},{"location":"docs/specs/otel/trace/api/#add-events","title":"Add Events","text":"<p>A <code>Span</code> MUST have the ability to add events. Events have a time associated with the moment when they are added to the <code>Span</code>.</p> <p>An <code>Event</code> is structurally defined by the following properties:</p> <ul> <li>Name of the event.</li> <li>A timestamp for the event. Either the time at which the event was added or a   custom timestamp provided by the user.</li> <li>Zero or more <code>Attributes</code> further describing   the event.</li> </ul> <p>The Span interface MUST provide:</p> <ul> <li>An API to record a single <code>Event</code> where the <code>Event</code> properties are passed as   arguments. This MAY be called <code>AddEvent</code>. This API takes the name of the   event, optional <code>Attributes</code> and an optional <code>Timestamp</code> which can be used to   specify the time at which the event occurred, either as individual parameters   or as an immutable object encapsulating them, whichever is most appropriate   for the language. If no custom timestamp is provided by the user, the   implementation automatically sets the time at which this API is called on the   event.</li> </ul> <p>Events SHOULD preserve the order in which they are recorded. This will typically match the ordering of the events' timestamps, but events may be recorded out-of-order using custom timestamps.</p> <p>Consumers should be aware that an event's timestamp might be before the start or after the end of the span if custom timestamps were provided by the user for the event or when starting or ending the span. The specification does not require any normalization if provided timestamps are out of range.</p> <p>Note that the OpenTelemetry project documents certain \"standard event names and keys\" which have prescribed semantic meanings.</p> <p>Note that <code>RecordException</code> is a specialized variant of <code>AddEvent</code> for recording exception events.</p>"},{"location":"docs/specs/otel/trace/api/#set-status","title":"Set Status","text":"<p>Sets the <code>Status</code> of the <code>Span</code>. If used, this will override the default <code>Span</code> status, which is <code>Unset</code>.</p> <p><code>Status</code> is structurally defined by the following properties:</p> <ul> <li><code>StatusCode</code>, one of the values listed below.</li> <li>Optional <code>Description</code> that provides a descriptive message of the <code>Status</code>.   <code>Description</code> MUST only be used with the <code>Error</code> <code>StatusCode</code> value. An empty   <code>Description</code> is equivalent with a not present one.</li> </ul> <p>Note: The OTLP protocol definition refers to the <code>Description</code> property as <code>message</code>.</p> <p><code>StatusCode</code> is one of the following values:</p> <ul> <li><code>Unset</code></li> <li>The default status.</li> <li><code>Ok</code></li> <li>The operation has been validated by an Application developer or Operator to     have completed successfully.</li> <li><code>Error</code></li> <li>The operation contains an error.</li> </ul> <p>These values form a total order: <code>Ok &gt; Error &gt; Unset</code>. This means that setting <code>Status</code> with <code>StatusCode=Ok</code> will override any prior or future attempts to set span <code>Status</code> with <code>StatusCode=Error</code> or <code>StatusCode=Unset</code>. See below for more specific rules.</p> <p>The Span interface MUST provide:</p> <ul> <li>An API to set the <code>Status</code>. This SHOULD be called <code>SetStatus</code>. This API takes   the <code>StatusCode</code>, and an optional <code>Description</code>, either as individual   parameters or as an immutable object encapsulating them, whichever is most   appropriate for the language. <code>Description</code> MUST be IGNORED for <code>StatusCode</code> <code>Ok</code> &amp; <code>Unset</code> values.</li> </ul> <p>The status code SHOULD remain unset, except for the following circumstances:</p> <p>An attempt to set value <code>Unset</code> SHOULD be ignored.</p> <p>When the status is set to <code>Error</code> by Instrumentation Libraries, the <code>Description</code> SHOULD be documented and predictable. The status code should only be set to <code>Error</code> according to the rules defined within the semantic conventions. For operations not covered by the semantic conventions, Instrumentation Libraries SHOULD publish their own conventions, including possible values of <code>Description</code> and what they mean.</p> <p>Generally, Instrumentation Libraries SHOULD NOT set the status code to <code>Ok</code>, unless explicitly configured to do so. Instrumentation Libraries SHOULD leave the status code as <code>Unset</code> unless there is an error, as described above.</p> <p>Application developers and Operators may set the status code to <code>Ok</code>.</p> <p>When span status is set to <code>Ok</code> it SHOULD be considered final and any further attempts to change it SHOULD be ignored.</p> <p>Analysis tools SHOULD respond to an <code>Ok</code> status by suppressing any errors they would otherwise generate. For example, to suppress noisy errors such as 404s.</p> <p>Only the value of the last call will be recorded, and implementations are free to ignore previous calls.</p>"},{"location":"docs/specs/otel/trace/api/#updatename","title":"UpdateName","text":"<p>Updates the <code>Span</code> name. Upon this update, any sampling behavior based on <code>Span</code> name will depend on the implementation.</p> <p>Note that Samplers can only consider information already present during span creation. Any changes done later, including updated span name, cannot change their decisions.</p> <p>Alternatives for the name update may be late <code>Span</code> creation, when Span is started with the explicit timestamp from the past at the moment where the final <code>Span</code> name is known, or reporting a <code>Span</code> with the desired name as a child <code>Span</code>.</p> <p>Required parameters:</p> <ul> <li>The new span name, which supersedes whatever was passed in when the <code>Span</code>   was started</li> </ul>"},{"location":"docs/specs/otel/trace/api/#end","title":"End","text":"<p>Signals that the operation described by this span has now (or at the time optionally specified) ended.</p> <p>Implementations SHOULD ignore all subsequent calls to <code>End</code> and any other Span methods, i.e. the Span becomes non-recording by being ended (there might be exceptions when Tracer is streaming events and has no mutable state associated with the <code>Span</code>).</p> <p>Language SIGs MAY provide methods other than <code>End</code> in the API that also end the span to support language-specific features like <code>with</code> statements in Python. However, all API implementations of such methods MUST internally call the <code>End</code> method and be documented to do so.</p> <p><code>End</code> MUST NOT have any effects on child spans. Those may still be running and can be ended later.</p> <p><code>End</code> MUST NOT inactivate the <code>Span</code> in any <code>Context</code> it is active in. It MUST still be possible to use an ended span as parent via a Context it is contained in. Also, any mechanisms for putting the Span into a Context MUST still work after the Span was ended.</p> <p>Parameters:</p> <ul> <li>(Optional) Timestamp to explicitly set the end timestamp. If omitted, this   MUST be treated equivalent to passing the current time.</li> </ul> <p>Expect this operation to be called in the \"hot path\" of production applications. It needs to be designed to complete fast, if not immediately. This operation itself MUST NOT perform blocking I/O on the calling thread. Any locking used needs be minimized and SHOULD be removed entirely if possible. Some downstream SpanProcessors and subsequent SpanExporters called from this operation may be used for testing, proof-of-concept ideas, or debugging and may not be designed for production use themselves. They are not in the scope of this requirement and recommendation.</p>"},{"location":"docs/specs/otel/trace/api/#record-exception","title":"Record Exception","text":"<p>To facilitate recording an exception languages SHOULD provide a <code>RecordException</code> method if the language uses exceptions. This is a specialized variant of <code>AddEvent</code>, so for anything not specified here, the same requirements as for <code>AddEvent</code> apply.</p> <p>The signature of the method is to be determined by each language and can be overloaded as appropriate. The method MUST record an exception as an <code>Event</code> with the conventions outlined in the exceptions document. The minimum required argument SHOULD be no more than only an exception object.</p> <p>If <code>RecordException</code> is provided, the method MUST accept an optional parameter to provide any additional event attributes (this SHOULD be done in the same way as for the <code>AddEvent</code> method). If attributes with the same name would be generated by the method already, the additional attributes take precedence.</p> <p>Note: <code>RecordException</code> may be seen as a variant of <code>AddEvent</code> with additional exception-specific parameters and all other parameters being optional (because they have defaults from the exception semantic convention).</p>"},{"location":"docs/specs/otel/trace/api/#span-lifetime","title":"Span lifetime","text":"<p>Span lifetime represents the process of recording the start and the end timestamps to the Span object:</p> <ul> <li>The start time is recorded when the Span is created.</li> <li>The end time needs to be recorded when the operation is ended.</li> </ul> <p>Start and end time as well as Event's timestamps MUST be recorded at a time of a calling of corresponding API.</p>"},{"location":"docs/specs/otel/trace/api/#wrapping-a-spancontext-in-a-span","title":"Wrapping a SpanContext in a Span","text":"<p>The API MUST provide an operation for wrapping a <code>SpanContext</code> with an object implementing the <code>Span</code> interface. This is done in order to expose a <code>SpanContext</code> as a <code>Span</code> in operations such as in-process <code>Span</code> propagation.</p> <p>If a new type is required for supporting this operation, it SHOULD NOT be exposed publicly if possible (e.g. by only exposing a function that returns something with the Span interface type). If a new type is required to be publicly exposed, it SHOULD be named <code>NonRecordingSpan</code>.</p> <p>The behavior is defined as follows:</p> <ul> <li><code>GetContext</code> MUST return the wrapped <code>SpanContext</code>.</li> <li><code>IsRecording</code> MUST return <code>false</code> to signal that events, attributes and other   elements are not being recorded, i.e. they are being dropped.</li> </ul> <p>The remaining functionality of <code>Span</code> MUST be defined as no-op operations. Note: This includes <code>End</code>, so as an exception from the general rule, it is not required (or even helpful) to end such a Span.</p> <p>This functionality MUST be fully implemented in the API, and SHOULD NOT be overridable.</p>"},{"location":"docs/specs/otel/trace/api/#spankind","title":"SpanKind","text":"<p><code>SpanKind</code> describes the relationship between the Span, its parents, and its children in a Trace. <code>SpanKind</code> describes two independent properties that benefit tracing systems during analysis.</p> <p>The first property described by <code>SpanKind</code> reflects whether the Span is a \"logical\" remote child or parent. By \"logical\", we mean that the span is logically a remote child or parent, from the point of view of the library that is being instrumented. Spans with a remote parent are interesting because they are sources of external load. Spans with a remote child are interesting because they reflect a non-local system dependency.</p> <p>The second property described by <code>SpanKind</code> reflects whether a child Span represents a synchronous call. When a child span is synchronous, the parent is expected to wait for it to complete under ordinary circumstances. It can be useful for tracing systems to know this property, since synchronous Spans may contribute to the overall trace latency. Asynchronous scenarios can be remote or local.</p> <p>In order for <code>SpanKind</code> to be meaningful, callers SHOULD arrange that a single Span does not serve more than one purpose. For example, a server-side span SHOULD NOT be used directly as the parent of another remote span. As a simple guideline, instrumentation should create a new Span prior to extracting and serializing the SpanContext for a remote call.</p> <p>Note: there are complex scenarios where a CLIENT span may have a child that is also logically a CLIENT span, or a PRODUCER span might have a local child that is a CLIENT span, depending on how the various libraries that are providing the functionality are built and instrumented. These scenarios, when they occur, should be detailed in the semantic conventions appropriate to the relevant libraries.</p> <p>These are the possible SpanKinds:</p> <ul> <li><code>SERVER</code> Indicates that the span covers server-side handling of a synchronous   RPC or other remote request. This span is often the child of a remote <code>CLIENT</code>   span that was expected to wait for a response.</li> <li><code>CLIENT</code> Indicates that the span describes a request to some remote service.   This span is usually the parent of a remote <code>SERVER</code> span and does not end   until the response is received.</li> <li><code>PRODUCER</code> Indicates that the span describes the initiators of an asynchronous   request. This parent span will often end before the corresponding child   <code>CONSUMER</code> span, possibly even before the child span starts. In messaging   scenarios with batching, tracing individual messages requires a new <code>PRODUCER</code>   span per message to be created.</li> <li><code>CONSUMER</code> Indicates that the span describes a child of an asynchronous   <code>PRODUCER</code> request.</li> <li><code>INTERNAL</code> Default value. Indicates that the span represents an internal   operation within an application, as opposed to an operations with remote   parents or children.</li> </ul> <p>To summarize the interpretation of these kinds:</p> <code>SpanKind</code> Synchronous Asynchronous Remote Incoming Remote Outgoing <code>CLIENT</code> yes yes <code>SERVER</code> yes yes <code>PRODUCER</code> yes maybe <code>CONSUMER</code> yes maybe <code>INTERNAL</code>"},{"location":"docs/specs/otel/trace/api/#concurrency","title":"Concurrency","text":"<p>For languages which support concurrent execution the Tracing APIs provide specific guarantees and safeties. Not all of API functions are safe to be called concurrently.</p> <p>TracerProvider - all methods are safe to be called concurrently.</p> <p>Tracer - all methods are safe to be called concurrently.</p> <p>Span - All methods of Span are safe to be called concurrently.</p> <p>Event - Events are immutable and safe to be used concurrently.</p> <p>Link - Links are immutable and safe to be used concurrently.</p>"},{"location":"docs/specs/otel/trace/api/#included-propagators","title":"Included Propagators","text":"<p>See Propagators Distribution for how propagators are to be distributed.</p>"},{"location":"docs/specs/otel/trace/api/#behavior-of-the-api-in-the-absence-of-an-installed-sdk","title":"Behavior of the API in the absence of an installed SDK","text":"<p>In general, in the absence of an installed SDK, the Trace API is a \"no-op\" API. This means that operations on a Tracer, or on Spans, should have no side effects and do nothing. However, there is one important exception to this general rule, and that is related to propagation of a <code>SpanContext</code>: The API MUST return a non-recording <code>Span</code> with the <code>SpanContext</code> in the parent <code>Context</code> (whether explicitly given or implicit current). If the <code>Span</code> in the parent <code>Context</code> is already non-recording, it SHOULD be returned directly without instantiating a new <code>Span</code>. If the parent <code>Context</code> contains no <code>Span</code>, an empty non-recording Span MUST be returned instead (i.e., having a <code>SpanContext</code> with all-zero Span and Trace IDs, empty Tracestate, and unsampled TraceFlags). This means that a <code>SpanContext</code> that has been provided by a configured <code>Propagator</code> will be propagated through to any child span and ultimately also <code>Inject</code>, but that no new <code>SpanContext</code>s will be created.</p>"},{"location":"docs/specs/otel/trace/exceptions/","title":"\u5f02\u5e38","text":"<p>Status: Experimental</p> <p>This document defines how to record exceptions and their required attributes.</p> <ul> <li>Recording an Exception</li> <li>Attributes</li> </ul>"},{"location":"docs/specs/otel/trace/exceptions/#recording-an-exception","title":"Recording an Exception","text":"<p>An exception SHOULD be recorded as an <code>Event</code> on the span during which it occurred. The name of the event MUST be <code>\"exception\"</code>.</p> <p>A typical template for an auto-instrumentation implementing this semantic convention using an API-provided <code>recordException</code> method could look like this (pseudo-Java):</p> <pre><code>Span span = myTracer.startSpan(/*...*/);\ntry {\n// Code that does the actual work which the Span represents\n} catch (Throwable e) {\nspan.recordException(e, Attributes.of(\"exception.escaped\", true));\nthrow e;\n} finally {\nspan.end();\n}\n</code></pre>"},{"location":"docs/specs/otel/trace/exceptions/#attributes","title":"Attributes","text":"<p>An event representing an exception MUST have an event name <code>exception</code>.</p> <p>Additionally, the following attributes SHOULD be filled out:</p> <ul> <li><code>exception.escaped</code></li> <li><code>exception.message</code></li> <li><code>exception.stacktrace</code></li> <li><code>exception.type</code></li> </ul> <p>The format and semantics of these attributes are defined in semantic conventions.</p>"},{"location":"docs/specs/otel/trace/sdk/","title":"\u8ffd\u8e2a SDK","text":"<p>Status: Stable</p> Table of Contents   - [Tracer Provider](#tracer-provider)   - [Tracer Creation](#tracer-creation)   - [Shutdown](#shutdown)   - [ForceFlush](#forceflush) - [Additional Span Interfaces](#additional-span-interfaces) - [Sampling](#sampling)   - [Recording Sampled reaction table](#recording-sampled-reaction-table)   - [SDK Span creation](#sdk-span-creation)   - [Sampler](#sampler)     - [ShouldSample](#shouldsample)     - [GetDescription](#getdescription)   - [Built-in samplers](#built-in-samplers)     - [AlwaysOn](#alwayson)     - [AlwaysOff](#alwaysoff)     - [TraceIdRatioBased](#traceidratiobased)       - [Requirements for `TraceIdRatioBased` sampler algorithm](#requirements-for-traceidratiobased-sampler-algorithm)     - [ParentBased](#parentbased)     - [JaegerRemoteSampler](#jaegerremotesampler)       - [Configuration](#configuration) - [Span Limits](#span-limits) - [Id Generators](#id-generators) - [Span processor](#span-processor)   - [Interface definition](#interface-definition)     - [OnStart](#onstart)     - [OnEnd(Span)](#onendspan)     - [Shutdown()](#shutdown)     - [ForceFlush()](#forceflush)   - [Built-in span processors](#built-in-span-processors)     - [Simple processor](#simple-processor)     - [Batching processor](#batching-processor) - [Span Exporter](#span-exporter)   - [Interface Definition](#interface-definition)     - [`Export(batch)`](#exportbatch)     - [`Shutdown()`](#shutdown)     - [`ForceFlush()`](#forceflush)   - [Further Language Specialization](#further-language-specialization)     - [Examples](#examples)       - [Go SpanExporter Interface](#go-spanexporter-interface)       - [Java SpanExporter Interface](#java-spanexporter-interface)"},{"location":"docs/specs/otel/trace/sdk/#tracer-provider","title":"Tracer Provider","text":""},{"location":"docs/specs/otel/trace/sdk/#tracer-creation","title":"Tracer Creation","text":"<p>New <code>Tracer</code> instances are always created through a <code>TracerProvider</code> (see API). The <code>name</code>, <code>version</code> (optional), <code>schema_url</code> (optional), and <code>attributes</code> (optional) arguments supplied to the <code>TracerProvider</code> must be used to create an <code>InstrumentationScope</code> instance which is stored on the created <code>Tracer</code>.</p> <p>Configuration (i.e., SpanProcessors, IdGenerator, SpanLimits and <code>Sampler</code>) MUST be managed solely by the <code>TracerProvider</code> and it MUST provide some way to configure all of them that are implemented in the SDK, at least when creating or initializing it.</p> <p>The TracerProvider MAY provide methods to update the configuration. If configuration is updated (e.g., adding a <code>SpanProcessor</code>), the updated configuration MUST also apply to all already returned <code>Tracers</code> (i.e. it MUST NOT matter whether a <code>Tracer</code> was obtained from the <code>TracerProvider</code> before or after the configuration change). Note: Implementation-wise, this could mean that <code>Tracer</code> instances have a reference to their <code>TracerProvider</code> and access configuration only via this reference.</p>"},{"location":"docs/specs/otel/trace/sdk/#shutdown","title":"Shutdown","text":"<p>This method provides a way for provider to do any cleanup required.</p> <p><code>Shutdown</code> MUST be called only once for each <code>TracerProvider</code> instance. After the call to <code>Shutdown</code>, subsequent attempts to get a <code>Tracer</code> are not allowed. SDKs SHOULD return a valid no-op Tracer for these calls, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry client authors can decide if they want to make the shutdown timeout configurable.</p> <p><code>Shutdown</code> MUST be implemented at least by invoking <code>Shutdown</code> within all internal processors.</p>"},{"location":"docs/specs/otel/trace/sdk/#forceflush","title":"ForceFlush","text":"<p>This method provides a way for provider to immediately export all spans that have not yet been exported for all the internal processors.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry client authors can decide if they want to make the flush timeout configurable.</p> <p><code>ForceFlush</code> MUST invoke <code>ForceFlush</code> on all registered <code>SpanProcessors</code>.</p>"},{"location":"docs/specs/otel/trace/sdk/#additional-span-interfaces","title":"Additional Span Interfaces","text":"<p>The API-level definition for Span's interface only defines write-only access to the span. This is good because instrumentations and applications are not meant to use the data stored in a span for application logic. However, the SDK needs to eventually read back the data in some locations. Thus, the SDK specification defines sets of possible requirements for <code>Span</code>-like parameters:</p> <ul> <li>Readable span: A function receiving this as argument MUST be able to   access all information that was added to the span, as listed in the API spec   for Span. Note: Below, a few particular properties are called   out for clarity, but for the complete list of required properties, the   Span API spec is authoritative.</li> </ul> <p>A function receiving this as argument MUST be able to access the   <code>InstrumentationScope</code> [since 1.10.0] and <code>Resource</code> information (implicitly)   associated with the span. For backwards compatibility it MUST also be able to   access the <code>InstrumentationLibrary</code> [deprecated since 1.10.0] having the same   name and version values as the <code>InstrumentationScope</code>.</p> <p>A function receiving this as argument MUST must be able to reliably determine   whether the Span has ended (some languages might implement this by having an   end timestamp of <code>null</code>, others might have an explicit <code>hasEnded</code> boolean).</p> <p>Counts for attributes, events and links dropped due to collection limits MUST   be available for exporters to report as described in the   exporters   specification.</p> <p>As an exception to the authoritative set of span properties defined in the API   spec, implementations MAY choose not to expose (and store) the full parent   Context of the Span but they MUST expose at least the   full parent SpanContext.</p> <p>A function receiving this as argument might not be able to modify the Span.</p> <p>Note: Typically this will be implemented with a new interface or (immutable)   value type. In some languages SpanProcessors may have a different readable   span type than exporters (e.g. a <code>SpanData</code> type might contain an immutable   snapshot and a <code>ReadableSpan</code> interface might read information directly from   the same underlying data structure that the <code>Span</code> interface manipulates).</p> <ul> <li>Read/write span: A function receiving this as argument must have access to   both the full span API as defined in the   API-level definition for span's interface and   additionally must be able to retrieve all information that was added to the   span (as with readable span).</li> </ul> <p>It MUST be possible for functions being called with this to somehow obtain the   same <code>Span</code> instance and type that the   span creation API returned (or will return) to the   user (for example, the <code>Span</code> could be one of the parameters passed to such a   function, or a getter could be provided).</p>"},{"location":"docs/specs/otel/trace/sdk/#sampling","title":"Sampling","text":"<p>Sampling is a mechanism to control the noise and overhead introduced by OpenTelemetry by reducing the number of samples of traces collected and sent to the backend.</p> <p>Sampling may be implemented on different stages of a trace collection. The earliest sampling could happen before the trace is actually created, and the latest sampling could happen on the Collector which is out of process.</p> <p>The OpenTelemetry API has two properties responsible for the data collection:</p> <ul> <li><code>IsRecording</code> field of a <code>Span</code>. If <code>false</code> the current <code>Span</code> discards all   tracing data (attributes, events, status, etc.). Users can use this property   to determine if collecting expensive trace data can be avoided.   Span Processor MUST receive only those spans which have   this field set to <code>true</code>. However, Span Exporter SHOULD NOT   receive them unless the <code>Sampled</code> flag was also set.</li> <li><code>Sampled</code> flag in <code>TraceFlags</code> on <code>SpanContext</code>. This flag is propagated via   the <code>SpanContext</code> to child Spans. For more details see the   W3C Trace Context specification.   This flag indicates that the <code>Span</code> has been <code>sampled</code> and will be exported.   Span Exporters MUST receive those spans which have <code>Sampled</code>   flag set to true and they SHOULD NOT receive the ones that do not.</li> </ul> <p>The flag combination <code>SampledFlag == false</code> and <code>IsRecording == true</code> means that the current <code>Span</code> does record information, but most likely the child <code>Span</code> will not.</p> <p>The flag combination <code>SampledFlag == true</code> and <code>IsRecording == false</code> could cause gaps in the distributed trace, and because of this the OpenTelemetry SDK MUST NOT allow this combination.</p>"},{"location":"docs/specs/otel/trace/sdk/#recording-sampled-reaction-table","title":"Recording Sampled reaction table","text":"<p>The following table summarizes the expected behavior for each combination of <code>IsRecording</code> and <code>SampledFlag</code>.</p> <code>IsRecording</code> <code>Sampled</code> Flag Span Processor receives Span? Span Exporter receives Span? true true true true true false true false false true Not allowed Not allowed false false false false <p>The SDK defines the interface <code>Sampler</code> as well as a set of built-in samplers and associates a <code>Sampler</code> with each [<code>TracerProvider</code>].</p>"},{"location":"docs/specs/otel/trace/sdk/#sdk-span-creation","title":"SDK Span creation","text":"<p>When asked to create a Span, the SDK MUST act as if doing the following in order:</p> <ol> <li>If there is a valid parent trace ID, use it. Otherwise generate a new trace    ID (note: this must be done before calling <code>ShouldSample</code>, because it expects    a valid trace ID as input).</li> <li>Query the <code>Sampler</code>'s <code>ShouldSample</code> method (Note that the    built-in <code>ParentBasedSampler</code> can be used to use the sampling    decision of the parent, translating a set SampledFlag to RECORD and an unset    one to DROP).</li> <li>Generate a new span ID for the <code>Span</code>, independently of the sampling    decision. This is done so other components (such as logs or exception    handling) can rely on a unique span ID, even if the <code>Span</code> is a non-recording    instance.</li> <li>Create a span depending on the decision returned by <code>ShouldSample</code>: see    description of <code>ShouldSample</code>'s return value below for how    to set <code>IsRecording</code> and <code>Sampled</code> on the Span, and the    table above on whether to pass the    <code>Span</code> to <code>SpanProcessor</code>s. A non-recording span MAY be implemented using the    same mechanism as when a <code>Span</code> is created without an SDK installed or as    described in    wrapping a SpanContext in a Span.</li> </ol>"},{"location":"docs/specs/otel/trace/sdk/#sampler","title":"Sampler","text":"<p><code>Sampler</code> interface allows users to create custom samplers which will return a sampling <code>SamplingResult</code> based on information that is typically available just before the <code>Span</code> was created.</p>"},{"location":"docs/specs/otel/trace/sdk/#shouldsample","title":"ShouldSample","text":"<p>Returns the sampling Decision for a <code>Span</code> to be created.</p> <p>Required arguments:</p> <ul> <li><code>Context</code> with parent <code>Span</code>. The Span's SpanContext   may be invalid to indicate a root span.</li> <li><code>TraceId</code> of the <code>Span</code> to be created. If the parent <code>SpanContext</code> contains a   valid <code>TraceId</code>, they MUST always match.</li> <li>Name of the <code>Span</code> to be created.</li> <li><code>SpanKind</code> of the <code>Span</code> to be created.</li> <li>Initial set of <code>Attributes</code> of the <code>Span</code> to be created.</li> <li>Collection of links that will be associated with the <code>Span</code> to be created.   Typically useful for batch operations, see   Links Between Spans.</li> </ul> <p>Note: Implementations may \"bundle\" all or several arguments together in a single object.</p> <p>Return value:</p> <p>It produces an output called <code>SamplingResult</code> which contains:</p> <ul> <li>A sampling <code>Decision</code>. One of the following enum values:</li> <li><code>DROP</code> - <code>IsRecording</code> will be <code>false</code>, the <code>Span</code> will not be recorded and     all events and attributes will be dropped.</li> <li><code>RECORD_ONLY</code> - <code>IsRecording</code> will be <code>true</code>, but the <code>Sampled</code> flag MUST     NOT be set.</li> <li><code>RECORD_AND_SAMPLE</code> - <code>IsRecording</code> will be <code>true</code> and the <code>Sampled</code> flag     MUST be set.</li> <li>A set of span Attributes that will also be added to the <code>Span</code>. The returned   object must be immutable (multiple calls may return different immutable   objects).</li> <li>A <code>Tracestate</code> that will be associated with the <code>Span</code> through the new   <code>SpanContext</code>. If the sampler returns an empty <code>Tracestate</code> here, the   <code>Tracestate</code> will be cleared, so samplers SHOULD normally return the passed-in   <code>Tracestate</code> if they do not intend to change it.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#getdescription","title":"GetDescription","text":"<p>Returns the sampler name or short description with the configuration. This may be displayed on debug pages or in the logs. Example: <code>\"TraceIdRatioBased{0.000100}\"</code>.</p> <p>Description MUST NOT change over time and caller can cache the returned value.</p>"},{"location":"docs/specs/otel/trace/sdk/#built-in-samplers","title":"Built-in samplers","text":"<p>OpenTelemetry supports a number of built-in samplers to choose from. The default sampler is <code>ParentBased(root=AlwaysOn)</code>.</p>"},{"location":"docs/specs/otel/trace/sdk/#alwayson","title":"AlwaysOn","text":"<ul> <li>Returns <code>RECORD_AND_SAMPLE</code> always.</li> <li>Description MUST be <code>AlwaysOnSampler</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#alwaysoff","title":"AlwaysOff","text":"<ul> <li>Returns <code>DROP</code> always.</li> <li>Description MUST be <code>AlwaysOffSampler</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#traceidratiobased","title":"TraceIdRatioBased","text":"<ul> <li>The <code>TraceIdRatioBased</code> MUST ignore the parent <code>SampledFlag</code>. To respect the   parent <code>SampledFlag</code>, the <code>TraceIdRatioBased</code> should be used as a delegate of   the <code>ParentBased</code> sampler specified below.</li> <li>Description MUST return a string of the form <code>\"TraceIdRatioBased{RATIO}\"</code> with   <code>RATIO</code> replaced with the Sampler instance's trace sampling ratio represented   as a decimal number. The precision of the number SHOULD follow implementation   language standards and SHOULD be high enough to identify when Samplers have   different ratios. For example, if a TraceIdRatioBased Sampler had a sampling   ratio of 1 to every 10,000 spans it COULD return   <code>\"TraceIdRatioBased{0.000100}\"</code> as its description.</li> </ul> <p>TODO: Add details about how the <code>TraceIdRatioBased</code> is implemented as a function of the <code>TraceID</code>. #1413</p>"},{"location":"docs/specs/otel/trace/sdk/#requirements-for-traceidratiobased-sampler-algorithm","title":"Requirements for <code>TraceIdRatioBased</code> sampler algorithm","text":"<ul> <li>The sampling algorithm MUST be deterministic. A trace identified by a given   <code>TraceId</code> is sampled or not independent of language, time, etc. To achieve   this, implementations MUST use a deterministic hash of the <code>TraceId</code> when   computing the sampling decision. By ensuring this, running the sampler on any   child <code>Span</code> will produce the same decision.</li> <li>A <code>TraceIdRatioBased</code> sampler with a given sampling rate MUST also sample all   traces that any <code>TraceIdRatioBased</code> sampler with a lower sampling rate would   sample. This is important when a backend system may want to run with a higher   sampling rate than the frontend system, this way all frontend traces will   still be sampled and extra traces will be sampled on the backend only.</li> <li>WARNING: Since the exact algorithm is not specified yet (see TODO above),   there will probably be changes to it in any language SDK once it is, which   would break code that relies on the algorithm results. Only the configuration   and creation APIs can be considered stable. It is recommended to use this   sampler algorithm only for root spans (in combination with   <code>ParentBased</code>) because different language SDKs or even   different versions of the same language SDKs may produce inconsistent results   for the same input.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#parentbased","title":"ParentBased","text":"<ul> <li>This is a composite sampler. <code>ParentBased</code> helps distinguish between the   following cases:</li> <li>No parent (root span).</li> <li>Remote parent (<code>SpanContext.IsRemote() == true</code>) with <code>SampledFlag</code> set</li> <li>Remote parent (<code>SpanContext.IsRemote() == true</code>) with <code>SampledFlag</code> not set</li> <li>Local parent (<code>SpanContext.IsRemote() == false</code>) with <code>SampledFlag</code> set</li> <li>Local parent (<code>SpanContext.IsRemote() == false</code>) with <code>SampledFlag</code> not set</li> </ul> <p>Required parameters:</p> <ul> <li><code>root(Sampler)</code> - Sampler called for spans with no parent (root spans)</li> </ul> <p>Optional parameters:</p> <ul> <li><code>remoteParentSampled(Sampler)</code> (default: AlwaysOn)</li> <li><code>remoteParentNotSampled(Sampler)</code> (default: AlwaysOff)</li> <li><code>localParentSampled(Sampler)</code> (default: AlwaysOn)</li> <li><code>localParentNotSampled(Sampler)</code> (default: AlwaysOff)</li> </ul> Parent parent.isRemote() parent.IsSampled() Invoke sampler absent n/a n/a <code>root()</code> present true true <code>remoteParentSampled()</code> present true false <code>remoteParentNotSampled()</code> present false true <code>localParentSampled()</code> present false false <code>localParentNotSampled()</code>"},{"location":"docs/specs/otel/trace/sdk/#jaegerremotesampler","title":"JaegerRemoteSampler","text":"<p>Jaeger remote sampler allows remotely controlling the sampling configuration for the SDKs. The sampling configuration is periodically loaded from the backend (see Remote Sampling API), where it can be managed by operators via configuration files or even automatically calculated (see Adaptive Sampling). The sampling configuration retrieved by the remote sampler can instruct it to use either a single sampling method for the whole service (e.g., <code>TraceIdRatioBased</code>), or different methods for different endpoints (span names), for example, sample <code>/product</code> endpoint at 10%, <code>/admin</code> endpoint at 100%, and never sample <code>/metrics</code> endpoint.</p> <p>The full Protobuf definition can be found at jaegertracing/jaeger-idl/api_v2/sampling.proto.</p>"},{"location":"docs/specs/otel/trace/sdk/#configuration","title":"Configuration","text":"<p>The following configuration properties should be available when creating the sampler:</p> <ul> <li>endpoint - address of a service that implements the Remote Sampling   API, such as Jaeger Collector or OpenTelemetry   Collector.</li> <li>polling interval - polling interval for getting configuration from remote</li> <li>initial sampler - initial sampler that is used before the first configuration   is fetched</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#span-limits","title":"Span Limits","text":"<p>Span attributes MUST adhere to the common rules of attribute limits.</p> <p>SDK Spans MAY also discard links and events that would increase the number of elements of each collection beyond the configured limit.</p> <p>If the SDK implements the limits above it MUST provide a way to change these limits, via a configuration to the TracerProvider, by allowing users to configure individual limits like in the Java example bellow.</p> <p>The name of the configuration options SHOULD be <code>EventCountLimit</code> and <code>LinkCountLimit</code>. The options MAY be bundled in a class, which then SHOULD be called <code>SpanLimits</code>. Implementations MAY provide additional configuration such as <code>AttributePerEventCountLimit</code> and <code>AttributePerLinkCountLimit</code>.</p> <pre><code>public final class SpanLimits {\nSpanLimits(int attributeCountLimit, int linkCountLimit, int eventCountLimit);\npublic int getAttributeCountLimit();\npublic int getAttributeCountPerEventLimit();\npublic int getAttributeCountPerLinkLimit();\npublic int getEventCountLimit();\npublic int getLinkCountLimit();\n}\n</code></pre> <p>Configurable parameters:</p> <ul> <li>all common options applicable to attributes</li> <li><code>EventCountLimit</code> (Default=128) - Maximum allowed span event count;</li> <li><code>LinkCountLimit</code> (Default=128) - Maximum allowed span link count;</li> <li><code>AttributePerEventCountLimit</code> (Default=128) - Maximum allowed attribute per   span event count;</li> <li><code>AttributePerLinkCountLimit</code> (Default=128) - Maximum allowed attribute per   span link count;</li> </ul> <p>There SHOULD be a message printed in the SDK's log to indicate to the user that an attribute was discarded due to such a limit. To prevent excessive logging, the message MUST be printed at most once per span (i.e., not per discarded attribute, event, or link).</p>"},{"location":"docs/specs/otel/trace/sdk/#id-generators","title":"Id Generators","text":"<p>The SDK MUST by default randomly generate both the <code>TraceId</code> and the <code>SpanId</code>.</p> <p>The SDK MUST provide a mechanism for customizing the way IDs are generated for both the <code>TraceId</code> and the <code>SpanId</code>.</p> <p>The SDK MAY provide this functionality by allowing custom implementations of an interface like the java example below (name of the interface MAY be <code>IdGenerator</code>, name of the methods MUST be consistent with SpanContext), which provides extension points for two methods, one to generate a <code>SpanId</code> and one for <code>TraceId</code>.</p> <pre><code>public interface IdGenerator {\nbyte[] generateSpanIdBytes();\nbyte[] generateTraceIdBytes();\n}\n</code></pre> <p>Additional <code>IdGenerator</code> implementing vendor-specific protocols such as AWS X-Ray trace id generator MUST NOT be maintained or distributed as part of the Core OpenTelemetry repositories.</p>"},{"location":"docs/specs/otel/trace/sdk/#span-processor","title":"Span processor","text":"<p>Span processor is an interface which allows hooks for span start and end method invocations. The span processors are invoked only when <code>IsRecording</code> is true.</p> <p>Built-in span processors are responsible for batching and conversion of spans to exportable representation and passing batches to exporters.</p> <p>Span processors can be registered directly on SDK <code>TracerProvider</code> and they are invoked in the same order as they were registered.</p> <p>Each processor registered on <code>TracerProvider</code> is a start of pipeline that consist of span processor and optional exporter. SDK MUST allow to end each pipeline with individual exporter.</p> <p>SDK MUST allow users to implement and configure custom processors.</p> <p>The following diagram shows <code>SpanProcessor</code>'s relationship to other components in the SDK:</p> <pre><code>  +-----+--------------+   +-------------------------+   +-------------------+\n  |     |              |   |                         |   |                   |\n  |     |              |   | Batching Span Processor |   |    SpanExporter   |\n  |     |              +---&gt; Simple Span Processor   +---&gt;  (JaegerExporter) |\n  |     |              |   |                         |   |                   |\n  | SDK | Span.start() |   +-------------------------+   +-------------------+\n  |     | Span.end()   |\n  |     |              |\n  |     |              |\n  |     |              |\n  |     |              |\n  +-----+--------------+\n</code></pre>"},{"location":"docs/specs/otel/trace/sdk/#interface-definition","title":"Interface definition","text":""},{"location":"docs/specs/otel/trace/sdk/#onstart","title":"OnStart","text":"<p><code>OnStart</code> is called when a span is started. This method is called synchronously on the thread that started the span, therefore it should not block or throw exceptions.</p> <p>Parameters:</p> <ul> <li><code>span</code> - a read/write span object for the   started span. It SHOULD be possible to keep a reference to this span object   and updates to the span SHOULD be reflected in it. For example, this is useful   for creating a SpanProcessor that periodically evaluates/prints information   about all active span from a background thread.</li> <li><code>parentContext</code> - the parent <code>Context</code> of the span that the SDK determined   (the explicitly passed <code>Context</code>, the current <code>Context</code> or an empty <code>Context</code>   if that was explicitly requested).</li> </ul> <p>Returns: <code>Void</code></p>"},{"location":"docs/specs/otel/trace/sdk/#onendspan","title":"OnEnd(Span)","text":"<p><code>OnEnd</code> is called after a span is ended (i.e., the end timestamp is already set). This method MUST be called synchronously within the <code>Span.End()</code> API, therefore it should not block or throw an exception.</p> <p>Parameters:</p> <ul> <li><code>Span</code> - a readable span object for the ended   span. Note: Even if the passed Span may be technically writable, since it's   already ended at this point, modifying it is not allowed.</li> </ul> <p>Returns: <code>Void</code></p>"},{"location":"docs/specs/otel/trace/sdk/#shutdown_1","title":"Shutdown()","text":"<p>Shuts down the processor. Called when SDK is shut down. This is an opportunity for processor to do any cleanup required.</p> <p><code>Shutdown</code> SHOULD be called only once for each <code>SpanProcessor</code> instance. After the call to <code>Shutdown</code>, subsequent calls to <code>OnStart</code>, <code>OnEnd</code>, or <code>ForceFlush</code> are not allowed. SDKs SHOULD ignore these calls gracefully, if possible.</p> <p><code>Shutdown</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>Shutdown</code> MUST include the effects of <code>ForceFlush</code>.</p> <p><code>Shutdown</code> SHOULD complete or abort within some timeout. <code>Shutdown</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry client authors can decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/trace/sdk/#forceflush_1","title":"ForceFlush()","text":"<p>This is a hint to ensure that any tasks associated with <code>Spans</code> for which the <code>SpanProcessor</code> had already received events prior to the call to <code>ForceFlush</code> SHOULD be completed as soon as possible, preferably before returning from this method.</p> <p>In particular, if any <code>SpanProcessor</code> has any associated exporter, it SHOULD try to call the exporter's <code>Export</code> with all spans for which this was not already done and then invoke <code>ForceFlush</code> on it. The built-in SpanProcessors MUST do so. If a timeout is specified (see below), the SpanProcessor MUST prioritize honoring the timeout over finishing all calls. It MAY skip or abort some or all Export or ForceFlush calls it has made to achieve this goal.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD only be called in cases where it is absolutely necessary, such as when using some FaaS providers that may suspend the process after an invocation, but before the <code>SpanProcessor</code> exports the completed spans.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry client authors can decide if they want to make the flush timeout configurable.</p>"},{"location":"docs/specs/otel/trace/sdk/#built-in-span-processors","title":"Built-in span processors","text":"<p>The standard OpenTelemetry SDK MUST implement both simple and batch processors, as described below. Other common processing scenarios should be first considered for implementation out-of-process in OpenTelemetry Collector.</p>"},{"location":"docs/specs/otel/trace/sdk/#simple-processor","title":"Simple processor","text":"<p>This is an implementation of <code>SpanProcessor</code> which passes finished spans and passes the export-friendly span data representation to the configured <code>SpanExporter</code>, as soon as they are finished.</p> <p>Configurable parameters:</p> <ul> <li><code>exporter</code> - the exporter where the spans are pushed.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#batching-processor","title":"Batching processor","text":"<p>This is an implementation of the <code>SpanProcessor</code> which create batches of finished spans and passes the export-friendly span data representations to the configured <code>SpanExporter</code>.</p> <p>The processor SHOULD export a batch when any of the following happens AND <code>SpanProcessor#Export()</code> has not yet returned (for additional concurrency details see the Export() specification):</p> <ul> <li><code>scheduledDelayMillis</code> after the processor is constructed OR the first span is   received by the span processor.</li> <li><code>scheduledDelayMillis</code> after the previous export timer ends, OR the previous   export completes, OR the first span is added to the queue after the previous   export timer ends or previous batch completes.</li> <li>The queue contains <code>maxExportBatchSize</code> or more spans.</li> <li><code>ForceFlush</code> is called.</li> </ul> <p>If any of the above events occurs before <code>Export()</code> returns, the span processor should wait until <code>Export()</code> returns before exporting the next batch. If the queue is empty when an export is triggered, the processor MAY export an empty batch OR skip the export and consider it to be completed immediately.</p> <p>Configurable parameters:</p> <ul> <li><code>exporter</code> - the exporter where the spans are pushed.</li> <li><code>maxQueueSize</code> - the maximum queue size. After the size is reached spans are   dropped. The default value is <code>2048</code>.</li> <li><code>scheduledDelayMillis</code> - the maximum delay interval in milliseconds between   two consecutive exports. The default value is <code>5000</code>.</li> <li><code>exportTimeoutMillis</code> - how long the export can run before it is cancelled.   The default value is <code>30000</code>.</li> <li><code>maxExportBatchSize</code> - the maximum batch size of every export. It must be   smaller or equal to <code>maxQueueSize</code>. If the queue reaches <code>maxExportBatchSize</code>   a batch will be exported even if <code>scheduledDelayMillis</code> milliseconds have not   elapsed. The default value is <code>512</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk/#span-exporter","title":"Span Exporter","text":"<p><code>Span Exporter</code> defines the interface that protocol-specific exporters must implement so that they can be plugged into OpenTelemetry SDK and support sending of telemetry data.</p> <p>The goal of the interface is to minimize burden of implementation for protocol-dependent telemetry exporters. The protocol exporter is expected to be primarily a simple telemetry data encoder and transmitter.</p>"},{"location":"docs/specs/otel/trace/sdk/#interface-definition_1","title":"Interface Definition","text":"<p>The exporter MUST support three functions: Export, Shutdown, and ForceFlush. In strongly typed languages typically there will be one separate <code>Exporter</code> interface per signal (<code>SpanExporter</code>, ...).</p>"},{"location":"docs/specs/otel/trace/sdk/#exportbatch","title":"<code>Export(batch)</code>","text":"<p>Exports a batch of readable spans. Protocol exporters that will implement this function are typically expected to serialize and transmit the data to the destination.</p> <p>Export() will never be called concurrently for the same exporter instance. Depending on the implementation the result of the export may be returned to the Processor not in the return value of the call to Export() but in a language specific way for signaling completion of an asynchronous task. This means that while an instance of an exporter will never have its Export() called concurrently it does not mean that the task of exporting can not be done concurrently. How this is done is outside the scope of this specification. Each implementation MUST document the concurrency characteristics the SDK requires of the exporter.</p> <p>Export() MUST NOT block indefinitely, there MUST be a reasonable upper limit after which the call must time out with an error result (<code>Failure</code>).</p> <p>Concurrent requests and retry logic is the responsibility of the exporter. The default SDK's Span Processors SHOULD NOT implement retry logic, as the required logic is likely to depend heavily on the specific protocol and backend the spans are being sent to. For example, the OpenTelemetry Protocol (OTLP) specification defines logic for both sending concurrent requests and retrying requests.</p> <p>Parameters:</p> <p>batch - a batch of readable spans. The exact data type of the batch is language specific, typically it is some kind of list, e.g. for spans in Java it will be typically <code>Collection&lt;SpanData&gt;</code>.</p> <p>Returns: ExportResult:</p> <p>The return of Export() is implementation specific. In what is idiomatic for the language the Exporter must send an <code>ExportResult</code> to the Processor. <code>ExportResult</code> has values of either <code>Success</code> or <code>Failure</code>:</p> <ul> <li><code>Success</code> - The batch has been successfully exported. For protocol exporters   this typically means that the data is sent over the wire and delivered to the   destination server.</li> <li><code>Failure</code> - exporting failed. The batch must be dropped. For example, this can   happen when the batch contains bad data and cannot be serialized.</li> </ul> <p>For example, in Java the return of Export() would be a Future which when completed returns the <code>ExportResult</code> object. While in Erlang the Exporter sends a message to the Processor with the <code>ExportResult</code> for a particular batch of spans.</p>"},{"location":"docs/specs/otel/trace/sdk/#shutdown_2","title":"<code>Shutdown()</code>","text":"<p>Shuts down the exporter. Called when SDK is shut down. This is an opportunity for exporter to do any cleanup required.</p> <p><code>Shutdown</code> should be called only once for each <code>Exporter</code> instance. After the call to <code>Shutdown</code> subsequent calls to <code>Export</code> are not allowed and should return a <code>Failure</code> result.</p> <p><code>Shutdown</code> should not block indefinitely (e.g. if it attempts to flush the data and the destination is unavailable). OpenTelemetry client authors can decide if they want to make the shutdown timeout configurable.</p>"},{"location":"docs/specs/otel/trace/sdk/#forceflush_2","title":"<code>ForceFlush()</code>","text":"<p>This is a hint to ensure that the export of any <code>Spans</code> the exporter has received prior to the call to <code>ForceFlush</code> SHOULD be completed as soon as possible, preferably before returning from this method.</p> <p><code>ForceFlush</code> SHOULD provide a way to let the caller know whether it succeeded, failed or timed out.</p> <p><code>ForceFlush</code> SHOULD only be called in cases where it is absolutely necessary, such as when using some FaaS providers that may suspend the process after an invocation, but before the exporter exports the completed spans.</p> <p><code>ForceFlush</code> SHOULD complete or abort within some timeout. <code>ForceFlush</code> can be implemented as a blocking API or an asynchronous API which notifies the caller via a callback or an event. OpenTelemetry client authors can decide if they want to make the flush timeout configurable.</p>"},{"location":"docs/specs/otel/trace/sdk/#further-language-specialization","title":"Further Language Specialization","text":"<p>Based on the generic interface definition laid out above library authors must define the exact interface for the particular language.</p> <p>Authors are encouraged to use efficient data structures on the interface boundary that are well suited for fast serialization to wire formats by protocol exporters and minimize the pressure on memory managers. The latter typically requires understanding of how to optimize the rapidly-generated, short-lived telemetry data structures to make life easier for the memory manager of the specific language. General recommendation is to minimize the number of allocations and use allocation arenas where possible, thus avoiding explosion of allocation/deallocation/collection operations in the presence of high rate of telemetry data generation.</p>"},{"location":"docs/specs/otel/trace/sdk/#examples","title":"Examples","text":"<p>These are examples on what the <code>Exporter</code> interface can look like in specific languages. Examples are for illustration purposes only. OpenTelemetry client authors are free to deviate from these provided that their design remain true to the spirit of <code>Exporter</code> concept.</p>"},{"location":"docs/specs/otel/trace/sdk/#go-spanexporter-interface","title":"Go SpanExporter Interface","text":"<pre><code>type SpanExporter interface {\nExport(batch []ExportableSpan) ExportResult\nShutdown()\n}\ntype ExportResult struct {\nCode         ExportResultCode\nWrappedError error\n}\ntype ExportResultCode int\nconst (\nSuccess ExportResultCode = iota\nFailure\n)\n</code></pre>"},{"location":"docs/specs/otel/trace/sdk/#java-spanexporter-interface","title":"Java SpanExporter Interface","text":"<pre><code>public interface SpanExporter {\npublic enum ResultCode {\nSuccess, Failure\n}\nResultCode export(Collection&lt;ExportableSpan&gt; batch);\nvoid shutdown();\n}\n</code></pre>"},{"location":"docs/specs/otel/trace/tracestate-handling/","title":"TraceState \u5904\u7406","text":"<p>Status: Experimental</p> <p>In alignment to the TraceContext specification, this section uses the Augmented Backus-Naur Form (ABNF) notation of RFC5234, including the DIGIT rule in that document.</p> <p>When setting TraceState values that are part of the OTel ecosystem, they MUST all be contained in a single entry using the <code>ot</code> key, with the value being a semicolon separated list of key-value pairs such as:</p> <ul> <li><code>ot=p:8;r:62</code></li> <li><code>ot=foo:bar;k1:13</code></li> </ul> <p>The TraceContext specification defines support for multiple \"tenants\" each to use their own <code>tracestate</code> entry by prefixing <code>tenant@</code> to tenant-specific values in a mixed tracing environment. OpenTelemetry recognizes this syntax but does not specify an interpretation for multi-tenant <code>tracestate</code>. The list can be formally defined as:</p> <pre><code>list        = list-member *( \";\" list-member )\nlist-member = key \":\" value\n</code></pre> <p>The complete list length MUST NOT exceed 256 characters, as defined by the TraceState value section, and the used keys MUST be unique.</p> <p>Instrumentation libraries and clients MUST NOT use this entry, and they MUST instead use their own entry.</p>"},{"location":"docs/specs/otel/trace/tracestate-handling/#key","title":"Key","text":"<p>The key is an identifier that describes an OTel concern. Simple examples are <code>p</code>, <code>ts</code>, or <code>s1</code>.</p> <p>The key can be formally defined as:</p> <pre><code>key        = lcalpha *(lcalpha / DIGIT )\nlcalpha    = %x61-7A ; a-z\n</code></pre> <p>Specific keys used by OTel concerns MUST be defined as part as the Specification, and hence it is forbidden to use to use any key that has not been defined in the Specification itself.</p>"},{"location":"docs/specs/otel/trace/tracestate-handling/#value","title":"Value","text":"<p>The value is an opaque string. Although it has no maximum allowed length, it is recommended to use short values, as the entire list of key-values MUST NOT exceed 256 characters.</p> <p>The value can be formally defined as:</p> <pre><code>value      = *(chr)\nchr        = ucalpha / lcalpha / DIGIT / \".\" / \"_\" / \"-\"\nucalpha    = %x41-5A ; A-Z\nlcalpha    = %x61-7A ; a-z\n</code></pre>"},{"location":"docs/specs/otel/trace/tracestate-handling/#setting-values","title":"Setting values","text":"<p>Set values MUST be either updated or added to the <code>ot</code> entry in <code>TraceState</code>, in order to preserve existing values belonging to other OTel concerns. For example, if a given concern K wants to set <code>k1:13</code>:</p> <ul> <li><code>ot=p:8;r:62</code> will become <code>ot=p:8;r:62;k1:13</code>.</li> <li><code>ot=p:8;k1:7;r:62</code> will become <code>ot=p:8;r:62;k1:13</code>. Preserving the order is   not required.</li> </ul> <p>If setting a value ends up making the entire <code>ot</code> entry exceed the 256 characters limit, SDKs are advised to abort the operation and signal the user about the error, e.g.</p> <pre><code>traceState, ok := SetTraceStateValue(traceState, value)\nif ok {\n// Successfully set the specified value, traceState was updated.\n} else {\n// traceState was not updated.\n}\n</code></pre>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/","title":"TraceState:\u6982\u7387\u62bd\u6837","text":"<p>Status: Experimental</p> Table of Contents   - [TraceState:\u6982\u7387\u62bd\u6837](#tracestate\u6982\u7387\u62bd\u6837)   - [Overview](#overview)     - [Definitions](#definitions)       - [Sampling](#sampling)       - [Adjusted count](#adjusted-count)       - [Sampler](#sampler)       - [Parent-based sampler](#parent-based-sampler)       - [Probability sampler](#probability-sampler)       - [Consistent probability sampler](#consistent-probability-sampler)       - [Trace completeness](#trace-completeness)       - [Non-probability sampler](#non-probability-sampler)       - [Always-on consistent probability sampler](#always-on-consistent-probability-sampler)       - [Always-off sampler](#always-off-sampler)   - [Consistent Probability sampling](#consistent-probability-sampling)     - [Conformance](#conformance)     - [Completeness guarantee](#completeness-guarantee)     - [Context invariants](#context-invariants)       - [Sampled flag](#sampled-flag)         - [Requirement: Inconsistent p-values are unset](#requirement-inconsistent-p-values-are-unset)       - [P-value](#p-value)         - [Requirement: Out-of-range p-values are unset](#requirement-out-of-range-p-values-are-unset)       - [R-value](#r-value)         - [Requirement: Out-of-range r-values unset both p and r](#requirement-out-of-range-r-values-unset-both-p-and-r)         - [Requirement: R-value is generated with the correct probabilities](#requirement-r-value-is-generated-with-the-correct-probabilities)       - [Examples: Context invariants](#examples-context-invariants)         - [Example: Probability sampled context](#example-probability-sampled-context)         - [Example: Probability unsampled](#example-probability-unsampled)     - [Samplers](#samplers)       - [ParentConsistentProbabilityBased sampler](#parentconsistentprobabilitybased-sampler)         - [Requirement: ParentConsistentProbabilityBased API](#requirement-parentconsistentprobabilitybased-api)         - [Requirement: ParentConsistentProbabilityBased does not modify valid tracestate](#requirement-parentconsistentprobabilitybased-does-not-modify-valid-tracestate)         - [Requirement: ParentConsistentProbabilityBased calls the configured root sampler for root spans](#requirement-parentconsistentprobabilitybased-calls-the-configured-root-sampler-for-root-spans)         - [Requirement: ParentConsistentProbabilityBased respects the sampled flag for non-root spans](#requirement-parentconsistentprobabilitybased-respects-the-sampled-flag-for-non-root-spans)       - [ConsistentProbabilityBased sampler](#consistentprobabilitybased-sampler)         - [Requirement: TraceIdRatioBased API compatibility](#requirement-traceidratiobased-api-compatibility)         - [Requirement: ConsistentProbabilityBased sampler sets r for root span](#requirement-consistentprobabilitybased-sampler-sets-r-for-root-span)         - [Requirement: ConsistentProbabilityBased sampler unsets p when not sampled](#requirement-consistentprobabilitybased-sampler-unsets-p-when-not-sampled)         - [Requirement: ConsistentProbabilityBased sampler sets p when sampled](#requirement-consistentprobabilitybased-sampler-sets-p-when-sampled)         - [Requirement: ConsistentProbabilityBased sampler records unbiased adjusted counts](#requirement-consistentprobabilitybased-sampler-records-unbiased-adjusted-counts)         - [Requirement: ConsistentProbabilityBased sampler sets r for non-root span](#requirement-consistentprobabilitybased-sampler-sets-r-for-non-root-span)         - [Requirement: ConsistentProbabilityBased sampler decides not to sample for probabilities less than 2\\*\\*-62](#requirement-consistentprobabilitybased-sampler-decides-not-to-sample-for-probabilities-less-than-2-62)       - [Examples: Consistent probability samplers](#examples-consistent-probability-samplers)         - [Example: Setting R-value for a root span](#example-setting-r-value-for-a-root-span)         - [Example: Handling inconsistent P-value](#example-handling-inconsistent-p-value)         - [Example: Handling corrupt R-value](#example-handling-corrupt-r-value)     - [Composition rules](#composition-rules)       - [List of requirements](#list-of-requirements)         - [Requirement: Combining multiple sampling decisions using logical `or`](#requirement-combining-multiple-sampling-decisions-using-logical-or)         - [Requirement: Combine multiple consistent probability samplers using the minimum p-value](#requirement-combine-multiple-consistent-probability-samplers-using-the-minimum-p-value)         - [Requirement: Unset p when multiple consistent probability samplers decide not to sample](#requirement-unset-p-when-multiple-consistent-probability-samplers-decide-not-to-sample)         - [Requirement: Use probability sampler p-value when its decision to sample is combined with non-probability samplers](#requirement-use-probability-sampler-p-value-when-its-decision-to-sample-is-combined-with-non-probability-samplers)         - [Requirement: Use p-value 63 when a probability sampler decision not to sample is combined with a non-probability sampler decision to sample](#requirement-use-p-value-63-when-a-probability-sampler-decision-not-to-sample-is-combined-with-a-non-probability-sampler-decision-to-sample)       - [Examples: Composition](#examples-composition)         - [Example: Probability and non-probability sampler in a root context](#example-probability-and-non-probability-sampler-in-a-root-context)         - [Example: Two consistent probability samplers](#example-two-consistent-probability-samplers)     - [Producer and consumer recommendations](#producer-and-consumer-recommendations)       - [Trace producer: completeness](#trace-producer-completeness)         - [Recommenendation: use non-descending power-of-two probabilities](#recommenendation-use-non-descending-power-of-two-probabilities)       - [Trace producer: correctness](#trace-producer-correctness)         - [Recommenendation: sampler delegation](#recommenendation-sampler-delegation)       - [Trace producer: interoperability with `ParentBased` sampler](#trace-producer-interoperability-with-parentbased-sampler)       - [Trace producer: interoperability with `TraceIDRatioBased` sampler](#trace-producer-interoperability-with-traceidratiobased-sampler)       - [Trace consumer](#trace-consumer)         - [Recommendation: Recognize inconsistent r-values](#recommendation-recognize-inconsistent-r-values)     - [Appendix: Statistical test requirements](#appendix-statistical-test-requirements)       - [Test procedure: non-powers of two](#test-procedure-non-powers-of-two)         - [Requirement: Pass 12 non-power-of-two statistical tests](#requirement-pass-12-non-power-of-two-statistical-tests)       - [Test procedure: exact powers of two](#test-procedure-exact-powers-of-two)         - [Requirement: Pass 3 power-of-two statistical tests](#requirement-pass-3-power-of-two-statistical-tests)       - [Test implementation](#test-implementation)   - [Appendix](#appendix)     - [Methods for generating R-values](#methods-for-generating-r-values)"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#overview","title":"Overview","text":"<p>Probability sampling allows OpenTelemetry tracing users to lower span collection costs by the use of randomized sampling techniques. The objectives are:</p> <ul> <li>Compatible with the existing W3C trace context <code>sampled</code> flag</li> <li>Spans can be accurately counted using a Span-to-metrics pipeline</li> <li>Traces tend to be complete, even though spans may make independent sampling   decisions.</li> </ul> <p>This document specifies an approach based on an \"r-value\" and a \"p-value\". At a very high level, r-value is a source of randomness and p-value encodes the sampling probability. A context is sampled when <code>p &lt;= r</code>.</p> <p>Significantly, by including the r-value and p-value in the OpenTelemetry <code>tracestate</code>, these two values automatically propagate through the context and are recorded on every Span. This allows Trace consumers to correctly count spans simply by interpreting the p-value on a given span.</p> <p>For efficiency, the supported sampling probabilities are limited to powers of two. P-value is derived from sampling probability, which equals <code>2**-p</code>, thus p-value is encoded using an unsigned integer.</p> <p>For example, a p-value of 3 indicates a sampling probability of \u215b.</p> <p>Since the W3C trace context does not specify that any of the 128 bits in a TraceID are true uniform-distributed random bits, the r-value is introduced as an additional source of randomness.</p> <p>The recommended method of generating an \"r-value\" is to count the number of leading 0s in a string of 62 random bits, however, it is not required to use this approach.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#definitions","title":"Definitions","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#sampling","title":"Sampling","text":"<p>Sampling is a family of techniques for collecting and analyzing only a fraction of a complete data set. Individual items that are \"sampled\" are taken to represent one or more spans when collected and counted. The representivity of each span is used in a Span-to-Metrics pipeline to accurately count spans.</p> <p>Sampling terminology uses \"population\" to refer to the complete set of data being sampled from. In OpenTelemetry tracing, \"population\" refers to all spans.</p> <p>In probability sampling, the representivity of individual sample items is generally known, whereas OpenTelemetry also recognizes \"non-probability\" sampling approaches, in which representivity is not explicitly quantified.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#adjusted-count","title":"Adjusted count","text":"<p>Adjusted count is a measure of representivity, the number of spans in the population that are represented by the individually sampled span. Span-to-metrics pipelines can be built by adding the adjusted count of each sample span to a counter of matching spans.</p> <p>For probability sampling, adjusted count is defined as the reciprocal (i.e., mathematical inverse) of sampling probability.</p> <p>For non-probability sampling, adjusted count is unknown.</p> <p>Zero adjusted count is defined in a way that supports composition of probability and non-probability sampling. Zero is assigned as the adjusted count when a probability sampler does not select a span.</p> <p>Thus, there are three meaningfully distinct categories of adjusted count:</p> Adjusted count is Interpretation Unknown The adjusted count is not known, possibly as a result of a non-probability sampler. Items in this category should not be counted. Zero The adjusted count is known; the effective count of the item is zero. Non-zero The adjusted count is known; the effective count of the item is greater than zero."},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#sampler","title":"Sampler","text":"<p>A Sampler provides configurable logic, used by the SDK, for selecting which Spans are \"recorded\" and/or \"sampled\" in a tracing client library. To \"record\" a span means to build a representation of it in the client's memory, which makes it eligible for being exported. To \"sample\" a span implies setting the W3C <code>sampled</code> flag, recording the span, and exporting the span when it is finished.</p> <p>OpenTelemetry supports spans that are \"recorded\" and not \"sampled\" for in-process observability of live spans (e.g., z-pages).</p> <p>The Sampler interface and the built-in Samplers defined by OpenTelemetry decide immediately whether to sample a span, and the child context immediately propagates the decision.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#parent-based-sampler","title":"Parent-based sampler","text":"<p>A Sampler that makes its decision to sample based on the W3C <code>sampled</code> flag from the context is said to use parent-based sampling.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#probability-sampler","title":"Probability sampler","text":"<p>A probability Sampler is a Sampler that knows immediately, for each of its decisions, the probability that the span had of being selected.</p> <p>Sampling probability is defined as a number less than or equal to 1 and greater than 0 (i.e., <code>0 &lt; probability &lt;= 1</code>). The case of 0 probability is treated as a special, non-probabilistic case.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#consistent-probability-sampler","title":"Consistent probability sampler","text":"<p>A consistent probability sampler is a Sampler that supports independent sampling decisions at each span in a trace while maintaining that traces will be complete with a certain minimum probability across the trace.</p> <p>Consistent probability sampling requires that for any span in a given trace, if a Sampler with lesser sampling probability selects the span for sampling, then the span would also be selected by a Sampler configured with greater sampling probability.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-completeness","title":"Trace completeness","text":"<p>A trace is said to be complete when all of the spans belonging to the trace are collected. When at least one span is collected but not all spans are collected, the trace is considered incomplete.</p> <p>Trace incompleteness may happen on purpose (e.g., through sampling configuration), or by accident (e.g., through collection errors). The OpenTelemetry trace data model supports a one-way test for incompleteness: for any non-root span, the trace is definitely incomplete if the span's parent span was not collected.</p> <p>Incomplete traces that result from sampling configuration (i.e., on purpose) are known as partial traces. An important subset of the partial traces are those which are also complete subtraces. A complete subtrace is defined at span S when every descendent span is collected.</p> <p>Since the test for an incompleteness is one-way, it is important to know which sampling configurations may lead to incomplete traces. Sampling configurations that lead naturally to complete traces and complete subtraces are discussed below.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#non-probability-sampler","title":"Non-probability sampler","text":"<p>A non-probability sampler is a Sampler that makes its decisions not based on chance, but instead uses arbitrary logic and internal state. The adjusted count of spans sampled by a non-probability sampler is unknown.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#always-on-consistent-probability-sampler","title":"Always-on consistent probability sampler","text":"<p>An always-on sampler is another name for a consistent probability sampler with probability equal to one.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#always-off-sampler","title":"Always-off sampler","text":"<p>An always-off Sampler has the effect of disabling a span completely, effectively excluding it from the population. This is defined as a non-probability sampler, not a zero-percent probability sampler, because the spans are effectively unrepresented.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#consistent-probability-sampling","title":"Consistent Probability sampling","text":"<p>The consistent sampling scheme adopted by OpenTelemetry propagates two values via the context, termed \"p-value\" and \"r-value\".</p> <p>Both fields are propagated via the OpenTelemetry <code>tracestate</code> under the <code>ot</code> vendor tag using the rules for tracestate handling. Both fields are represented as unsigned decimal integers requiring at most 6 bits of information.</p> <p>This sampling scheme selects items from among a fixed set of 63 distinct probability values. The set of supported probabilities includes the integer powers of two between 1 and 2**-62. Zero probability and probabilities smaller than 2**-62 are treated as a special case of \"ConsistentAlwaysOff\" sampler, just as unit probability (i.e., 100%) describes a special case of \"ConsistentAlwaysOn\" sampler.</p> <p>R-value encodes which among the 63 possibilities will consistently decide to sample for a given trace. Specifically, r-value specifies the smallest probability that will decide to sample a given trace in terms of the corresponding p-value. For example, a trace with r-value 0 will sample spans configured for 100% sampling, while r-value 1 will sample spans configured for 50% or 100% sampling, and so on through r-value 62, for which a consistent probability sampler will decide \"yes\" at every supported probability (i.e., greater than or equal to 2**-62).</p> <p>P-value encodes the adjusted count for child contexts (i.e., consumers of <code>tracestate</code>) and consumers of sampled spans to record for use in Span-to-metrics pipelines. A special p-value of 63 is defined to mean zero adjusted count, which helps define composition rules for non-probability samplers.</p> <p>An invariant will be stated that connects the <code>sampled</code> trace flag found in <code>traceparent</code> context to the r-value and p-value found in <code>tracestate</code> context.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#conformance","title":"Conformance","text":"<p>Consumers of OpenTelemetry <code>tracestate</code> data are expected to validate the probability sampling fields before interpreting the data. This applies to the two samplers specified here as well as consumers of span data, who are expected to validate <code>tracestate</code> before interpreting span adjusted counts.</p> <p>Producers of OpenTelemetry <code>tracestate</code> containing p-value and r-value fields are required to meet the behavioral requirements stated for the <code>ConsistentProbabilityBased</code> sampler and to ensure statistically valid outcomes. A test suite is included in this specification so that users and consumers of OpenTelemetry <code>tracestate</code> can be assured of accuracy in Span-to-metrics pipelines.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#completeness-guarantee","title":"Completeness guarantee","text":"<p>This specification defines consistent sampling for power-of-two sampling probabilities. When a sampler is configured with a non-power-of-two sampling probability, the sampler will probabilistically choose between the nearest powers of two.</p> <p>When a single consistent probability sampler is used at the root of a trace and all other spans use a parent-based sampler, the resulting traces are always complete (ignoring collection errors). This property holds even for non-power-of-two sampling probabilities.</p> <p>When multiple consistent probability samplers are used in the same trace, in general, trace completeness is ensured at the smallest power of two greater than or equal to the minimum sampling probability across the trace.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#context-invariants","title":"Context invariants","text":"<p>The W3C <code>traceparent</code> (version 0) contains three fields of information: the TraceId, the SpanId, and the trace flags. The <code>sampled</code> trace flag has been defined by W3C to signal an intent to sample the context.</p> <p>The Sampler API is responsible for setting the <code>sampled</code> flag and the <code>tracestate</code>.</p> <p>P-value and r-value are set in the OpenTelemetry <code>tracestate</code>, under the vendor tag <code>ot</code>, using the identifiers <code>p</code> and <code>r</code>. P-value is an unsigned integer valid in the inclusive range <code>[0, 63]</code> (i.e., there are 64 valid values). R-value is an unsigned integer valid in the inclusive range <code>[0, 62]</code> (i.e., there are 63 valid values). P-value and r-value are independent settings, each can be meaningfully set without the other present.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#sampled-flag","title":"Sampled flag","text":"<p>Probability sampling uses additional information to enable consistent decision making and to record the adjusted count of sampled spans. When both values are defined and in the specified range, the invariant between r-value and p-value and the <code>sampled</code> trace flag states that <code>((p &lt;= r) == sampled) OR (sampled AND (p == 63)) == TRUE</code>.</p> <p>The invariant between <code>sampled</code>, <code>p</code>, and <code>r</code> only applies when both <code>p</code> and <code>r</code> are present. When the invariant is violated, the <code>sampled</code> flag takes precedence and <code>p</code> is unset from <code>tracestate</code> in order to signal unknown adjusted count.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-inconsistent-p-values-are-unset","title":"Requirement: Inconsistent p-values are unset","text":"<p>Samplers SHOULD unset <code>p</code> when the invariant between the <code>sampled</code>, <code>p</code>, and <code>r</code> values is violated before using the <code>tracestate</code> to make a sampling decision or interpret adjusted count.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#p-value","title":"P-value","text":"<p>Zero adjusted count is represented by the special p-value 63, otherwise the p-value is set to the negative base-2 logarithm of sampling probability:</p> p-value Parent Probability Adjusted count 0 1 1 1 \u00bd 2 2 \u00bc 4 ... ... ... N 2**-N 2**N ... ... ... 61 2**-61 2**61 62 2**-62 2**62 63 0 0"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-out-of-range-p-values-are-unset","title":"Requirement: Out-of-range p-values are unset","text":"<p>Consumers SHOULD unset <code>p</code> from the <code>tracestate</code> if the unsigned decimal value is greater than 63 before using the <code>tracestate</code> to make a sampling decision or interpret adjusted count.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#r-value","title":"R-value","text":"<p>R-value is set in the <code>tracestate</code> by the Sampler at the root of the trace, in order to support consistent probability sampling. When the value is omitted or not present, child spans in the trace are not able to participate in consistent probability sampling.</p> <p>R-value determines which sampling probabilities will decide to sample or not decide to sample for spans of a given trace, as follows:</p> r-value Implied sampling probabilities 0 1 1 \u00bd and above 2 \u00bc and above 3 \u215b and above ... ... 0 &lt;= r &lt;= 61 2**-r and above ... ... 59 2**-59 and above 60 2**-60 and above 61 2**-61 and above 62 2**-62 and above <p>These probabilities are specified to ensure that conforming Sampler implementations record spans with correct adjusted counts. The recommended method of generating r-values is to count the number of leading 0s in a string of 62 random bits, however it is not required to use this approach.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-out-of-range-r-values-unset-both-p-and-r","title":"Requirement: Out-of-range r-values unset both p and r","text":"<p>Samplers SHOULD unset both <code>r</code> and <code>p</code> from the <code>tracestate</code> if the unsigned decimal value of <code>r</code> is greater than 62 before using the <code>tracestate</code> to make a sampling decision.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-r-value-is-generated-with-the-correct-probabilities","title":"Requirement: R-value is generated with the correct probabilities","text":"<p>Samplers MUST generate r-values using a randomized scheme that produces each value with the probabilities equivalent to those produced by counting the number of leading 0s in a string of 62 random bits.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#examples-context-invariants","title":"Examples: Context invariants","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-probability-sampled-context","title":"Example: Probability sampled context","text":"<p>Consider a trace context with the following headers:</p> <pre><code>traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\ntracestate: ot=r:3;p:2\n</code></pre> <p>The <code>traceparent</code> contents in this example example are repeated from the W3C specification) and have the following base64-encoded field values:</p> <pre><code>base16(version) = 00\nbase16(trace-id) = 4bf92f3577b34da6a3ce929d0e0e4736\nbase16(parent-id) = 00f067aa0ba902b7\nbase16(trace-flags) = 01  // (i.e., sampled)\n</code></pre> <p>The <code>tracestate</code> header contains OpenTelemetry string <code>r:3;p:2</code>, containing decimal-encoded p-value and r-value:</p> <pre><code>base10(r) = 3\nbase10(p) = 2\n</code></pre> <p>Here, r-value 3 indicates that a consistent probability sampler configured with probability 12.5% (i.e., 1-in-8) or greater will sample the trace. The p-value 2 indicates that the parent that set the <code>sampled</code> flag was configured to sample at 25% (i.e., 1-in-4). This trace context is consistent because <code>p &lt;= r</code> is true and the <code>sampled</code> flag is set.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-probability-unsampled","title":"Example: Probability unsampled","text":"<p>This example has an unsampled context where only the r-value is set.</p> <pre><code>traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-00\ntracestate: ot=r:3\n</code></pre> <p>This supports consistent probability sampling in child contexts by virtue of having an r-value. P-value is not set, consistent with an unsampled context.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#samplers","title":"Samplers","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#parentconsistentprobabilitybased-sampler","title":"ParentConsistentProbabilityBased sampler","text":"<p>The <code>ParentConsistentProbabilityBased</code> sampler is meant as an optional replacement for the <code>ParentBased</code> Sampler. It is required to first validate the <code>tracestate</code> and then respect the <code>sampled</code> flag in the W3C traceparent.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-parentconsistentprobabilitybased-api","title":"Requirement: ParentConsistentProbabilityBased API","text":"<p>The <code>ParentConsistentProbabilityBased</code> Sampler constructor SHOULD take a single Sampler argument, which is the Sampler to use in case the <code>ParentConsistentProbabilityBased</code> Sampler is called for a root span.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-parentconsistentprobabilitybased-does-not-modify-valid-tracestate","title":"Requirement: ParentConsistentProbabilityBased does not modify valid tracestate","text":"<p>The <code>ParentConsistentProbabilityBased</code> Sampler MUST NOT modify a valid <code>tracestate</code>.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-parentconsistentprobabilitybased-calls-the-configured-root-sampler-for-root-spans","title":"Requirement: ParentConsistentProbabilityBased calls the configured root sampler for root spans","text":"<p>The <code>ParentConsistentProbabilityBased</code> Sampler MUST delegate to the configured root Sampler when there is not a valid parent trace context.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-parentconsistentprobabilitybased-respects-the-sampled-flag-for-non-root-spans","title":"Requirement: ParentConsistentProbabilityBased respects the sampled flag for non-root spans","text":"<p>The <code>ParentConsistentProbabilityBased</code> Sampler MUST decide to sample the span according to the value of the <code>sampled</code> flag in the W3C traceparent header.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#consistentprobabilitybased-sampler","title":"ConsistentProbabilityBased sampler","text":"<p>The <code>ConsistentProbabilityBased</code> sampler is meant as an optional replacement for the <code>TraceIdRatioBased</code> Sampler. In the case where it is used as a root sampler, the <code>ConsistentProbabilityBased</code> sampler is required to produce a valid <code>tracestate</code>. In the case where it is used in a non-root context, it is required to validate the incoming <code>tracestate</code> and to produce a valid <code>tracestate</code> for the outgoing context.</p> <p>The <code>ConsistentProbabilityBased</code> sampler is required to support probabilities that are not exact powers of two. To do so, implementations are required to select between the nearest powers of two probabilistically. For example, 5% sampling can be achieved by selecting 1/16 sampling 60% of the time and 1/32 sampling 40% of the time.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-traceidratiobased-api-compatibility","title":"Requirement: TraceIdRatioBased API compatibility","text":"<p>The <code>ConsistentProbabilityBased</code> Sampler MUST have the same constructor signature as the built-in <code>TraceIdRatioBased</code> sampler in each OpenTelemetry SDK.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-sets-r-for-root-span","title":"Requirement: ConsistentProbabilityBased sampler sets r for root span","text":"<p>The <code>ConsistentProbabilityBased</code> Sampler MUST set <code>r</code> when it makes a root sampling decision.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-unsets-p-when-not-sampled","title":"Requirement: ConsistentProbabilityBased sampler unsets p when not sampled","text":"<p>The <code>ConsistentProbabilityBased</code> Sampler MUST unset <code>p</code> from the <code>tracestate</code> when it decides not to sample.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-sets-p-when-sampled","title":"Requirement: ConsistentProbabilityBased sampler sets p when sampled","text":"<p>The <code>ConsistentProbabilityBased</code> Sampler MUST set <code>p</code> when it decides to sample according to its configured sampling probability.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-records-unbiased-adjusted-counts","title":"Requirement: ConsistentProbabilityBased sampler records unbiased adjusted counts","text":"<p>The <code>ConsistentProbabilityBased</code> Sampler with non-zero probability MUST set <code>p</code> so that the adjusted count interpreted from the <code>tracestate</code> is an unbiased estimate of the number of representative spans in the population.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-sets-r-for-non-root-span","title":"Requirement: ConsistentProbabilityBased sampler sets r for non-root span","text":"<p>If <code>r</code> is not set on the input <code>tracecontext</code> and the Span is not a root span, <code>ConsistentProbabilityBased</code> SHOULD set <code>r</code> as if it were a root span and warn the user that a potentially inconsistent trace is being produced.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-consistentprobabilitybased-sampler-decides-not-to-sample-for-probabilities-less-than-2-62","title":"Requirement: ConsistentProbabilityBased sampler decides not to sample for probabilities less than 2**-62","text":"<p>If the configured sampling probability is in the interval <code>[0, 2**-62)</code>, the Sampler MUST decide not to sample.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#examples-consistent-probability-samplers","title":"Examples: Consistent probability samplers","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-setting-r-value-for-a-root-span","title":"Example: Setting R-value for a root span","text":"<p>A new root span is sampled by a consistent probability sampler at 25%. A new r-value should be generated (see the appendix for suitable methods), in this example r-value 5 is used which happens 1.5625% of the time and indicates to sample:</p> <pre><code>tracestate: ot=r:5;p:2\n</code></pre> <p>The span would be sampled because p-value 2 is less than or equal to r-value 5. An example <code>tracestate</code> where r-value 1 indicates not to sample at 25%:</p> <pre><code>tracestate: ot=r:1\n</code></pre> <p>This span would not be sampled because p-value 2 (corresponding with 25% sampling) is greater than r-value 1.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-handling-inconsistent-p-value","title":"Example: Handling inconsistent P-value","text":"<p>When either the consistent probability sampler or the parent-based consistent probability sampler receives a sampled context but invalid p-value, for example,</p> <pre><code>tracestate: ot=r:4;p:73\n</code></pre> <p>the <code>tracestate</code> will have its p-value stripped. The r-value is kept, and the sampler should act as if the following had been received:</p> <pre><code>tracestate: ot=r:4\n</code></pre> <p>The consistent probability sampler will make its own (consistent) decision using the r-value that was received.</p> <p>The parent-based consistent probability sampler will in this case follow the <code>sampled</code> flag. If the context is sampled, the resulting span will have an r-value without a p-value, which indicates unknown adjusted count.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-handling-corrupt-r-value","title":"Example: Handling corrupt R-value","text":"<p>A non-root span receives:</p> <pre><code>tracestate: ot=r:100;p:10\n</code></pre> <p>where the r-value is out of its valid range. The r-value and p-value are stripped during validation, according to the invariants. In this case, the sampler will act as though no <code>tracestate</code> were received.</p> <p>The parent-based consistent probability sampler will sample or not sample based on the <code>sampled</code> flag, in this case. If the context is sampled, the recorded span will have an r-value without a p-value, which indicates unknown adjusted count.</p> <p>The consistent probability sampler will generate a new r-value and make a new sampling decision while warning the user of a corrupt and potentially inconsistent r-value.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#composition-rules","title":"Composition rules","text":"<p>When more than one Sampler participates in the decision to sample a context, their decisions can be combined using composition rules. In all cases, the combined decision to sample is the logical-OR of the Samplers' decisions (i.e., sample if at least one of the composite Samplers decides to sample).</p> <p>To combine p-values from two consistent probability Sampler decisions, the Sampler with the greater probability takes effect. The output p-value becomes the minimum of the two values for <code>p</code>.</p> <p>To combine a consistent probability Sampler decision with a non-probability Sampler decision, p-value 63 is used to signify zero adjusted count. If the probability Sampler decides to sample, its p-value takes effect. If the probability Sampler decides not to sample when the non-probability sample does sample, p-value 63 takes effect signifying zero adjusted count.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#list-of-requirements","title":"List of requirements","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-combining-multiple-sampling-decisions-using-logical-or","title":"Requirement: Combining multiple sampling decisions using logical <code>or</code>","text":"<p>When multiple samplers are combined using composition, the sampling decision MUST be to sample if at least one of the combined samplers decides to sample.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-combine-multiple-consistent-probability-samplers-using-the-minimum-p-value","title":"Requirement: Combine multiple consistent probability samplers using the minimum p-value","text":"<p>When combining Sampler decisions for multiple consistent probability Samplers and at least one decides to sample, the minimum of the \"yes\" decision <code>p</code> values MUST be set in the <code>tracestate</code>.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-unset-p-when-multiple-consistent-probability-samplers-decide-not-to-sample","title":"Requirement: Unset p when multiple consistent probability samplers decide not to sample","text":"<p>When combining Sampler decisions for multiple consistent probability Samplers and none decides to sample, p-value MUST be unset in the <code>tracestate</code>.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-use-probability-sampler-p-value-when-its-decision-to-sample-is-combined-with-non-probability-samplers","title":"Requirement: Use probability sampler p-value when its decision to sample is combined with non-probability samplers","text":"<p>When combining Sampler decisions for a consistent probability Sampler and a non-probability Sampler, and the probability Sampler decides to sample, its p-value MUST be set in the <code>tracestate</code> regardless of the non-probability Sampler decision.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-use-p-value-63-when-a-probability-sampler-decision-not-to-sample-is-combined-with-a-non-probability-sampler-decision-to-sample","title":"Requirement: Use p-value 63 when a probability sampler decision not to sample is combined with a non-probability sampler decision to sample","text":"<p>When combining Sampler decisions for a consistent probability Sampler and a non-probability Sampler, and the probability Sampler decides not to sample but the non-probability does sample, p-value 63 MUST be set in the <code>tracestate</code>.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#examples-composition","title":"Examples: Composition","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-probability-and-non-probability-sampler-in-a-root-context","title":"Example: Probability and non-probability sampler in a root context","text":"<p>In a new root context, a consistent probability sampler decides not to set the sampled flag, adds <code>r:4</code> indicating that the trace is consistently sampled at 6.5% (i.e., 1-in-16) and larger probabilities.</p> <p>The probability sampler decision is composed with a non-probability sampler that decides to sample the context. Setting <code>sampled</code> when the probability sampler has not sampled requires setting <code>p:63</code>, indicating zero adjusted count.</p> <p>The resulting context:</p> <pre><code>traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\ntracestate: ot=r:4;p:63\n</code></pre>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#example-two-consistent-probability-samplers","title":"Example: Two consistent probability samplers","text":"<p>Whether a root or non-root, if multiple consistent probability samplers make a decision to sample a given context, the minimum p-value is output in the tracestate.</p> <p>If a root context, the first of the samplers generates <code>r:15</code> and its own p-value <code>p:10</code> (i.e., adjusted count 1024). The second of the two probability samplers outputs a smaller adjusted count <code>p:8</code> (i.e., adjusted count 256).</p> <p>The resulting context takes the smaller p-value:</p> <pre><code>traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01\ntracestate: ot=r:15;p:8\n</code></pre>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#producer-and-consumer-recommendations","title":"Producer and consumer recommendations","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-producer-completeness","title":"Trace producer: completeness","text":"<p>As stated in the completeness guarantee, traces will be possibly incomplete when configuring multiple consistent probability samplers in the same trace. One way to avoid producing incomplete traces is to use parent-based samplers except for root spans.</p> <p>There is a simple test for trace incompleteness, but it is a one-way test and does not detect when child spans are uncollected. One way to avoid producing incomplete traces is to avoid configuring non-power-of-two sampling probabilities for non-root spans, because completeness is not guaranteed for non-power-of-two sampling probabilities.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#recommenendation-use-non-descending-power-of-two-probabilities","title":"Recommenendation: use non-descending power-of-two probabilities","text":"<p>Complete subtraces will be produced when the sequence of sampling probabilities from the root of a trace to its leaves consists of non-descending powers of two. To ensure complete sub-traces are produced, child samplers SHOULD be configured with a power-of-two probability greater than or equal to the parent span's sampling probability.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-producer-correctness","title":"Trace producer: correctness","text":"<p>The use of tracestate to convey adjusted count information rests upon trust between participants in a trace. Users are advised not to use a Span-to-metrics pipeline when the parent sampling decision's corresponding adjusted count is untrustworthy.</p> <p>The <code>ConsistentProbabilityBased</code> and <code>ParentConsistentProbabilityBased</code> samplers can be used as delegates of another sampler, for conditioning the choice of sampler on span and other fixed attributes. However, for adjusted counts to be trustworthy, the choice of non-root sampler cannot be conditioned on the parent's sampled trace flag or the OpenTelemetry tracestate r-value and p-value, as these decisions would lead to incorrect adjusted counts.</p> <p>For example, the built-in <code>ParentBased</code> sampler supports configuring the delegated-to sampler based on whether the parent context is remote or non-remote, sampled or unsampled. If a <code>ParentBased</code> sampler delegates to a <code>ConsistentProbabilityBased</code> sampler only for unsampled contexts, the resulting Span-to-metrics pipeline will (probably) overcount spans.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#recommenendation-sampler-delegation","title":"Recommenendation: sampler delegation","text":"<p>For non-root spans, composite samplers SHOULD NOT condition the choice of delegated-to sampler based on the parent's sampled flag or OpenTelemetry tracestate.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-producer-interoperability-with-parentbased-sampler","title":"Trace producer: interoperability with <code>ParentBased</code> sampler","text":"<p>The OpenTelemetry built-in <code>ParentBased</code> sampler is interoperable with the <code>ConsistentProbabilityBased</code> sampler, provided that the delegated-to sampler does not change the decision that determined its selection. For example, it is safe to configure an alternate <code>ParentBased</code> sampler delegate for unsampled spans, provided the decision does not change to sampled.</p> <p>Because the <code>ParentBased</code> sampler honors the sampled trace flag, and OpenTelemetry SDKs include the tracestate in the <code>Span</code> data, which means a system can be upgraded to probability sampling by just replacing <code>TraceIDRatioBased</code> samplers with conforming <code>ConsistentProbabilityBased</code> samplers everywhere in the trace.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-producer-interoperability-with-traceidratiobased-sampler","title":"Trace producer: interoperability with <code>TraceIDRatioBased</code> sampler","text":"<p>The <code>TraceIDRatioBased</code> specification includes a RECOMMENDATION against being used for non-root spans because it does not specify how to make the sampler decision consistent across the trace. A <code>TraceIDRatioBased</code> sampler at the root span is interoperable with a <code>ConsistentParentProbabilityBased</code> sampler in terms of completeness, although the resulting spans will have unknown adjusted count.</p> <p>When a <code>TraceIDRatioBased</code> sampler is configured for a non-root span, several cases arise where an incorrect OpenTelemetry tracestate can be generated. Consider for example a trace with three spans where the root (R) has a <code>ConsistentProbabilityBased</code> sampler, the root's child (P) has a <code>TraceIDRatioBased</code> sampler, and the grand-child (C) has a <code>ParentBased</code> sampler. Because the <code>TraceIDRatioBased</code> sampler change the intermediate sampled flag without updating the OpenTelemetry tracestate, we have the following cases:</p> <ol> <li>If <code>TraceIDRatioBased</code> does not change P's decision, the trace is complete    and all spans' adjusted counts are correct.</li> <li>If <code>TraceIDRatioBased</code> changes P's decision from no to yes, the consumer will    observe a (definitely) incomplete trace containing P and C. Both spans will    have invalid OpenTelemetry tracestate, leading to unknown adjusted count in    this case.</li> <li>If <code>TraceIDRatioBased</code> changes the sampling decision from yes to no, the    consumer will observe singleton trace with correct adjusted count. The    consumer cannot determine that R has two unsampled descendents.</li> </ol> <p>As these cases demonstrate, users can expect incompleteness and unknown adjusted count when using <code>TraceIDRatioBased</code> samplers for non-root spans, but this goes against the originally specified warning.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#trace-consumer","title":"Trace consumer","text":"<p>Trace consumers are expected to apply the simple one-way test for incompleteness. When non-root spans are configured with independent sampling probabilities, traces may be complete in a way that cannot be detected. Because of the one-way test, consumers wanting to ensure complete traces are expected to know the minimum sampling probability across the system.</p> <p>Ignoring accidental data loss, a trace will be complete if all its spans are sampled with consistent probability samplers and the trace's r-value is larger than the corresponding smallest power of two greater than or equal to the minimum sampling probability across the trace.</p> <p>Due to the <code>ConsistentProbabilityBased</code> Sampler requirement about setting <code>r</code> when it is unset for a non-root span, trace consumers are advised to check traces for r-value consistency. When a single trace contains more than a single distinct <code>r</code> value, it means the trace was not correctly sampled at the root for probability sampling. While the adjusted count of each span is correct in this scenario, it may be impossible to detect complete traces.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#recommendation-recognize-inconsistent-r-values","title":"Recommendation: Recognize inconsistent r-values","text":"<p>When a single trace contains spans with <code>tracestate</code> values containing more than one distinct value for <code>r</code>, the consumer SHOULD recognize the trace as inconsistently sampled.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#appendix-statistical-test-requirements","title":"Appendix: Statistical test requirements","text":"<p>This section specifies a test that can be implemented to ensure basic conformance with the requirement that sampling decisions are unbiased.</p> <p>The goal of this test specification is to be simple to implement and not require advanced statistical skills or libraries to be successful.</p> <p>This test is not meant to evaluate the performance of a random number generator. This test assumes the underlying RNG is of good quality and checks that the sampler produces the expected proportionality with a high degree of statistical confidence.</p> <p>One of the challenges of this kind of test is that probabilistic tests are expected to occasionally produce exceptional results. To make this a strict test for random behavior, we take the following approach:</p> <ul> <li>Generate a pre-determined list of 20 random seeds</li> <li>Use fixed values for significance level (5%) and trials (20)</li> <li>Use a population size of 100,000 spans</li> <li>For each trial, simulate the population and compute ChiSquared test statistic</li> <li>Locate the first seed value in the ordered list such that the Chi-Squared   significance test fails exactly once out of 20 trials</li> </ul> <p>To create this test, perform the above sequence using the seed values from the predetermined list, in order, until a seed value is found with exactly one failure. This is expected to happen fairly often and is required to happen once among the 20 available seeds. After calculating the index of the first seed with exactly one ChiSquared failure, record it in the test. For continuous integration testing, it is only necessary to re-run the test using the predetermined seed index.</p> <p>As specified, the Chi-Squared test has either one or two degrees of freedom, depending on whether the sampling probability is an exact power of two or not.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#test-procedure-non-powers-of-two","title":"Test procedure: non-powers of two","text":"<p>In this case there are two degrees of freedom for the Chi-Squared test. The following table summarizes the test parameters.</p> Test case Sampling probability Lower, Upper p-value when sampled Expectlower Expectupper Expectunsampled 1 0.900000 0, 1 10000 80000 10000 2 0.600000 0, 1 40000 20000 40000 3 0.330000 1, 2 17000 16000 67000 4 0.130000 2, 3 12000 1000 87000 5 0.100000 3, 4 2500 7500 90000 6 0.050000 4, 5 1250 3750 95000 7 0.017000 5, 6 1425 275 98300 8 0.010000 6, 7 562.5 437.5 99000 9 0.005000 7, 8 281.25 218.75 99500 10 0.002900 8, 9 100.625 189.375 99710 11 0.001000 9, 10 95.3125 4.6875 99900 12 0.000500 10, 11 47.65625 2.34375 99950 <p>The formula for computing Chi-Squared in this case is:</p> <pre><code>ChiSquared = math.Pow(sampled_lowerP - expect_lowerP, 2) / expect_lowerP +\n             math.Pow(sampled_upperP - expect_upperP, 2) / expect_upperP +\n             math.Pow(100000 - sampled_lowerP - sampled_upperP - expect_unsampled, 2) / expect_unsampled\n</code></pre> <p>This should be compared with 0.102587, the value of the Chi-Squared distribution for two degrees of freedom with significance level 5%. For each probability in the table above, the test is required to demonstrate a seed that produces exactly one ChiSquared value less than 0.102587.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-pass-12-non-power-of-two-statistical-tests","title":"Requirement: Pass 12 non-power-of-two statistical tests","text":"<p>For the test with 20 trials and 100,000 spans each, the test MUST demonstrate a random number generator seed such that the ChiSquared test statistic is below 0.102587 exactly 1 out of 20 times.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#test-procedure-exact-powers-of-two","title":"Test procedure: exact powers of two","text":"<p>In this case there is one degree of freedom for the Chi-Squared test. The following table summarizes the test parameters.</p> Test case Sampling probability P-value when sampled Expectsampled Expectunsampled 13 0x1p-01 (0.500000) 1 50000 50000 14 0x1p-04 (0.062500) 4 6250 93750 15 0x1p-07 (0.007812) 7 781.25 99218.75 <p>The formula for computing Chi-Squared in this case is:</p> <pre><code>ChiSquared = math.Pow(sampled - expect_sampled, 2) / expect_sampled +\n             math.Pow(100000 - sampled - expect_unsampled, 2) / expect_unsampled\n</code></pre> <p>This should be compared with 0.003932, the value of the Chi-Squared distribution for one degree of freedom with significance level 5%. For each probability in the table above, the test is required to demonstrate a seed that produces exactly one ChiSquared value less than 0.003932.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#requirement-pass-3-power-of-two-statistical-tests","title":"Requirement: Pass 3 power-of-two statistical tests","text":"<p>For the test with 20 trials and 100,000 spans each, the test MUST demonstrate a random number generator seed such that the ChiSquared test statistic is below 0.003932 exactly 1 out of 20 times.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#test-implementation","title":"Test implementation","text":"<p>The recommended structure for this test uses a table listing the 15 probability values, the expected p-values, whether the ChiSquared statistic has one or two degrees of freedom, and the index into the predetermined list of seeds.</p> <pre><code>    for _, test := range []testCase{\n        // Non-powers of two\n        {0.90000, 1, twoDegrees, 3},\n        {0.60000, 1, twoDegrees, 2},\n        {0.33000, 2, twoDegrees, 2},\n        {0.13000, 3, twoDegrees, 1},\n        {0.10000, 4, twoDegrees, 0},\n        {0.05000, 5, twoDegrees, 0},\n        {0.01700, 6, twoDegrees, 2},\n        {0.01000, 7, twoDegrees, 2},\n        {0.00500, 8, twoDegrees, 2},\n        {0.00290, 9, twoDegrees, 4},\n        {0.00100, 10, twoDegrees, 6},\n        {0.00050, 11, twoDegrees, 0},\n\n        // Powers of two\n        {0x1p-1, 1, oneDegree, 0},\n        {0x1p-4, 4, oneDegree, 0},\n        {0x1p-7, 7, oneDegree, 1},\n    } {\n</code></pre> <p>Note that seed indexes in the example above have what appears to be the correct distribution. The five 0s, two 1s, five 2s, one 3s, and one 4 demonstrate that it is relatively easy to find examples where there is exactly one failure. Probability 0.001, with seed index 6 in this case, is a reminder that outliers exist. Further significance testing of this distribution is not recommended.</p>"},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#appendix","title":"Appendix","text":""},{"location":"docs/specs/otel/trace/tracestate-probability-sampling/#methods-for-generating-r-values","title":"Methods for generating R-values","text":"<p>The method used for generating r-values is not specified, in order to leave the implementation freedom to optimize. Typically, when the TraceId is known to contain at a 62-bit substring of random bits, R-values can be derived directly from the 62 random bits of TraceId by:</p> <ol> <li>Count the leading zeros</li> <li>Count the leading ones</li> <li>Count the trailing zeros</li> <li>Count the trailing ones.</li> </ol> <pre><code>import (\n\"math/rand\"\n\"math/bits\"\n)\nfunc nextRValueLeading() int {\nx := uint64(rand.Int63()) // 63 least-significant bits are random\ny := x &lt;&lt; 1 | 0x3         // 62 most-significant bits are random\nreturn bits.LeadingZeros64(y)\n}\n</code></pre> <p>If the TraceId contains unknown or insufficient randomness, another approach is to generate random bits until the first true or false value.</p> <pre><code>func nextRValueGenerated() int {\n    for r := 0; r &lt; 62; r++ {\n        if rand.Bool() == true {\n            return r\n        }\n    }\n    return 62\n}\n</code></pre> <p>Any scheme that produces r-values shown in the following table is considered conforming.</p> r-value Probability of r-value 0 \u00bd 1 \u00bc 2 \u215b 3 1/16 ... ... 0 &lt;= r &lt;= 61 2**-(r+1) ... ... 59 2**-60 60 2**-61 61 2**-62 62 2**-62"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/","title":"OpenTelemetry to Jaeger Transformation","text":"<p>Status: Stable</p> <p>This document defines the transformation between OpenTelemetry and Jaeger Spans. The generic transformation rules specified here also apply. If a particular generic transformation rule and the rule in this document contradict then the rule in this document MUST be used.</p> <p>Jaeger accepts spans in the following formats:</p> <ul> <li>OpenTelemetry Protocol (OTLP), defined in opentelemetry-proto</li> <li>Thrift <code>Batch</code>, defined in jaeger-idl/.../jaeger.thrift, accepted via UDP or HTTP</li> <li>Protobuf <code>Batch</code>, defined in jaeger-idl/.../model.proto, accepted via gRPC</li> </ul> <p>See also:</p> <ul> <li>Jaeger APIs</li> <li>Reference implementation of this translation in the OpenTelemetry Collector Contrib repository</li> </ul>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#summary","title":"Summary","text":"<p>The following table summarizes the major transformations between OpenTelemetry and Jaeger.</p> OpenTelemetry Jaeger Thrift Jaeger Proto Notes Span.TraceId Span.traceIdLow/High Span.trace_id See IDs Span.ParentId Span.parentSpanId as SpanReference See Parent ID Span.SpanId Span.spanId Span.span_id Span.TraceState TBD TBD Span.Name Span.operationName Span.operation_name Span.Kind Span.tags[\"span.kind\"] same See SpanKind for values mapping Span.StartTime Span.startTime Span.start_time See Unit of time Span.EndTime Span.duration same Calculated as EndTime - StartTime. See also Unit of time Span.Attributes Span.tags same See Attributes for data types for the mapping. Span.DroppedAttributesCount Add to Span.tags same See Dropped Attributes Count for tag name to use. Span.Events Span.logs same See Events for the mapping format. Span.DroppedEventsCount Add to Span.tags same See Dropped Events Count for tag name to use. Span.Links Span.references same See Links Span.DroppedLinksCount Add to Span.tags same See Dropped Links Count for tag name to use. Span.Status Add to Span.tags same See Status for tag names to use."},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#mappings","title":"Mappings","text":"<p>This section discusses the details of the transformations between OpenTelemetry and Jaeger.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#resource","title":"Resource","text":"<p>OpenTelemetry resources MUST be mapped to Jaeger's <code>Span.Process</code> tags. Multiple resources can exist for a single process and exporters need to handle this case accordingly.</p> <p>Critically, Jaeger backend depends on <code>Span.Process.ServiceName</code> to identify the service that produced the spans. That field MUST be populated from the <code>service.name</code> attribute of the <code>service</code> resource. If no <code>service.name</code> is contained in a Span's Resource, that field MUST be populated from the default <code>Resource</code>.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#ids","title":"IDs","text":"<p>Trace and span IDs in Jaeger are random sequences of bytes. However, Thrift model represents IDs using <code>i64</code> type, or in case of a 128-bit wide Trace ID as two <code>i64</code> fields <code>traceIdLow</code> and <code>traceIdHigh</code>. The bytes MUST be converted to/from unsigned ints using Big Endian byte order, e.g. <code>[0x10, 0x00, 0x00, 0x00] == 268435456</code>. The unsigned ints MUST be converted to <code>i64</code> by re-interpreting the existing 64bit value as signed <code>i64</code>. For example (in Go):</p> <pre><code>var (\nid       []byte = []byte{0xFF, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00}\nunsigned uint64 = binary.BigEndian.Uint64(id)\nsigned   int64  = int64(unsigned)\n)\nfmt.Println(\"unsigned:\", unsigned)\nfmt.Println(\"  signed:\", signed)\n// Output:\n// unsigned: 18374686479671623680\n//   signed: -72057594037927936\n</code></pre>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#parent-id","title":"Parent ID","text":"<p>Jaeger Thrift format allows capturing parent ID in a top-level Span field. Jaeger Proto format does not support parent ID field; instead the parent MUST be recorded as a <code>SpanReference</code> of type <code>CHILD_OF</code>, e.g.:</p> <pre><code>    SpanReference(\nref_type=opentracing.CHILD_OF,\ntrace_id=span.context.trace_id,\nspan_id=parent_id,\n)\n</code></pre> <p>This span reference MUST be the first in the list of references.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#spankind","title":"SpanKind","text":"<p>OpenTelemetry <code>SpanKind</code> field MUST be encoded as <code>span.kind</code> tag in Jaeger span, except for <code>SpanKind.INTERNAL</code>, which SHOULD NOT be translated to a tag.</p> OpenTelemetry Jaeger <code>SpanKind.CLIENT</code> <code>\"client\"</code> <code>SpanKind.SERVER</code> <code>\"server\"</code> <code>SpanKind.CONSUMER</code> <code>\"consumer\"</code> <code>SpanKind.PRODUCER</code> <code>\"producer\"</code> <code>SpanKind.INTERNAL</code> do not add <code>span.kind</code> tag"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#unit-of-time","title":"Unit of time","text":"<p>In Jaeger Thrift format the timestamps and durations MUST be represented in microseconds (since epoch for timestamps). If the original value in OpenTelemetry is expressed in nanoseconds, it MUST be rounded or truncated to microseconds.</p> <p>In Jaeger Proto format the timestamps and durations MUST be represented with nanosecond precision using <code>google.protobuf.Timestamp</code> and <code>google.protobuf.Duration</code> types.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#status","title":"Status","text":"<p>The Status is recorded as Span tags. See Status for tag names to use.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#error-flag","title":"Error flag","text":"<p>When Span <code>Status</code> is set to <code>ERROR</code>, an <code>error</code> span tag MUST be added with the Boolean value of <code>true</code>. The added <code>error</code> tag MAY override any previous value.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#attributes","title":"Attributes","text":"<p>OpenTelemetry Span <code>Attribute</code>(s) MUST be reported as <code>tags</code> to Jaeger.</p> <p>Primitive types MUST be represented by the corresponding types of Jaeger tags.</p> <p>Array values MUST be serialized to string like a JSON list as mentioned in semantic conventions.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#links","title":"Links","text":"<p>OpenTelemetry <code>Link</code>(s) MUST be converted to <code>SpanReference</code>(s) in Jaeger, using <code>FOLLOWS_FROM</code> reference type. The Link's attributes cannot be represented in Jaeger explicitly. The exporter MAY additionally convert <code>Link</code>(s) to span <code>Log</code>(s):</p> <ul> <li>use Span start time as the timestamp of the Log</li> <li>set Log tag <code>event=link</code></li> <li>set Log tags <code>trace_id</code> and <code>span_id</code> from the respective <code>SpanContext</code>'s fields</li> <li>store <code>Link</code>'s attributes as Log tags</li> </ul> <p>Span references generated from <code>Link</code>(s) MUST be added after the span reference generated from Parent ID, if any.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/jaeger/#events","title":"Events","text":"<p>Events MUST be converted to Jaeger Logs. OpenTelemetry Event's <code>time_unix_nano</code> and <code>attributes</code> fields map directly to Jaeger Log's <code>timestamp</code> and <code>fields</code> fields. Jaeger Log has no direct equivalent for OpenTelemetry Event's <code>name</code> field but OpenTracing semantic conventions specify some special attribute names here. OpenTelemetry Event's <code>name</code> field should be added to Jaeger Log's <code>fields</code> map as follows:</p> OpenTelemetry Event Field Jaeger Attribute <code>name</code> <code>event</code> <ul> <li>If OpenTelemetry Event contains an attributes with the key <code>event</code>, it should take precedence over Event's <code>name</code> field.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/","title":"OpenTelemetry to Zipkin Transformation","text":"<p>Status: Stable</p> <p>This document defines the transformation between OpenTelemetry and Zipkin Spans. The generic transformation rules specified here also apply. If a particular generic transformation rule and the rule in this document contradict then the rule in this document MUST be used.</p> <p>Zipkin's v2 API is defined in the zipkin.proto</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#summary","title":"Summary","text":"<p>The following table summarizes the major transformations between OpenTelemetry and Zipkin.</p> OpenTelemetry Zipkin Notes Span.TraceId Span.trace_id Span.ParentId Span.parent_id Span.SpanId Span.id Span.TraceState TBD TBD Span.Name Span.name Span.Kind Span.kind See SpanKind for values mapping Span.StartTime Span.timestamp See Unit of time Span.EndTime Span.duration Duration is calculated based on StartTime and EndTime. See also Unit of time Span.Attributes Add to Span.tags See Attributes for data types for the mapping. Span.DroppedAttributesCount Add to Span.tags See Dropped Attributes Count for tag name to use. Span.Events Span.annotations See Events for the mapping format. Span.DroppedEventsCount Add to Span.tags See Dropped Events Count for tag name to use. Span.Links TBD TBD Span.DroppedLinksCount Add to Span.tags See Dropped Links Count for tag name to use. Span.Status Add to Span.tags See Status for tag names to use. <p>TBD : This is work in progress document and it is currently doesn't specify mapping for these fields:</p> <p>OpenTelemetry fields:</p> <ul> <li>Resource attributes</li> <li>Tracestate</li> <li>Links</li> </ul> <p>Zipkin fields:</p> <ul> <li>local_endpoint</li> <li>debug</li> <li>shared</li> </ul>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#mappings","title":"Mappings","text":"<p>This section discusses the details of the transformations between OpenTelemetry and Zipkin.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#service-name","title":"Service name","text":"<p>Zipkin service name MUST be set to the value of the resource attribute: <code>service.name</code>. If no <code>service.name</code> is contained in a Span's Resource, it MUST be populated from the default <code>Resource</code>. In Zipkin it is important that the service name is consistent for all spans in a local root. Otherwise service graph and aggregations would not work properly. OpenTelemetry doesn't provide this consistency guarantee. Exporter may chose to override the value for service name based on a local root span to improve Zipkin user experience.</p> <p>Note, the attribute <code>service.namespace</code> MUST NOT be used for the Zipkin service name and should be sent as a Zipkin tag.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#spankind","title":"SpanKind","text":"<p>The following table lists all the <code>SpanKind</code> mappings between OpenTelemetry and Zipkin.</p> OpenTelemetry Zipkin Note <code>SpanKind.CLIENT</code> <code>SpanKind.CLIENT</code> <code>SpanKind.SERVER</code> <code>SpanKind.SERVER</code> <code>SpanKind.CONSUMER</code> <code>SpanKind.CONSUMER</code> <code>SpanKind.PRODUCER</code> <code>SpanKind.PRODUCER</code> <code>SpanKind.INTERNAL</code> <code>null</code> must be omitted (set to <code>null</code>)"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#remote-endpoint","title":"Remote endpoint","text":""},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#otlp-zipkin","title":"OTLP -&gt; Zipkin","text":"<p>If Zipkin <code>SpanKind</code> resolves to either <code>SpanKind.CLIENT</code> or <code>SpanKind.PRODUCER</code> then the service SHOULD specify remote endpoint otherwise Zipkin won't treat the Span as a dependency. <code>peer.service</code> is the preferred attribute but is not always available. The following table lists the possible attributes for <code>remoteEndpoint</code> by preferred ranking:</p> Rank Attribute Name Reason 1 peer.service OpenTelemetry adopted attribute for remote service. 2 server.address OpenTelemetry adopted attribute for remote hostname, or similar. 3 server.socket.domain OpenTelemetry adopted attribute for remote socket hostname of the peer. 4 server.socket.address &amp; server.socket.port OpenTelemetry adopted attribute for remote socket address of the peer. 5 peer.hostname Remote hostname defined in OpenTracing specification. 6 peer.address Remote address defined in OpenTracing specification. 7 db.name Commonly used database name attribute for DB Spans. <ul> <li>Ranking should control the selection order. For example, <code>server.address</code> (Rank   2) should be selected before <code>peer.address</code> (Rank 6).</li> <li><code>server.socket.domain</code> and <code>server.socket.address</code> can be used by themselves as <code>remoteEndpoint</code> but should be combined   with <code>server.socket.port</code> if it is also present.</li> </ul>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#zipkin-otlp","title":"Zipkin -&gt; OTLP","text":"<p>When mapping from Zipkin to OTLP set <code>peer.service</code> tag from <code>remoteEndpoint</code> unless there is a <code>peer.service</code> tag defined explicitly.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#attribute","title":"Attribute","text":"<p>OpenTelemetry Span <code>Attribute</code>(s) MUST be reported as <code>tags</code> to Zipkin.</p> <p>Some attributes defined in semantic convention document maps to the strongly-typed fields of Zipkin spans.</p> <p>Primitive types MUST be converted to string using en-US culture settings. Boolean values MUST use lower case strings <code>\"true\"</code> and <code>\"false\"</code>.</p> <p>Array values MUST be serialized to string like a JSON list as mentioned in semantic conventions.</p> <p>TBD: add examples</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#status","title":"Status","text":"<p>This section overrides the generic Status mapping rule.</p> <p>Span <code>Status</code> MUST be reported as a key-value pair in <code>tags</code> to Zipkin, unless it is <code>UNSET</code>. In the latter case it MUST NOT be reported.</p> <p>The following table defines the OpenTelemetry <code>Status</code> to Zipkin <code>tags</code> mapping.</p> Status Tag Key Tag Value Code <code>otel.status_code</code> Name of the code, either <code>OK</code> or <code>ERROR</code>. MUST NOT be set if the code is <code>UNSET</code>. Description <code>error</code> Description of the <code>Status</code>. MUST be set if the code is <code>ERROR</code>, use an empty string if Description has no value. MUST NOT be set for <code>OK</code> and <code>UNSET</code> codes. <p>Note: The <code>error</code> tag should only be set if <code>Status</code> is <code>Error</code>. If a boolean version (<code>{\"error\":false}</code> or <code>{\"error\":\"false\"}</code>) is present, it SHOULD be removed. Zipkin will treat any span with <code>error</code> sent as failed.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#events","title":"Events","text":"<p>OpenTelemetry <code>Event</code> has an optional <code>Attribute</code>(s) which is not supported by Zipkin. Events MUST be converted to the Annotations with the names which will include attribute values like this:</p> <pre><code>\"my-event-name\": { \"key1\" : \"value1\", \"key2\": \"value2\" }\n</code></pre>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#unit-of-time","title":"Unit of Time","text":"<p>Zipkin times like <code>timestamp</code>, <code>duration</code> and <code>annotation.timestamp</code> MUST be reported in microseconds as whole numbers. For example, <code>duration</code> of 1234 nanoseconds will be represented as 1 microsecond.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#request-payload","title":"Request Payload","text":"<p>For performance considerations, Zipkin fields that can be absent SHOULD be omitted from the payload when they are empty in the OpenTelemetry <code>Span</code>.</p> <p>For example, an OpenTelemetry <code>Span</code> without any <code>Event</code> should not have an <code>annotations</code> field in the Zipkin payload.</p>"},{"location":"docs/specs/otel/trace/sdk_exporters/zipkin/#considerations-for-legacy-v1-format","title":"Considerations for Legacy (v1) Format","text":"<p>Zipkin's v2 json format was defined in 2017, followed up by a protobuf format in 2018.</p> <p>Frameworks made before then use a more complex v1 Thrift or json format that notably differs in so far as it uses terminology such as Binary Annotation, and repeats endpoint information on each attribute.</p> <p>Consider using V1SpanConverter.java as a reference implementation for converting v1 model to OpenTelemetry.</p> <p>The span timestamp and duration were late additions to the V1 format. As in the code link above, it is possible to heuristically derive them from annotations.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/","title":"\u8ddf\u8e2a\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>In OpenTelemetry spans can be created freely and it\u2019s up to the implementor to annotate them with attributes specific to the represented operation. Spans represent specific operations in and between systems. Some of these operations represent calls that use well-known protocols like HTTP or database calls. Depending on the protocol and the type of operation, additional information is needed to represent and analyze a span correctly in monitoring systems. It is also important to unify how this attribution is made in different languages. This way, the operator will not need to learn specifics of a language and telemetry collected from polyglot (multi-language) micro-service environments can still be easily correlated and cross-analyzed.</p> <p>The following semantic conventions for spans are defined:</p> <ul> <li>General: General semantic attributes that may be used in   describing different kinds of operations.</li> <li>HTTP: For HTTP client and server spans.</li> <li>Database: For SQL and NoSQL client call spans.</li> <li>RPC/RMI: For remote procedure call (e.g., gRPC) spans.</li> <li>Messaging: For messaging systems (queues, publish/subscribe,   etc.) spans.</li> <li>FaaS: For   Function as a Service   (e.g., AWS Lambda) spans.</li> <li>Exceptions: For recording exceptions associated with a span.</li> <li>Compatibility: For spans generated by compatibility   components, e.g. OpenTracing Shim layer.</li> <li>Feature Flags: For recording feature flag evaluations   associated with a span.</li> </ul> <p>The following library-specific semantic conventions are defined:</p> <ul> <li>AWS Lambda: For AWS Lambda spans.</li> <li>AWS SDK: For AWS SDK spans.</li> <li>GraphQL: For GraphQL spans.</li> </ul> <p>Apart from semantic conventions for traces and metrics, OpenTelemetry also defines the concept of overarching Resources with their own Resource Semantic Conventions.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/#event-name-reuse-prohibition","title":"Event Name Reuse Prohibition","text":"<p>A new event MUST NOT be added with the same name as an event that existed in the past but was renamed (with a corresponding schema file).</p> <p>When introducing a new event name check all existing schema files to make sure the name does not appear as a key of any \"rename_events\" section (keys denote old event names in rename operations).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/","title":"CloudEvents","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <ul> <li>Definitions</li> <li>Overview</li> <li>Conventions</li> <li>Spans<ul> <li>Creation</li> <li>Processing</li> </ul> </li> <li>Attributes</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#definitions","title":"Definitions","text":"<p>From the  CloudEvents specification:</p> <p>CloudEvents is a specification for describing event data in common formats to provide interoperability across services, platforms and systems. </p> <p>For more information on the concepts, terminology and background of CloudEvents consult the CloudEvents Primer document.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#overview","title":"Overview","text":"<p>A CloudEvent can be sent directly from producer to consumer. For such a scenario, the traditional parent-child trace model works well. However, CloudEvents are also used in distributed systems where an event can go through many hops until it reaches a consumer. In this scenario, the traditional parent-child trace model is not sufficient to produce a meaningful trace.</p> <p>Consider the following scenario:</p> <pre><code>+----------+                  +--------------+\n| Producer | ---------------&gt; | Intermediary |\n+----------+                  +--------------+\n                                     |        \n                                     |        \n                                     |        \n                                     v        \n+----------+                    +----------+  \n| Consumer | &lt;----------------- |  Queue   |  \n+----------+                    +----------+ \n</code></pre> <p>With the traditional parent-child trace model, the above scenario would produce two traces, completely independent from each other because the consumer starts receiving (and thus has to specify a parent span) before it receives the event. It is not possible to correlate a producer with a consumer(s) solely via a parent-child relationship.</p> <pre><code>+-------------------------------------------------+\n|    Trace 1                                      |\n|                                                 |\n|    +---------------------------------------+    |\n|    | Send (auto-instr)                     |    |\n|    +---------------------------------------+    |\n|       +------------------------------------+    |\n|       | Intermediary: Received (auto-instr)|    |\n|       +------------------------------------+    |\n|       +------------------------------------+    |\n|       | Intermediary: Send (auto-instr)    |    |\n|       +------------------------------------+    |\n|                                                 |\n|    Trace 2                                      |\n|                                                 |\n|    +---------------------------------------+    |\n|    | Consumer: Receive (auto-instr)        |    |\n|    +---------------------------------------+    |\n|                                                 |\n+-------------------------------------------------+\n</code></pre> <p>This document defines semantic conventions to model the different stages a CloudEvent can go through in a system, making it possible to create traces that are meaningful and consistent. It covers creation, processing, context propagation between producer and consumer(s) and attributes to be added to spans.</p> <p>With the proposed model, it is possible to have an overview of everything that happened as the result of an event. One can, for example, answer the following questions:</p> <ul> <li>What components in a system reacted to an event</li> <li>What further events were sent due to an incoming event</li> <li>Which event caused the exception</li> </ul> <p>With the conventions in this document, the application scenario above would produce a trace where it is possible to correlate a producer with a consumer(s):</p> <pre><code>+-------------------------------------------------------+\n|          Trace 1                                      |\n|                                                       |\n|          +---------------------------------------+    |\n|    +---&gt; | Create event                          |    |\n|    |     +---------------------------------------+    |\n|    |     +---------------------------------------+    |\n|    |     | Send (auto-instr)                     |    |\n|    |     +---------------------------------------+    |\n|    |        +------------------------------------+    |\n|    |        | Intermediary: Received (auto-instr)|    |\n|    |        +------------------------------------+    |\n|    |        +------------------------------------+    |\n|    |        | Intermediary: Send (auto-instr)    |    |\n|    |Link    +------------------------------------+    |\n|    |                                                  |\n|    |                                                  |\n|    |                                                  |\n|    |     Trace 2                                      |\n|    |                                                  |\n|    |     +---------------------------------------+    |\n|    |     | Consumer: Receive (auto-instr)        |    |\n|    |     +---------------------------------------+    |\n|    |       +-------------------------------------+    |\n|    +------ | Consumer: Process                   |    |\n|            +-------------------------------------+    |\n|                                                       |\n+-------------------------------------------------------+\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#conventions","title":"Conventions","text":"<p>To achieve the trace above, it is necessary to capture the context of the event creation so that when the CloudEvent reaches its destination(s), this context can be continued. Each CloudEvent acts then as the medium of this context propagation.</p> <p>This document relies on the CloudEvents specification, which defines this context propagation mechanism via the CloudEvents Distributed Tracing Extension. Once the trace context is set on the event via the Distributed Tracing Extension, it MUST not be modified.</p> <p>The remainder of this section describes the semantic conventions for Spans required to achieve the proposed trace.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#spans","title":"Spans","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#creation","title":"Creation","text":"<p>Instrumentation SHOULD create a new span and populate the CloudEvents Distributed Tracing Extension on the event. This applies when:</p> <ul> <li>A CloudEvent is created by the instrumented library. It may be impossible or impractical to create the Span during event creation (e.g. inside the constructor or in a factory method), so instrumentation MAY create the Span later, when passing the event to the transport layer.</li> <li>A CloudEvent is created outside of the instrumented library (e.g. directly constructed by the application owner, without calling a constructor or factory method), and passed without the Distributed Tracing Extension populated.</li> </ul> <p>In case a CloudEvent is passed to the instrumented library with the Distributed Tracing Extension already populated, instrumentation MUST NOT create a span and MUST NOT modify the Distributed Tracing Extension on the event.</p> <ul> <li> <p>Span name: <code>CloudEvents Create &lt;event_type&gt;</code></p> </li> <li> <p>Span kind: PRODUCER</p> </li> <li> <p>Span attributes: Instrumentation MUST add the required attributes defined in the table below.</p> </li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#processing","title":"Processing","text":"<p>When an instrumented library supports processing of a single CloudEvent, instrumentation SHOULD create a new span to trace it.</p> <p>Instrumentation SHOULD set the remote trace context from the Distributed Tracing Extension as a link on the processing span. Instrumentation MAY add attributes to the link to further describe it.</p> <ul> <li> <p>Span name: <code>CloudEvents Process &lt;event_type&gt;</code></p> </li> <li> <p>Span kind: CONSUMER</p> </li> <li> <p>Span attributes: Instrumentation MUST add the required attributes defined in the table below.</p> </li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/cloudevents/#attributes","title":"Attributes","text":"<p>The following attributes are applicable to creation and processing Spans.</p> Attribute Type Description Examples Requirement Level <code>cloudevents.event_id</code> string The event_id uniquely identifies the event. <code>123e4567-e89b-12d3-a456-426614174000</code>; <code>0001</code> Required <code>cloudevents.event_source</code> string The source identifies the context in which an event happened. <code>https://github.com/cloudevents</code>; <code>/cloudevents/spec/pull/123</code>; <code>my-service</code> Required <code>cloudevents.event_spec_version</code> string The version of the CloudEvents specification which the event uses. <code>1.0</code> Recommended <code>cloudevents.event_type</code> string The event_type contains a value describing the type of event related to the originating occurrence. <code>com.github.pull_request.opened</code>; <code>com.example.object.deleted.v2</code> Recommended <code>cloudevents.event_subject</code> string The subject of the event in the context of the event producer (identified by source). <code>mynewfile.jpg</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/compatibility/","title":"\u517c\u5bb9\u6027\u7ec4\u4ef6\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines trace semantic conventions used by the compatibility components, e.g. OpenTracing Shim layer.</p> <ul> <li>OpenTracing</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/compatibility/#opentracing","title":"OpenTracing","text":"<p><code>Link</code>s created by the OpenTracing Shim MUST set <code>opentracing.ref_type</code> with one of the accepted values, describing the direct causal relationships between a child Span and a parent Span, as defined by OpenTracing.</p> Attribute Type Description Examples Requirement Level <code>opentracing.ref_type</code> string Parent-child Reference type [1] <code>child_of</code> Recommended <p>[1]: The causal relationship between a child Span and a parent Span.</p> <p><code>opentracing.ref_type</code> MUST be one of the following:</p> Value Description <code>child_of</code> The parent Span depends on the child Span in some capacity <code>follows_from</code> The parent Span does not depend in any way on the result of the child Span"},{"location":"docs/specs/otel/trace/semantic_conventions/database/","title":"\u6570\u636e\u5e93\u5ba2\u6237\u673a\u8c03\u7528\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <ul> <li>Connection-level attributes</li> <li>Notes and well-known identifiers for <code>db.system</code></li> <li>Connection-level attributes for specific technologies</li> <li>Call-level attributes</li> <li>Call-level attributes for specific technologies<ul> <li>Cassandra</li> <li>Microsoft Azure Cosmos DB Attributes</li> </ul> </li> <li>Examples</li> <li>MySQL</li> <li>Redis</li> <li>MongoDB</li> <li>Microsoft Azure Cosmos DB</li> </ul> <p>Warning Existing Database instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the networking attributes that they emit   until the HTTP semantic conventions are marked stable (HTTP stabilization   will include stabilization of a core set of networking attributes which are   also used in Database instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental     database attributes the instrumentation was emitting previously. This is     the default value.</li> <li><code>http</code> - emit the new, stable networking attributes, and stop emitting the     old experimental networking attributes that the instrumentation emitted     previously.</li> <li><code>http/dup</code> - emit both the old and the stable networking attributes,     allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul> <p>Span kind: MUST always be <code>CLIENT</code>.</p> <p>The span name SHOULD be set to a low cardinality value representing the statement executed on the database. It MAY be a stored procedure name (without arguments), DB statement without variable arguments, operation name, etc. Since SQL statements may have very high cardinality even without arguments, SQL spans SHOULD be named the following way, unless the statement is known to be of low cardinality: <code>&lt;db.operation&gt; &lt;db.name&gt;.&lt;db.sql.table&gt;</code>, provided that <code>db.operation</code> and <code>db.sql.table</code> are available. If <code>db.sql.table</code> is not available due to its semantics, the span SHOULD be named <code>&lt;db.operation&gt; &lt;db.name&gt;</code>. It is not recommended to attempt any client-side parsing of <code>db.statement</code> just to get these properties, they should only be used if the library being instrumented already provides them. When it's otherwise impossible to get any meaningful span name, <code>db.name</code> or the tech-specific database name MAY be used.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#connection-level-attributes","title":"Connection-level attributes","text":"<p>These attributes will usually be the same for all operations performed over the same database connection. Some database systems may allow a connection to switch to a different <code>db.user</code>, for example, and other database systems may not even have the concept of a connection at all.</p> Attribute Type Description Examples Requirement Level <code>db.system</code> string An identifier for the database management system (DBMS) product being used. See below for a list of well-known identifiers. <code>other_sql</code> Required <code>db.connection_string</code> string The connection string used to connect to the database. It is recommended to remove embedded credentials. <code>Server=(localdb)\\v11.0;Integrated Security=true;</code> Recommended <code>db.user</code> string Username for accessing the database. <code>readonly_user</code>; <code>reporting_user</code> Recommended <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>server.address</code> string Name of the database host. <code>example.com</code> Conditionally Required: See alternative attributes below. <code>server.port</code> int Logical server port number <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [1] <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> See below <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: If different than <code>server.port</code>. <p>[1]: If using a port other than the default port for this DBMS and if <code>server.address</code> is set.</p> <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>server.address</code></li> <li><code>server.socket.address</code></li> </ul> <p><code>db.system</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>other_sql</code> Some other SQL database. Fallback only. See notes. <code>mssql</code> Microsoft SQL Server <code>mssqlcompact</code> Microsoft SQL Server Compact <code>mysql</code> MySQL <code>oracle</code> Oracle Database <code>db2</code> IBM Db2 <code>postgresql</code> PostgreSQL <code>redshift</code> Amazon Redshift <code>hive</code> Apache Hive <code>cloudscape</code> Cloudscape <code>hsqldb</code> HyperSQL DataBase <code>progress</code> Progress Database <code>maxdb</code> SAP MaxDB <code>hanadb</code> SAP HANA <code>ingres</code> Ingres <code>firstsql</code> FirstSQL <code>edb</code> EnterpriseDB <code>cache</code> InterSystems Cach\u00e9 <code>adabas</code> Adabas (Adaptable Database System) <code>firebird</code> Firebird <code>derby</code> Apache Derby <code>filemaker</code> FileMaker <code>informix</code> Informix <code>instantdb</code> InstantDB <code>interbase</code> InterBase <code>mariadb</code> MariaDB <code>netezza</code> Netezza <code>pervasive</code> Pervasive PSQL <code>pointbase</code> PointBase <code>sqlite</code> SQLite <code>sybase</code> Sybase <code>teradata</code> Teradata <code>vertica</code> Vertica <code>h2</code> H2 <code>coldfusion</code> ColdFusion IMQ <code>cassandra</code> Apache Cassandra <code>hbase</code> Apache HBase <code>mongodb</code> MongoDB <code>redis</code> Redis <code>couchbase</code> Couchbase <code>couchdb</code> CouchDB <code>cosmosdb</code> Microsoft Azure Cosmos DB <code>dynamodb</code> Amazon DynamoDB <code>neo4j</code> Neo4j <code>geode</code> Apache Geode <code>elasticsearch</code> Elasticsearch <code>memcached</code> Memcached <code>cockroachdb</code> CockroachDB <code>opensearch</code> OpenSearch <code>clickhouse</code> ClickHouse <code>spanner</code> Cloud Spanner <code>trino</code> Trino"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#notes-and-well-known-identifiers-for-dbsystem","title":"Notes and well-known identifiers for <code>db.system</code>","text":"<p>The list above is a non-exhaustive list of well-known identifiers to be specified for <code>db.system</code>.</p> <p>If a value defined in this list applies to the DBMS to which the request is sent, this value MUST be used. If no value defined in this list is suitable, a custom value MUST be provided. This custom value MUST be the name of the DBMS in lowercase and without a version number to stay consistent with existing identifiers.</p> <p>It is encouraged to open a PR towards this specification to add missing values to the list, especially when instrumentations for those missing databases are written. This allows multiple instrumentations for the same database to be aligned and eases analyzing for backends.</p> <p>The value <code>other_sql</code> is intended as a fallback and MUST only be used if the DBMS is known to be SQL-compliant but the concrete product is not known to the instrumentation. If the concrete DBMS is known to the instrumentation, its specific identifier MUST be used.</p> <p>Back ends could, for example, use the provided identifier to determine the appropriate SQL dialect for parsing the <code>db.statement</code>.</p> <p>When additional attributes are added that only apply to a specific DBMS, its identifier SHOULD be used as a namespace in the attribute key as for the attributes in the sections below.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#connection-level-attributes-for-specific-technologies","title":"Connection-level attributes for specific technologies","text":"Attribute Type Description Examples Requirement Level <code>db.jdbc.driver_classname</code> string The fully-qualified class name of the Java Database Connectivity (JDBC) driver used to connect. <code>org.postgresql.Driver</code>; <code>com.microsoft.sqlserver.jdbc.SQLServerDriver</code> Recommended <code>db.mssql.instance_name</code> string The Microsoft SQL Server instance name connecting to. This name is used to determine the port of a named instance. [1] <code>MSSQLSERVER</code> Recommended <p>[1]: If setting a <code>db.mssql.instance_name</code>, <code>server.port</code> is no longer required (but still recommended if non-standard).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#call-level-attributes","title":"Call-level attributes","text":"<p>These attributes may be different for each operation performed, even if the same connection is used for multiple operations. Usually only one <code>db.name</code> will be used per connection though.</p> Attribute Type Description Examples Requirement Level <code>db.name</code> string This attribute is used to report the name of the database being accessed. For commands that switch the database, this should be set to the target database (even if the command fails). [1] <code>customers</code>; <code>main</code> Conditionally Required: If applicable. <code>db.statement</code> string The database statement being executed. <code>SELECT * FROM wuser_table</code>; <code>SET mykey \"WuValue\"</code> Recommended: [2] <code>db.operation</code> string The name of the operation being executed, e.g. the MongoDB command name such as <code>findAndModify</code>, or the SQL keyword. [3] <code>findAndModify</code>; <code>HMSET</code>; <code>SELECT</code> Conditionally Required: If <code>db.statement</code> is not applicable. <p>[1]: In some SQL databases, the database name to be used is called \"schema name\". In case there are multiple layers that could be considered for database name (e.g. Oracle instance name and schema name), the database name to be used is the more specific layer (e.g. Oracle schema name).</p> <p>[2]: Should be collected by default only if there is sanitization that excludes sensitive information.</p> <p>[3]: When setting this to an SQL keyword, it is not recommended to attempt any client-side parsing of <code>db.statement</code> just to get this property, but it should be set if the operation name is provided by the library being instrumented. If the SQL statement has an ambiguous operation, or performs more than one operation, this value may be omitted.</p> <p>For Redis, the value provided for <code>db.statement</code> SHOULD correspond to the syntax of the Redis CLI. If, for example, the [<code>HMSET</code> command][] is invoked, <code>\"HMSET myhash field1 'Hello' field2 'World'\"</code> would be a suitable value for <code>db.statement</code>.</p> <p>In CouchDB, <code>db.operation</code> should be set to the HTTP method + the target REST route according to the API reference documentation. For example, when retrieving a document, <code>db.operation</code> would be set to (literally, i.e., without replacing the placeholders with concrete values): <code>GET /{db}/{docid}</code>.</p> <p>In Cassandra, <code>db.name</code> SHOULD be set to the keyspace name.</p> <p>In HBase, <code>db.name</code> SHOULD be set to the HBase namespace.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#call-level-attributes-for-specific-technologies","title":"Call-level attributes for specific technologies","text":"Attribute Type Description Examples Requirement Level <code>db.redis.database_index</code> int The index of the database being accessed as used in the <code>SELECT</code> command, provided as an integer. To be used instead of the generic <code>db.name</code> attribute. <code>0</code>; <code>1</code>; <code>15</code> Conditionally Required: If other than the default database (<code>0</code>). <code>db.mongodb.collection</code> string The collection being accessed within the database stated in <code>db.name</code>. <code>customers</code>; <code>products</code> Required <code>db.sql.table</code> string The name of the primary table that the operation is acting upon, including the database name (if applicable). [1] <code>public.users</code>; <code>customers</code> Recommended <p>[1]: It is not recommended to attempt any client-side parsing of <code>db.statement</code> just to get this property, but it should be set if it is provided by the library being instrumented. If the operation is acting upon an anonymous table, or more than one table, this value MUST NOT be set.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#cassandra","title":"Cassandra","text":"<p>Separated for clarity.</p> Attribute Type Description Examples Requirement Level <code>db.cassandra.page_size</code> int The fetch size used for paging, i.e. how many rows will be returned at once. <code>5000</code> Recommended <code>db.cassandra.consistency_level</code> string The consistency level of the query. Based on consistency values from CQL. <code>all</code> Recommended <code>db.cassandra.table</code> string The name of the primary table that the operation is acting upon, including the keyspace name (if applicable). [1] <code>mytable</code> Recommended <code>db.cassandra.idempotence</code> boolean Whether or not the query is idempotent. Recommended <code>db.cassandra.speculative_execution_count</code> int The number of times a query was speculatively executed. Not set or <code>0</code> if the query was not executed speculatively. <code>0</code>; <code>2</code> Recommended <code>db.cassandra.coordinator.id</code> string The ID of the coordinating node for a query. <code>be13faa2-8574-4d71-926d-27f16cf8a7af</code> Recommended <code>db.cassandra.coordinator.dc</code> string The data center of the coordinating node for a query. <code>us-west-2</code> Recommended <p>[1]: This mirrors the db.sql.table attribute but references cassandra rather than sql. It is not recommended to attempt any client-side parsing of <code>db.statement</code> just to get this property, but it should be set if it is provided by the library being instrumented. If the operation is acting upon an anonymous table, or more than one table, this value MUST NOT be set.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#microsoft-azure-cosmos-db-attributes","title":"Microsoft Azure Cosmos DB Attributes","text":"<p>Cosmos DB instrumentation includes call-level (public API) surface spans and network spans. Depending on the connection mode (Gateway or Direct), network-level spans may also be created.</p> Attribute Type Description Examples Requirement Level <code>db.cosmosdb.client_id</code> string Unique Cosmos client instance id. <code>3ba4827d-4422-483f-b59f-85b74211c11d</code> Recommended <code>db.cosmosdb.operation_type</code> string CosmosDB Operation Type. <code>Invalid</code> Conditionally Required: [1] <code>db.cosmosdb.connection_mode</code> string Cosmos client connection mode. <code>gateway</code> Conditionally Required: if not <code>direct</code> (or pick gw as default) <code>db.cosmosdb.container</code> string Cosmos DB container name. <code>anystring</code> Conditionally Required: if available <code>db.cosmosdb.request_content_length</code> int Request payload size in bytes Recommended <code>db.cosmosdb.status_code</code> int Cosmos DB status code. <code>200</code>; <code>201</code> Conditionally Required: if response was received <code>db.cosmosdb.sub_status_code</code> int Cosmos DB sub status code. <code>1000</code>; <code>1002</code> Conditionally Required: [2] <code>db.cosmosdb.request_charge</code> double RU consumed for that operation <code>46.18</code>; <code>1.0</code> Conditionally Required: when available <code>user_agent.original</code> string Full user-agent string is generated by Cosmos DB SDK [3] <code>cosmos-netstandard-sdk/3.23.0\\|3.23.1\\|1\\|X64\\|Linux 5.4.0-1098-azure 104 18\\|.NET Core 3.1.32\\|S\\|</code> Recommended <p>[1]: when performing one of the operations in this list</p> <p>[2]: when response was received and contained sub-code.</p> <p>[3]: The user-agent value is generated by SDK which is a combination of <code>sdk_version</code> : Current version of SDK. e.g. 'cosmos-netstandard-sdk/3.23.0' <code>direct_pkg_version</code> : Direct package version used by Cosmos DB SDK. e.g. '3.23.1' <code>number_of_client_instances</code> : Number of cosmos client instances created by the application. e.g. '1' <code>type_of_machine_architecture</code> : Machine architecture. e.g. 'X64' <code>operating_system</code> : Operating System. e.g. 'Linux 5.4.0-1098-azure 104 18' <code>runtime_framework</code> : Runtime Framework. e.g. '.NET Core 3.1.32' <code>failover_information</code> : Generated key to determine if region failover enabled. Format Reg-{D (Disabled discovery)}-S(application region)|L(List of preferred regions)|N(None, user did not configure it). Default value is \"NS\".</p> <p><code>db.cosmosdb.operation_type</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>Invalid</code> invalid <code>Create</code> create <code>Patch</code> patch <code>Read</code> read <code>ReadFeed</code> read_feed <code>Delete</code> delete <code>Replace</code> replace <code>Execute</code> execute <code>Query</code> query <code>Head</code> head <code>HeadFeed</code> head_feed <code>Upsert</code> upsert <code>Batch</code> batch <code>QueryPlan</code> query_plan <code>ExecuteJavaScript</code> execute_javascript <p><code>db.cosmosdb.connection_mode</code> MUST be one of the following:</p> Value Description <code>gateway</code> Gateway (HTTP) connections mode <code>direct</code> Direct connection. <p>In addition to Cosmos DB attributes, all spans include <code>az.namespace</code> attribute representing Azure Resource Provider namespace that MUST be equal to <code>Microsoft.DocumentDB</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#examples","title":"Examples","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/database/#mysql","title":"MySQL","text":"Key Value Span name <code>\"SELECT ShopDb.orders\"</code> <code>db.system</code> <code>\"mysql\"</code> <code>db.connection_string</code> <code>\"Server=shopdb.example.com;Database=ShopDb;Uid=billing_user;TableCache=true;UseCompression=True;MinimumPoolSize=10;MaximumPoolSize=50;\"</code> <code>db.user</code> <code>\"billing_user\"</code> <code>server.address</code> <code>\"shopdb.example.com\"</code> <code>server.socket.address</code> <code>\"192.0.2.12\"</code> <code>server.port</code> <code>3306</code> <code>network.transport</code> <code>\"IP.TCP\"</code> <code>db.name</code> <code>\"ShopDb\"</code> <code>db.statement</code> <code>\"SELECT * FROM orders WHERE order_id = 'o4711'\"</code> <code>db.operation</code> <code>\"SELECT\"</code> <code>db.sql.table</code> <code>\"orders\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#redis","title":"Redis","text":"<p>In this example, Redis is connected using a unix domain socket and therefore the connection string and <code>server.address</code> are left out. Furthermore, <code>db.name</code> is not specified as there is no database name in Redis and <code>db.redis.database_index</code> is set instead.</p> Key Value Span name <code>\"HMSET myhash\"</code> <code>db.system</code> <code>\"redis\"</code> <code>db.connection_string</code> not set <code>db.user</code> not set <code>server.socket.address</code> <code>\"/tmp/redis.sock\"</code> <code>network.transport</code> <code>\"Unix\"</code> <code>db.name</code> not set <code>db.statement</code> <code>\"HMSET myhash field1 'Hello' field2 'World\"</code> <code>db.operation</code> not set <code>db.redis.database_index</code> <code>15</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#mongodb","title":"MongoDB","text":"Key Value Span name <code>\"products.findAndModify\"</code> <code>db.system</code> <code>\"mongodb\"</code> <code>db.connection_string</code> not set <code>db.user</code> <code>\"the_user\"</code> <code>server.address</code> <code>\"mongodb0.example.com\"</code> <code>server.socket.address</code> <code>\"192.0.2.14\"</code> <code>server.port</code> <code>27017</code> <code>network.transport</code> <code>\"IP.TCP\"</code> <code>db.name</code> <code>\"shopDb\"</code> <code>db.statement</code> not set <code>db.operation</code> <code>\"findAndModify\"</code> <code>db.mongodb.collection</code> <code>\"products\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/database/#microsoft-azure-cosmos-db","title":"Microsoft Azure Cosmos DB","text":"Key Value Span name <code>\"ReadItemsAsync\"</code> <code>kind</code> <code>\"internal\"</code> <code>az.namespace</code> <code>\"Microsoft.DocumentDB\"</code> <code>db.system</code> <code>\"cosmosdb\"</code> <code>db.name</code> <code>\"database name\"</code> <code>db.operation</code> <code>\"ReadItemsAsync\"</code> <code>server.address</code> <code>\"account.documents.azure.com\"</code> <code>db.cosmosdb.client_id</code> <code>3ba4827d-4422-483f-b59f-85b74211c11d</code> <code>db.cosmosdb.operation_type</code> <code>Read</code> <code>user_agent.original</code> <code>cosmos-netstandard-sdk/3.23.0\\|3.23.1\\|1\\|X64\\|Linux 5.4.0-1098-azure 104 18\\|.NET Core 3.1.32\\|S\\|</code> <code>db.cosmosdb.connection_mode</code> <code>\"Direct\"</code> <code>db.cosmosdb.container</code> <code>\"container name\"</code> <code>db.cosmosdb.request_content_length</code> <code>20</code> <code>db.cosmosdb.status_code</code> <code>201</code> <code>db.cosmosdb.sub_status_code</code> <code>0</code> <code>db.cosmosdb.request_charge</code> <code>7.43</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/exceptions/","title":"\u5f02\u5e38\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions for recording application exceptions.</p> <ul> <li>Recording an Exception</li> <li>Attributes</li> <li>Stacktrace Representation</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/exceptions/#recording-an-exception","title":"Recording an Exception","text":"<p>An exception SHOULD be recorded as an <code>Event</code> on the span during which it occurred. The name of the event MUST be <code>\"exception\"</code>.</p> <p>A typical template for an auto-instrumentation implementing this semantic convention using an API-provided <code>recordException</code> method could look like this (pseudo-Java):</p> <pre><code>Span span = myTracer.startSpan(/*...*/);\ntry {\n// Code that does the actual work which the Span represents\n} catch (Throwable e) {\nspan.recordException(e, Attributes.of(\"exception.escaped\", true));\nthrow e;\n} finally {\nspan.end();\n}\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/exceptions/#attributes","title":"Attributes","text":"<p>The table below indicates which attributes should be added to the <code>Event</code> and their types.</p> <p>The event name MUST be <code>exception</code>.</p> Attribute Type Description Examples Requirement Level <code>exception.escaped</code> boolean SHOULD be set to true if the exception event is recorded at a point where it is known that the exception is escaping the scope of the span. [1] Recommended <code>exception.message</code> string The exception message. <code>Division by zero</code>; <code>Can't convert 'int' object to str implicitly</code> See below <code>exception.stacktrace</code> string A stacktrace as a string in the natural representation for the language runtime. The representation is to be determined and documented by each language SIG. <code>Exception in thread \"main\" java.lang.RuntimeException: Test exception\\n at com.example.GenerateTrace.methodB(GenerateTrace.java:13)\\n at com.example.GenerateTrace.methodA(GenerateTrace.java:9)\\n at com.example.GenerateTrace.main(GenerateTrace.java:5)</code> Recommended <code>exception.type</code> string The type of the exception (its fully-qualified class name, if applicable). The dynamic type of the exception should be preferred over the static type in languages that support it. <code>java.net.ConnectException</code>; <code>OSError</code> See below <p>[1]: An exception is considered to have escaped (or left) the scope of a span, if that span is ended while the exception is still logically \"in flight\". This may be actually \"in flight\" in some languages (e.g. if the exception is passed to a Context manager's <code>__exit__</code> method in Python) but will usually be caught at the point of recording the exception in most languages.</p> <p>It is usually not possible to determine at the point where an exception is thrown whether it will escape the scope of a span. However, it is trivial to know that an exception will escape, if one checks for an active exception just before ending the span, as done in the example above.</p> <p>It follows that an exception may still escape the scope of the span even if the <code>exception.escaped</code> attribute was not set or set to false, since the event might have been recorded at a time where it was not clear whether the exception will escape.</p> <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>exception.type</code></li> <li><code>exception.message</code></li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/exceptions/#stacktrace-representation","title":"Stacktrace Representation","text":"<p>The table below, adapted from Google Cloud, includes possible representations of stacktraces in various languages. The table is not meant to be a recommendation for any particular language, although SIGs are free to adopt them if they see fit.</p> Language Format C# the return value of Exception.ToString() Elixir the return value of Exception.format/3 Erlang the return value of <code>erl_error:format</code> Go the return value of runtime.Stack Java the contents of Throwable.printStackTrace() Javascript the return value of error.stack as returned by V8 Python the return value of traceback.format_exc() Ruby the return value of Exception.full_message <p>Backends can use the language specified methodology for generating a stacktrace combined with platform information from the telemetry sdk resource in order to extract more fine grained information from a stacktrace, if necessary.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/","title":"FaaS \u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines how to describe an instance of a function that runs without provisioning or managing of servers (also known as serverless functions or Function as a Service (FaaS)) with spans.</p> <p>See also the additional instructions for instrumenting AWS Lambda.</p> <ul> <li>General Attributes</li> <li>Function Name</li> <li>Difference between invocation and instance</li> <li>Incoming Invocations</li> <li>Incoming FaaS Span attributes</li> <li>Resource attributes as incoming FaaS span attributes</li> <li>Outgoing Invocations</li> <li>Function Trigger Type</li> <li>Datasource</li> <li>HTTP</li> <li>PubSub</li> <li>Timer</li> <li>Other</li> <li>Example</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#general-attributes","title":"General Attributes","text":"<p>Span <code>name</code> should be set to the function name being executed. Depending on the value of the <code>faas.trigger</code> attribute, additional attributes MUST be set. For example, an <code>http</code> trigger SHOULD follow the HTTP Server semantic conventions. For more information, refer to the Function Trigger Type section.</p> <p>If Spans following this convention are produced, a Resource of type <code>faas</code> MUST exist following the Resource semantic convention.</p> Attribute Type Description Examples Requirement Level <code>faas.trigger</code> string Type of the trigger which caused this function invocation. [1] <code>datasource</code> Recommended <code>faas.invocation_id</code> string The invocation ID of the current function invocation. <code>af9d5aa4-a685-4c5f-a22b-444f80b3cc28</code> Recommended <code>cloud.resource_id</code> string Cloud provider-specific native identifier of the monitored cloud resource (e.g. an ARN on AWS, a fully qualified resource ID on Azure, a full resource name on GCP) [2] <code>arn:aws:lambda:REGION:ACCOUNT_ID:function:my-function</code>; <code>//run.googleapis.com/projects/PROJECT_ID/locations/LOCATION_ID/services/SERVICE_ID</code>; <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code> Recommended <p>[1]: For the server/consumer span on the incoming side, <code>faas.trigger</code> MUST be set.</p> <p>Clients invoking FaaS instances usually cannot set <code>faas.trigger</code>, since they would typically need to look in the payload to determine the event type. If clients set it, it should be the same as the trigger that corresponding incoming would have (i.e., this has nothing to do with the underlying transport used to make the API call to invoke the lambda, which is often HTTP).</p> <p>[2]: On some cloud providers, it may not be possible to determine the full ID at startup, so it may be necessary to set <code>cloud.resource_id</code> as a span attribute instead.</p> <p>The exact value to use for <code>cloud.resource_id</code> depends on the cloud provider. The following well-known definitions MUST be used if you set this attribute and they apply:</p> <ul> <li>AWS Lambda: The function   ARN.   Take care not to use the \"invoked ARN\" directly but replace any   alias suffix   with the resolved function version, as the same runtime instance may be   invokable with multiple different aliases.</li> <li>GCP: The   URI of the resource</li> <li>Azure: The   Fully Qualified Resource ID   of the invoked function, not the function app, having the form   <code>/subscriptions/&lt;SUBSCIPTION_GUID&gt;/resourceGroups/&lt;RG&gt;/providers/Microsoft.Web/sites/&lt;FUNCAPP&gt;/functions/&lt;FUNC&gt;</code>.   This means that a span attribute MUST be used, as an Azure function app can   host multiple functions that would usually share a TracerProvider.</li> </ul> <p><code>faas.trigger</code> MUST be one of the following:</p> Value Description <code>datasource</code> A response to some data source operation such as a database or filesystem read/write. <code>http</code> To provide an answer to an inbound HTTP request <code>pubsub</code> A function is set to be executed when messages are sent to a messaging system. <code>timer</code> A function is scheduled to be executed regularly. <code>other</code> If none of the others apply"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#function-name","title":"Function Name","text":"<p>There are 2 locations where the function's name can be recorded: the span name and the <code>faas.name</code> Resource attribute.</p> <p>It is guaranteed that if <code>faas.name</code> attribute is present it will contain the function name, since it is defined in the semantic convention strictly for that purpose. It is also highly likely that Span name will contain the function name (e.g. for Span displaying purposes), but it is not guaranteed (since it is a weaker \"SHOULD\" requirement). Consumers that needs such guarantee can use <code>faas.name</code> attribute as the source.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#difference-between-invocation-and-instance","title":"Difference between invocation and instance","text":"<p>For performance reasons (e.g. AWS lambda, or Azure functions), FaaS providers allocate an execution environment for a single instance of a function that is used to serve multiple requests. Developers exploit this fact to solve the cold start issue, caching expensive resource computations between different function invocations. Furthermore, FaaS providers encourage this behavior, e.g. Google functions. The <code>faas.instance</code> resource attribute MAY be set to help correlate function invocations that belong to the same execution environment. The span attribute <code>faas.invocation_id</code> differs from the resource attribute <code>faas.instance</code> in the following:</p> <ul> <li><code>faas.invocation_id</code> refers to the ID of the current invocation of the   function;</li> <li><code>faas.instance</code> refers to the execution environment ID of the function.</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#incoming-invocations","title":"Incoming Invocations","text":"<p>This section describes incoming FaaS invocations as they are reported by the FaaS instance itself.</p> <p>For incoming FaaS spans, the span kind MUST be <code>Server</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#incoming-faas-span-attributes","title":"Incoming FaaS Span attributes","text":"Attribute Type Description Examples Requirement Level <code>faas.coldstart</code> boolean A boolean that is true if the serverless function is executed for the first time (aka cold-start). Recommended <code>faas.trigger</code> string Type of the trigger which caused this function invocation. [1] <code>datasource</code> Required <p>[1]: For the server/consumer span on the incoming side, <code>faas.trigger</code> MUST be set.</p> <p>Clients invoking FaaS instances usually cannot set <code>faas.trigger</code>, since they would typically need to look in the payload to determine the event type. If clients set it, it should be the same as the trigger that corresponding incoming would have (i.e., this has nothing to do with the underlying transport used to make the API call to invoke the lambda, which is often HTTP).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#resource-attributes-as-incoming-faas-span-attributes","title":"Resource attributes as incoming FaaS span attributes","text":"<p>In addition to the attributes listed above, any FaaS or cloud resource attributes MAY instead be set as span attributes on incoming FaaS invocation spans: In some FaaS environments some of the information required for resource attributes is only readily available in the context of an invocation (e.g. as part of a \"request context\" argument) and while a separate API call to look up the resource information is often possible, it may be prohibitively expensive due to cold start duration concerns. The <code>cloud.resource_id</code> and <code>cloud.account.id</code> attributes on AWS are some examples. In principle, the above considerations apply to any resource attribute that fulfills the criteria above (not being readily available without some extra effort that could be expensive).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#outgoing-invocations","title":"Outgoing Invocations","text":"<p>This section describes outgoing FaaS invocations as they are reported by a client calling a FaaS instance.</p> <p>For outgoing FaaS spans, the span kind MUST be <code>Client</code>.</p> <p>The values reported by the client for the attributes listed below SHOULD be equal to the corresponding FaaS resource attributes and Cloud resource attributes, which the invoked FaaS instance reports about itself, if it's instrumented.</p> Attribute Type Description Examples Requirement Level <code>faas.invoked_name</code> string The name of the invoked function. [1] <code>my-function</code> Required <code>faas.invoked_provider</code> string The cloud provider of the invoked function. [2] <code>alibaba_cloud</code> Required <code>faas.invoked_region</code> string The cloud region of the invoked function. [3] <code>eu-central-1</code> Conditionally Required: [4] <p>[1]: SHOULD be equal to the <code>faas.name</code> resource attribute of the invoked function.</p> <p>[2]: SHOULD be equal to the <code>cloud.provider</code> resource attribute of the invoked function.</p> <p>[3]: SHOULD be equal to the <code>cloud.region</code> resource attribute of the invoked function.</p> <p>[4]: For some cloud providers, like AWS or GCP, the region in which a function is hosted is essential to uniquely identify the function and also part of its endpoint. Since it's part of the endpoint being called, the region is always known to clients. In these cases, <code>faas.invoked_region</code> MUST be set accordingly. If the region is unknown to the client or not required for identifying the invoked function, setting <code>faas.invoked_region</code> is optional.</p> <p><code>faas.invoked_provider</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>alibaba_cloud</code> Alibaba Cloud <code>aws</code> Amazon Web Services <code>azure</code> Microsoft Azure <code>gcp</code> Google Cloud Platform <code>tencent_cloud</code> Tencent Cloud"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#function-trigger-type","title":"Function Trigger Type","text":"<p>This section describes how to handle the span creation and additional attributes based on the value of the attribute <code>faas.trigger</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#datasource","title":"Datasource","text":"<p>A datasource function is triggered as a response to some data source operation such as a database or filesystem read/write. FaaS instrumentations that produce <code>faas</code> spans with trigger <code>datasource</code>, SHOULD use the following set of attributes.</p> Attribute Type Description Examples Requirement Level <code>faas.document.collection</code> string The name of the source on which the triggering operation was performed. For example, in Cloud Storage or S3 corresponds to the bucket name, and in Cosmos DB to the database name. <code>myBucketName</code>; <code>myDbName</code> Required <code>faas.document.operation</code> string Describes the type of the operation that was performed on the data. <code>insert</code> Required <code>faas.document.time</code> string A string containing the time when the data was accessed in the ISO 8601 format expressed in UTC. <code>2020-01-23T13:47:06Z</code> Recommended <code>faas.document.name</code> string The document name/table subjected to the operation. For example, in Cloud Storage or S3 is the name of the file, and in Cosmos DB the table name. <code>myFile.txt</code>; <code>myTableName</code> Recommended <p><code>faas.document.operation</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>insert</code> When a new object is created. <code>edit</code> When an object is modified. <code>delete</code> When an object is deleted."},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#http","title":"HTTP","text":"<p>The function responsibility is to provide an answer to an inbound HTTP request. The <code>faas</code> span SHOULD follow the recommendations described in the HTTP Server semantic conventions.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#pubsub","title":"PubSub","text":"<p>A function is set to be executed when messages are sent to a messaging system. In this case, multiple messages could be batch and forwarded at once to the same function invocation. Therefore, a different root span of type <code>faas</code> MUST be created for each message processed by the function, following the Messaging systems semantic conventions. This way, it is possible to correlate each individual message with its invocation sender.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#timer","title":"Timer","text":"<p>A function is scheduled to be executed regularly. The following additional attributes are recommended.</p> Attribute Type Description Examples Requirement Level <code>faas.time</code> string A string containing the function invocation time in the ISO 8601 format expressed in UTC. <code>2020-01-23T13:47:06Z</code> Recommended <code>faas.cron</code> string A string containing the schedule period as Cron Expression. <code>0/5 * * * ? *</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#other","title":"Other","text":"<p>Function as a Service offers such flexibility that it is not possible to fully cover with semantic conventions. When a function does not satisfy any of the aforementioned cases, a span MUST set the attribute <code>faas.trigger</code> to <code>\"other\"</code>. In this case, it is responsibility of the framework or instrumentation library to define the most appropriate attributes.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/faas/#example","title":"Example","text":"<p>This example shows the FaaS attributes for a (non-FaaS) process hosted on Google Cloud Platform (Span A with kind <code>Client</code>), which invokes a Lambda function called \"my-lambda-function\" in Amazon Web Services (Span B with kind <code>Server</code>).</p> Attribute Kind Attribute Span A (Client, GCP) Span B (Server, AWS Lambda) Resource <code>cloud.provider</code> <code>\"gcp\"</code> <code>\"aws\"</code> Resource <code>cloud.region</code> <code>\"europe-west3\"</code> <code>\"eu-central-1\"</code> Span <code>faas.invoked_name</code> <code>\"my-lambda-function\"</code> n/a Span <code>faas.invoked_provider</code> <code>\"aws\"</code> n/a Span <code>faas.invoked_region</code> <code>\"eu-central-1\"</code> n/a Span <code>faas.trigger</code> n/a <code>\"http\"</code> Span <code>faas.invocation_id</code> n/a <code>\"af9d5aa4-a685-4c5f-a22b-444f80b3cc28\"</code> Span <code>faas.coldstart</code> n/a <code>true</code> Resource <code>faas.name</code> n/a <code>\"my-lambda-function\"</code> Resource <code>faas.version</code> n/a <code>\"semver:2.0.0\"</code> Resource <code>faas.instance</code> n/a <code>\"my-lambda-function:instance-0001\"</code> Resource <code>cloud.resource_id</code> n/a <code>\"arn:aws:lambda:us-west-2:123456789012:function:my-lambda-function\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/feature-flags/","title":"\u7279\u6027\u6807\u5fd7\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions for recording dynamic feature flag evaluations within a transaction as span events. To record an evaluation outside of a transaction context, consider recording it as a log record.</p> <ul> <li>Motivation</li> <li>Overview</li> <li>Convention</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/feature-flags/#motivation","title":"Motivation","text":"<p>Features flags are commonly used in modern applications to decouple feature releases from deployments. Many feature flagging tools support the ability to update flag configurations in near real-time from a remote feature flag management service. They also commonly allow rulesets to be defined that return values based on contextual information. For example, a feature could be enabled only for a specific subset of users based on context (e.g. users email domain, membership tier, country).</p> <p>Since feature flags are dynamic and affect runtime behavior, it's important to collect relevant feature flag telemetry signals. This can be used to determine the impact a feature has on a request, enabling enhanced observability use cases, such as A/B testing or progressive feature releases.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/feature-flags/#overview","title":"Overview","text":"<p>The following semantic convention defines how feature flags can be represented as an <code>Event</code> in OpenTelemetry. The terminology was defined in the OpenFeature specification, which represents an industry consensus. It's intended to be vendor neutral and provide flexibility for current and future use cases.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/feature-flags/#convention","title":"Convention","text":"<p>A flag evaluation SHOULD be recorded as an Event on the span during which it occurred.</p> <p>The event name MUST be <code>feature_flag</code>.</p> Attribute Type Description Examples Requirement Level <code>feature_flag.key</code> string The unique identifier of the feature flag. <code>logo-color</code> Required <code>feature_flag.provider_name</code> string The name of the service provider that performs the flag evaluation. <code>Flag Manager</code> Recommended <code>feature_flag.variant</code> string SHOULD be a semantic identifier for a value. If one is unavailable, a stringified version of the value can be used. [1] <code>red</code>; <code>true</code>; <code>on</code> Recommended <p>[1]: A semantic identifier, commonly referred to as a variant, provides a means for referring to a value without including the value itself. This can provide additional context for understanding the meaning behind a value. For example, the variant <code>red</code> maybe be used for the value <code>#c05543</code>.</p> <p>A stringified version of the value can be used in situations where a semantic identifier is unavailable. String representation of the value should be determined by the implementer.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/","title":"HTTP \u8de8\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions for HTTP client and server Spans. They can be used for http and https schemes and various HTTP versions like 1.1, 2 and SPDY.</p> <ul> <li>Name</li> <li>Status</li> <li>Common Attributes</li> <li>HTTP request and response headers</li> <li>HTTP client</li> <li>HTTP request retries and redirects</li> <li>HTTP server</li> <li>HTTP server definitions</li> <li>HTTP Server semantic conventions</li> <li>Examples</li> <li>HTTP client-server example</li> <li>HTTP client retries examples</li> <li>HTTP client authorization retry examples</li> <li>HTTP client redirects examples</li> </ul> <p>Warning Existing HTTP instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the HTTP or networking attributes that they   emit until the HTTP semantic conventions are marked stable (HTTP   stabilization will include stabilization of a core set of networking   attributes which are also used in HTTP instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental HTTP     and networking attributes the instrumentation was emitting previously.     This is the default value.</li> <li><code>http</code> - emit the new, stable HTTP and networking attributes, and stop     emitting the old experimental HTTP and networking attributes that the     instrumentation emitted previously.</li> <li><code>http/dup</code> - emit both the old and the stable HTTP and networking     attributes, allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#name","title":"Name","text":"<p>HTTP spans MUST follow the overall guidelines for span names. HTTP server span names SHOULD be <code>{http.request.method} {http.route}</code> if there is a (low-cardinality) <code>http.route</code> available. HTTP server span names SHOULD be <code>{http.method}</code> if there is no (low-cardinality) <code>http.route</code> available. HTTP client spans have no <code>http.route</code> attribute since client-side instrumentation is not generally aware of the \"route\", and therefore HTTP client spans SHOULD use <code>{http.method}</code>. Instrumentation MUST NOT default to using URI path as span name, but MAY provide hooks to allow custom logic to override the default span name.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#status","title":"Status","text":"<p>Span Status MUST be left unset if HTTP status code was in the 1xx, 2xx or 3xx ranges, unless there was another error (e.g., network error receiving the response body; or 3xx codes with max redirects exceeded), in which case status MUST be set to <code>Error</code>.</p> <p>For HTTP status codes in the 4xx range span status MUST be left unset in case of <code>SpanKind.SERVER</code> and MUST be set to <code>Error</code> in case of <code>SpanKind.CLIENT</code>.</p> <p>For HTTP status codes in the 5xx range, as well as any other code the client failed to interpret, span status MUST be set to <code>Error</code>.</p> <p>Don't set the span status description if the reason can be inferred from <code>http.response.status_code</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#common-attributes","title":"Common Attributes","text":"<p>The common attributes listed in this section apply to both HTTP clients and servers in addition to the specific attributes listed in the HTTP client and HTTP server sections below.</p> Attribute Type Description Examples Requirement Level <code>http.response.status_code</code> int HTTP response status code. <code>200</code> Conditionally Required: If and only if one was received/sent. <code>http.request.body.size</code> int The size of the request payload body in bytes. This is the number of bytes transferred excluding headers and is often, but not always, present as the Content-Length header. For requests using transport encoding, this should be the compressed size. <code>3495</code> Recommended <code>http.response.body.size</code> int The size of the response payload body in bytes. This is the number of bytes transferred excluding headers and is often, but not always, present as the Content-Length header. For requests using transport encoding, this should be the compressed size. <code>3495</code> Recommended <code>http.request.method</code> string HTTP request method. <code>GET</code>; <code>POST</code>; <code>HEAD</code> Required <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>http</code>; <code>spdy</code> Recommended: if not default (<code>http</code>). <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [1] <code>1.0</code>; <code>1.1</code>; <code>2.0</code> Recommended <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Conditionally Required: [2] <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>user_agent.original</code> string Value of the HTTP User-Agent header sent by the client. <code>CERN-LineMode/2.15 libwww/2.17b3</code> Recommended <p>[1]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[2]: If not default (<code>tcp</code> for <code>HTTP/1.1</code> and <code>HTTP/2</code>, <code>udp</code> for <code>HTTP/3</code>).</p> <p>Following attributes MUST be provided at span creation time (when provided at all), so they can be considered for sampling decisions:</p> <ul> <li><code>http.request.method</code></li> </ul> <p><code>network.transport</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>tcp</code> TCP <code>udp</code> UDP <code>pipe</code> Named or anonymous pipe. See note below. <code>unix</code> Unix domain socket <p><code>network.type</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>ipv4</code> IPv4 <code>ipv6</code> IPv6"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-request-and-response-headers","title":"HTTP request and response headers","text":"Attribute Type Description Examples Requirement Level <code>http.request.header.&lt;key&gt;</code> string[] HTTP request headers, <code>&lt;key&gt;</code> being the normalized HTTP Header name (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the header values. [1] [2] <code>http.request.header.content_type=[\"application/json\"]</code>; <code>http.request.header.x_forwarded_for=[\"1.2.3.4\", \"1.2.3.5\"]</code> Opt-In <code>http.response.header.&lt;key&gt;</code> string[] HTTP response headers, <code>&lt;key&gt;</code> being the normalized HTTP Header name (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the header values. [1] [2] <code>http.response.header.content_type=[\"application/json\"]</code>; <code>http.response.header.my_custom_header=[\"abc\", \"def\"]</code> Opt-In <p>[1]: Instrumentations SHOULD require an explicit configuration of which headers are to be captured. Including all request/response headers can be a security risk - explicit configuration helps avoid leaking sensitive information.</p> <p>The <code>User-Agent</code> header is already captured in the <code>user_agent.original</code> attribute. Users MAY explicitly configure instrumentations to capture them even though it is not recommended.</p> <p>[2]: The attribute value MUST consist of either multiple header values as an array of strings or a single-item array containing a possibly comma-concatenated string, depending on the way the HTTP library provides access to headers.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-client","title":"HTTP client","text":"<p>This span type represents an outbound HTTP request. There are two ways this can be achieved in an instrumentation:</p> <ol> <li> <p>Instrumentations SHOULD create an HTTP span for each attempt to send an HTTP    request over the wire. In case the request is resent, the resend attempts    MUST follow the HTTP resend spec. In    this case, instrumentations SHOULD NOT (also) emit a logical encompassing    HTTP client span.</p> </li> <li> <p>If for some reason it is not possible to emit a span for each send attempt    (because e.g. the instrumented library does not expose hooks that would allow    this), instrumentations MAY create an HTTP span for the top-most operation of    the HTTP client. In this case, the <code>url.full</code> MUST be the absolute URL that    was originally requested, before any HTTP-redirects that may happen when    executing the request.</p> </li> </ol> <p>For an HTTP client span, <code>SpanKind</code> MUST be <code>Client</code>.</p> Attribute Type Description Examples Requirement Level <code>http.resend_count</code> int The ordinal number of request resending attempt (for any reason, including redirects). [1] <code>3</code> Recommended: if and only if request was retried. <code>server.address</code> string Host identifier of the \"URI origin\" HTTP request is sent to. [2] <code>example.com</code> Required <code>server.port</code> int Port identifier of the \"URI origin\" HTTP request is sent to. [3] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [4] <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <code>server.socket.domain</code> string The domain name of an immediate peer. [5] <code>proxy.example.com</code> Recommended <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: If different than <code>server.port</code>. <code>url.full</code> string Absolute URL describing a network resource according to RFC3986 [6] <code>https://www.foo.bar/search?q=OpenTelemetry#SemConv</code>; <code>//localhost</code> Required <p>[1]: The resend count SHOULD be updated each time an HTTP request gets resent by the client, regardless of what was the cause of the resending (e.g. redirection, authorization failure, 503 Server Unavailable, network issues, or any other).</p> <p>[2]: Determined by using the first of the following that applies</p> <ul> <li>Host identifier of the   request target   if it's sent in absolute-form</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>If an HTTP client request is explicitly made to an IP address, e.g. <code>http://x.x.x.x:8080</code>, then <code>server.address</code> SHOULD be the IP address <code>x.x.x.x</code>. A DNS lookup SHOULD NOT be used.</p> <p>[3]: When request target is absolute URI, <code>server.port</code> MUST match URI port identifier, otherwise it MUST match <code>Host</code> header port identifier.</p> <p>[4]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p> <p>[5]: Typically observed from the client side, and represents a proxy or other intermediary domain name.</p> <p>[6]: For network calls, URL usually has <code>scheme://host[:port][path][?query][#fragment]</code> format, where the fragment is not transmitted over HTTP, but if it is known, it should be included nevertheless. <code>url.full</code> MUST NOT contain credentials passed via URL in form of <code>https://username:password@www.example.com/</code>. In such case username and password should be redacted and attribute's value should be <code>https://REDACTED:REDACTED@www.example.com/</code>. <code>url.full</code> SHOULD capture the absolute URL when it is available (or can be reconstructed) and SHOULD NOT be validated or modified except for sanitizing purposes.</p> <p>Following attributes MUST be provided at span creation time (when provided at all), so they can be considered for sampling decisions:</p> <ul> <li><code>server.address</code></li> <li><code>server.port</code></li> <li><code>url.full</code></li> </ul> <p>Note that in some cases host and port identifiers in the <code>Host</code> header might be different from the <code>server.address</code> and <code>server.port</code>, in this case instrumentation MAY populate <code>Host</code> header on <code>http.request.header.host</code> attribute even if it's not enabled by user.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-request-retries-and-redirects","title":"HTTP request retries and redirects","text":"<p>Retries and redirects cause more than one physical HTTP request to be sent. A request is resent when an HTTP client library sends more than one HTTP request to satisfy the same API call. This may happen due to following redirects, authorization challenges, 503 Server Unavailable, network issues, or any other.</p> <p>Each time an HTTP request is resent, the <code>http.resend_count</code> attribute SHOULD be added to each repeated span and set to the ordinal number of the request resend attempt.</p> <p>See the examples for more details about:</p> <ul> <li>retrying a server error,</li> <li>redirects,</li> <li>authorization.</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-server","title":"HTTP server","text":"<p>To understand the attributes defined in this section, it is helpful to read the \"Definitions\" subsection.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-server-definitions","title":"HTTP server definitions","text":"<p>This section gives a short summary of some concepts in web server configuration and web app deployment that are relevant to tracing.</p> <p>Usually, on a physical host, reachable by one or multiple IP addresses, a single HTTP listener process runs. If multiple processes are running, they must listen on distinct TCP/UDP ports so that the OS can route incoming TCP/UDP packets to the right one.</p> <p>Within a single server process, there can be multiple virtual hosts. The HTTP host header (in combination with a port number) is normally used to determine to which of them to route incoming HTTP requests.</p> <p>The host header value that matches some virtual host is called the virtual hosts's server name. If there are multiple aliases for the virtual host, one of them (often the first one listed in the configuration) is called the primary server name. See for example, the Apache <code>ServerName</code> or NGINX <code>server_name</code> directive or the CGI specification on <code>SERVER_NAME</code> (RFC 3875). In practice the HTTP host header is often ignored when just a single virtual host is configured for the IP.</p> <p>Within a single virtual host, some servers support the concepts of an HTTP application (for example in Java, the Servlet JSR defines an application as \"a collection of servlets, HTML pages, classes, and other resources that make up a complete application on a Web server\" -- SRV.9 in JSR 53; in a deployment of a Python application to Apache, the application would be the PEP 3333 conformant callable that is configured using the <code>WSGIScriptAlias</code> directive of <code>mod_wsgi</code>).</p> <p>An application can be \"mounted\" under an application root (also known as a context root, context prefix, or document base) which is a fixed path prefix of the URL that determines to which application a request is routed (e.g., the server could be configured to route all requests that go to an URL path starting with <code>/webshop/</code> at a particular virtual host to the <code>com.example.webshop</code> web application).</p> <p>Some servers allow to bind the same HTTP application to multiple <code>(virtual host, application root)</code> pairs.</p> <p>TODO: Find way to trace HTTP application and application root (opentelemetry/opentelementry-specification#335)</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-server-semantic-conventions","title":"HTTP Server semantic conventions","text":"<p>This span type represents an inbound HTTP request.</p> <p>For an HTTP server span, <code>SpanKind</code> MUST be <code>Server</code>.</p> <p>Given an inbound request for a route (e.g. <code>\"/users/:userID?\"</code>) the <code>name</code> attribute of the span SHOULD be set to this route.</p> <p>If the route cannot be determined, the <code>name</code> attribute MUST be set as defined in the general semantic conventions for HTTP.</p> Attribute Type Description Examples Requirement Level <code>http.route</code> string The matched route (path template in the format used by the respective server framework). See note below [1] <code>/users/:userID?</code>; <code>{controller}/{action}/{id?}</code> Conditionally Required: If and only if it's available <code>client.address</code> string Client address - unix domain socket name, IPv4 or IPv6 address. [2] <code>83.164.160.102</code> Recommended <code>client.port</code> int The port of the original client behind all proxies, if known (e.g. from Forwarded or a similar header). Otherwise, the immediate client peer port. [3] <code>65123</code> Recommended <code>client.socket.address</code> string Immediate client peer address - unix domain socket name, IPv4 or IPv6 address. <code>/tmp/my.sock</code>; <code>127.0.0.1</code> Recommended: If different than <code>client.address</code>. <code>client.socket.port</code> int Immediate client peer port number <code>35555</code> Recommended: If different than <code>client.port</code>. <code>server.address</code> string Name of the local HTTP server that received the request. [4] <code>example.com</code> Required <code>server.port</code> int Port of the local HTTP server that received the request. [5] <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: [6] <code>server.socket.address</code> string Local socket address. Useful in case of a multi-IP host. <code>10.5.3.2</code> Opt-In <code>server.socket.port</code> int Local socket port. Useful in case of a multi-port host. <code>16456</code> Opt-In <code>url.path</code> string The URI path component [7] <code>/search</code> Required <code>url.query</code> string The URI query component [8] <code>q=OpenTelemetry</code> Recommended <code>url.scheme</code> string The URI scheme component identifying the used protocol. <code>http</code>; <code>https</code> Required <p>[1]: MUST NOT be populated when this is not supported by the HTTP server framework as the route attribute should have low-cardinality and the URI path can NOT substitute it. SHOULD include the application root if there is one.</p> <p>[2]: The IP address of the original client behind all proxies, if known (e.g. from Forwarded, X-Forwarded-For, or a similar header). Otherwise, the immediate client peer address.</p> <p>[3]: When observed from the server side, and when communicating through an intermediary, <code>client.port</code> SHOULD represent client port behind any intermediaries (e.g. proxies) if it's available.</p> <p>[4]: Determined by using the first of the following that applies</p> <ul> <li>The   primary server name   of the matched virtual host. MUST only include host identifier.</li> <li>Host identifier of the   request target   if it's sent in absolute-form.</li> <li>Host identifier of the <code>Host</code> header</li> </ul> <p>SHOULD NOT be set if only IP address is available and capturing name would require a reverse DNS lookup.</p> <p>[5]: Determined by using the first of the following that applies</p> <ul> <li>Port identifier of the   primary server host   of the matched virtual host.</li> <li>Port identifier of the   request target   if it's sent in absolute-form.</li> <li>Port identifier of the <code>Host</code> header</li> </ul> <p>[6]: If not default (<code>80</code> for <code>http</code> scheme, <code>443</code> for <code>https</code>).</p> <p>[7]: When missing, the value is assumed to be <code>/</code></p> <p>[8]: Sensitive content provided in query string SHOULD be scrubbed when instrumentations can identify it.</p> <p>Following attributes MUST be provided at span creation time (when provided at all), so they can be considered for sampling decisions:</p> <ul> <li><code>server.address</code></li> <li><code>server.port</code></li> <li><code>url.path</code></li> <li><code>url.query</code></li> <li><code>url.scheme</code></li> </ul> <p><code>http.route</code> MUST be provided at span creation time if and only if it's already available. If it becomes available after span starts, instrumentation MUST populate it anytime before span ends.</p> <p>Note that in some cases host and port identifiers in the <code>Host</code> header might be different from the <code>server.address</code> and <code>server.port</code>, in this case instrumentation MAY populate <code>Host</code> header on <code>http.request.header.host</code> attribute even if it's not enabled by user.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#examples","title":"Examples","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-client-server-example","title":"HTTP client-server example","text":"<p>As an example, if a browser request for <code>https://example.com:8080/webshop/articles/4?s=1</code> is invoked from a host with IP 192.0.2.4, we may have the following Span on the client side:</p> <p>Span name: <code>GET</code></p> Attribute name Value <code>http.request.method</code> <code>\"GET\"</code> <code>network.protocol.version</code> <code>\"1.1\"</code> <code>url.full</code> <code>\"https://example.com:8080/webshop/articles/4?s=1\"</code> <code>server.address</code> <code>example.com</code> <code>server.port</code> 8080 <code>server.socket.address</code> <code>\"192.0.2.5\"</code> <code>http.response.status_code</code> <code>200</code> <p>The corresponding server Span may look like this:</p> <p>Span name: <code>GET /webshop/articles/:article_id</code>.</p> Attribute name Value <code>http.request.method</code> <code>\"GET\"</code> <code>network.protocol.version</code> <code>\"1.1\"</code> <code>url.path</code> <code>\"/webshop/articles/4\"</code> <code>url.query</code> <code>\"?s=1\"</code> <code>server.address</code> <code>\"example.com\"</code> <code>server.port</code> <code>8080</code> <code>url.scheme</code> <code>\"https\"</code> <code>http.route</code> <code>\"/webshop/articles/:article_id\"</code> <code>http.response.status_code</code> <code>200</code> <code>client.address</code> <code>\"192.0.2.4\"</code> <code>client.socket.address</code> <code>\"192.0.2.5\"</code> (the client goes through a proxy) <code>user_agent.original</code> <code>\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-client-retries-examples","title":"HTTP client retries examples","text":"<p>Example of retries in the presence of a trace started by an inbound request:</p> <pre><code>request (SERVER, trace=t1, span=s1)\n  |\n  -- GET / - 500 (CLIENT, trace=t1, span=s2)\n  |   |\n  |   --- server (SERVER, trace=t1, span=s3)\n  |\n  -- GET / - 500 (CLIENT, trace=t1, span=s4, http.resend_count=1)\n  |   |\n  |   --- server (SERVER, trace=t1, span=s5)\n  |\n  -- GET / - 200 (CLIENT, trace=t1, span=s6, http.resend_count=2)\n      |\n      --- server (SERVER, trace=t1, span=s7)\n</code></pre> <p>Example of retries with no trace started upfront:</p> <pre><code>GET / - 500 (CLIENT, trace=t1, span=s1)\n |\n --- server (SERVER, trace=t1, span=s2)\n\nGET / - 500 (CLIENT, trace=t2, span=s1, http.resend_count=1)\n |\n --- server (SERVER, trace=t2, span=s2)\n\nGET / - 200 (CLIENT, trace=t3, span=s1, http.resend_count=2)\n |\n --- server (SERVER, trace=t3, span=s1)\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-client-authorization-retry-examples","title":"HTTP client authorization retry examples","text":"<p>Example of retries in the presence of a trace started by an inbound request:</p> <pre><code>request (SERVER, trace=t1, span=s1)\n  |\n  -- GET /hello - 401 (CLIENT, trace=t1, span=s2)\n  |   |\n  |   --- server (SERVER, trace=t1, span=s3)\n  |\n  -- GET /hello - 200 (CLIENT, trace=t1, span=s4, http.resend_count=1)\n      |\n      --- server (SERVER, trace=t1, span=s5)\n</code></pre> <p>Example of retries with no trace started upfront:</p> <pre><code>GET /hello - 401 (CLIENT, trace=t1, span=s1)\n |\n --- server (SERVER, trace=t1, span=s2)\n\nGET /hello - 200 (CLIENT, trace=t2, span=s1, http.resend_count=1)\n |\n --- server (SERVER, trace=t2, span=s2)\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/http/#http-client-redirects-examples","title":"HTTP client redirects examples","text":"<p>Example of redirects in the presence of a trace started by an inbound request:</p> <pre><code>request (SERVER, trace=t1, span=s1)\n  |\n  -- GET / - 302 (CLIENT, trace=t1, span=s2)\n  |   |\n  |   --- server (SERVER, trace=t1, span=s3)\n  |\n  -- GET /hello - 200 (CLIENT, trace=t1, span=s4, http.resend_count=1)\n      |\n      --- server (SERVER, trace=t1, span=s5)\n</code></pre> <p>Example of redirects with no trace started upfront:</p> <pre><code>GET / - 302 (CLIENT, trace=t1, span=s1)\n |\n --- server (SERVER, trace=t1, span=s2)\n\nGET /hello - 200 (CLIENT, trace=t2, span=s1, http.resend_count=1)\n |\n --- server (SERVER, trace=t2, span=s2)\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/","title":"\u6d88\u606f\u4f20\u9012\u7cfb\u7edf","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <ul> <li>Definitions</li> <li>Message</li> <li>Producer</li> <li>Consumer</li> <li>Intermediary</li> <li>Destinations and sources</li> <li>Message consumption</li> <li>Conversations</li> <li>Temporary and anonymous destinations</li> <li>Conventions</li> <li>Context propagation</li> <li>Span name</li> <li>Span kind</li> <li>Operation names</li> <li>Messaging attributes</li> <li>Attribute namespaces</li> <li>Producer attributes</li> <li>Consumer attributes</li> <li>Per-message attributes</li> <li>Attributes specific to certain messaging systems<ul> <li>RabbitMQ</li> <li>Apache Kafka</li> <li>Apache RocketMQ</li> </ul> </li> <li>Examples</li> <li>Topic with multiple consumers</li> <li>Apache Kafka with Quarkus or Spring Boot Example</li> <li>Batch receiving</li> <li>Batch processing</li> </ul> <p>Warning Existing Messaging instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the networking attributes that they emit   until the HTTP semantic conventions are marked stable (HTTP stabilization   will include stabilization of a core set of networking attributes which are   also used in Messaging instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental     networking attributes the instrumentation was emitting previously. This is     the default value.</li> <li><code>http</code> - emit the new, stable networking attributes, and stop emitting the     old experimental networking attributes that the instrumentation emitted     previously.</li> <li><code>http/dup</code> - emit both the old and the stable networking attributes,     allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#definitions","title":"Definitions","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#message","title":"Message","text":"<p>Although messaging systems are not as standardized as, e.g., HTTP, it is assumed that the following definitions are applicable to most of them that have similar concepts at all (names borrowed mostly from JMS):</p> <p>A message is an envelope with a potentially empty payload. This envelope may offer the possibility to convey additional metadata, often in key/value form.</p> <p>A message is sent by a message producer to:</p> <ul> <li>Physically: some message broker (which can be e.g., a single server, or a   cluster, or a local process reached via IPC). The broker handles the actual   delivery, re-delivery, persistence, etc. In some messaging systems the broker   may be identical or co-located with (some) message consumers. With Apache   Kafka, the physical broker a message is written to depends on the number of   partitions, and which broker is the leader of the partition the record is   written to.</li> <li>Logically: some particular message destination.</li> </ul> <p>Messages can be delivered to 0, 1, or multiple consumers depending on the dispatching semantic of the protocol.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#producer","title":"Producer","text":"<p>The \"producer\" is a specific instance, process or device that creates and publishes a message. \"Publishing\" is the process of sending a message or batch of messages to the intermediary or consumer.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#consumer","title":"Consumer","text":"<p>A \"consumer\" receives the message and acts upon it. It uses the context and data to execute some logic, which might lead to the occurrence of new events.</p> <p>The consumer receives, processes, and settles a message. \"Receiving\" is the process of obtaining a message from the intermediary, \"processing\" is the process of acting on the information a message contains, \"settling\" is the process of notifying an intermediary that a message was processed successfully.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#intermediary","title":"Intermediary","text":"<p>An \"intermediary\" receives a message to forward it to the next receiver, which might be another intermediary or a consumer.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#destinations-and-sources","title":"Destinations and sources","text":"<p>A destination is usually uniquely identified by name within the messaging system instance. Examples of a destination name would be a URL or a simple one-word identifier. Sending messages to a destination is called \"publish\" in context of this specification.</p> <p>A source represents an entity within messaging system messages are consumed from. Source and destination for specific message may be the same. However, if message is routed within one or multiple brokers, source and destination can be different.</p> <p>Typical examples of destinations and sources include Kafka topics, RabbitMQ queues and topics.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#message-consumption","title":"Message consumption","text":"<p>The consumption of a message can happen in multiple steps. First, the lower-level receiving of a message at a consumer, and then the logical processing of the message. Often, the waiting for a message is not particularly interesting and hidden away in a framework that only invokes some handler function to process a message once one is received (in the same way that the listening on a TCP port for an incoming HTTP message is not particularly interesting).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#conversations","title":"Conversations","text":"<p>In some messaging systems, a message can receive one or more reply messages that answers a particular other message that was sent earlier. All messages that are grouped together by such a reply-relationship are called a conversation. The grouping usually happens through some sort of \"In-Reply-To:\" meta information or an explicit conversation ID (sometimes called correlation ID). Sometimes a conversation can span multiple message destinations (e.g. initiated via a topic, continued on a temporary one-to-one queue).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#temporary-and-anonymous-destinations","title":"Temporary and anonymous destinations","text":"<p>Some messaging systems support the concept of temporary destination (often only temporary queues) that are established just for a particular set of communication partners (often one to one) or conversation. Often such destinations are also unnamed (anonymous) or have an auto-generated name.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#conventions","title":"Conventions","text":"<p>Given these definitions, the remainder of this section describes the semantic conventions for Spans describing interactions with messaging systems.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#context-propagation","title":"Context propagation","text":"<p>A message may traverse many different components and layers in one or more intermediaries when it is propagated from the producer to the consumer(s). To be able to correlate consumer traces with producer traces using the existing context propagation mechanisms, all components must propagate context down the chain.</p> <p>Messaging systems themselves may trace messages as the messages travels from producers to consumers. Such tracing would cover the transport layer but would not help in correlating producers with consumers. To be able to directly correlate producers with consumers, another context that is propagated with the message is required.</p> <p>A message creation context allows correlating producers with consumers of a message and model the dependencies between them, regardless of the underlying messaging transport mechanism and its instrumentation.</p> <p>The message creation context is created by the producer and should be propagated to the consumer(s). Consumer traces cannot be directly correlated with producer traces if the message creation context is not attached and propagated with the message.</p> <p>A producer SHOULD attach a message creation context to each message. If possible, the message creation context SHOULD be attached in such a way that it cannot be changed by intermediaries.</p> <p>This document does not specify the exact mechanisms on how the creation context is attached/extracted to/from messages. Future versions of these conventions will give clear recommendations, following industry standards including, but not limited to Trace Context: AMQP protocol and Trace Context: MQTT protocol once those standards reach a stable state.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#span-name","title":"Span name","text":"<p>The span name SHOULD be set to the message destination name and the operation being performed in the following format:</p> <pre><code>&lt;destination name&gt; &lt;operation name&gt;\n</code></pre> <p>The destination name SHOULD only be used for the span name if it is known to be of low cardinality (cf. general span name guidelines). This can be assumed if it is statically derived from application code or configuration. Wherever possible, the real destination names after resolving logical or aliased names SHOULD be used. If the destination name is dynamic, such as a conversation ID or a value obtained from a <code>Reply-To</code> header, it SHOULD NOT be used for the span name. In these cases, an artificial destination name that best expresses the destination, or a generic, static fallback like <code>\"(anonymous)\"</code> for anonymous destinations SHOULD be used instead.</p> <p>The values allowed for <code>&lt;operation name&gt;</code> are defined in the section Operation names below. If the format above is used, the operation name MUST match the <code>messaging.operation</code> attribute defined for message consumer spans below.</p> <p>Examples:</p> <ul> <li><code>shop.orders publish</code></li> <li><code>shop.orders receive</code></li> <li><code>shop.orders process</code></li> <li><code>print_jobs publish</code></li> <li><code>topic with spaces process</code></li> <li><code>AuthenticationRequest-Conversations process</code></li> <li><code>(anonymous) publish</code> (<code>(anonymous)</code> being a stable identifier for an unnamed   destination)</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#span-kind","title":"Span kind","text":"<p>A producer of a message should set the span kind to <code>PRODUCER</code> unless it synchronously waits for a response: then it should use <code>CLIENT</code>. The processor of the message should set the kind to <code>CONSUMER</code>, unless it always sends back a reply that is directed to the producer of the message (as opposed to e.g., a queue on which the producer happens to listen): then it should use <code>SERVER</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#operation-names","title":"Operation names","text":"<p>The following operations related to messages are defined for these semantic conventions:</p> Operation name Description <code>publish</code> A message is sent to a destination by a message producer/client. <code>receive</code> A message is received from a destination by a message consumer/server. <code>process</code> A message that was previously received from a destination is processed by a message consumer/server."},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#messaging-attributes","title":"Messaging attributes","text":"Attribute Type Description Examples Requirement Level <code>messaging.system</code> string A string identifying the messaging system. <code>kafka</code>; <code>rabbitmq</code>; <code>rocketmq</code>; <code>activemq</code>; <code>AmazonSQS</code> Required <code>messaging.operation</code> string A string identifying the kind of messaging operation as defined in the Operation names section above. [1] <code>publish</code> Required <code>messaging.batch.message_count</code> int The number of messages sent, received, or processed in the scope of the batching operation. [2] <code>0</code>; <code>1</code>; <code>2</code> Conditionally Required: [3] <code>messaging.client_id</code> string A unique identifier for the client that consumes or produces a message. <code>client-5</code>; <code>myhost@8742@s8083jm</code> Recommended: If a client id is available <code>messaging.message.conversation_id</code> string The conversation ID identifying the conversation to which the message belongs, represented as a string. Sometimes called \"Correlation ID\". <code>MyConversationId</code> Recommended: [4] <code>messaging.message.id</code> string A value used by the messaging system as an identifier for the message, represented as a string. <code>452a7c7c7c7048c2f887f61572b18fc2</code> Recommended: [5] <code>messaging.message.payload_compressed_size_bytes</code> int The compressed size of the message payload in bytes. <code>2048</code> Recommended: [6] <code>messaging.message.payload_size_bytes</code> int The (uncompressed) size of the message payload in bytes. Also use this attribute if it is unknown whether the compressed or uncompressed payload size is reported. <code>2738</code> Recommended: [7] <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [8] <code>3.1.1</code> Recommended <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>server.address</code> string Logical server hostname, matches server FQDN if available, and IP or socket address if FQDN is not known. [9] <code>example.com</code> Conditionally Required: If available. <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <code>server.socket.domain</code> string The domain name of an immediate peer. [10] <code>proxy.example.com</code> Recommended: [11] <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: If different than <code>server.port</code>. <p>[1]: If a custom value is used, it MUST be of low cardinality.</p> <p>[2]: Instrumentations SHOULD NOT set <code>messaging.batch.message_count</code> on spans that operate with a single message. When a messaging client library supports both batch and single-message API for the same operation, instrumentations SHOULD use <code>messaging.batch.message_count</code> for batching APIs and SHOULD NOT use it for single-message APIs.</p> <p>[3]: If the span describes an operation on a batch of messages.</p> <p>[4]: Only if span represents operation on a single message.</p> <p>[5]: Only for spans that represent an operation on a single message.</p> <p>[6]: Only if span represents operation on a single message.</p> <p>[7]: Only if span represents operation on a single message.</p> <p>[8]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p>[9]: This should be the IP/hostname of the broker (or other network-level peer) this specific message is sent to/received from.</p> <p>[10]: Typically observed from the client side, and represents a proxy or other intermediary domain name.</p> <p>[11]: If different than <code>server.address</code> and if <code>server.socket.address</code> is set.</p> <p><code>messaging.operation</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>publish</code> publish <code>receive</code> receive <code>process</code> process <p>Additionally <code>server.port</code> from the network attributes is recommended. Furthermore, it is strongly recommended to add the [<code>network.transport</code>][] attribute and follow its guidelines, especially for in-process queueing systems (like Hangfire, for example). These attributes should be set to the broker to which the message is sent/from which it is received.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#attribute-namespaces","title":"Attribute namespaces","text":"<ul> <li><code>messaging.message</code>: Contains attributes that describe individual messages</li> <li><code>messaging.destination</code>: Contains attributes that describe the logical entity   messages are published to. See   Destinations and sources for more details</li> <li><code>messaging.source</code>: Contains attributes that describe the logical entity   messages are received from</li> <li><code>messaging.batch</code>: Contains attributes that describe batch operations</li> <li><code>messaging.consumer</code>: Contains attributes that describe application instance   that consumes a message. See consumer for more details</li> </ul> <p>Communication with broker is described with general network attributes.</p> <p>Messaging system-specific attributes MUST be defined in the corresponding <code>messaging.{system}</code> namespace as described in Attributes specific to certain messaging systems.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#producer-attributes","title":"Producer attributes","text":"<p>The following additional attributes describe message producer operations.</p> Attribute Type Description Examples Requirement Level <code>messaging.destination.anonymous</code> boolean A boolean that is true if the message destination is anonymous (could be unnamed or have auto-generated name). Conditionally Required: [1] <code>messaging.destination.name</code> string The message destination name [2] <code>MyQueue</code>; <code>MyTopic</code> Conditionally Required: [3] <code>messaging.destination.template</code> string Low cardinality representation of the messaging destination name [4] <code>/customers/{customerId}</code> Conditionally Required: [5] <code>messaging.destination.temporary</code> boolean A boolean that is true if the message destination is temporary and might not exist anymore after messages are processed. Conditionally Required: [6] <p>[1]: If value is <code>true</code>. When missing, the value is assumed to be <code>false</code>.</p> <p>[2]: Destination name SHOULD uniquely identify a specific queue, topic or other entity within the broker. If the broker does not have such notion, the destination name SHOULD uniquely identify the broker.</p> <p>[3]: If one message is being published or if the value applies to all messages in the batch.</p> <p>[4]: Destination names could be constructed from templates. An example would be a destination name involving a user name or product id. Although the destination name in this case is of high cardinality, the underlying template is of low cardinality and can be effectively used for grouping and aggregation.</p> <p>[5]: If available. Instrumentations MUST NOT use <code>messaging.destination.name</code> as template unless low-cardinality of destination name is guaranteed.</p> <p>[6]: If value is <code>true</code>. When missing, the value is assumed to be <code>false</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#consumer-attributes","title":"Consumer attributes","text":"<p>The following additional attributes describe message consumer operations.</p> <p>Note: Consumer spans can have attributes describing source and destination. Since messages could be routed by brokers, source and destination don't always match. If original destination information is available on the consumer, consumer instrumentations SHOULD populate corresponding <code>messaging.destination</code> attributes.</p> Attribute Type Description Examples Requirement Level <code>messaging.destination.anonymous</code> boolean A boolean that is true if the message destination is anonymous (could be unnamed or have auto-generated name). Recommended: If known on consumer <code>messaging.destination.name</code> string The message destination name [1] <code>MyQueue</code>; <code>MyTopic</code> Recommended: If known on consumer <code>messaging.destination.temporary</code> boolean A boolean that is true if the message destination is temporary and might not exist anymore after messages are processed. Recommended: If known on consumer <code>messaging.source.anonymous</code> boolean A boolean that is true if the message source is anonymous (could be unnamed or have auto-generated name). Recommended: [2] <code>messaging.source.name</code> string The message source name [3] <code>MyQueue</code>; <code>MyTopic</code> Conditionally Required: [4] <code>messaging.source.template</code> string Low cardinality representation of the messaging source name [5] <code>/customers/{customerId}</code> Conditionally Required: [6] <code>messaging.source.temporary</code> boolean A boolean that is true if the message source is temporary and might not exist anymore after messages are processed. Recommended: [7] <p>[1]: Destination name SHOULD uniquely identify a specific queue, topic or other entity within the broker. If the broker does not have such notion, the destination name SHOULD uniquely identify the broker.</p> <p>[2]: When supported by messaging system and only if the source is anonymous. When missing, the value is assumed to be <code>false</code>.</p> <p>[3]: Source name SHOULD uniquely identify a specific queue, topic, or other entity within the broker. If the broker does not have such notion, the source name SHOULD uniquely identify the broker.</p> <p>[4]: If the value applies to all messages in the batch.</p> <p>[5]: Source names could be constructed from templates. An example would be a source name involving a user name or product id. Although the source name in this case is of high cardinality, the underlying template is of low cardinality and can be effectively used for grouping and aggregation.</p> <p>[6]: If available. Instrumentations MUST NOT use <code>messaging.source.name</code> as template unless low-cardinality of source name is guaranteed.</p> <p>[7]: When supported by messaging system and only if the source is temporary. When missing, the value is assumed to be <code>false</code>.</p> <p>The receive span is be used to track the time used for receiving the message(s), whereas the process span(s) track the time for processing the message(s). Note that one or multiple Spans with <code>messaging.operation</code> = <code>process</code> may often be the children of a Span with <code>messaging.operation</code> = <code>receive</code>. The distinction between receiving and processing of messages is not always of particular interest or sometimes hidden away in a framework (see the Message consumption section above) and therefore the attribute can be left out. For batch receiving and processing (see the Batch receiving and Batch processing examples below) in particular, the attribute SHOULD be set. Even though in that case one might think that the processing span's kind should be <code>INTERNAL</code>, that kind MUST NOT be used. Instead span kind should be set to either <code>CONSUMER</code> or <code>SERVER</code> according to the rules defined above.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#per-message-attributes","title":"Per-message attributes","text":"<p>All messaging operations (<code>publish</code>, <code>receive</code>, <code>process</code>, or others not covered by this specification) can describe both single and/or batch of messages. Attributes in the <code>messaging.message</code> or <code>messaging.{system}.message</code> namespace describe individual messages. For single-message operations they SHOULD be set on corresponding span.</p> <p>For batch operations, per-message attributes are usually different and cannot be set on the corresponding span. In such cases the attributes MAY be set on links. See Batch Receiving and Batch Processing for more information on correlation using links.</p> <p>Some messaging systems (e.g., Kafka, Azure EventGrid) allow publishing a single batch of messages to different topics. In such cases, the attributes in <code>messaging.destination</code> and <code>messaging.source</code> MAY be set on links. Instrumentations MAY set source and destination attributes on the span if all messages in the batch share the same destination or source.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#attributes-specific-to-certain-messaging-systems","title":"Attributes specific to certain messaging systems","text":"<p>All attributes that are specific for a messaging system SHOULD be populated in <code>messaging.{system}</code> namespace. Attributes that describe a message, a destination, a source, a consumer or a batch of messages SHOULD be populated under the corresponding namespace:</p> <ul> <li><code>messaging.{system}.message</code>: Describes attributes for individual messages</li> <li><code>messaging.{system}.destination</code> and <code>messaging.{system}.source</code>: Describe the   destination and source a message (or a batch) are published to and received   from respectively. The combination of attributes in these namespaces should   uniquely identify the entity and include properties significant for this   messaging system. For example, Kafka instrumentations should include partition   identifier.</li> <li><code>messaging.{system}.consumer</code>: Describes message consumer properties</li> <li><code>messaging.{system}.batch</code>: Describes message batch properties</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#rabbitmq","title":"RabbitMQ","text":"<p>In RabbitMQ, the destination is defined by an exchange and a routing key. <code>messaging.destination.name</code> MUST be set to the name of the exchange. This will be an empty string if the default exchange is used.</p> Attribute Type Description Examples Requirement Level <code>messaging.rabbitmq.destination.routing_key</code> string RabbitMQ message routing key. <code>myKey</code> Conditionally Required: If not empty."},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#apache-kafka","title":"Apache Kafka","text":"<p>For Apache Kafka, the following additional attributes are defined:</p> Attribute Type Description Examples Requirement Level <code>messaging.kafka.message.key</code> string Message keys in Kafka are used for grouping alike messages to ensure they're processed on the same partition. They differ from <code>messaging.message.id</code> in that they're not unique. If the key is <code>null</code>, the attribute MUST NOT be set. [1] <code>myKey</code> Recommended <code>messaging.kafka.consumer.group</code> string Name of the Kafka Consumer Group that is handling the message. Only applies to consumers, not producers. <code>my-group</code> Recommended <code>messaging.kafka.destination.partition</code> int Partition the message is sent to. <code>2</code> Recommended <code>messaging.kafka.source.partition</code> int Partition the message is received from. <code>2</code> Recommended <code>messaging.kafka.message.offset</code> int The offset of a record in the corresponding Kafka partition. <code>42</code> Recommended <code>messaging.kafka.message.tombstone</code> boolean A boolean that is true if the message is a tombstone. Conditionally Required: [2] <p>[1]: If the key type is not string, it's string representation has to be supplied for the attribute. If the key has no unambiguous, canonical string form, don't include its value.</p> <p>[2]: If value is <code>true</code>. When missing, the value is assumed to be <code>false</code>.</p> <p>For Apache Kafka producers, <code>peer.service</code> SHOULD be set to the name of the broker or service the message will be sent to. The <code>service.name</code> of a Consumer's Resource SHOULD match the <code>peer.service</code> of the Producer, when the message is directly passed to another service. If an intermediary broker is present, <code>service.name</code> and <code>peer.service</code> will not be the same.</p> <p><code>messaging.client_id</code> SHOULD be set to the <code>client-id</code> of consumers, or to the <code>client.id</code> property of producers.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#apache-rocketmq","title":"Apache RocketMQ","text":"<p>Specific attributes for Apache RocketMQ are defined below.</p> Attribute Type Description Examples Requirement Level <code>messaging.rocketmq.namespace</code> string Namespace of RocketMQ resources, resources in different namespaces are individual. <code>myNamespace</code> Required <code>messaging.rocketmq.client_group</code> string Name of the RocketMQ producer/consumer group that is handling the message. The client type is identified by the SpanKind. <code>myConsumerGroup</code> Required <code>messaging.rocketmq.message.delivery_timestamp</code> int The timestamp in milliseconds that the delay message is expected to be delivered to consumer. <code>1665987217045</code> Conditionally Required: [1] <code>messaging.rocketmq.message.delay_time_level</code> int The delay time level for delay message, which determines the message delay time. <code>3</code> Conditionally Required: [2] <code>messaging.rocketmq.message.group</code> string It is essential for FIFO message. Messages that belong to the same message group are always processed one by one within the same consumer group. <code>myMessageGroup</code> Conditionally Required: If the message type is FIFO. <code>messaging.rocketmq.message.type</code> string Type of message. <code>normal</code> Recommended <code>messaging.rocketmq.message.tag</code> string The secondary classifier of message besides topic. <code>tagA</code> Recommended <code>messaging.rocketmq.message.keys</code> string[] Key(s) of message, another way to mark message besides message id. <code>[keyA, keyB]</code> Recommended <code>messaging.rocketmq.consumption_model</code> string Model of message consumption. This only applies to consumer spans. <code>clustering</code> Recommended <p>[1]: If the message type is delay and delay time level is not specified.</p> <p>[2]: If the message type is delay and delivery timestamp is not specified.</p> <p><code>messaging.rocketmq.message.type</code> MUST be one of the following:</p> Value Description <code>normal</code> Normal message <code>fifo</code> FIFO message <code>delay</code> Delay message <code>transaction</code> Transaction message <p><code>messaging.rocketmq.consumption_model</code> MUST be one of the following:</p> Value Description <code>clustering</code> Clustering consumption model <code>broadcasting</code> Broadcasting consumption model <p><code>messaging.client_id</code> SHOULD be set to the client ID that is automatically generated by the Apache RocketMQ SDK.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#examples","title":"Examples","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#topic-with-multiple-consumers","title":"Topic with multiple consumers","text":"<p>Given is a process P, that publishes a message to a topic T on messaging system MS, and two processes CA and CB, which both receive the message and process it.</p> <pre><code>Process P:  | Span Prod1 |\n--\nProcess CA:              | Span CA1 |\n--\nProcess CB:                 | Span CB1 |\n</code></pre> Field or Attribute Span Prod1 Span CA1 Span CB1 Span name <code>\"T publish\"</code> <code>\"T process\"</code> <code>\"T process\"</code> Parent Span Prod1 Span Prod1 Links SpanKind <code>PRODUCER</code> <code>CONSUMER</code> <code>CONSUMER</code> Status <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>server.address</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>server.port</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>messaging.system</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>messaging.destination.name</code> <code>\"T\"</code> <code>messaging.source.name</code> <code>\"T\"</code> <code>\"T\"</code> <code>messaging.operation</code> <code>\"process\"</code> <code>\"process\"</code> <code>messaging.message.id</code> <code>\"a1\"</code> <code>\"a1\"</code> <code>\"a1\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#apache-kafka-with-quarkus-or-spring-boot-example","title":"Apache Kafka with Quarkus or Spring Boot Example","text":"<p>Given is a process P, that publishes a message to a topic T1 on Apache Kafka. One process, CA, receives the message and publishes a new message to a topic T2 that is then received and processed by CB.</p> <p>Frameworks such as Quarkus and Spring Boot separate processing of a received message from producing subsequent messages out. For this reason, receiving (Span Rcv1) is the parent of both processing (Span Proc1) and producing a new message (Span Prod2). The span representing message receiving (Span Rcv1) should not set <code>messaging.operation</code> to <code>receive</code>, as it does not only receive the message but also converts the input message to something suitable for the processing operation to consume and creates the output message from the result of processing.</p> <pre><code>Process P:  | Span Prod1 |\n--\nProcess CA:              | Span Rcv1 |\n                                | Span Proc1 |\n                                  | Span Prod2 |\n--\nProcess CB:                           | Span Rcv2 |\n</code></pre> Field or Attribute Span Prod1 Span Rcv1 Span Proc1 Span Prod2 Span Rcv2 Span name <code>\"T1 publish\"</code> <code>\"T1 receive\"</code> <code>\"T1 process\"</code> <code>\"T2 publish\"</code> <code>\"T2 receive</code>\" Parent Span Prod1 Span Rcv1 Span Rcv1 Span Prod2 Links SpanKind <code>PRODUCER</code> <code>CONSUMER</code> <code>CONSUMER</code> <code>PRODUCER</code> <code>CONSUMER</code> Status <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>peer.service</code> <code>\"myKafka\"</code> <code>\"myKafka\"</code> <code>service.name</code> <code>\"myConsumer1\"</code> <code>\"myConsumer1\"</code> <code>\"myConsumer2\"</code> <code>messaging.system</code> <code>\"kafka\"</code> <code>\"kafka\"</code> <code>\"kafka\"</code> <code>\"kafka\"</code> <code>\"kafka\"</code> <code>messaging.destination.name</code> <code>\"T1\"</code> <code>messaging.source.name</code> <code>\"T1\"</code> <code>\"T1\"</code> <code>\"T2\"</code> <code>\"T2\"</code> <code>messaging.operation</code> <code>\"process\"</code> <code>\"receive\"</code> <code>messaging.client_id</code> <code>\"5\"</code> <code>\"5\"</code> <code>\"5\"</code> <code>\"8\"</code> <code>messaging.kafka.message.key</code> <code>\"myKey\"</code> <code>\"myKey\"</code> <code>\"myKey\"</code> <code>\"anotherKey\"</code> <code>\"anotherKey\"</code> <code>messaging.kafka.consumer.group</code> <code>\"my-group\"</code> <code>\"my-group\"</code> <code>\"another-group\"</code> <code>messaging.kafka.partition</code> <code>\"1\"</code> <code>\"1\"</code> <code>\"1\"</code> <code>\"3\"</code> <code>\"3\"</code> <code>messaging.kafka.message.offset</code> <code>\"12\"</code> <code>\"12\"</code> <code>\"12\"</code> <code>\"32\"</code> <code>\"32\"</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#batch-receiving","title":"Batch receiving","text":"<p>Given is a process P, that publishes two messages to a queue Q on messaging system MS, and a process C, which receives both of them in one batch (Span Recv1) and processes each message separately (Spans Proc1 and Proc2).</p> <p>Since a span can only have one parent and the propagated trace and span IDs are not known when the receiving span is started, the receiving span will have no parent and the processing spans are correlated with the producing spans using links.</p> <pre><code>Process P: | Span Prod1 | Span Prod2 |\n--\nProcess C:                      | Span Recv1 |\n                                        | Span Proc1 |\n                                               | Span Proc2 |\n</code></pre> Field or Attribute Span Prod1 Span Prod2 Span Recv1 Span Proc1 Span Proc2 Span name <code>\"Q publish\"</code> <code>\"Q publish\"</code> <code>\"Q receive\"</code> <code>\"Q process\"</code> <code>\"Q process\"</code> Parent Span Recv1 Span Recv1 Links Span Prod1 Span Prod2 SpanKind <code>PRODUCER</code> <code>PRODUCER</code> <code>CONSUMER</code> <code>CONSUMER</code> <code>CONSUMER</code> Status <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>server.address</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>server.port</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>messaging.system</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>messaging.destination.name</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>messaging.source.name</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>messaging.operation</code> <code>\"receive\"</code> <code>\"process\"</code> <code>\"process\"</code> <code>messaging.message.id</code> <code>\"a1\"</code> <code>\"a2\"</code> <code>\"a1\"</code> <code>\"a2\"</code> <code>messaging.batch.message_count</code> 2"},{"location":"docs/specs/otel/trace/semantic_conventions/messaging/#batch-processing","title":"Batch processing","text":"<p>Given is a process P, that publishes two messages to a queue Q on messaging system MS, and a process C, which receives them separately in two different operations (Span Recv1 and Recv2) and processes both messages in one batch (Span Proc1).</p> <p>Since each span can only have one parent, C3 should not choose a random parent out of C1 and C2, but rather rely on the implicitly selected parent as defined by the tracing API spec. Depending on the implementation, the producing spans might still be available in the meta data of the messages and should be added to C3 as links. The client library or application could also add the receiver span's SpanContext to the data structure it returns for each message. In this case, C3 could also add links to the receiver spans C1 and C2.</p> <p>The status of the batch processing span is selected by the application. Depending on the semantics of the operation. A span status <code>Ok</code> could, for example, be set only if all messages or if just at least one were properly processed.</p> <pre><code>Process P: | Span Prod1 | Span Prod2 |\n--\nProcess C:                              | Span Recv1 | Span Recv2 |\n                                                                   | Span Proc1 |\n</code></pre> Field or Attribute Span Prod1 Span Prod2 Span Recv1 Span Recv2 Span Proc1 Span name <code>\"Q publish\"</code> <code>\"Q publish\"</code> <code>\"Q receive\"</code> <code>\"Q receive\"</code> <code>\"Q process\"</code> Parent Span Prod1 Span Prod2 Links [Span Prod1, Span Prod2 ] Link attributes Span Prod1: <code>messaging.message.id</code>: <code>\"a1\"</code> Span Prod2: <code>messaging.message.id</code>: <code>\"a2\"</code> SpanKind <code>PRODUCER</code> <code>PRODUCER</code> <code>CONSUMER</code> <code>CONSUMER</code> <code>CONSUMER</code> Status <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>server.address</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>\"ms\"</code> <code>server.port</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>1234</code> <code>messaging.system</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>\"rabbitmq\"</code> <code>messaging.destination.name</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>messaging.source.name</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>\"Q\"</code> <code>messaging.operation</code> <code>\"receive\"</code> <code>\"receive\"</code> <code>\"process\"</code> <code>messaging.message.id</code> <code>\"a1\"</code> <code>\"a2\"</code> <code>\"a1\"</code> <code>\"a2\"</code> <code>messaging.batch.message_count</code> 1 1 2"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/","title":"RPC \u8de8\u8d8a\u7684\u8bed\u4e49\u7ea6\u5b9a","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines how to describe remote procedure calls (also called \"remote method invocations\" / \"RMI\") with spans.</p> <ul> <li>Common remote procedure call conventions</li> <li>Span name</li> <li>Common attributes<ul> <li>Service name</li> </ul> </li> <li>Client attributes</li> <li>Server attributes</li> <li>Events</li> <li>Distinction from HTTP spans</li> <li>gRPC</li> <li>gRPC Attributes</li> <li>gRPC Status</li> <li>gRPC Request and Response Metadata</li> <li>Connect RPC conventions</li> <li>Connect RPC Attributes</li> <li>Connect RPC Status</li> <li>Connect RPC Request and Response Metadata</li> <li>JSON RPC</li> <li>JSON RPC Attributes</li> </ul> <p>Warning Existing RPC instrumentations that are using v1.20.0 of this document (or prior):</p> <ul> <li>SHOULD NOT change the version of the networking attributes that they emit   until the HTTP semantic conventions are marked stable (HTTP stabilization   will include stabilization of a core set of networking attributes which are   also used in RPC instrumentations).</li> <li>SHOULD introduce an environment variable <code>OTEL_SEMCONV_STABILITY_OPT_IN</code> in   the existing major version which supports the following values:</li> <li><code>none</code> - continue emitting whatever version of the old experimental     networking attributes the instrumentation was emitting previously. This is     the default value.</li> <li><code>http</code> - emit the new, stable networking attributes, and stop emitting the     old experimental networking attributes that the instrumentation emitted     previously.</li> <li><code>http/dup</code> - emit both the old and the stable networking attributes,     allowing for a seamless transition.</li> <li>SHOULD maintain (security patching at a minimum) the existing major version   for at least six months after it starts emitting both sets of attributes.</li> <li>SHOULD drop the environment variable in the next major version (stable next   major version SHOULD NOT be released prior to October 1, 2023).</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#common-remote-procedure-call-conventions","title":"Common remote procedure call conventions","text":"<p>A remote procedure calls is described by two separate spans, one on the client-side and one on the server-side.</p> <p>For outgoing requests, the <code>SpanKind</code> MUST be set to <code>CLIENT</code> and for incoming requests to <code>SERVER</code>.</p> <p>Remote procedure calls can only be represented with these semantic conventions, when the names of the called service and method are known and available.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#span-name","title":"Span name","text":"<p>The span name MUST be the full RPC method name formatted as:</p> <pre><code>$package.$service/$method\n</code></pre> <p>(where $service MUST NOT contain dots and $method MUST NOT contain slashes)</p> <p>If there is no package name or if it is unknown, the <code>$package.</code> part (including the period) is omitted.</p> <p>Examples of span names:</p> <ul> <li><code>grpc.test.EchoService/Echo</code></li> <li><code>com.example.ExampleRmiService/exampleMethod</code></li> <li><code>MyCalcService.Calculator/Add</code> reported by the server and   <code>MyServiceReference.ICalculator/Add</code> reported by the client for .NET WCF calls</li> <li><code>MyServiceWithNoPackage/theMethod</code></li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#common-attributes","title":"Common attributes","text":"Attribute Type Description Examples Requirement Level <code>rpc.system</code> string A string identifying the remoting system. See below for a list of well-known identifiers. <code>grpc</code> Required <code>rpc.service</code> string The full (logical) name of the service being called, including its package name, if applicable. [1] <code>myservice.EchoService</code> Recommended <code>rpc.method</code> string The name of the (logical) method being called, must be equal to the $method part in the span name. [2] <code>exampleMethod</code> Recommended <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>server.address</code> string RPC server host name. [3] <code>example.com</code> Required <code>server.port</code> int Logical server port number <code>80</code>; <code>8080</code>; <code>443</code> Conditionally Required: See below <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> See below <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: [4] <p>[1]: This is the logical name of the service from the RPC interface perspective, which can be different from the name of any implementing class. The <code>code.namespace</code> attribute may be used to store the latter (despite the attribute name, it may include a class name; e.g., class with method actually executing the call on the server side, RPC client stub class on the client side).</p> <p>[2]: This is the logical name of the method from the RPC interface perspective, which can be different from the name of any implementing method/function. The <code>code.function</code> attribute may be used to store the latter (e.g., method actually executing the call on the server side, RPC client stub method on the client side).</p> <p>[3]: May contain server IP address, DNS name, or local socket name. When host component is an IP address, instrumentations SHOULD NOT do a reverse proxy lookup to obtain DNS name and SHOULD set <code>server.address</code> to the IP address provided in the host component.</p> <p>[4]: If different than <code>server.port</code> and if <code>server.socket.address</code> is set.</p> <p>Additional attribute requirements: At least one of the following sets of attributes is required:</p> <ul> <li><code>server.socket.address</code></li> <li><code>server.address</code></li> </ul> <p><code>rpc.system</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>grpc</code> gRPC <code>java_rmi</code> Java RMI <code>dotnet_wcf</code> .NET WCF <code>apache_dubbo</code> Apache Dubbo <code>connect_rpc</code> Connect RPC <p>For client-side spans <code>server.port</code> is required if the connection is IP-based and the port is available (it describes the server port they are connecting to). For server-side spans <code>client.socket.port</code> is optional (it describes the port the client is connecting from).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#service-name","title":"Service name","text":"<p>On the server process receiving and handling the remote procedure call, the service name provided in <code>rpc.service</code> does not necessarily have to match the [<code>service.name</code>][] resource attribute. One process can expose multiple RPC endpoints and thus have multiple RPC service names. From a deployment perspective, as expressed by the <code>service.*</code> resource attributes, it will be treated as one deployed service with one <code>service.name</code>. Likewise, on clients sending RPC requests to a server, the service name provided in <code>rpc.service</code> does not have to match the [<code>peer.service</code>][] span attribute.</p> <p>As an example, given a process deployed as <code>QuoteService</code>, this would be the name that goes into the <code>service.name</code> resource attribute which applies to the entire process. This process could expose two RPC endpoints, one called <code>CurrencyQuotes</code> (= <code>rpc.service</code>) with a method called <code>getMeanRate</code> (= <code>rpc.method</code>) and the other endpoint called <code>StockQuotes</code> (= <code>rpc.service</code>) with two methods <code>getCurrentBid</code> and <code>getLastClose</code> (= <code>rpc.method</code>). In this example, spans representing client request should have their <code>peer.service</code> attribute set to <code>QuoteService</code> as well to match the server's <code>service.name</code> resource attribute. Generally, a user SHOULD NOT set <code>peer.service</code> to a fully qualified RPC service name.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#client-attributes","title":"Client attributes","text":"Attribute Type Description Examples Requirement Level <code>server.socket.domain</code> string The domain name of an immediate peer. [1] <code>proxy.example.com</code> Recommended: [2] <p>[1]: Typically observed from the client side, and represents a proxy or other intermediary domain name.</p> <p>[2]: If different than <code>server.address</code> and if <code>server.socket.address</code> is set.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#server-attributes","title":"Server attributes","text":"Attribute Type Description Examples Requirement Level <code>client.address</code> string Client address - unix domain socket name, IPv4 or IPv6 address. [1] <code>/tmp/my.sock</code>; <code>10.1.2.80</code> Recommended <code>client.port</code> int Client port number [2] <code>65123</code> Recommended <code>client.socket.address</code> string Immediate client peer address - unix domain socket name, IPv4 or IPv6 address. <code>/tmp/my.sock</code>; <code>127.0.0.1</code> Recommended: If different than <code>client.address</code>. <code>client.socket.port</code> int Immediate client peer port number <code>35555</code> Recommended: If different than <code>client.port</code>. <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <p>[1]: When observed from the server side, and when communicating through an intermediary, <code>client.address</code> SHOULD represent client address behind any intermediaries (e.g. proxies) if it's available.</p> <p>[2]: When observed from the server side, and when communicating through an intermediary, <code>client.port</code> SHOULD represent client port behind any intermediaries (e.g. proxies) if it's available.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#events","title":"Events","text":"<p>In the lifetime of an RPC stream, an event for each message sent/received on client and server spans SHOULD be created. In case of unary calls only one sent and one received message will be recorded for both client and server spans.</p> <p>The event name MUST be <code>message</code>.</p> Attribute Type Description Examples Requirement Level <code>message.type</code> string Whether this is a received or sent message. <code>SENT</code> Recommended <code>message.id</code> int MUST be calculated as two different counters starting from <code>1</code> one for sent messages and one for received message. [1] Recommended <code>message.compressed_size</code> int Compressed size of the message in bytes. Recommended <code>message.uncompressed_size</code> int Uncompressed size of the message in bytes. Recommended <p>[1]: This way we guarantee that the values will be consistent between different implementations.</p> <p><code>message.type</code> MUST be one of the following:</p> Value Description <code>SENT</code> sent <code>RECEIVED</code> received"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#distinction-from-http-spans","title":"Distinction from HTTP spans","text":"<p>HTTP calls can generally be represented using just HTTP spans. If they address a particular remote service and method known to the caller, i.e., when it is a remote procedure call transported over HTTP, the <code>rpc.*</code> attributes might be added additionally on that span, or in a separate RPC span that is a parent of the transporting HTTP call. Note that method in this context is about the called remote procedure and not the HTTP verb (GET, POST, etc.).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#grpc","title":"gRPC","text":"<p>For remote procedure calls via gRPC, additional conventions are described in this section.</p> <p><code>rpc.system</code> MUST be set to <code>\"grpc\"</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#grpc-attributes","title":"gRPC Attributes","text":"Attribute Type Description Examples Requirement Level <code>rpc.grpc.status_code</code> int The numeric status code of the gRPC request. <code>0</code> Required <p><code>rpc.grpc.status_code</code> MUST be one of the following:</p> Value Description <code>0</code> OK <code>1</code> CANCELLED <code>2</code> UNKNOWN <code>3</code> INVALID_ARGUMENT <code>4</code> DEADLINE_EXCEEDED <code>5</code> NOT_FOUND <code>6</code> ALREADY_EXISTS <code>7</code> PERMISSION_DENIED <code>8</code> RESOURCE_EXHAUSTED <code>9</code> FAILED_PRECONDITION <code>10</code> ABORTED <code>11</code> OUT_OF_RANGE <code>12</code> UNIMPLEMENTED <code>13</code> INTERNAL <code>14</code> UNAVAILABLE <code>15</code> DATA_LOSS <code>16</code> UNAUTHENTICATED"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#grpc-status","title":"gRPC Status","text":"<p>The table below describes when the Span Status MUST be set to <code>Error</code> or remain unset depending on the gRPC status code and Span Kind.</p> gRPC Status Code <code>SpanKind.SERVER</code> Span Status <code>SpanKind.CLIENT</code> Span Status OK unset unset CANCELLED unset <code>Error</code> UNKNOWN <code>Error</code> <code>Error</code> INVALID_ARGUMENT unset <code>Error</code> DEADLINE_EXCEEDED <code>Error</code> <code>Error</code> NOT_FOUND unset <code>Error</code> ALREADY_EXISTS unset <code>Error</code> PERMISSION_DENIED unset <code>Error</code> RESOURCE_EXHAUSTED unset <code>Error</code> FAILED_PRECONDITION unset <code>Error</code> ABORTED unset <code>Error</code> OUT_OF_RANGE unset <code>Error</code> UNIMPLEMENTED <code>Error</code> <code>Error</code> INTERNAL <code>Error</code> <code>Error</code> UNAVAILABLE <code>Error</code> <code>Error</code> DATA_LOSS <code>Error</code> <code>Error</code> UNAUTHENTICATED unset <code>Error</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#grpc-request-and-response-metadata","title":"gRPC Request and Response Metadata","text":"Attribute Type Description Examples Requirement Level <code>rpc.grpc.request.metadata.&lt;key&gt;</code> string[] gRPC request metadata, <code>&lt;key&gt;</code> being the normalized gRPC Metadata key (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the metadata values. [1] <code>rpc.grpc.request.metadata.my_custom_metadata_attribute=[\"1.2.3.4\", \"1.2.3.5\"]</code> Opt-In <code>rpc.grpc.response.metadata.&lt;key&gt;</code> string[] gRPC response metadata, <code>&lt;key&gt;</code> being the normalized gRPC Metadata key (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the metadata values. [1] <code>rpc.grpc.response.metadata.my_custom_metadata_attribute=[\"attribute_value\"]</code> Opt-In <p>[1]: Instrumentations SHOULD require an explicit configuration of which metadata values are to be captured. Including all request/response metadata values can be a security risk - explicit configuration helps avoid leaking sensitive information.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#connect-rpc-conventions","title":"Connect RPC conventions","text":"<p>For remote procedure calls via connect, additional conventions are described in this section.</p> <p><code>rpc.system</code> MUST be set to <code>\"connect_rpc\"</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#connect-rpc-attributes","title":"Connect RPC Attributes","text":"<p>Below is a table of attributes that SHOULD be included on client and server RPC measurements when <code>rpc.system</code> is <code>\"connect_rpc\"</code>.</p> Attribute Type Description Examples Requirement Level <code>rpc.connect_rpc.error_code</code> string The error codes of the Connect request. Error codes are always string values. <code>cancelled</code> Conditionally Required: [1] <p>[1]: If response is not successful and if error code available.</p> <p><code>rpc.connect_rpc.error_code</code> MUST be one of the following:</p> Value Description <code>cancelled</code> cancelled <code>unknown</code> unknown <code>invalid_argument</code> invalid_argument <code>deadline_exceeded</code> deadline_exceeded <code>not_found</code> not_found <code>already_exists</code> already_exists <code>permission_denied</code> permission_denied <code>resource_exhausted</code> resource_exhausted <code>failed_precondition</code> failed_precondition <code>aborted</code> aborted <code>out_of_range</code> out_of_range <code>unimplemented</code> unimplemented <code>internal</code> internal <code>unavailable</code> unavailable <code>data_loss</code> data_loss <code>unauthenticated</code> unauthenticated"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#connect-rpc-status","title":"Connect RPC Status","text":"<p>If <code>rpc.connect_rpc.error_code</code> is set, Span Status MUST be set to <code>Error</code> and left unset in all other cases.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#connect-rpc-request-and-response-metadata","title":"Connect RPC Request and Response Metadata","text":"Attribute Type Description Examples Requirement Level <code>rpc.connect_rpc.request.metadata.&lt;key&gt;</code> string[] Connect request metadata, <code>&lt;key&gt;</code> being the normalized Connect Metadata key (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the metadata values. [1] <code>rpc.request.metadata.my_custom_metadata_attribute=[\"1.2.3.4\", \"1.2.3.5\"]</code> Optional <code>rpc.connect_rpc.response.metadata.&lt;key&gt;</code> string[] Connect response metadata, <code>&lt;key&gt;</code> being the normalized Connect Metadata key (lowercase, with <code>-</code> characters replaced by <code>_</code>), the value being the metadata values. [1] <code>rpc.response.metadata.my_custom_metadata_attribute=[\"attribute_value\"]</code> Optional <p>[1]: Instrumentations SHOULD require an explicit configuration of which metadata values are to be captured. Including all request/response metadata values can be a security risk - explicit configuration helps avoid leaking sensitive information.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#json-rpc","title":"JSON RPC","text":"<p>Conventions specific to JSON RPC.</p> <p><code>rpc.system</code> MUST be set to <code>\"jsonrpc\"</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/rpc/#json-rpc-attributes","title":"JSON RPC Attributes","text":"Attribute Type Description Examples Requirement Level <code>rpc.jsonrpc.version</code> string Protocol version as in <code>jsonrpc</code> property of request/response. Since JSON-RPC 1.0 does not specify this, the value can be omitted. <code>2.0</code>; <code>1.0</code> Conditionally Required: If other than the default version (<code>1.0</code>) <code>rpc.jsonrpc.request_id</code> string <code>id</code> property of request or response. Since protocol allows id to be int, string, <code>null</code> or missing (for notifications), value is expected to be cast to string for simplicity. Use empty string in case of <code>null</code> value. Omit entirely if this is a notification. <code>10</code>; <code>request-7</code>; `` Recommended <code>rpc.jsonrpc.error_code</code> int <code>error.code</code> property of response if it is an error response. <code>-32700</code>; <code>100</code> Conditionally Required: If response is not successful. <code>rpc.jsonrpc.error_message</code> string <code>error.message</code> property of response if it is an error response. <code>Parse error</code>; <code>User already exists</code> Recommended <code>rpc.method</code> string The name of the (logical) method being called, must be equal to the $method part in the span name. [1] <code>exampleMethod</code> Required <p>[1]: This is always required for jsonrpc. See the note in the general RPC conventions for more information.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/","title":"\u901a\u7528\u5c5e\u6027","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>The attributes described in this section are not specific to a particular operation but rather generic. They may be used in any Span they apply to. Particular operations may refer to or require some of these attributes.</p> <ul> <li>\u901a\u7528\u5c5e\u6027</li> <li>Server and client attributes<ul> <li>Server attributes</li> <li><code>server.address</code></li> <li><code>server.socket.*</code> attributes</li> <li>Client attributes</li> <li>Connecting through intermediary</li> </ul> </li> <li>Network attributes<ul> <li>Source and destination attributes</li> <li>Source</li> <li>Destination</li> <li>Network connection and carrier attributes</li> </ul> </li> <li>General remote service attributes</li> <li>General identity attributes</li> <li>General thread attributes</li> <li>Source Code Attributes</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#server-and-client-attributes","title":"Server and client attributes","text":"<p>These attributes may be used to describe the client and server in a connection-based network interaction where there is one side that initiates the connection (the client is the side that initiates the connection). This covers all TCP network interactions since TCP is connection-based and one side initiates the connection (an exception is made for peer-to-peer communication over TCP where the \"user-facing\" surface of the protocol / API does not expose a clear notion of client and server). This also covers UDP network interactions where one side initiates the interaction, e.g. QUIC (HTTP/3) and DNS.</p> <p>In an ideal situation, not accounting for proxies, multiple IP addresses or host names, the <code>server.*</code> attributes are the same on the client and server.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#server-attributes","title":"Server attributes","text":"<p>Warning Attributes in this section are in use by the HTTP semantic conventions. Once the HTTP semantic conventions are declared stable, changes to the attributes in this section will only be allowed if they do not cause breaking changes to HTTP semantic conventions.</p> Attribute Type Description Examples Requirement Level <code>server.address</code> string Logical server hostname, matches server FQDN if available, and IP or socket address if FQDN is not known. <code>example.com</code> Recommended <code>server.port</code> int Logical server port number <code>80</code>; <code>8080</code>; <code>443</code> Recommended <code>server.socket.domain</code> string The domain name of an immediate peer. [1] <code>proxy.example.com</code> Recommended <code>server.socket.address</code> string Physical server IP address or Unix socket address. <code>10.5.3.2</code> Recommended: If different than <code>server.address</code>. <code>server.socket.port</code> int Physical server port. <code>16456</code> Recommended: If different than <code>server.port</code>. <p>[1]: Typically observed from the client side, and represents a proxy or other intermediary domain name.</p> <p><code>server.address</code> and <code>server.port</code> represent logical server name and port. Semantic conventions that refer to these attributes SHOULD specify what these attributes mean in their context.</p> <p>Semantic conventions and instrumentations that populate both logical (<code>server.address</code> and <code>server.port</code>) and socket-level (<code>server.socket.*</code>) attributes SHOULD set socket-level attributes only when they don't match logical ones. For example, when direct connection to the remote destination is established and <code>server.address</code> is populated, <code>server.socket.domain</code> SHOULD NOT be set. Check out Connecting through intermediary for more information.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#serveraddress","title":"<code>server.address</code>","text":"<p>For IP-based communication, the name should be a DNS host name of the service. On client side it matches remote service name, on server side, it represents local service name as seen externally on clients.</p> <p>When connecting to an URL <code>https://example.com/foo</code>, <code>server.address</code> matches <code>\"example.com\"</code> on both client and server side.</p> <p>On client side, it's usually passed in form of URL, connection string, host name, etc. Sometimes host name is only available to instrumentation as a string which may contain DNS name or IP address. <code>server.address</code> SHOULD be set to the available known hostname (e.g., <code>\"127.0.0.1\"</code> if connecting to an URL <code>https://127.0.0.1/foo</code>).</p> <p>If only IP address is available, it should be populated on <code>server.address</code>. Reverse DNS lookup SHOULD NOT be used to obtain DNS name.</p> <p>If <code>network.transport</code> is <code>\"pipe\"</code>, the absolute path to the file representing it should be used as <code>server.address</code>. If there is no such file (e.g., anonymous pipe), the name should explicitly be set to the empty string to distinguish it from the case where the name is just unknown or not covered by the instrumentation.</p> <p>For Unix domain socket, <code>server.address</code> attribute represents remote endpoint address on the client side and local endpoint address on the server side.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#serversocket-attributes","title":"<code>server.socket.*</code> attributes","text":"<p>Note: this section applies to socket connections visible to instrumentations. Instrumentations have limited knowledge about intermediaries communications goes through such as transparent proxies or VPN servers. Higher-level instrumentations such as HTTP don't always have access to the socket-level information and may not be able to populate socket-level attributes.</p> <p>Socket-level attributes identify peer and host that are directly connected to each other. Since instrumentations may have limited knowledge on network information, instrumentations SHOULD populate such attributes to the best of their knowledge when populate them at all.</p> <p>Note: Specific structures and methods to obtain socket-level attributes are mentioned here only as examples. Instrumentations would usually use Socket API provided by their environment or sockets implementations.</p> <p>For IP-based communication, <code>server.socket.domain</code> represents either fully qualified domain name of immediate peer and <code>server.socket.address</code> to the IP address (or one specific to network family).</p> <p><code>server.socket.domain</code>, <code>server.socket.address</code>, and <code>server.socket.port</code> describe server side of socket communication. For example, when connecting using <code>connect(2)</code> on Linux or Windows with <code>AF_INET</code> address family, they represent <code>sin_addr</code> and <code>sin_port</code> fields of <code>sockaddr_in</code> structure.</p> <p>On client side, address and port can be obtained by calling <code>getpeername</code> method on Linux or Windows.</p> <p>On server side address and port can be obtained by calling <code>getsockname</code> method on Linux or Windows.</p> <p><code>server.socket.port</code> SHOULD only be populated for families that have notion of port.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#client-attributes","title":"Client attributes","text":"<p>Warning Attributes in this section are in use by the HTTP semantic conventions. Once the HTTP semantic conventions are declared stable, changes to the attributes in this section will only be allowed if they do not cause breaking changes to HTTP semantic conventions.</p> Attribute Type Description Examples Requirement Level <code>client.address</code> string Client address - unix domain socket name, IPv4 or IPv6 address. [1] <code>/tmp/my.sock</code>; <code>10.1.2.80</code> Recommended <code>client.port</code> int Client port number [2] <code>65123</code> Recommended <code>client.socket.address</code> string Immediate client peer address - unix domain socket name, IPv4 or IPv6 address. <code>/tmp/my.sock</code>; <code>127.0.0.1</code> Recommended: If different than <code>client.address</code>. <code>client.socket.port</code> int Immediate client peer port number <code>35555</code> Recommended: If different than <code>client.port</code>. <p>[1]: When observed from the server side, and when communicating through an intermediary, <code>client.address</code> SHOULD represent client address behind any intermediaries (e.g. proxies) if it's available.</p> <p>[2]: When observed from the server side, and when communicating through an intermediary, <code>client.port</code> SHOULD represent client port behind any intermediaries (e.g. proxies) if it's available.</p> <p><code>client.socket.address</code> and <code>client.socket.port</code> represent physical client name and port.</p> <p>For IP-based communication, the <code>client.socket.address</code> should be a IP address, Unix domain name, or another address specific to network type.</p> <p>On server side, <code>client.socket.address</code> identifies the direct peer endpoint socket address. For example, when using <code>bind(2)</code> on Linux or Windows with <code>AF_INET</code> address family, represent <code>sin_addr</code> and <code>sin_port</code> fields of <code>sockaddr_in</code> structure.</p> <p>On client side it represents local socket address and port can be obtained by calling <code>getsockname</code> method on Linux, Windows.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#connecting-through-intermediary","title":"Connecting through intermediary","text":"<p>When connecting to the remote destination through an intermediary (e.g. proxy), client instrumentations SHOULD set <code>server.address</code> and <code>server.port</code> to logical remote destination address and <code>server.socket.name</code>, <code>server.socket.address</code> and <code>server.socket.port</code> to the socket peer connection is established with - the intermediary.</p> <p><code>server.socket.domain</code> SHOULD be set to the DNS name used to resolve <code>server.socket.address</code> if it's readily available. Instrumentations SHOULD NOT do DNS lookups to obtain <code>server.socket.address</code>. If peer information available to instrumentation can represent DNS name or IP address, instrumentation SHOULD NOT attempt to parse it and SHOULD only set <code>server.socket.domain</code>.</p> <p>Note: Telemetry consumers can obtain IP address from telemetry item by first checking <code>server.socket.address</code> and if not present, falling back to <code>server.socket.domain</code>.</p> <p>For example, URL Host component can contain IP address or DNS name and instrumentations that don't have access to socket-level communication can only populate <code>server.socket.domain</code>. Instrumentations that have access to socket connection, may be able to populate valid <code>server.socket.address</code> instead of or in addition to DNS name.</p> <p>Server instrumentations that leverage <code>client.address</code> and <code>client.port</code> attributes SHOULD set them to originating client address and port behind all proxies if this information is available. The <code>client.socket.address</code> and <code>client.socket.port</code> attributes then SHOULD contain immediate client peer address and port.</p> <p>If only immediate peer information is available, it should be set on <code>client.address</code> and <code>client.port</code> and <code>client.socket.*</code> attributes SHOULD NOT be set.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#network-attributes","title":"Network attributes","text":"<p>Warning Attributes in this section are in use by the HTTP semantic conventions. Once the HTTP semantic conventions are declared stable, changes to the attributes in this section will only be allowed if they do not cause breaking changes to HTTP semantic conventions.</p> Attribute Type Description Examples Requirement Level <code>network.transport</code> string OSI Transport Layer or Inter-process Communication method. The value SHOULD be normalized to lowercase. <code>tcp</code>; <code>udp</code> Recommended <code>network.type</code> string OSI Network Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>ipv4</code>; <code>ipv6</code> Recommended <code>network.protocol.name</code> string OSI Application Layer or non-OSI equivalent. The value SHOULD be normalized to lowercase. <code>amqp</code>; <code>http</code>; <code>mqtt</code> Recommended <code>network.protocol.version</code> string Version of the application layer protocol used. See note below. [1] <code>3.1.1</code> Recommended <p>[1]: <code>network.protocol.version</code> refers to the version of the protocol used and might be different from the protocol client's version. If the HTTP client used has a version of <code>0.27.2</code>, but sends HTTP version <code>1.1</code>, this attribute should be set to <code>1.1</code>.</p> <p><code>network.transport</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>tcp</code> TCP <code>udp</code> UDP <code>pipe</code> Named or anonymous pipe. See note below. <code>unix</code> Unix domain socket <p><code>network.type</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>ipv4</code> IPv4 <code>ipv6</code> IPv6"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#source-and-destination-attributes","title":"Source and destination attributes","text":"<p>These attributes may be used to describe the sender and receiver of a network exchange/packet. These should be used when there is no client/server relationship between the two sides, or when that relationship is unknown. This covers low-level network interactions (e.g. packet tracing) where you don't know if there was a connection or which side initiated it. This also covers unidirectional UDP flows and peer-to-peer communication where the \"user-facing\" surface of the protocol / API does not expose a clear notion of client and server.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#source","title":"Source","text":"Attribute Type Description Examples Requirement Level <code>source.domain</code> string The domain name of the source system. [1] <code>foo.example.com</code> Recommended <code>source.address</code> string Source address, for example IP address or Unix socket name. <code>10.5.3.2</code> Recommended <code>source.port</code> int Source port number <code>3389</code>; <code>2888</code> Recommended <p>[1]: This value may be a host name, a fully qualified domain name, or another host naming format.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#destination","title":"Destination","text":"<p>Destination fields capture details about the receiver of a network exchange/packet.</p> Attribute Type Description Examples Requirement Level <code>destination.domain</code> string The domain name of the destination system. [1] <code>foo.example.com</code> Recommended <code>destination.address</code> string Peer address, for example IP address or UNIX socket name. <code>10.5.3.2</code> Recommended <code>destination.port</code> int Peer port number <code>3389</code>; <code>2888</code> Recommended <p>[1]: This value may be a host name, a fully qualified domain name, or another host naming format.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#network-connection-and-carrier-attributes","title":"Network connection and carrier attributes","text":"Attribute Type Description Examples Requirement Level <code>network.connection.type</code> string The internet connection type. <code>wifi</code> Recommended <code>network.connection.subtype</code> string This describes more details regarding the connection.type. It may be the type of cell technology connection, but it could be used for describing details about a wifi connection. <code>LTE</code> Recommended <code>network.carrier.name</code> string The name of the mobile carrier. <code>sprint</code> Recommended <code>network.carrier.mcc</code> string The mobile carrier country code. <code>310</code> Recommended <code>network.carrier.mnc</code> string The mobile carrier network code. <code>001</code> Recommended <code>network.carrier.icc</code> string The ISO 3166-1 alpha-2 2-character country code associated with the mobile carrier network. <code>DE</code> Recommended <p><code>network.connection.type</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>wifi</code> wifi <code>wired</code> wired <code>cell</code> cell <code>unavailable</code> unavailable <code>unknown</code> unknown <p><code>network.connection.subtype</code> has the following list of well-known values. If one of them applies, then the respective value MUST be used, otherwise a custom value MAY be used.</p> Value Description <code>gprs</code> GPRS <code>edge</code> EDGE <code>umts</code> UMTS <code>cdma</code> CDMA <code>evdo_0</code> EVDO Rel. 0 <code>evdo_a</code> EVDO Rev. A <code>cdma2000_1xrtt</code> CDMA2000 1XRTT <code>hsdpa</code> HSDPA <code>hsupa</code> HSUPA <code>hspa</code> HSPA <code>iden</code> IDEN <code>evdo_b</code> EVDO Rev. B <code>lte</code> LTE <code>ehrpd</code> EHRPD <code>hspap</code> HSPAP <code>gsm</code> GSM <code>td_scdma</code> TD-SCDMA <code>iwlan</code> IWLAN <code>nr</code> 5G NR (New Radio) <code>nrnsa</code> 5G NRNSA (New Radio Non-Standalone) <code>lte_ca</code> LTE CA <p>For <code>Unix</code> and <code>pipe</code>, since the connection goes over the file system instead of being directly to a known peer, <code>server.address</code> is the only attribute that usually makes sense (see description of <code>server.address</code> below).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#general-remote-service-attributes","title":"General remote service attributes","text":"<p>This attribute may be used for any operation that accesses some remote service. Users can define what the name of a service is based on their particular semantics in their distributed system. Instrumentations SHOULD provide a way for users to configure this name.</p> Attribute Type Description Examples Requirement Level <code>peer.service</code> string The <code>service.name</code> of the remote service. SHOULD be equal to the actual <code>service.name</code> resource attribute of the remote service if any. <code>AuthTokenCache</code> Recommended <p>Examples of <code>peer.service</code> that users may specify:</p> <ul> <li>A Redis cache of auth tokens as <code>peer.service=\"AuthTokenCache\"</code>.</li> <li>A gRPC service <code>rpc.service=\"io.opentelemetry.AuthService\"</code> may be hosted in   both a gateway, <code>peer.service=\"ExternalApiService\"</code> and a backend,   <code>peer.service=\"AuthService\"</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#general-identity-attributes","title":"General identity attributes","text":"<p>These attributes may be used for any operation with an authenticated and/or authorized enduser.</p> Attribute Type Description Examples Requirement Level <code>enduser.id</code> string Username or client_id extracted from the access token or Authorization header in the inbound request from outside the system. <code>username</code> Recommended <code>enduser.role</code> string Actual/assumed role the client is making the request under extracted from token or application security context. <code>admin</code> Recommended <code>enduser.scope</code> string Scopes or granted authorities the client currently possesses extracted from token or application security context. The value would come from the scope associated with an OAuth 2.0 Access Token or an attribute value in a SAML 2.0 Assertion. <code>read:message, write:files</code> Recommended <p>These attributes describe the authenticated user driving the user agent making requests to the instrumented system. It is expected this information would be propagated unchanged from node-to-node within the system using the Baggage mechanism. These attributes should not be used to record system-to-system authentication attributes.</p> <p>Examples of where the <code>enduser.id</code> value is extracted from:</p> Authentication protocol Field or description HTTP Basic/Digest Authentication <code>username</code> OAuth 2.0 Bearer Token OAuth 2.0 Client Identifier value from <code>client_id</code> for the OAuth 2.0 Client Credentials Grant flow and <code>subject</code> or <code>username</code> from get token info response for other flows using opaque tokens. OpenID Connect 1.0 IDToken <code>sub</code> SAML 2.0 Assertion <code>urn:oasis:names:tc:SAML:2.0:assertion:Subject</code> Kerberos <code>PrincipalName</code> Framework Field or description JavaEE/JakartaEE Servlet <code>javax.servlet.http.HttpServletRequest.getUserPrincipal()</code> Windows Communication Foundation <code>ServiceSecurityContext.Current.PrimaryIdentity</code> <p>Given the sensitive nature of this information, SDKs and exporters SHOULD drop these attributes by default and then provide a configuration parameter to turn on retention for use cases where the information is required and would not violate any policies or regulations.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#general-thread-attributes","title":"General thread attributes","text":"<p>These attributes may be used for any operation to store information about a thread that started a span.</p> Attribute Type Description Examples Requirement Level <code>thread.id</code> int Current \"managed\" thread ID (as opposed to OS thread ID). <code>42</code> Recommended <code>thread.name</code> string Current thread name. <code>main</code> Recommended <p>Examples of where <code>thread.id</code> and <code>thread.name</code> can be extracted from:</p> Language or platform <code>thread.id</code> <code>thread.name</code> JVM <code>Thread.currentThread().getId()</code> <code>Thread.currentThread().getName()</code> .NET <code>Thread.CurrentThread.ManagedThreadId</code> <code>Thread.CurrentThread.Name</code> Python <code>threading.current_thread().ident</code> <code>threading.current_thread().name</code> Ruby <code>Thread.current.object_id</code> <code>Thread.current.name</code> C++ <code>std::this_thread::get_id()</code> Erlang <code>erlang:system_info(scheduler_id)</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/span-general/#source-code-attributes","title":"Source Code Attributes","text":"<p>Often a span is closely tied to a certain unit of code that is logically responsible for handling the operation that the span describes (usually the method that starts the span). For an HTTP server span, this would be the function that handles the incoming request, for example. The attributes listed below allow to report this unit of code and therefore to provide more context about the span.</p> Attribute Type Description Examples Requirement Level <code>code.function</code> string The method or function name, or equivalent (usually rightmost part of the code unit's name). <code>serveRequest</code> Recommended <code>code.namespace</code> string The \"namespace\" within which <code>code.function</code> is defined. Usually the qualified class or module name, such that <code>code.namespace</code> + some separator + <code>code.function</code> form a unique identifier for the code unit. <code>com.example.MyHttpService</code> Recommended <code>code.filepath</code> string The source code file name that identifies the code unit as uniquely as possible (preferably an absolute file path). <code>/usr/local/MyApplication/content_root/app/index.php</code> Recommended <code>code.lineno</code> int The line number in <code>code.filepath</code> best representing the operation. It SHOULD point within the code unit named in <code>code.function</code>. <code>42</code> Recommended <code>code.column</code> int The column number in <code>code.filepath</code> best representing the operation. It SHOULD point within the code unit named in <code>code.function</code>. <code>16</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/","title":"Instrumenting AWS Lambda","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines how to apply semantic conventions when instrumenting an AWS Lambda request handler. AWS Lambda largely follows the conventions for FaaS while HTTP conventions are also applicable when handlers are for HTTP requests.</p> <p>There are a variety of triggers for Lambda functions, and this document will grow over time to cover all the use cases.</p> <ul> <li>All triggers</li> <li>AWS X-Ray Environment Span Link</li> <li>API Gateway</li> <li>SQS</li> <li>SQS Event</li> <li>SQS Message</li> <li>Examples</li> <li>API Gateway Request Proxy (Lambda tracing passive)</li> <li>API Gateway Request Proxy (Lambda tracing active)</li> <li>SQS (Lambda tracing passive)</li> <li>SQS (Lambda tracing active)</li> <li>Resource Detector</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#all-triggers","title":"All triggers","text":"<p>For all events, a span with kind <code>SERVER</code> MUST be created corresponding to the function invocation unless stated otherwise below. Unless stated otherwise below, the name of the span MUST be set to the function name from the Lambda <code>Context</code>.</p> <p>The following attributes SHOULD be set:</p> <ul> <li><code>faas.invocation_id</code> - The value of the AWS Request ID, which is always available through an accessor on the Lambda <code>Context</code>.</li> <li><code>cloud.account.id</code> - In some languages, this is available as an accessor on the Lambda <code>Context</code>. Otherwise, it can be parsed from the ARN as the fifth item when splitting on <code>:</code></li> </ul> <p>Also consider setting other attributes of the <code>faas</code> resource and trace conventions and the cloud resource conventions. The following AWS Lambda-specific attribute MAY also be set:</p> Attribute Type Description Examples Requirement Level <code>aws.lambda.invoked_arn</code> string The full invoked ARN as provided on the <code>Context</code> passed to the function (<code>Lambda-Runtime-Invoked-Function-Arn</code> header on the <code>/runtime/invocation/next</code> applicable). [1] <code>arn:aws:lambda:us-east-1:123456:function:myfunction:myalias</code> Recommended <p>[1]: This may be different from <code>cloud.resource_id</code> if an alias is involved.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#aws-x-ray-environment-span-link","title":"AWS X-Ray Environment Span Link","text":"<p>If the <code>_X_AMZN_TRACE_ID</code> environment variable is set, instrumentation SHOULD try to parse an OpenTelemetry <code>Context</code> out of it using the AWS X-Ray Propagator. If the resulting <code>Context</code> is valid then a Span Link SHOULD be added to the new Span's start options with an associated attribute of <code>source=x-ray-env</code> to indicate the source of the linked span. Instrumentation MUST check if the context is valid before using it because the <code>_X_AMZN_TRACE_ID</code> environment variable can contain an incomplete trace context which indicates X-Ray isn\u2019t enabled. The environment variable will be set and the <code>Context</code> will be valid and sampled only if AWS X-Ray has been enabled for the Lambda function. A user can disable AWS X-Ray for the function if the X-Ray Span Link is not desired.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#api-gateway","title":"API Gateway","text":"<p>API Gateway allows a user to trigger a Lambda function in response to HTTP requests. It can be configured to be a pure proxy, where the information about the original HTTP request is passed to the Lambda function, or as a configuration for a REST API, in which case only a deserialized body payload is available.  In the case the API gateway is configured to proxy to the Lambda function, the instrumented request handler will have access to all the information about the HTTP request in the form of an API Gateway Proxy Request Event.</p> <p>The Lambda span name and the <code>http.route</code> span attribute SHOULD be set to the resource property from the proxy request event, which corresponds to the user configured HTTP route instead of the function name.</p> <p><code>faas.trigger</code> MUST be set to <code>http</code>. HTTP attributes SHOULD be set based on the available information in the Lambda event initiated by the proxy request. <code>http.scheme</code> is available as the <code>x-forwarded-proto</code> header in the Lambda event. Refer to the input event format for more details.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#sqs","title":"SQS","text":"<p>Amazon Simple Queue Service (SQS) is a message queue that triggers a Lambda function with a batch of messages. So we consider processing both of a batch and of each individual message. The function invocation span MUST correspond to the SQS event, which is the batch of messages. For each message, an additional span SHOULD be created to correspond with the handling of the SQS message. Because handling of a message will be inside user business logic, not the Lambda framework, automatic instrumentation mechanisms without code change will often not be able to instrument the processing of the individual messages. Instrumentation SHOULD provide utilities for creating message processing spans within user code.</p> <p>The span kind for both types of SQS spans MUST be <code>CONSUMER</code>.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#sqs-event","title":"SQS Event","text":"<p>For the SQS event span, if all the messages in the event have the same event source, the name of the span MUST be <code>&lt;event source&gt; process</code>. If there are multiple sources in the batch, the name MUST be <code>multiple_sources process</code>. The parent MUST be the <code>SERVER</code> span corresponding to the function invocation.</p> <p>For every message in the event, the message system attributes (not message attributes, which are provided by the user) SHOULD be checked for the key <code>AWSTraceHeader</code>. If it is present, an OpenTelemetry <code>Context</code> SHOULD be parsed from the value of the attribute using the AWS X-Ray Propagator and added as a link to the span. This means the span may have as many links as messages in the batch. See compatibility for more info.</p> <ul> <li><code>faas.trigger</code> MUST be set to <code>pubsub</code>.</li> <li><code>messaging.operation</code> MUST be set to <code>process</code>.</li> <li><code>messaging.system</code> MUST be set to <code>AmazonSQS</code>.</li> <li><code>messaging.destination.kind</code> or <code>messaging.source.kind</code> MUST be set to <code>queue</code>.</li> </ul>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#sqs-message","title":"SQS Message","text":"<p>For the SQS message span, the name MUST be <code>&lt;event source&gt; process</code>.  The parent MUST be the <code>CONSUMER</code> span corresponding to the SQS event. The message system attributes (not message attributes, which are provided by the user) SHOULD be checked for the key <code>AWSTraceHeader</code>. If it is present, an OpenTelemetry <code>Context</code> SHOULD be parsed from the value of the attribute using the AWS X-Ray Propagator and added as a link to the span. See compatibility for more info.</p> <ul> <li><code>faas.trigger</code> MUST be set to <code>pubsub</code>.</li> <li><code>messaging.operation</code> MUST be set to <code>process</code>.</li> <li><code>messaging.system</code> MUST be set to <code>AmazonSQS</code>.</li> </ul> <p>Other Messaging attributes SHOULD be set based on the available information in the SQS message event.</p> <p>Note that <code>AWSTraceHeader</code> is the only supported mechanism for propagating <code>Context</code> in instrumentation for SQS to prevent conflicts with other sources. Notably, message attributes (user-provided, not system) are not supported - the linked contexts are always expected to have been sent as HTTP headers of the <code>SQS.SendMessage</code> request that the message originated from. This is a function of AWS SDK instrumentation, not Lambda instrumentation.</p> <p>Using the <code>AWSTraceHeader</code> ensures that propagation will work across AWS services that may be integrated to Lambda via SQS, for example a flow that goes through S3 -&gt; SNS -&gt; SQS -&gt; Lambda. <code>AWSTraceHeader</code> is only a means of propagating context and not tied to any particular observability backend. Notably, using it does not imply using AWS X-Ray - any observability backend will fully function using this propagation mechanism.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#examples","title":"Examples","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#api-gateway-request-proxy-lambda-tracing-passive","title":"API Gateway Request Proxy (Lambda tracing passive)","text":"<p>Given a process C that sends an HTTP request to an API Gateway endpoint with path <code>/pets/{petId}</code> configured for a Lambda function F:</p> <pre><code>Process C: | Span Client        |\n--\nFunction F:    | Span Function |\n</code></pre> Field or Attribute <code>Span Client</code> <code>Span Function</code> Span name <code>HTTP GET</code> <code>/pets/{petId}</code> Parent Span Client SpanKind <code>CLIENT</code> <code>SERVER</code> Status <code>Ok</code> <code>Ok</code> <code>faas.invocation_id</code> <code>79104EXAMPLEB723</code> <code>faas.trigger</code> <code>http</code> <code>cloud.account.id</code> <code>12345678912</code> <code>server.address</code> <code>foo.execute-api.us-east-1.amazonaws.com</code> <code>server.port</code> <code>413</code> <code>http.method</code> <code>GET</code> <code>GET</code> <code>user_agent.original</code> <code>okhttp 3.0</code> <code>okhttp 3.0</code> <code>url.scheme</code> <code>https</code> <code>url.path</code> <code>/pets/10</code> <code>http.route</code> <code>/pets/{petId}</code> <code>http.response.status_code</code> <code>200</code> <code>200</code>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#api-gateway-request-proxy-lambda-tracing-active","title":"API Gateway Request Proxy (Lambda tracing active)","text":"<p>Active tracing in Lambda means an API Gateway span <code>Span APIGW</code> and a Lambda runtime invocation span <code>Span Lambda</code> will be exported to AWS X-Ray by the infrastructure (not instrumentation). All attributes above are the same except that in this case, the parent of <code>APIGW</code> is <code>Span Client</code> and the parent of <code>Span Function</code> is <code>Span Lambda</code>. This means the hierarchy looks like:</p> <pre><code>Span Client --&gt; Span APIGW --&gt; Span Lambda --&gt; Span Function\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#sqs-lambda-tracing-passive","title":"SQS (Lambda tracing passive)","text":"<p>Given a process P, that sends two messages to a queue Q on SQS, and a Lambda function F, which processes both of them in one batch (Span ProcBatch) and generates a processing span for each message separately (Spans Proc1 and Proc2).</p> <pre><code>Process P: | Span Prod1 | Span Prod2 |\n--\nFunction F:                      | Span ProcBatch |\n                                        | Span Proc1 |\n                                               | Span Proc2 |\n</code></pre> Field or Attribute Span Prod1 Span Prod2 Span ProcBatch Span Proc1 Span Proc2 Span name <code>Q send</code> <code>Q send</code> <code>Q process</code> <code>Q process</code> <code>Q process</code> Parent Span ProcBatch Span ProcBatch Links Span Prod1 Span Prod2 SpanKind <code>PRODUCER</code> <code>PRODUCER</code> <code>CONSUMER</code> <code>CONSUMER</code> <code>CONSUMER</code> Status <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>Ok</code> <code>messaging.system</code> <code>AmazonSQS</code> <code>AmazonSQS</code> <code>AmazonSQS</code> <code>AmazonSQS</code> <code>AmazonSQS</code> <code>messaging.destination.name</code> <code>Q</code> <code>Q</code> <code>messaging.source.name</code> <code>Q</code> <code>Q</code> <code>Q</code> <code>messaging.destination.kind</code> <code>queue</code> <code>queue</code> <code>messaging.source.kind</code> <code>queue</code> <code>queue</code> <code>queue</code> <code>messaging.operation</code> <code>process</code> <code>process</code> <code>process</code> <code>messaging.message.id</code> <code>\"a1\"</code> <code>\"a2\"</code> <p>Note that if Span Prod1 and Span Prod2 were sent to different queues, Span ProcBatch would not have <code>messaging.source.name</code> set as it would correspond to multiple sources.</p> <p>The above requires user code change to create <code>Span Proc1</code> and <code>Span Proc2</code>. In Java, the user would inherit from TracingSqsMessageHandler instead of Lambda's standard <code>RequestHandler</code> to enable them. Otherwise these two spans would not exist.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#sqs-lambda-tracing-active","title":"SQS (Lambda tracing active)","text":"<p>Active tracing in Lambda means a Lambda runtime invocation span <code>Span Lambda</code> will be exported to X-Ray by the infrastructure (not instrumentation). In this case, all of the above is the same except <code>Span ProcBatch</code> will have a parent of <code>Span Lambda</code>. This means the hierarchy looks like:</p> <pre><code>Span Lambda --&gt; Span ProcBatch --&gt; Span Proc1 (links to Span Prod1 and Span Prod2)\n                               \\-&gt; Span Proc2 (links to Span Prod1 and Span Prod2)\n</code></pre>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-lambda/#resource-detector","title":"Resource Detector","text":"<p>AWS Lambda resource information is available as environment variables provided by the runtime.</p> <ul> <li><code>cloud.provider</code> MUST be set to <code>aws</code></li> <li><code>cloud.region</code> MUST be set to the value of the <code>AWS_REGION</code> environment variable</li> <li><code>faas.name</code> MUST be set to the value of the <code>AWS_LAMBDA_FUNCTION_NAME</code> environment variable</li> <li><code>faas.version</code> MUST be set to the value of the <code>AWS_LAMBDA_FUNCTION_VERSION</code> environment variable</li> </ul> <p>Note that <code>cloud.resource_id</code> currently cannot be populated as a resource because it is not available until function invocation.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/","title":"Semantic conventions for AWS SDK","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions to apply when instrumenting the AWS SDK. They map request or response parameters in AWS SDK API calls to attributes on a Span. The conventions have been collected over time based on feedback from AWS users of tracing and will continue to increase as new interesting conventions are found.</p> <p>Some descriptions are also provided for populating general OpenTelemetry semantic conventions based on these APIs.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#context-propagation","title":"Context Propagation","text":"<p>See compatibility.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#common-attributes","title":"Common Attributes","text":"<p>The span name MUST be of the format <code>Service.Operation</code> as per the AWS HTTP API, e.g., <code>DynamoDB.GetItem</code>, <code>S3.ListBuckets</code>. This is equivalent to concatenating <code>rpc.service</code> and <code>rpc.method</code> with <code>.</code> and consistent with the naming guidelines for RPC client spans.</p> Attribute Type Description Examples Requirement Level <code>aws.request_id</code> string The AWS request ID as returned in the response headers <code>x-amz-request-id</code> or <code>x-amz-requestid</code>. <code>79b9da39-b7ae-508a-a6bc-864b2829c622</code>; <code>C9ER4AJX75574TDJ</code> Recommended <code>rpc.method</code> string The name of the operation corresponding to the request, as returned by the AWS SDK [1] <code>GetItem</code>; <code>PutItem</code> Recommended <code>rpc.service</code> string The name of the service to which a request is made, as returned by the AWS SDK. [2] <code>DynamoDB</code>; <code>S3</code> Recommended <code>rpc.system</code> string The value <code>aws-api</code>. <code>aws-api</code> Required <p>[1]: This is the logical name of the method from the RPC interface perspective, which can be different from the name of any implementing method/function. The <code>code.function</code> attribute may be used to store the latter (e.g., method actually executing the call on the server side, RPC client stub method on the client side).</p> <p>[2]: This is the logical name of the service from the RPC interface perspective, which can be different from the name of any implementing class. The <code>code.namespace</code> attribute may be used to store the latter (despite the attribute name, it may include a class name; e.g., class with method actually executing the call on the server side, RPC client stub class on the client side).</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodb","title":"DynamoDB","text":""},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#common-attributes_1","title":"Common Attributes","text":"<p>These attributes are filled in for all DynamoDB request types.</p> Attribute Type Description Examples Requirement Level <code>db.system</code> string The value <code>dynamodb</code>. <code>dynamodb</code> Required"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbbatchgetitem","title":"DynamoDB.BatchGetItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.table_names</code> string[] The keys in the <code>RequestItems</code> object field. <code>[Users, Cats]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbbatchwriteitem","title":"DynamoDB.BatchWriteItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.item_collection_metrics</code> string The JSON-serialized value of the <code>ItemCollectionMetrics</code> response field. <code>{ \"string\" : [ { \"ItemCollectionKey\": { \"string\" : { \"B\": blob, \"BOOL\": boolean, \"BS\": [ blob ], \"L\": [ \"AttributeValue\" ], \"M\": { \"string\" : \"AttributeValue\" }, \"N\": \"string\", \"NS\": [ \"string\" ], \"NULL\": boolean, \"S\": \"string\", \"SS\": [ \"string\" ] } }, \"SizeEstimateRangeGB\": [ number ] } ] }</code> Recommended <code>aws.dynamodb.table_names</code> string[] The keys in the <code>RequestItems</code> object field. <code>[Users, Cats]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbcreatetable","title":"DynamoDB.CreateTable","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.global_secondary_indexes</code> string[] The JSON-serialized value of each item of the <code>GlobalSecondaryIndexes</code> request field <code>[{ \"IndexName\": \"string\", \"KeySchema\": [ { \"AttributeName\": \"string\", \"KeyType\": \"string\" } ], \"Projection\": { \"NonKeyAttributes\": [ \"string\" ], \"ProjectionType\": \"string\" }, \"ProvisionedThroughput\": { \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }]</code> Recommended <code>aws.dynamodb.local_secondary_indexes</code> string[] The JSON-serialized value of each item of the <code>LocalSecondaryIndexes</code> request field. <code>[{ \"IndexArn\": \"string\", \"IndexName\": \"string\", \"IndexSizeBytes\": number, \"ItemCount\": number, \"KeySchema\": [ { \"AttributeName\": \"string\", \"KeyType\": \"string\" } ], \"Projection\": { \"NonKeyAttributes\": [ \"string\" ], \"ProjectionType\": \"string\" } }]</code> Recommended <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.item_collection_metrics</code> string The JSON-serialized value of the <code>ItemCollectionMetrics</code> response field. <code>{ \"string\" : [ { \"ItemCollectionKey\": { \"string\" : { \"B\": blob, \"BOOL\": boolean, \"BS\": [ blob ], \"L\": [ \"AttributeValue\" ], \"M\": { \"string\" : \"AttributeValue\" }, \"N\": \"string\", \"NS\": [ \"string\" ], \"NULL\": boolean, \"S\": \"string\", \"SS\": [ \"string\" ] } }, \"SizeEstimateRangeGB\": [ number ] } ] }</code> Recommended <code>aws.dynamodb.provisioned_read_capacity</code> double The value of the <code>ProvisionedThroughput.ReadCapacityUnits</code> request parameter. <code>1.0</code>; <code>2.0</code> Recommended <code>aws.dynamodb.provisioned_write_capacity</code> double The value of the <code>ProvisionedThroughput.WriteCapacityUnits</code> request parameter. <code>1.0</code>; <code>2.0</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbdeleteitem","title":"DynamoDB.DeleteItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.item_collection_metrics</code> string The JSON-serialized value of the <code>ItemCollectionMetrics</code> response field. <code>{ \"string\" : [ { \"ItemCollectionKey\": { \"string\" : { \"B\": blob, \"BOOL\": boolean, \"BS\": [ blob ], \"L\": [ \"AttributeValue\" ], \"M\": { \"string\" : \"AttributeValue\" }, \"N\": \"string\", \"NS\": [ \"string\" ], \"NULL\": boolean, \"S\": \"string\", \"SS\": [ \"string\" ] } }, \"SizeEstimateRangeGB\": [ number ] } ] }</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbdeletetable","title":"DynamoDB.DeleteTable","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbdescribetable","title":"DynamoDB.DescribeTable","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbgetitem","title":"DynamoDB.GetItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consistent_read</code> boolean The value of the <code>ConsistentRead</code> request parameter. Recommended <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.projection</code> string The value of the <code>ProjectionExpression</code> request parameter. <code>Title</code>; <code>Title, Price, Color</code>; <code>Title, Description, RelatedItems, ProductReviews</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodblisttables","title":"DynamoDB.ListTables","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.exclusive_start_table</code> string The value of the <code>ExclusiveStartTableName</code> request parameter. <code>Users</code>; <code>CatsTable</code> Recommended <code>aws.dynamodb.table_count</code> int The the number of items in the <code>TableNames</code> response parameter. <code>20</code> Recommended <code>aws.dynamodb.limit</code> int The value of the <code>Limit</code> request parameter. <code>10</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbputitem","title":"DynamoDB.PutItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.item_collection_metrics</code> string The JSON-serialized value of the <code>ItemCollectionMetrics</code> response field. <code>{ \"string\" : [ { \"ItemCollectionKey\": { \"string\" : { \"B\": blob, \"BOOL\": boolean, \"BS\": [ blob ], \"L\": [ \"AttributeValue\" ], \"M\": { \"string\" : \"AttributeValue\" }, \"N\": \"string\", \"NS\": [ \"string\" ], \"NULL\": boolean, \"S\": \"string\", \"SS\": [ \"string\" ] } }, \"SizeEstimateRangeGB\": [ number ] } ] }</code> Recommended <code>aws.dynamodb.table_names</code> string[] The keys in the <code>RequestItems</code> object field. <code>[Users, Cats]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbquery","title":"DynamoDB.Query","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.scan_forward</code> boolean The value of the <code>ScanIndexForward</code> request parameter. Recommended <code>aws.dynamodb.attributes_to_get</code> string[] The value of the <code>AttributesToGet</code> request parameter. <code>[lives, id]</code> Recommended <code>aws.dynamodb.consistent_read</code> boolean The value of the <code>ConsistentRead</code> request parameter. Recommended <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.index_name</code> string The value of the <code>IndexName</code> request parameter. <code>name_to_group</code> Recommended <code>aws.dynamodb.limit</code> int The value of the <code>Limit</code> request parameter. <code>10</code> Recommended <code>aws.dynamodb.projection</code> string The value of the <code>ProjectionExpression</code> request parameter. <code>Title</code>; <code>Title, Price, Color</code>; <code>Title, Description, RelatedItems, ProductReviews</code> Recommended <code>aws.dynamodb.select</code> string The value of the <code>Select</code> request parameter. <code>ALL_ATTRIBUTES</code>; <code>COUNT</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbscan","title":"DynamoDB.Scan","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.segment</code> int The value of the <code>Segment</code> request parameter. <code>10</code> Recommended <code>aws.dynamodb.total_segments</code> int The value of the <code>TotalSegments</code> request parameter. <code>100</code> Recommended <code>aws.dynamodb.count</code> int The value of the <code>Count</code> response parameter. <code>10</code> Recommended <code>aws.dynamodb.scanned_count</code> int The value of the <code>ScannedCount</code> response parameter. <code>50</code> Recommended <code>aws.dynamodb.attributes_to_get</code> string[] The value of the <code>AttributesToGet</code> request parameter. <code>[lives, id]</code> Recommended <code>aws.dynamodb.consistent_read</code> boolean The value of the <code>ConsistentRead</code> request parameter. Recommended <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.index_name</code> string The value of the <code>IndexName</code> request parameter. <code>name_to_group</code> Recommended <code>aws.dynamodb.limit</code> int The value of the <code>Limit</code> request parameter. <code>10</code> Recommended <code>aws.dynamodb.projection</code> string The value of the <code>ProjectionExpression</code> request parameter. <code>Title</code>; <code>Title, Price, Color</code>; <code>Title, Description, RelatedItems, ProductReviews</code> Recommended <code>aws.dynamodb.select</code> string The value of the <code>Select</code> request parameter. <code>ALL_ATTRIBUTES</code>; <code>COUNT</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbupdateitem","title":"DynamoDB.UpdateItem","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.item_collection_metrics</code> string The JSON-serialized value of the <code>ItemCollectionMetrics</code> response field. <code>{ \"string\" : [ { \"ItemCollectionKey\": { \"string\" : { \"B\": blob, \"BOOL\": boolean, \"BS\": [ blob ], \"L\": [ \"AttributeValue\" ], \"M\": { \"string\" : \"AttributeValue\" }, \"N\": \"string\", \"NS\": [ \"string\" ], \"NULL\": boolean, \"S\": \"string\", \"SS\": [ \"string\" ] } }, \"SizeEstimateRangeGB\": [ number ] } ] }</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#dynamodbupdatetable","title":"DynamoDB.UpdateTable","text":"Attribute Type Description Examples Requirement Level <code>aws.dynamodb.attribute_definitions</code> string[] The JSON-serialized value of each item in the <code>AttributeDefinitions</code> request field. <code>[{ \"AttributeName\": \"string\", \"AttributeType\": \"string\" }]</code> Recommended <code>aws.dynamodb.global_secondary_index_updates</code> string[] The JSON-serialized value of each item in the the <code>GlobalSecondaryIndexUpdates</code> request field. <code>[{ \"Create\": { \"IndexName\": \"string\", \"KeySchema\": [ { \"AttributeName\": \"string\", \"KeyType\": \"string\" } ], \"Projection\": { \"NonKeyAttributes\": [ \"string\" ], \"ProjectionType\": \"string\" }, \"ProvisionedThroughput\": { \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }]</code> Recommended <code>aws.dynamodb.consumed_capacity</code> string[] The JSON-serialized value of each item in the <code>ConsumedCapacity</code> response field. <code>[{ \"CapacityUnits\": number, \"GlobalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"LocalSecondaryIndexes\": { \"string\" : { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number } }, \"ReadCapacityUnits\": number, \"Table\": { \"CapacityUnits\": number, \"ReadCapacityUnits\": number, \"WriteCapacityUnits\": number }, \"TableName\": \"string\", \"WriteCapacityUnits\": number }]</code> Recommended <code>aws.dynamodb.provisioned_read_capacity</code> double The value of the <code>ProvisionedThroughput.ReadCapacityUnits</code> request parameter. <code>1.0</code>; <code>2.0</code> Recommended <code>aws.dynamodb.provisioned_write_capacity</code> double The value of the <code>ProvisionedThroughput.WriteCapacityUnits</code> request parameter. <code>1.0</code>; <code>2.0</code> Recommended <code>aws.dynamodb.table_names</code> string[] A single-element array with the value of the TableName request parameter. <code>[Users]</code> Recommended"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/aws-sdk/#s3","title":"S3","text":"Attribute Type Description Examples Requirement Level <code>aws.s3.bucket</code> string The S3 bucket name the request refers to. Corresponds to the <code>--bucket</code> parameter of the S3 API operations. [1] <code>some-bucket-name</code> Recommended <code>aws.s3.key</code> string The S3 object key the request refers to. Corresponds to the <code>--key</code> parameter of the S3 API operations. [2] <code>someFile.yml</code> Recommended <code>aws.s3.copy_source</code> string The source object (in the form <code>bucket</code>/<code>key</code>) for the copy operation. [3] <code>someFile.yml</code> Recommended <code>aws.s3.upload_id</code> string Upload ID that identifies the multipart upload. [4] <code>dfRtDYWFbkRONycy.Yxwh66Yjlx.cph0gtNBtJ</code> Recommended <code>aws.s3.delete</code> string The delete request container that specifies the objects to be deleted. [5] <code>Objects=[{Key=string,VersionId=string},{Key=string,VersionId=string}],Quiet=boolean</code> Recommended <code>aws.s3.part_number</code> int The part number of the part being uploaded in a multipart-upload operation. This is a positive integer between 1 and 10,000. [6] <code>3456</code> Recommended <p>[1]: The <code>bucket</code> attribute is applicable to all S3 operations that reference a bucket, i.e. that require the bucket name as a mandatory parameter. This applies to almost all S3 operations except <code>list-buckets</code>.</p> <p>[2]: The <code>key</code> attribute is applicable to all object-related S3 operations, i.e. that require the object key as a mandatory parameter. This applies in particular to the following operations:</p> <ul> <li>copy-object</li> <li>delete-object</li> <li>get-object</li> <li>head-object</li> <li>put-object</li> <li>restore-object</li> <li>select-object-content</li> <li>abort-multipart-upload</li> <li>complete-multipart-upload</li> <li>create-multipart-upload</li> <li>list-parts</li> <li>upload-part</li> <li>upload-part-copy</li> </ul> <p>[3]: The <code>copy_source</code> attribute applies to S3 copy operations and corresponds to the <code>--copy-source</code> parameter of the copy-object operation within the S3 API. This applies in particular to the following operations:</p> <ul> <li>copy-object</li> <li>upload-part-copy</li> </ul> <p>[4]: The <code>upload_id</code> attribute applies to S3 multipart-upload operations and corresponds to the <code>--upload-id</code> parameter of the S3 API multipart operations. This applies in particular to the following operations:</p> <ul> <li>abort-multipart-upload</li> <li>complete-multipart-upload</li> <li>list-parts</li> <li>upload-part</li> <li>upload-part-copy</li> </ul> <p>[5]: The <code>delete</code> attribute is only applicable to the delete-object operation. The <code>delete</code> attribute corresponds to the <code>--delete</code> parameter of the delete-objects operation within the S3 API.</p> <p>[6]: The <code>part_number</code> attribute is only applicable to the upload-part and upload-part-copy operations. The <code>part_number</code> attribute corresponds to the <code>--part-number</code> parameter of the upload-part operation within the S3 API.</p>"},{"location":"docs/specs/otel/trace/semantic_conventions/instrumentation/graphql/","title":"Semantic conventions for GraphQL Server","text":"<p>NOTICE Semantic Conventions are moving to a new location.</p> <p>No changes to this document are allowed.</p> <p>Status: Experimental</p> <p>This document defines semantic conventions to apply when instrumenting the GraphQL implementation. They map GraphQL operations to attributes on a Span.</p> <p>The span name MUST be of the format <code>&lt;graphql.operation.type&gt; &lt;graphql.operation.name&gt;</code> provided that <code>graphql.operation.type</code> and <code>graphql.operation.name</code> are available. If <code>graphql.operation.name</code> is not available, the span SHOULD be named <code>&lt;graphql.operation.type&gt;</code>. When <code>&lt;graphql.operation.type&gt;</code> is not available, <code>GraphQL Operation</code> MAY be used as span name.</p> Attribute Type Description Examples Requirement Level <code>graphql.operation.name</code> string The name of the operation being executed. <code>findBookById</code> Recommended <code>graphql.operation.type</code> string The type of the operation being executed. <code>query</code>; <code>mutation</code>; <code>subscription</code> Recommended <code>graphql.document</code> string The GraphQL document being executed. [1] <code>query findBookById { bookById(id: ?) { name } }</code> Recommended <p>[1]: The value may be sanitized to exclude sensitive information.</p> <p><code>graphql.operation.type</code> MUST be one of the following:</p> Value Description <code>query</code> GraphQL query <code>mutation</code> GraphQL mutation <code>subscription</code> GraphQL subscription"},{"location":"ecosystem/demo/","title":"\u5f00\u9065\u6f14\u793a","text":"<p>\u5f00\u9065\u6f14\u793a\u7684\u529f\u80fd\u5982\u4e0b:</p> <ul> <li>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u5b9e\u9645\u793a\u4f8b\uff0c\u8be5\u793a\u4f8b\u53ef\u7528\u4e8e\u6f14\u793a\u5f00\u9065\u63d2\u88c5\u548c\u53ef\u89c2\u5bdf\u6027\u3002</li> <li>\u4e3a\u4f9b\u5e94\u5546\u3001\u5de5\u5177\u4f5c\u8005\u548c\u5176\u4ed6\u4eba\u6784\u5efa\u4e00\u4e2a\u57fa\u7840\uff0c\u4ee5\u6269\u5c55\u548c\u6f14\u793a\u4ed6\u4eec\u7684\u5f00\u9065\u96c6\u6210\u3002</li> <li>\u4e3a\u5f00\u9065\u8d21\u732e\u8005\u521b\u5efa\u4e00\u4e2a\u6d3b\u751f\u751f\u7684\u793a\u4f8b\uff0c\u7528\u4e8e\u6d4b\u8bd5 API\u3001SDK \u548c\u5176\u4ed6\u7ec4\u4ef6\u6216\u589e\u5f3a\u7684\u65b0\u7248\u672c\u3002</li> </ul> <p>\u5982\u679c\u4f60\u53d1\u73b0\u81ea\u5df1\u5728\u95ee\u8fd9\u6837\u7684\u95ee\u9898:</p> <ul> <li>\u6211\u5e94\u8be5\u5982\u4f55\u4e3a\u6211\u7684\u8bed\u8a00\u4f7f\u7528 SDK ?</li> <li>\u4f7f\u7528\u5f00\u9065 api \u7684\u6700\u4f73\u65b9\u5f0f\u662f\u4ec0\u4e48?</li> <li>\u6211\u7684\u670d\u52a1\u5e94\u8be5\u5982\u4f55\u914d\u7f6e?</li> <li>\u6211\u7684\u5f00\u9065\u6536\u96c6\u5668\u5e94\u8be5\u5982\u4f55\u914d\u7f6e?</li> <li>\u6211\u5982\u4f55\u8003\u8651\u4f7f\u7528\u5f00\u9065\u7cfb\u7edf\u7684\u67b6\u6784?</li> </ul> <p>\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1:</p> <ul> <li>\u6f14\u793a\u6587\u6863</li> <li>\u6f14\u793a\u56de\u8d2d</li> </ul>"},{"location":"ecosystem/integrations/","title":"\u96c6\u6210","text":"<p>OpenTelemetry\u96c6\u6210\u6216\u96c6\u6210\u5230\u5404\u79cd\u5f00\u6e90\u9879\u76ee\u4e2d\u3002</p>"},{"location":"ecosystem/integrations/#within-opentelemetry","title":"Within OpenTelemetry","text":"<p>OpenTelemetry\u63d0\u4f9b\u4e86\u4e0e\u4ee5\u4e0b\u5f00\u6e90\u9879\u76ee\u7684\u96c6\u6210\u3002</p> External Project* OpenTelemetry Supported Components Apache Kafka Collector Apache SkyWalking Collector Elasticsearch Collector, C++, Java, Python Fluent Bit Collector Graphite Collector Jaeger Collector, DotNet, Go, Java, JS, PHP, Python, Ruby, Rust OpenCensus Collector, Python OpenTracing DotNet, Go, Java, JS, Python, Ruby OpenMetrics 1 Collector Prometheus 1 Collector, C++, Go, Java, JS, Rust Zipkin Collector, DotNet, Go, Java, JS, PHP, Python, Rust W3C trace-context DotNet, Go, Java, JS, Python, Ruby <p>* \u9879\u76ee\u6309\u5b57\u6bcd\u987a\u5e8f\u5217\u51fa.</p>"},{"location":"ecosystem/integrations/#outside-opentelemetry","title":"Outside OpenTelemetry","text":"<p>\u4ee5\u4e0b\u5f00\u6e90\u9879\u76ee\u4f7f\u7528OpenTelemetry\u7ec4\u4ef6\u3002</p> External Project Applicable OpenTelemetry Components containerd Go CRI-O Go Docker buildx Go Jaeger Collector, Go Kubernetes Go Micrometer Java Quarkus Java <p>* \u9879\u76ee\u6309\u5b57\u6bcd\u987a\u5e8f\u5217\u51fa.</p> <ol> <li> <p>\u76ee\u524d\u53ea\u652f\u6301\u90e8\u5206\u9879\u76ee\u3002\u5168\u529b\u652f\u6301\u5373\u5c06\u5230\u6765!\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"ecosystem/vendors/","title":"\u4f9b\u5e94\u5546","text":"<p>\u53d1\u884c\u7248\u548c\u5728\u5176\u5546\u4e1a\u4ea7\u54c1\u4e2d\u539f\u751f\u652f\u6301OpenTelemetry\u7684\u4f9b\u5e94\u5546\u3002</p> \u516c\u53f8* Distri\u00adbution Native OTLP \u4e86\u89e3\u66f4\u591a AppDynamics (Cisco) Yes Yes docs.appdynamics.com/... Aria by VMware (Wavefront) No Yes docs.wavefront.com/... Aspecto Yes Yes aspecto.io AWS Yes No aws-otel.github.io Azure Yes No docs.microsoft.com/... Coralogix Yes Yes coralogix.com/... DaoCloud Yes Yes docs.daocloud.io/... Datadog Yes Yes docs.datadoghq.com/... Dynatrace Yes Yes dynatrace.com/... Elastic Yes Yes elastic.co/... F5 No Yes opentelemetry-collector-contrib/... Google Cloud Platform No Yes opentelemetry-collector-contrib/... Grafana Labs No Yes grafana.com/... Helios Yes Yes gethelios.dev Honeycomb Yes Yes docs.honeycomb.io/... Instana No Yes ibm.com/docs/... ITRS Yes Yes docs.itrsgroup.com/docs/geneos/... KloudFuse No Yes kloudfuse.com Lightstep Yes Yes github.com/lightstep LogicMonitor Yes Yes logicmonitor.com/... Logz.io Yes No docs.logz.io/... LogScale by Crowdstrike (Humio) No Yes library.humio.com/... Lumigo Yes Yes lumigo.io New Relic No Yes newrelic.com/... Observe, Inc. Yes Yes observeinc.com observIQ Yes Yes observiq.com/... Oracle No Yes docs.oracle.com/... Promscale No Yes timescale.com/promscale Sentry Software Yes Yes sentrysoftware.com/... ServicePilot No Yes servicepilot.com/... SigNoz Yes Yes signoz.io SolarWinds Yes Yes documentation.solarwinds.com/... Splunk Yes Yes splunk.com/blog/... Sumo Logic Yes Yes help.sumologic.com/ TelemetryHub No Yes telemetryhub.com Teletrace Yes Yes docs.teletrace.io ThousandEyes (Cisco) No Yes docs.thousandeyes.com Traceloop No Yes traceloop.com Uptrace Yes Yes uptrace.dev <p>* Vendors are listed alphabetically.</p>"},{"location":"ecosystem/registry/","title":"\u6ce8\u518c\u8868","text":"<p>{{% blocks/lead color=\"white\" %}}</p>"},{"location":"ecosystem/registry/#param-title","title":"{{% param title %}}","text":"<p>{{% param description %}}</p> <p>{{% /blocks/lead %}}</p> <p>{{% blocks/section color=\"dark\" %}}</p>"},{"location":"ecosystem/registry/#_1","title":"\u4f60\u9700\u8981\u4ec0\u4e48?","text":"<p>\u5f00\u653e\u9065\u6d4b\u6ce8\u518c\u8868\u5141\u8bb8\u60a8\u5728\u5f00\u653e\u9065\u6d4b\u751f\u6001\u7cfb\u7edf\u4e2d\u641c\u7d22\u4eea\u5668\u5e93\u3001\u8ddf\u8e2a\u5668\u5b9e\u73b0\u3001\u5b9e\u7528\u7a0b\u5e8f\u548c\u5176\u4ed6\u6709\u7528\u7684\u9879\u76ee\u3002</p> <ul> <li>Not able to find an exporter for your language? Remember, the   OpenTelemetry Collector supports exporting to a variety of   systems and works with all OpenTelemetry Core Components!</li> <li>Are you a project maintainer? See,  Adding a project to the OpenTelemetry Registry.</li> <li>Check back regularly, the community and registry are growing!</li> </ul> <p>{{% /blocks/section %}}</p> <p>{{&lt; blocks/section color=\"white\" type=\"container-lg\" &gt;}}</p> <p>{{}} <p>{{&lt; /blocks/section &gt;}}</p>"},{"location":"ecosystem/registry/adding/","title":"\u6dfb\u52a0\u5230\u6ce8\u518c\u8868","text":"<p>\u60a8\u662f\u5426\u7ef4\u62a4\u6216\u4e3aOpenTelemetry\u7684\u96c6\u6210\u505a\u51fa\u8d21\u732e? \u6211\u4eec\u5f88\u4e50\u610f\u5728\u6ce8\u518c\u8868\u4e2d\u5c55\u793a\u60a8\u7684\u9879\u76ee!</p> <p>\u8981\u6dfb\u52a0\u9879\u76ee\uff0c\u8bf7\u63d0\u4ea4pull request\u3002 \u4f60\u9700\u8981\u5728data/registry\u4e2d\u4e3a\u4f60\u7684\u9879\u76ee\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u6587\u4ef6\uff0c\u4f7f\u7528\u4ee5\u4e0b\u6a21\u677f:registry-entry.yml.</p>"}]}